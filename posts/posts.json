[
  {
    "path": "posts/2023-04-30-networkpsych/",
    "title": "Examining Big 5 Personality Inventory Data with Network Psychometrics",
    "description": "In this post, I will be examining response data from Big 5 Personality Inventory with the Network psychometrics approach.\n\n(7 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2023-04-30",
    "categories": [
      "R",
      "Psychometric"
    ],
    "contents": "\r\n\r\nContents\r\nSaying Hi\r\nNetwork Psychometrics\r\nExploratory Graph Analysis for Dimensional Stability\r\nMeasurement Invariance with Network Model\r\nConcluding Remark\r\n\r\nSaying Hi\r\nHi, everyone. We are heading into spring here in Edmonton. There is no class to teach, so I have some more free time to work on projects that I have on hands. I took an advance psychometric course this past winter 2023 semester and learned a lot of useful techniques. One of them is Network psychometrics.\r\nNetwork psychometrics is a relatively novel approach to psychometrics research that examines relationships between observed variables (assessment items) without relying on the assumption of latent variables (Borsboom et al., 2021). Items that are related to each other may appear closer while items that are less relevant may be positioned further on a network graph. This approach offers an alternative evidence to validity of the interpretations and uses of a test in addition to the traditional factor analysis method\r\n(Christensen & Golino, 2021).\r\nIn this post, I used response data to the Big 5 personality inventory from the open-source psychometrics project. Items from the test were put together from the international personality item pool. The data set has N = 19,719 responses to 50 Likert-scale items asking of respondents’ agreement to the presented statements such as “I don’t talk a lot”, “I shirk my duties”, or “I am quick to understand things”. Responses were coded as 1 to, where 1=Disagree, 3=Neutral, 5=Agree. Five personality traits of extraversion, neuroticism, agreeableness, conscientiousness, and openness to experience are reported as results.\r\nWe will start by loading packages that we will use.\r\n\r\n\r\nShow code\r\n\r\nlibrary(\"dplyr\")\r\n# for network modeling \r\nlibrary(\"qgraph\")\r\nlibrary(\"psychonetrics\")\r\nlibrary(\"bootnet\")  \r\nlibrary(\"EGAnet\")  \r\n\r\n\r\nNext, we will extract response data from the whole data set, as well as group items into their respective dimensions.\r\n\r\n\r\nShow code\r\n\r\ndf_response <- df[, 8:57]\r\n\r\ngroups <- list(Extraversion = 1:10,\r\n               Neuroticism = 11:20,\r\n               Agreeableness = 21:30,\r\n               Conscientiousness = 31:40,\r\n               Openness = 41:50)\r\n\r\nobsvars <- colnames(df_response)\r\n\r\n\r\nNetwork Psychometrics\r\nTo construct a psychometrics network, we will use the estimateNetwork function from bootnet package. Relationships between items will be determined by polychoric correlations, and the network graph was estimated with the graphical least absolute shrinkage and selection operator (Glasso) estimator.\r\n\r\n\r\nShow code\r\n\r\nnetwork_big5 <- bootnet::estimateNetwork(\r\n  data = df_response,\r\n  # Alternatively, \"cov\" for covariances, \"cor\" for correlations \r\n  corMethod = \"cor_auto\", # for polychoric and polyserial correlations\r\n  # Alternatively, \"ggmModSelect\" for an unregularized GGM using glasso\r\n  default = \"EBICglasso\", # for estimating GGM with gLASSO and EBIC\r\n  tuning = 0.5 # EBIC tuning parameter; set to zero for BIC model selection. If you make it large, you should justify it.\r\n)\r\n\r\n\r\nBelow is th initial network of the Big 5 personality data set that we used. Items of the same dimensions are grouped together, meaning that they measure similar construct to each other. Note that edges (or linkages) between items can be adjusted. We can remove (or prune) statistically insignificant edges and add edges that improve model fit.\r\n\r\n\r\nShow code\r\n\r\nplot(network_big5, \r\n     layout = \"spring\", \r\n     palette = \"colorblind\", \r\n     groups = groups, # to label each group\r\n     font = 2,\r\n     label.cex = 1\r\n     ) \r\n\r\n\r\n\r\nWe will use the prune function from psychonetrics package to prune edges that do not achieve statistical significance at alpha 0.05 (step down method). Then, we will add more edges until model fit of the data set based on Bayesian Information Criterion (BIC) does not improve anymore based on alpha = 0.05 with the stepup function (step up method).\r\n\r\n\r\nShow code\r\n\r\nnetwork_big5_optimized <- psychonetrics::ggm(df_response, \r\n                                     vars = obsvars) %>%\r\n  psychonetrics::runmodel() %>%\r\n  psychonetrics::prune(adjust = \"fdr\", alpha = 0.05) %>%\r\n  # To automatically add edges at  alpha=0.05 until BIC is no longer be improved\r\n  psychonetrics::stepup(criterion = \"bic\", alpha = 0.05) %>%\r\n  psychonetrics::modelsearch()\r\n\r\n\r\nWe can check fit indices of the optimized network model and its network graph with the code below. The network is a lot more complex with more edges added to the graph. The items still stay with their peers in their respective domain, meaning that structure of the test still holds after the optimization.\r\n\r\n\r\nShow code\r\n\r\n# Look at the model fit\r\nnetwork_big5_optimized %>% psychonetrics::fit()\r\n\r\n           Measure       Value\r\n              logl -1373902.53\r\n unrestricted.logl -1373377.80\r\n     baseline.logl -1561979.97\r\n              nvar          50\r\n              nobs        1325\r\n              npar         747\r\n                df         578\r\n         objective       47.45\r\n             chisq     1049.45\r\n            pvalue         ~ 0\r\n    baseline.chisq   377204.34\r\n       baseline.df        1225\r\n   baseline.pvalue         ~ 0\r\n               nfi         1.0\r\n              pnfi        0.47\r\n               tli         1.0\r\n              nnfi         1.0\r\n               rfi        0.99\r\n               ifi         1.0\r\n               rni         1.0\r\n               cfi         1.0\r\n             rmsea      0.0064\r\n    rmsea.ci.lower      0.0058\r\n    rmsea.ci.upper      0.0070\r\n      rmsea.pvalue           1\r\n            aic.ll  2749299.06\r\n           aic.ll2  2749357.96\r\n             aic.x     -106.55\r\n            aic.x2     2543.45\r\n               bic  2755192.39\r\n              bic2  2752818.46\r\n           ebic.25  2758114.67\r\n            ebic.5  2761036.96\r\n           ebic.75  2763374.78\r\n             ebic1  2766881.52\r\n\r\nShow code\r\n\r\n# Obtain the network plot\r\nnet_optimized <- psychonetrics::getmatrix(network_big5_optimized, \"omega\")\r\n\r\nqgraph::qgraph(net_optimized, \r\n               layout = \"spring\", \r\n               theme = \"colorblind\",\r\n               labels = obsvars,\r\n               groups = groups,\r\n               negDashed = T, # should negative edges be dashed?\r\n               font = 2)\r\n\r\n\r\n\r\nExploratory Graph Analysis for Dimensional Stability\r\nAside from investigating the network structure, we can also use the network approach to examine dimensional stability of the test with the exploratory graph analysis (EGA) method. EGA tests dimensional stability of a test by examining its structure across several resampling iterations. In other words, EGA checks if structure of the test is similar across different response patterns.\r\nWe will use the bootEGA function from the EGAnet package to perform EGA with the big 5 personality inventory data. Usually 500 resampling iterations is recommended, but we will stick to 100 to make it computationally feasible.\r\n\r\n\r\nShow code\r\n\r\nbootEGA_big5 <- EGAnet::bootEGA(\r\n  # we could also provide the cor matrix but then\r\n  # n (i.e., number of rows) must also be specified\r\n  data = df_response, \r\n  cor = \"cor_auto\",\r\n  uni.method = \"louvain\",\r\n  iter = 100, # Number of replica samples to generate\r\n  # resampling\" for n random subsamples of the original data\r\n  # parametric\" for n synthetic samples from multivariate normal dist.\r\n  type = \"parametric\", \r\n  # EGA Uses standard exploratory graph analysis\r\n  # EGA.fit Uses total entropy fit index (tefi) to determine best fit of EGA\r\n  # hierEGA Uses hierarchical exploratory graph analysis\r\n  EGA.type = \"EGA\", \r\n  model = \"glasso\", \r\n  algorithm = \"walktrap\", # or \"louvain\" (better for unidimensional structures)\r\n  # use \"highest_modularity\", \"most_common\", or \"lowest_tefi\"\r\n  consensus.method = \"highest_modularity\", \r\n  typicalStructure = TRUE, # typical network of partial correlations\r\n  plot.typicalStructure = TRUE, # returns a plot of the typical network\r\n  ncores = 4, # Number of cores to use in computing results\r\n  progress = FALSE ,\r\n  summary.table  = TRUE\r\n)\r\n\r\n\r\nThe network below shows that structure of our data set holds despite being tested on different data sets across 100 resampling iterations. This means that the 5 dimensions of our test is quite stable.\r\nExploratory Graph Analysis ResultWe can also request for written results of EGA. All items are loaded onto their respective dimensions. For example, all 10 extraversion items are loaded onto the 5th dimension. The same applies to the remaining 40 items as well.\r\n\r\n\r\nShow code\r\n\r\n# View the number of communities\r\nbootEGA_big5$EGA\r\n\r\nNumber of communities: 5 \r\n\r\n E1  E2  E3  E4  E5  E6  E7  E8  E9 E10  N1  N2  N3  N4  N5  N6  N7 \r\n  5   5   5   5   5   5   5   5   5   5   3   3   3   3   3   3   3 \r\n N8  N9 N10  A1  A2  A3  A4  A5  A6  A7  A8  A9 A10  C1  C2  C3  C4 \r\n  3   3   3   2   2   2   2   2   2   2   2   2   2   4   4   4   4 \r\n C5  C6  C7  C8  C9 C10  O1  O2  O3  O4  O5  O6  O7  O8  O9 O10 \r\n  4   4   4   4   4   4   1   1   1   1   1   1   1   1   1   1 \r\n\r\nMethods:\r\n                                                         \r\nCorrelations =          auto (from qgraph)               \r\nModel =                 glasso                           \r\nAlgorithm =             walktrap                         \r\nUnidimensional Method = louvain with consensus clustering\r\n\r\nShow code\r\n\r\nbootEGA_big5$typicalGraph$typical.dim.variables\r\n\r\n    items dimension\r\nO1     O1         1\r\nO2     O2         1\r\nO3     O3         1\r\nO4     O4         1\r\nO5     O5         1\r\nO6     O6         1\r\nO7     O7         1\r\nO8     O8         1\r\nO9     O9         1\r\nO10   O10         1\r\nA1     A1         2\r\nA2     A2         2\r\nA3     A3         2\r\nA4     A4         2\r\nA5     A5         2\r\nA6     A6         2\r\nA7     A7         2\r\nA8     A8         2\r\nA9     A9         2\r\nA10   A10         2\r\nN1     N1         3\r\nN2     N2         3\r\nN3     N3         3\r\nN4     N4         3\r\nN5     N5         3\r\nN6     N6         3\r\nN7     N7         3\r\nN8     N8         3\r\nN9     N9         3\r\nN10   N10         3\r\nC1     C1         4\r\nC2     C2         4\r\nC3     C3         4\r\nC4     C4         4\r\nC5     C5         4\r\nC6     C6         4\r\nC7     C7         4\r\nC8     C8         4\r\nC9     C9         4\r\nC10   C10         4\r\nE1     E1         5\r\nE2     E2         5\r\nE3     E3         5\r\nE4     E4         5\r\nE5     E5         5\r\nE6     E6         5\r\nE7     E7         5\r\nE8     E8         5\r\nE9     E9         5\r\nE10   E10         5\r\n\r\nWe can also check if all items are stable across 100 iterations. The empirical EGA Communities plot below indicates that all items and dimensions have value = 1 across all iterations, meaning that the items are loaded into the same test community (aka measuring the same construct) across different response patterns.\r\n\r\n\r\nShow code\r\n\r\n# Dimension (i.e., structural) stability results\r\ndim_big5 <- EGAnet::dimensionStability(bootEGA_big5)\r\n\r\n\r\nShow code\r\n\r\ndim_big5$dimension.stability\r\n\r\n$structural.consistency\r\n1 2 3 4 5 \r\n1 1 1 1 1 \r\n\r\n$average.item.stability\r\n1 2 3 4 5 \r\n1 1 1 1 1 \r\n\r\nShow code\r\n\r\n# Item stability results\r\ndim_big5$item.stability\r\n\r\n$membership\r\n$membership$empirical\r\n E1  E2  E3  E4  E5  E6  E7  E8  E9 E10  N1  N2  N3  N4  N5  N6  N7 \r\n  5   5   5   5   5   5   5   5   5   5   3   3   3   3   3   3   3 \r\n N8  N9 N10  A1  A2  A3  A4  A5  A6  A7  A8  A9 A10  C1  C2  C3  C4 \r\n  3   3   3   2   2   2   2   2   2   2   2   2   2   4   4   4   4 \r\n C5  C6  C7  C8  C9 C10  O1  O2  O3  O4  O5  O6  O7  O8  O9 O10 \r\n  4   4   4   4   4   4   1   1   1   1   1   1   1   1   1   1 \r\n\r\n$membership$unique\r\n[1] 5 3 2 4 1\r\n\r\n$membership$bootstrap\r\n    [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\r\nE1     5    5    5    5    5    5    5    5    5     5     5     5\r\nE2     5    5    5    5    5    5    5    5    5     5     5     5\r\nE3     5    5    5    5    5    5    5    5    5     5     5     5\r\nE4     5    5    5    5    5    5    5    5    5     5     5     5\r\nE5     5    5    5    5    5    5    5    5    5     5     5     5\r\nE6     5    5    5    5    5    5    5    5    5     5     5     5\r\nE7     5    5    5    5    5    5    5    5    5     5     5     5\r\nE8     5    5    5    5    5    5    5    5    5     5     5     5\r\nE9     5    5    5    5    5    5    5    5    5     5     5     5\r\nE10    5    5    5    5    5    5    5    5    5     5     5     5\r\nN1     3    3    3    3    3    3    3    3    3     3     3     3\r\nN2     3    3    3    3    3    3    3    3    3     3     3     3\r\nN3     3    3    3    3    3    3    3    3    3     3     3     3\r\nN4     3    3    3    3    3    3    3    3    3     3     3     3\r\nN5     3    3    3    3    3    3    3    3    3     3     3     3\r\nN6     3    3    3    3    3    3    3    3    3     3     3     3\r\nN7     3    3    3    3    3    3    3    3    3     3     3     3\r\nN8     3    3    3    3    3    3    3    3    3     3     3     3\r\nN9     3    3    3    3    3    3    3    3    3     3     3     3\r\nN10    3    3    3    3    3    3    3    3    3     3     3     3\r\nA1     2    2    2    2    2    2    2    2    2     2     2     2\r\nA2     2    2    2    2    2    2    2    2    2     2     2     2\r\nA3     2    2    2    2    2    2    2    2    2     2     2     2\r\nA4     2    2    2    2    2    2    2    2    2     2     2     2\r\nA5     2    2    2    2    2    2    2    2    2     2     2     2\r\nA6     2    2    2    2    2    2    2    2    2     2     2     2\r\nA7     2    2    2    2    2    2    2    2    2     2     2     2\r\nA8     2    2    2    2    2    2    2    2    2     2     2     2\r\nA9     2    2    2    2    2    2    2    2    2     2     2     2\r\nA10    2    2    2    2    2    2    2    2    2     2     2     2\r\nC1     4    4    4    4    4    4    4    4    4     4     4     4\r\nC2     4    4    4    4    4    4    4    4    4     4     4     4\r\nC3     4    4    4    4    4    4    4    4    4     4     4     4\r\nC4     4    4    4    4    4    4    4    4    4     4     4     4\r\nC5     4    4    4    4    4    4    4    4    4     4     4     4\r\nC6     4    4    4    4    4    4    4    4    4     4     4     4\r\nC7     4    4    4    4    4    4    4    4    4     4     4     4\r\nC8     4    4    4    4    4    4    4    4    4     4     4     4\r\nC9     4    4    4    4    4    4    4    4    4     4     4     4\r\nC10    4    4    4    4    4    4    4    4    4     4     4     4\r\nO1     1    1    1    1    1    1    1    1    1     1     1     1\r\nO2     1    1    1    1    1    1    1    1    1     1     1     1\r\nO3     1    1    1    1    1    1    1    1    1     1     1     1\r\nO4     1    1    1    1    1    1    1    1    1     1     1     1\r\nO5     1    1    1    1    1    1    1    1    1     1     1     1\r\nO6     1    1    1    1    1    1    1    1    1     1     1     1\r\nO7     1    1    1    1    1    1    1    1    1     1     1     1\r\nO8     1    1    1    1    1    1    1    1    1     1     1     1\r\nO9     1    1    1    1    1    1    1    1    1     1     1     1\r\nO10    1    1    1    1    1    1    1    1    1     1     1     1\r\n    [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23]\r\nE1      5     5     5     5     5     5     5     5     5     5     5\r\nE2      5     5     5     5     5     5     5     5     5     5     5\r\nE3      5     5     5     5     5     5     5     5     5     5     5\r\nE4      5     5     5     5     5     5     5     5     5     5     5\r\nE5      5     5     5     5     5     5     5     5     5     5     5\r\nE6      5     5     5     5     5     5     5     5     5     5     5\r\nE7      5     5     5     5     5     5     5     5     5     5     5\r\nE8      5     5     5     5     5     5     5     5     5     5     5\r\nE9      5     5     5     5     5     5     5     5     5     5     5\r\nE10     5     5     5     5     5     5     5     5     5     5     5\r\nN1      3     3     3     3     3     3     3     3     3     3     3\r\nN2      3     3     3     3     3     3     3     3     3     3     3\r\nN3      3     3     3     3     3     3     3     3     3     3     3\r\nN4      3     3     3     3     3     3     3     3     3     3     3\r\nN5      3     3     3     3     3     3     3     3     3     3     3\r\nN6      3     3     3     3     3     3     3     3     3     3     3\r\nN7      3     3     3     3     3     3     3     3     3     3     3\r\nN8      3     3     3     3     3     3     3     3     3     3     3\r\nN9      3     3     3     3     3     3     3     3     3     3     3\r\nN10     3     3     3     3     3     3     3     3     3     3     3\r\nA1      2     2     2     2     2     2     2     2     2     2     2\r\nA2      2     2     2     2     2     2     2     2     2     2     2\r\nA3      2     2     2     2     2     2     2     2     2     2     2\r\nA4      2     2     2     2     2     2     2     2     2     2     2\r\nA5      2     2     2     2     2     2     2     2     2     2     2\r\nA6      2     2     2     2     2     2     2     2     2     2     2\r\nA7      2     2     2     2     2     2     2     2     2     2     2\r\nA8      2     2     2     2     2     2     2     2     2     2     2\r\nA9      2     2     2     2     2     2     2     2     2     2     2\r\nA10     2     2     2     2     2     2     2     2     2     2     2\r\nC1      4     4     4     4     4     4     4     4     4     4     4\r\nC2      4     4     4     4     4     4     4     4     4     4     4\r\nC3      4     4     4     4     4     4     4     4     4     4     4\r\nC4      4     4     4     4     4     4     4     4     4     4     4\r\nC5      4     4     4     4     4     4     4     4     4     4     4\r\nC6      4     4     4     4     4     4     4     4     4     4     4\r\nC7      4     4     4     4     4     4     4     4     4     4     4\r\nC8      4     4     4     4     4     4     4     4     4     4     4\r\nC9      4     4     4     4     4     4     4     4     4     4     4\r\nC10     4     4     4     4     4     4     4     4     4     4     4\r\nO1      1     1     1     1     1     1     1     1     1     1     1\r\nO2      1     1     1     1     1     1     1     1     1     1     1\r\nO3      1     1     1     1     1     1     1     1     1     1     1\r\nO4      1     1     1     1     1     1     1     1     1     1     1\r\nO5      1     1     1     1     1     1     1     1     1     1     1\r\nO6      1     1     1     1     1     1     1     1     1     1     1\r\nO7      1     1     1     1     1     1     1     1     1     1     1\r\nO8      1     1     1     1     1     1     1     1     1     1     1\r\nO9      1     1     1     1     1     1     1     1     1     1     1\r\nO10     1     1     1     1     1     1     1     1     1     1     1\r\n    [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34]\r\nE1      5     5     5     5     5     5     5     5     5     5     5\r\nE2      5     5     5     5     5     5     5     5     5     5     5\r\nE3      5     5     5     5     5     5     5     5     5     5     5\r\nE4      5     5     5     5     5     5     5     5     5     5     5\r\nE5      5     5     5     5     5     5     5     5     5     5     5\r\nE6      5     5     5     5     5     5     5     5     5     5     5\r\nE7      5     5     5     5     5     5     5     5     5     5     5\r\nE8      5     5     5     5     5     5     5     5     5     5     5\r\nE9      5     5     5     5     5     5     5     5     5     5     5\r\nE10     5     5     5     5     5     5     5     5     5     5     5\r\nN1      3     3     3     3     3     3     3     3     3     3     3\r\nN2      3     3     3     3     3     3     3     3     3     3     3\r\nN3      3     3     3     3     3     3     3     3     3     3     3\r\nN4      3     3     3     3     3     3     3     3     3     3     3\r\nN5      3     3     3     3     3     3     3     3     3     3     3\r\nN6      3     3     3     3     3     3     3     3     3     3     3\r\nN7      3     3     3     3     3     3     3     3     3     3     3\r\nN8      3     3     3     3     3     3     3     3     3     3     3\r\nN9      3     3     3     3     3     3     3     3     3     3     3\r\nN10     3     3     3     3     3     3     3     3     3     3     3\r\nA1      2     2     2     2     2     2     2     2     2     2     2\r\nA2      2     2     2     2     2     2     2     2     2     2     2\r\nA3      2     2     2     2     2     2     2     2     2     2     2\r\nA4      2     2     2     2     2     2     2     2     2     2     2\r\nA5      2     2     2     2     2     2     2     2     2     2     2\r\nA6      2     2     2     2     2     2     2     2     2     2     2\r\nA7      2     2     2     2     2     2     2     2     2     2     2\r\nA8      2     2     2     2     2     2     2     2     2     2     2\r\nA9      2     2     2     2     2     2     2     2     2     2     2\r\nA10     2     2     2     2     2     2     2     2     2     2     2\r\nC1      4     4     4     4     4     4     4     4     4     4     4\r\nC2      4     4     4     4     4     4     4     4     4     4     4\r\nC3      4     4     4     4     4     4     4     4     4     4     4\r\nC4      4     4     4     4     4     4     4     4     4     4     4\r\nC5      4     4     4     4     4     4     4     4     4     4     4\r\nC6      4     4     4     4     4     4     4     4     4     4     4\r\nC7      4     4     4     4     4     4     4     4     4     4     4\r\nC8      4     4     4     4     4     4     4     4     4     4     4\r\nC9      4     4     4     4     4     4     4     4     4     4     4\r\nC10     4     4     4     4     4     4     4     4     4     4     4\r\nO1      1     1     1     1     1     1     1     1     1     1     1\r\nO2      1     1     1     1     1     1     1     1     1     1     1\r\nO3      1     1     1     1     1     1     1     1     1     1     1\r\nO4      1     1     1     1     1     1     1     1     1     1     1\r\nO5      1     1     1     1     1     1     1     1     1     1     1\r\nO6      1     1     1     1     1     1     1     1     1     1     1\r\nO7      1     1     1     1     1     1     1     1     1     1     1\r\nO8      1     1     1     1     1     1     1     1     1     1     1\r\nO9      1     1     1     1     1     1     1     1     1     1     1\r\nO10     1     1     1     1     1     1     1     1     1     1     1\r\n    [,35] [,36] [,37] [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45]\r\nE1      5     5     5     5     5     5     5     5     5     5     5\r\nE2      5     5     5     5     5     5     5     5     5     5     5\r\nE3      5     5     5     5     5     5     5     5     5     5     5\r\nE4      5     5     5     5     5     5     5     5     5     5     5\r\nE5      5     5     5     5     5     5     5     5     5     5     5\r\nE6      5     5     5     5     5     5     5     5     5     5     5\r\nE7      5     5     5     5     5     5     5     5     5     5     5\r\nE8      5     5     5     5     5     5     5     5     5     5     5\r\nE9      5     5     5     5     5     5     5     5     5     5     5\r\nE10     5     5     5     5     5     5     5     5     5     5     5\r\nN1      3     3     3     3     3     3     3     3     3     3     3\r\nN2      3     3     3     3     3     3     3     3     3     3     3\r\nN3      3     3     3     3     3     3     3     3     3     3     3\r\nN4      3     3     3     3     3     3     3     3     3     3     3\r\nN5      3     3     3     3     3     3     3     3     3     3     3\r\nN6      3     3     3     3     3     3     3     3     3     3     3\r\nN7      3     3     3     3     3     3     3     3     3     3     3\r\nN8      3     3     3     3     3     3     3     3     3     3     3\r\nN9      3     3     3     3     3     3     3     3     3     3     3\r\nN10     3     3     3     3     3     3     3     3     3     3     3\r\nA1      2     2     2     2     2     2     2     2     2     2     2\r\nA2      2     2     2     2     2     2     2     2     2     2     2\r\nA3      2     2     2     2     2     2     2     2     2     2     2\r\nA4      2     2     2     2     2     2     2     2     2     2     2\r\nA5      2     2     2     2     2     2     2     2     2     2     2\r\nA6      2     2     2     2     2     2     2     2     2     2     2\r\nA7      2     2     2     2     2     2     2     2     2     2     2\r\nA8      2     2     2     2     2     2     2     2     2     2     2\r\nA9      2     2     2     2     2     2     2     2     2     2     2\r\nA10     2     2     2     2     2     2     2     2     2     2     2\r\nC1      4     4     4     4     4     4     4     4     4     4     4\r\nC2      4     4     4     4     4     4     4     4     4     4     4\r\nC3      4     4     4     4     4     4     4     4     4     4     4\r\nC4      4     4     4     4     4     4     4     4     4     4     4\r\nC5      4     4     4     4     4     4     4     4     4     4     4\r\nC6      4     4     4     4     4     4     4     4     4     4     4\r\nC7      4     4     4     4     4     4     4     4     4     4     4\r\nC8      4     4     4     4     4     4     4     4     4     4     4\r\nC9      4     4     4     4     4     4     4     4     4     4     4\r\nC10     4     4     4     4     4     4     4     4     4     4     4\r\nO1      1     1     1     1     1     1     1     1     1     1     1\r\nO2      1     1     1     1     1     1     1     1     1     1     1\r\nO3      1     1     1     1     1     1     1     1     1     1     1\r\nO4      1     1     1     1     1     1     1     1     1     1     1\r\nO5      1     1     1     1     1     1     1     1     1     1     1\r\nO6      1     1     1     1     1     1     1     1     1     1     1\r\nO7      1     1     1     1     1     1     1     1     1     1     1\r\nO8      1     1     1     1     1     1     1     1     1     1     1\r\nO9      1     1     1     1     1     1     1     1     1     1     1\r\nO10     1     1     1     1     1     1     1     1     1     1     1\r\n    [,46] [,47] [,48] [,49] [,50] [,51] [,52] [,53] [,54] [,55] [,56]\r\nE1      5     5     5     5     5     5     5     5     5     5     5\r\nE2      5     5     5     5     5     5     5     5     5     5     5\r\nE3      5     5     5     5     5     5     5     5     5     5     5\r\nE4      5     5     5     5     5     5     5     5     5     5     5\r\nE5      5     5     5     5     5     5     5     5     5     5     5\r\nE6      5     5     5     5     5     5     5     5     5     5     5\r\nE7      5     5     5     5     5     5     5     5     5     5     5\r\nE8      5     5     5     5     5     5     5     5     5     5     5\r\nE9      5     5     5     5     5     5     5     5     5     5     5\r\nE10     5     5     5     5     5     5     5     5     5     5     5\r\nN1      3     3     3     3     3     3     3     3     3     3     3\r\nN2      3     3     3     3     3     3     3     3     3     3     3\r\nN3      3     3     3     3     3     3     3     3     3     3     3\r\nN4      3     3     3     3     3     3     3     3     3     3     3\r\nN5      3     3     3     3     3     3     3     3     3     3     3\r\nN6      3     3     3     3     3     3     3     3     3     3     3\r\nN7      3     3     3     3     3     3     3     3     3     3     3\r\nN8      3     3     3     3     3     3     3     3     3     3     3\r\nN9      3     3     3     3     3     3     3     3     3     3     3\r\nN10     3     3     3     3     3     3     3     3     3     3     3\r\nA1      2     2     2     2     2     2     2     2     2     2     2\r\nA2      2     2     2     2     2     2     2     2     2     2     2\r\nA3      2     2     2     2     2     2     2     2     2     2     2\r\nA4      2     2     2     2     2     2     2     2     2     2     2\r\nA5      2     2     2     2     2     2     2     2     2     2     2\r\nA6      2     2     2     2     2     2     2     2     2     2     2\r\nA7      2     2     2     2     2     2     2     2     2     2     2\r\nA8      2     2     2     2     2     2     2     2     2     2     2\r\nA9      2     2     2     2     2     2     2     2     2     2     2\r\nA10     2     2     2     2     2     2     2     2     2     2     2\r\nC1      4     4     4     4     4     4     4     4     4     4     4\r\nC2      4     4     4     4     4     4     4     4     4     4     4\r\nC3      4     4     4     4     4     4     4     4     4     4     4\r\nC4      4     4     4     4     4     4     4     4     4     4     4\r\nC5      4     4     4     4     4     4     4     4     4     4     4\r\nC6      4     4     4     4     4     4     4     4     4     4     4\r\nC7      4     4     4     4     4     4     4     4     4     4     4\r\nC8      4     4     4     4     4     4     4     4     4     4     4\r\nC9      4     4     4     4     4     4     4     4     4     4     4\r\nC10     4     4     4     4     4     4     4     4     4     4     4\r\nO1      1     1     1     1     1     1     1     1     1     1     1\r\nO2      1     1     1     1     1     1     1     1     1     1     1\r\nO3      1     1     1     1     1     1     1     1     1     1     1\r\nO4      1     1     1     1     1     1     1     1     1     1     1\r\nO5      1     1     1     1     1     1     1     1     1     1     1\r\nO6      1     1     1     1     1     1     1     1     1     1     1\r\nO7      1     1     1     1     1     1     1     1     1     1     1\r\nO8      1     1     1     1     1     1     1     1     1     1     1\r\nO9      1     1     1     1     1     1     1     1     1     1     1\r\nO10     1     1     1     1     1     1     1     1     1     1     1\r\n    [,57] [,58] [,59] [,60] [,61] [,62] [,63] [,64] [,65] [,66] [,67]\r\nE1      5     5     5     5     5     5     5     5     5     5     5\r\nE2      5     5     5     5     5     5     5     5     5     5     5\r\nE3      5     5     5     5     5     5     5     5     5     5     5\r\nE4      5     5     5     5     5     5     5     5     5     5     5\r\nE5      5     5     5     5     5     5     5     5     5     5     5\r\nE6      5     5     5     5     5     5     5     5     5     5     5\r\nE7      5     5     5     5     5     5     5     5     5     5     5\r\nE8      5     5     5     5     5     5     5     5     5     5     5\r\nE9      5     5     5     5     5     5     5     5     5     5     5\r\nE10     5     5     5     5     5     5     5     5     5     5     5\r\nN1      3     3     3     3     3     3     3     3     3     3     3\r\nN2      3     3     3     3     3     3     3     3     3     3     3\r\nN3      3     3     3     3     3     3     3     3     3     3     3\r\nN4      3     3     3     3     3     3     3     3     3     3     3\r\nN5      3     3     3     3     3     3     3     3     3     3     3\r\nN6      3     3     3     3     3     3     3     3     3     3     3\r\nN7      3     3     3     3     3     3     3     3     3     3     3\r\nN8      3     3     3     3     3     3     3     3     3     3     3\r\nN9      3     3     3     3     3     3     3     3     3     3     3\r\nN10     3     3     3     3     3     3     3     3     3     3     3\r\nA1      2     2     2     2     2     2     2     2     2     2     2\r\nA2      2     2     2     2     2     2     2     2     2     2     2\r\nA3      2     2     2     2     2     2     2     2     2     2     2\r\nA4      2     2     2     2     2     2     2     2     2     2     2\r\nA5      2     2     2     2     2     2     2     2     2     2     2\r\nA6      2     2     2     2     2     2     2     2     2     2     2\r\nA7      2     2     2     2     2     2     2     2     2     2     2\r\nA8      2     2     2     2     2     2     2     2     2     2     2\r\nA9      2     2     2     2     2     2     2     2     2     2     2\r\nA10     2     2     2     2     2     2     2     2     2     2     2\r\nC1      4     4     4     4     4     4     4     4     4     4     4\r\nC2      4     4     4     4     4     4     4     4     4     4     4\r\nC3      4     4     4     4     4     4     4     4     4     4     4\r\nC4      4     4     4     4     4     4     4     4     4     4     4\r\nC5      4     4     4     4     4     4     4     4     4     4     4\r\nC6      4     4     4     4     4     4     4     4     4     4     4\r\nC7      4     4     4     4     4     4     4     4     4     4     4\r\nC8      4     4     4     4     4     4     4     4     4     4     4\r\nC9      4     4     4     4     4     4     4     4     4     4     4\r\nC10     4     4     4     4     4     4     4     4     4     4     4\r\nO1      1     1     1     1     1     1     1     1     1     1     1\r\nO2      1     1     1     1     1     1     1     1     1     1     1\r\nO3      1     1     1     1     1     1     1     1     1     1     1\r\nO4      1     1     1     1     1     1     1     1     1     1     1\r\nO5      1     1     1     1     1     1     1     1     1     1     1\r\nO6      1     1     1     1     1     1     1     1     1     1     1\r\nO7      1     1     1     1     1     1     1     1     1     1     1\r\nO8      1     1     1     1     1     1     1     1     1     1     1\r\nO9      1     1     1     1     1     1     1     1     1     1     1\r\nO10     1     1     1     1     1     1     1     1     1     1     1\r\n    [,68] [,69] [,70] [,71] [,72] [,73] [,74] [,75] [,76] [,77] [,78]\r\nE1      5     5     5     5     5     5     5     5     5     5     5\r\nE2      5     5     5     5     5     5     5     5     5     5     5\r\nE3      5     5     5     5     5     5     5     5     5     5     5\r\nE4      5     5     5     5     5     5     5     5     5     5     5\r\nE5      5     5     5     5     5     5     5     5     5     5     5\r\nE6      5     5     5     5     5     5     5     5     5     5     5\r\nE7      5     5     5     5     5     5     5     5     5     5     5\r\nE8      5     5     5     5     5     5     5     5     5     5     5\r\nE9      5     5     5     5     5     5     5     5     5     5     5\r\nE10     5     5     5     5     5     5     5     5     5     5     5\r\nN1      3     3     3     3     3     3     3     3     3     3     3\r\nN2      3     3     3     3     3     3     3     3     3     3     3\r\nN3      3     3     3     3     3     3     3     3     3     3     3\r\nN4      3     3     3     3     3     3     3     3     3     3     3\r\nN5      3     3     3     3     3     3     3     3     3     3     3\r\nN6      3     3     3     3     3     3     3     3     3     3     3\r\nN7      3     3     3     3     3     3     3     3     3     3     3\r\nN8      3     3     3     3     3     3     3     3     3     3     3\r\nN9      3     3     3     3     3     3     3     3     3     3     3\r\nN10     3     3     3     3     3     3     3     3     3     3     3\r\nA1      2     2     2     2     2     2     2     2     2     2     2\r\nA2      2     2     2     2     2     2     2     2     2     2     2\r\nA3      2     2     2     2     2     2     2     2     2     2     2\r\nA4      2     2     2     2     2     2     2     2     2     2     2\r\nA5      2     2     2     2     2     2     2     2     2     2     2\r\nA6      2     2     2     2     2     2     2     2     2     2     2\r\nA7      2     2     2     2     2     2     2     2     2     2     2\r\nA8      2     2     2     2     2     2     2     2     2     2     2\r\nA9      2     2     2     2     2     2     2     2     2     2     2\r\nA10     2     2     2     2     2     2     2     2     2     2     2\r\nC1      4     4     4     4     4     4     4     4     4     4     4\r\nC2      4     4     4     4     4     4     4     4     4     4     4\r\nC3      4     4     4     4     4     4     4     4     4     4     4\r\nC4      4     4     4     4     4     4     4     4     4     4     4\r\nC5      4     4     4     4     4     4     4     4     4     4     4\r\nC6      4     4     4     4     4     4     4     4     4     4     4\r\nC7      4     4     4     4     4     4     4     4     4     4     4\r\nC8      4     4     4     4     4     4     4     4     4     4     4\r\nC9      4     4     4     4     4     4     4     4     4     4     4\r\nC10     4     4     4     4     4     4     4     4     4     4     4\r\nO1      1     1     1     1     1     1     1     1     1     1     1\r\nO2      1     1     1     1     1     1     1     1     1     1     1\r\nO3      1     1     1     1     1     1     1     1     1     1     1\r\nO4      1     1     1     1     1     1     1     1     1     1     1\r\nO5      1     1     1     1     1     1     1     1     1     1     1\r\nO6      1     1     1     1     1     1     1     1     1     1     1\r\nO7      1     1     1     1     1     1     1     1     1     1     1\r\nO8      1     1     1     1     1     1     1     1     1     1     1\r\nO9      1     1     1     1     1     1     1     1     1     1     1\r\nO10     1     1     1     1     1     1     1     1     1     1     1\r\n    [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86] [,87] [,88] [,89]\r\nE1      5     5     5     5     5     5     5     5     5     5     5\r\nE2      5     5     5     5     5     5     5     5     5     5     5\r\nE3      5     5     5     5     5     5     5     5     5     5     5\r\nE4      5     5     5     5     5     5     5     5     5     5     5\r\nE5      5     5     5     5     5     5     5     5     5     5     5\r\nE6      5     5     5     5     5     5     5     5     5     5     5\r\nE7      5     5     5     5     5     5     5     5     5     5     5\r\nE8      5     5     5     5     5     5     5     5     5     5     5\r\nE9      5     5     5     5     5     5     5     5     5     5     5\r\nE10     5     5     5     5     5     5     5     5     5     5     5\r\nN1      3     3     3     3     3     3     3     3     3     3     3\r\nN2      3     3     3     3     3     3     3     3     3     3     3\r\nN3      3     3     3     3     3     3     3     3     3     3     3\r\nN4      3     3     3     3     3     3     3     3     3     3     3\r\nN5      3     3     3     3     3     3     3     3     3     3     3\r\nN6      3     3     3     3     3     3     3     3     3     3     3\r\nN7      3     3     3     3     3     3     3     3     3     3     3\r\nN8      3     3     3     3     3     3     3     3     3     3     3\r\nN9      3     3     3     3     3     3     3     3     3     3     3\r\nN10     3     3     3     3     3     3     3     3     3     3     3\r\nA1      2     2     2     2     2     2     2     2     2     2     2\r\nA2      2     2     2     2     2     2     2     2     2     2     2\r\nA3      2     2     2     2     2     2     2     2     2     2     2\r\nA4      2     2     2     2     2     2     2     2     2     2     2\r\nA5      2     2     2     2     2     2     2     2     2     2     2\r\nA6      2     2     2     2     2     2     2     2     2     2     2\r\nA7      2     2     2     2     2     2     2     2     2     2     2\r\nA8      2     2     2     2     2     2     2     2     2     2     2\r\nA9      2     2     2     2     2     2     2     2     2     2     2\r\nA10     2     2     2     2     2     2     2     2     2     2     2\r\nC1      4     4     4     4     4     4     4     4     4     4     4\r\nC2      4     4     4     4     4     4     4     4     4     4     4\r\nC3      4     4     4     4     4     4     4     4     4     4     4\r\nC4      4     4     4     4     4     4     4     4     4     4     4\r\nC5      4     4     4     4     4     4     4     4     4     4     4\r\nC6      4     4     4     4     4     4     4     4     4     4     4\r\nC7      4     4     4     4     4     4     4     4     4     4     4\r\nC8      4     4     4     4     4     4     4     4     4     4     4\r\nC9      4     4     4     4     4     4     4     4     4     4     4\r\nC10     4     4     4     4     4     4     4     4     4     4     4\r\nO1      1     1     1     1     1     1     1     1     1     1     1\r\nO2      1     1     1     1     1     1     1     1     1     1     1\r\nO3      1     1     1     1     1     1     1     1     1     1     1\r\nO4      1     1     1     1     1     1     1     1     1     1     1\r\nO5      1     1     1     1     1     1     1     1     1     1     1\r\nO6      1     1     1     1     1     1     1     1     1     1     1\r\nO7      1     1     1     1     1     1     1     1     1     1     1\r\nO8      1     1     1     1     1     1     1     1     1     1     1\r\nO9      1     1     1     1     1     1     1     1     1     1     1\r\nO10     1     1     1     1     1     1     1     1     1     1     1\r\n    [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98] [,99]\r\nE1      5     5     5     5     5     5     5     5     5     5\r\nE2      5     5     5     5     5     5     5     5     5     5\r\nE3      5     5     5     5     5     5     5     5     5     5\r\nE4      5     5     5     5     5     5     5     5     5     5\r\nE5      5     5     5     5     5     5     5     5     5     5\r\nE6      5     5     5     5     5     5     5     5     5     5\r\nE7      5     5     5     5     5     5     5     5     5     5\r\nE8      5     5     5     5     5     5     5     5     5     5\r\nE9      5     5     5     5     5     5     5     5     5     5\r\nE10     5     5     5     5     5     5     5     5     5     5\r\nN1      3     3     3     3     3     3     3     3     3     3\r\nN2      3     3     3     3     3     3     3     3     3     3\r\nN3      3     3     3     3     3     3     3     3     3     3\r\nN4      3     3     3     3     3     3     3     3     3     3\r\nN5      3     3     3     3     3     3     3     3     3     3\r\nN6      3     3     3     3     3     3     3     3     3     3\r\nN7      3     3     3     3     3     3     3     3     3     3\r\nN8      3     3     3     3     3     3     3     3     3     3\r\nN9      3     3     3     3     3     3     3     3     3     3\r\nN10     3     3     3     3     3     3     3     3     3     3\r\nA1      2     2     2     2     2     2     2     2     2     2\r\nA2      2     2     2     2     2     2     2     2     2     2\r\nA3      2     2     2     2     2     2     2     2     2     2\r\nA4      2     2     2     2     2     2     2     2     2     2\r\nA5      2     2     2     2     2     2     2     2     2     2\r\nA6      2     2     2     2     2     2     2     2     2     2\r\nA7      2     2     2     2     2     2     2     2     2     2\r\nA8      2     2     2     2     2     2     2     2     2     2\r\nA9      2     2     2     2     2     2     2     2     2     2\r\nA10     2     2     2     2     2     2     2     2     2     2\r\nC1      4     4     4     4     4     4     4     4     4     4\r\nC2      4     4     4     4     4     4     4     4     4     4\r\nC3      4     4     4     4     4     4     4     4     4     4\r\nC4      4     4     4     4     4     4     4     4     4     4\r\nC5      4     4     4     4     4     4     4     4     4     4\r\nC6      4     4     4     4     4     4     4     4     4     4\r\nC7      4     4     4     4     4     4     4     4     4     4\r\nC8      4     4     4     4     4     4     4     4     4     4\r\nC9      4     4     4     4     4     4     4     4     4     4\r\nC10     4     4     4     4     4     4     4     4     4     4\r\nO1      1     1     1     1     1     1     1     1     1     1\r\nO2      1     1     1     1     1     1     1     1     1     1\r\nO3      1     1     1     1     1     1     1     1     1     1\r\nO4      1     1     1     1     1     1     1     1     1     1\r\nO5      1     1     1     1     1     1     1     1     1     1\r\nO6      1     1     1     1     1     1     1     1     1     1\r\nO7      1     1     1     1     1     1     1     1     1     1\r\nO8      1     1     1     1     1     1     1     1     1     1\r\nO9      1     1     1     1     1     1     1     1     1     1\r\nO10     1     1     1     1     1     1     1     1     1     1\r\n    [,100]\r\nE1       5\r\nE2       5\r\nE3       5\r\nE4       5\r\nE5       5\r\nE6       5\r\nE7       5\r\nE8       5\r\nE9       5\r\nE10      5\r\nN1       3\r\nN2       3\r\nN3       3\r\nN4       3\r\nN5       3\r\nN6       3\r\nN7       3\r\nN8       3\r\nN9       3\r\nN10      3\r\nA1       2\r\nA2       2\r\nA3       2\r\nA4       2\r\nA5       2\r\nA6       2\r\nA7       2\r\nA8       2\r\nA9       2\r\nA10      2\r\nC1       4\r\nC2       4\r\nC3       4\r\nC4       4\r\nC5       4\r\nC6       4\r\nC7       4\r\nC8       4\r\nC9       4\r\nC10      4\r\nO1       1\r\nO2       1\r\nO3       1\r\nO4       1\r\nO5       1\r\nO6       1\r\nO7       1\r\nO8       1\r\nO9       1\r\nO10      1\r\n\r\n\r\n$item.stability\r\n$item.stability$empirical.dimensions\r\n O1  O2  O3  O4  O5  O6  O7  O8  O9 O10  A1  A2  A3  A4  A5  A6  A7 \r\n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \r\n A8  A9 A10  N1  N2  N3  N4  N5  N6  N7  N8  N9 N10  C1  C2  C3  C4 \r\n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \r\n C5  C6  C7  C8  C9 C10  E1  E2  E3  E4  E5  E6  E7  E8  E9 E10 \r\n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \r\n\r\n$item.stability$all.dimensions\r\n    1 2 3 4 5\r\nO1  1 0 0 0 0\r\nO2  1 0 0 0 0\r\nO3  1 0 0 0 0\r\nO4  1 0 0 0 0\r\nO5  1 0 0 0 0\r\nO6  1 0 0 0 0\r\nO7  1 0 0 0 0\r\nO8  1 0 0 0 0\r\nO9  1 0 0 0 0\r\nO10 1 0 0 0 0\r\nA1  0 1 0 0 0\r\nA2  0 1 0 0 0\r\nA3  0 1 0 0 0\r\nA4  0 1 0 0 0\r\nA5  0 1 0 0 0\r\nA6  0 1 0 0 0\r\nA7  0 1 0 0 0\r\nA8  0 1 0 0 0\r\nA9  0 1 0 0 0\r\nA10 0 1 0 0 0\r\nN1  0 0 1 0 0\r\nN2  0 0 1 0 0\r\nN3  0 0 1 0 0\r\nN4  0 0 1 0 0\r\nN5  0 0 1 0 0\r\nN6  0 0 1 0 0\r\nN7  0 0 1 0 0\r\nN8  0 0 1 0 0\r\nN9  0 0 1 0 0\r\nN10 0 0 1 0 0\r\nC1  0 0 0 1 0\r\nC2  0 0 0 1 0\r\nC3  0 0 0 1 0\r\nC4  0 0 0 1 0\r\nC5  0 0 0 1 0\r\nC6  0 0 0 1 0\r\nC7  0 0 0 1 0\r\nC8  0 0 0 1 0\r\nC9  0 0 0 1 0\r\nC10 0 0 0 1 0\r\nE1  0 0 0 0 1\r\nE2  0 0 0 0 1\r\nE3  0 0 0 0 1\r\nE4  0 0 0 0 1\r\nE5  0 0 0 0 1\r\nE6  0 0 0 0 1\r\nE7  0 0 0 0 1\r\nE8  0 0 0 0 1\r\nE9  0 0 0 0 1\r\nE10 0 0 0 0 1\r\n\r\n\r\n$plot\r\n\r\n\r\n$mean.loadings\r\n           1        2        3        4        5\r\nO1   0.30720 -0.00221 -0.00160  0.00456 -0.01138\r\nO2  -0.29093  0.00492  0.04399  0.00098  0.00649\r\nO3   0.25147  0.00162  0.00959 -0.02048 -0.00347\r\nO4  -0.23413 -0.01246 -0.00031  0.00609  0.00063\r\nO5   0.25141  0.01206 -0.00408  0.04092  0.02301\r\nO6  -0.26856 -0.00680  0.00000  0.00000  0.01373\r\nO7   0.18369  0.00716 -0.01910  0.08451  0.00000\r\nO8   0.24276  0.03165  0.00532 -0.01625 -0.00529\r\nO9   0.10796  0.03400  0.04442  0.02276 -0.00433\r\nO10  0.34513  0.00150  0.00000  0.00780  0.02958\r\nA1  -0.01025  0.14071  0.00551 -0.01950  0.01227\r\nA2   0.00160  0.19575  0.00000 -0.01028  0.07268\r\nA3   0.02445  0.11329  0.06184 -0.06368  0.03116\r\nA4   0.00985  0.40845  0.01414  0.00000  0.00002\r\nA5  -0.00415 -0.28542 -0.00007 -0.00001  0.00073\r\nA6  -0.01330  0.22036  0.04996  0.00000  0.00014\r\nA7  -0.00937 -0.30437  0.00907 -0.00492 -0.00137\r\nA8   0.00252  0.22963 -0.00030  0.00958  0.00943\r\nA9   0.02002  0.31385  0.00795  0.00829  0.00000\r\nA10  0.02088  0.12357 -0.01261  0.02884  0.09563\r\nN1  -0.01954  0.00033  0.31789 -0.00424 -0.00012\r\nN2   0.00568  0.00702 -0.18227 -0.00387  0.02775\r\nN3   0.03103  0.03924  0.24779  0.04157  0.01181\r\nN4  -0.00130  0.00480 -0.12695  0.01135  0.00284\r\nN5  -0.02750  0.00220  0.15908 -0.02181  0.00023\r\nN6  -0.01337  0.03106  0.32793 -0.01479  0.00000\r\nN7   0.00457  0.00432  0.27570 -0.01549  0.00000\r\nN8   0.00001  0.00201  0.34925 -0.03390  0.00000\r\nN9  -0.00086 -0.07244  0.25831  0.00091  0.00575\r\nN10  0.02728 -0.00005  0.25664 -0.03733 -0.05148\r\nC1   0.02873  0.00000 -0.00310  0.27463  0.00220\r\nC2   0.01296  0.01085 -0.00389 -0.22476 -0.00018\r\nC3   0.06676  0.00850  0.01771  0.16283  0.00213\r\nC4   0.00748  0.03473  0.08806 -0.24955 -0.00499\r\nC5  -0.00905  0.01259 -0.01097  0.28608  0.00661\r\nC6   0.00438  0.00000  0.00755 -0.31143  0.00000\r\nC7   0.00435  0.00000  0.01701  0.23044  0.00832\r\nC8   0.00000 -0.00894  0.03057 -0.19143  0.00135\r\nC9  -0.00687  0.00684  0.00180  0.28896  0.00155\r\nC10  0.05659  0.02013  0.00000  0.20565  0.00000\r\nE1   0.00301  0.03257 -0.00035  0.00000  0.23667\r\nE2   0.00000 -0.01167 -0.00103  0.00013  0.26156\r\nE3  -0.00591  0.09845 -0.07217  0.01457  0.21283\r\nE4   0.01271 -0.00009  0.01909 -0.00082 -0.30472\r\nE5   0.00311  0.06126  0.00000  0.00122  0.30649\r\nE6  -0.05808 -0.01897  0.00038 -0.00121  0.20654\r\nE7   0.00000  0.03499  0.00000  0.00000  0.31879\r\nE8   0.00067  0.00474  0.00000  0.01106  0.21917\r\nE9   0.02462  0.01562  0.00000 -0.00006  0.24571\r\nE10  0.01618  0.00024  0.02189  0.00000  0.17401\r\n\r\nattr(,\"class\")\r\n[1] \"itemStability\"\r\n\r\nMeasurement Invariance with Network Model\r\nWe can also test whether structure of the test is the same or similar across subgroup of populations such as age groups, gender, and race with the measurement invariance analysis. This technique allows us to check whether our test is consistent across different populations.\r\nWe begin by setting up our model parameter first. We will create a matrix called ‘Lambda’ and load all 50 items into their respective dimensions.\r\n\r\n\r\nShow code\r\n\r\n#paste(1:10, collapse = \", \")\r\n\r\nLambda <- matrix(0, 50, 5)\r\nLambda[c(1,2,3,4,5,6,7,8,9,10)] <- 1 # first factor (E)\r\nLambda[c(11,12,13,14,15,16,17,18,19,20), 2] <- 1 # second factor (N)\r\nLambda[c(21,22,23,24,25,26,27,28,29,30), 3] <- 1 # second factor (A)\r\nLambda[c(31,32,33,34,35,36,37,38,39,40), 4] <- 1 # second factor (C)\r\nLambda[c(41,42,43,44,45,46,47,48,49,50), 5] <- 1 # second factor (O)\r\n\r\nlatents <- c(\"E\", \"N\", \"A\", \"C\", \"O\")\r\n\r\n\r\nThen, we will recode the gender variable into factor, as well as removing missing data (coded 0) and gender ‘other’ (coded 3) because there is not enough sample size (less than 100).\r\n\r\n\r\nShow code\r\n\r\n#recode gender\r\n\r\ndf_mi <- subset(df, gender != 3 & gender != 0)\r\n\r\ndf_mi$gender <-as.factor  (df_mi$gender)\r\n\r\nlevels(df_mi$gender) <- c(\"male\", \"female\")\r\n\r\n\r\nWe will then establish our network models based on subgroups with the lvm function in psychonetrics package.\r\n\r\n\r\nShow code\r\n\r\n# Configural model with free residuals across groups\r\nmod_configural <- psychonetrics::lvm(data = df_mi, # data\r\n                                     lambda = Lambda, # factor structure\r\n                                     vars = obsvars, # items to be analyzed\r\n                                     latents = latents, # factor names\r\n                                     identification = \"variance\", # Fix variance to 1\r\n                                     groups =  \"gender\", # group variable\r\n                                     # if \"full\", it frees the residual variance-covariance matrix\r\n                                     # Default: \"empty\" --> fixing them to zero\r\n                                     omega_epsilon = \"full\",\r\n                                     residual = \"ggm\") %>%\r\n  psychonetrics::runmodel()\r\n\r\n\r\nThen, we can visualize the network models based on gender male and female. We can see that the structure of both models are similar, but edges between items may be different. For example, an edge between item A7 to A9 in female subgroup is more prominent than male subgroup. This may indicate differences how the two items work between respondents of different gender.\r\n\r\n\r\nShow code\r\n\r\n# Get the residual/error matrix\r\nnet_male <- mod_configural %>% \r\n  psychonetrics::getmatrix(\"omega_epsilon\") %>%\r\n  .$\"male\"\r\n\r\nnet_female <- mod_configural %>% \r\n  psychonetrics::getmatrix(\"omega_epsilon\") %>%\r\n  .$\"female\"\r\n\r\n# Obtain an average/joint layout over several graphs\r\nLayout <- qgraph::averageLayout(net_male, net_female)\r\n\r\n# Plot both networks together\r\nlayout(t(1:2)) # 1 row and 2 columns layout\r\n\r\nqgraph::qgraph(net_male, \r\n               layout = Layout, \r\n               theme = \"colorblind\", \r\n               title = \"Male\", \r\n               labels = obsvars)\r\n\r\nqgraph::qgraph(net_female, \r\n               layout = Layout, \r\n               theme = \"colorblind\", \r\n               title = \"Female\",\r\n               labels = obsvars)\r\n\r\n\r\nShow code\r\n\r\n# Reset the layout to 1:1 again\r\nlayout(t(1:1)) \r\n\r\n\r\nWe can further test the invariance of our model across levels like weak invariance (equal factor loadings), strong invariance (equal factor loadings and intercepts), and strict invariance (equal factor loadings, intercepts, and even residuals) across all subgroups. See Finch et al. (2023) and Bulut (2020) for more information.\r\nConcluding Remark\r\nTo support validity of a test, we need as much evidence as possible to argue that our test, its structure, and its functions actually hold across different conditions. For this reason, network psychometric is a useful approach to examine validity evidence of a test and ensure its continuing interpretations and uses. Thank you so much for reading this!\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-04-30-networkpsych/ega.png",
    "last_modified": "2023-04-30T01:06:56-06:00",
    "input_file": "networkpsych.knit.md",
    "preview_width": 700,
    "preview_height": 432
  },
  {
    "path": "posts/2023-03-20-bibanalysis/",
    "title": "Examining State of the Field with Bibliometric Analysis",
    "description": "In this post, I will be performing bibliometric analysis to examine state of the field from bibliographic records.  \n\n(7 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2023-03-22",
    "categories": [
      "R",
      "Data Visualization"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction, import, and convert the data\r\nExamining relationships between authors with co-citation Analysis\r\nExamining conceptual structure with co-word analysis\r\nExamining keyword usage across time with historiograph\r\nExamining trends of the field with thematic map\r\nExamining collaboration with social structure analysis\r\nConcluding remarks\r\n\r\nIntroduction, import, and convert the data\r\nHi Everyone. It’s been a busy semester for me. I taught a class (it’s over). I am interning at Alberta Education. I am also taking a doctoral-level coursework. I am also doing research regularly as always. But I am managing, I guess. I found an interesting analysis technique from one of my reading club that I think would be beneficial to you. It’s called bibliometric analysis.\r\nSome key reading that inspired me to try my hands on this techniques are Donthu et al. (2021), Moral-Muñoz et al. (2020), and Sudakova et al. (2022). This post will revolve around the three papers I mentioned, as well as some online tutorial of the package. I will be performing a bibliometric analysis with bibliographic records of the learning analytics field in R. The package I will mainly use is the bibliometrix package (Aria & Cuccurullo, 2017) to perform the analysis, with some additional helper packages such as ggplot2.\r\nBibliometric analysis is an exploratory method that analyzes large volumes of bibliometric records to examine trends of scientific publications, journal performance, collaboration pattern, and relationships between topics in the field (Donthu et al., 2021). Researchers can use this technique to gain a broad perspective of the field to identify a research gap that they can study, as well as determine its relative importance to other topics. Bibliometric records can be extracted from any scientific databases such as Web of science, SCOPUS, Digital science dimensions, and PubMed.\r\nFor the data set in this post, I extracted a set of bibliometric records from Web of Science because it provides more information on meta-data (e.g., keyword plus, cited references) and less missing data than Scopus. However, note that Scopus has more publication data for the arts and humanities field.\r\nI used the search syntax of: Learning analytic* (All fields) AND Education* (All fields) NOT Medical (All fields), with filter for review article or proceeding Paper. Then, I extracted the first 500 entries from the database based on their relevance to the search term. Note that Web of Science only allows for the maximum of 500 entries to be exported if I requested for full bibliographic record. Here is the link to the search result.\r\nFirst, we will set our working directory and load the packages as usual.\r\nI exported the bibliographic file in as a plain text document. I will use the convert2df function to import the data set into R environment. We have to specify source and format of data set for the package to correctly process the data\r\n\r\n\r\nShow code\r\n\r\n#using plain text file. \r\nfile <- c(\"la.txt\")\r\n\r\nM <- convert2df(file, dbsource = \"wos\", format = \"plaintext\")\r\n\r\n\r\nConverting your wos collection into a bibliographic dataframe\r\n\r\nDone!\r\n\r\n\r\nGenerating affiliation field tag AU_UN from C1:  Done!\r\n\r\nWe can use the biblioAnalysis function to perform descriptive analysis of the data set. We can check for information such as time span of the literature, number of journals, document types and so on. Some of the more interesting results are most relevant sources (i.e., journals or conferences), and top manuscripts per citation. We can identify popular topics and papers from these results.\r\n\r\n\r\nShow code\r\n\r\n#Summary\r\nresults <- biblioAnalysis(M, sep = \";\")\r\nS <- summary(object = results, k = 10, pause = FALSE)\r\n\r\n\r\n\r\nMAIN INFORMATION ABOUT DATA\r\n\r\n Timespan                              2007 : 2023 \r\n Sources (Journals, Books, etc)        238 \r\n Documents                             500 \r\n Annual Growth Rate %                  11.85 \r\n Document Average Age                  4.52 \r\n Average citations per doc             9.354 \r\n Average citations per year per doc    1.638 \r\n References                            11674 \r\n \r\nDOCUMENT TYPES                     \r\n article                         162 \r\n article; book chapter           3 \r\n article; early access           19 \r\n article; proceedings paper      1 \r\n proceedings paper               300 \r\n review                          11 \r\n review; book chapter            1 \r\n review; early access            3 \r\n \r\nDOCUMENT CONTENTS\r\n Keywords Plus (ID)                    321 \r\n Author's Keywords (DE)                1206 \r\n \r\nAUTHORS\r\n Authors                               1274 \r\n Author Appearances                    1722 \r\n Authors of single-authored docs       47 \r\n \r\nAUTHORS COLLABORATION\r\n Single-authored docs                  49 \r\n Documents per Author                  0.392 \r\n Co-Authors per Doc                    3.44 \r\n International co-authorships %        29.8 \r\n \r\n\r\nAnnual Scientific Production\r\n\r\n Year    Articles\r\n    2007        1\r\n    2011        1\r\n    2012        4\r\n    2013       15\r\n    2014       14\r\n    2015       17\r\n    2016       41\r\n    2017       70\r\n    2018       76\r\n    2019       62\r\n    2020       53\r\n    2021       66\r\n    2022       52\r\n    2023        6\r\n\r\nAnnual Percentage Growth Rate 11.85 \r\n\r\n\r\nMost Productive Authors\r\n\r\n       Authors        Articles Authors        Articles Fractionalized\r\n1  OGATA H                  19   OGATA H                         5.33\r\n2  GASEVIC D                16   GASEVIC D                       3.87\r\n3  RIENTIES B               12   RIENTIES B                      3.68\r\n4  YAMADA M                 11   NGUYEN Q                        2.98\r\n5  NGUYEN Q                 10   YAMADA M                        2.85\r\n6  DRACHSLER H               9   IFENTHALER D                    2.57\r\n7  KINSHUK                   8   GIANNAKOS M                     2.33\r\n8  PARDO A                   8   TEMPELAAR D                     2.25\r\n9  FERNANDEZ-MANJON B        7   KINSHUK                         2.16\r\n10 FREIRE M                  7   DRACHSLER H                     2.15\r\n\r\n\r\nTop manuscripts per citations\r\n\r\n                                                                                                                                       Paper         \r\n1  GRELLER W, 2012, EDUC TECHNOL SOC                                                                                                                 \r\n2  SHUM SB, 2012, EDUC TECHNOL SOC                                                                                                                   \r\n3  LU OHT, 2018, EDUC TECHNOL SOC                                                                                                                    \r\n4  JIVET I, 2018, PROCEEDINGS OF THE 8TH INTERNATIONAL CONFERENCE ON LEARNING ANALYTICS & KNOWLEDGE (LAK'18): TOWARDS USER-CENTRED LEARNING ANALYTICS\r\n5  TABUENCA B, 2015, COMPUT EDUC                                                                                                                     \r\n6  LEITNER P, 2017, STUD SYST DECIS CONT                                                                                                             \r\n7  WILLIAMS R, 2011, INT REV RES OPEN DIS                                                                                                            \r\n8  TSAI YS, 2017, SEVENTH INTERNATIONAL LEARNING ANALYTICS & KNOWLEDGE CONFERENCE (LAK'17)                                                           \r\n9  JIVET I, 2017, LECT NOTES COMPUT SC                                                                                                               \r\n10 BODILY R, 2017, SEVENTH INTERNATIONAL LEARNING ANALYTICS & KNOWLEDGE CONFERENCE (LAK'17)                                                          \r\n                             DOI  TC TCperYear   NTC\r\n1  NA                            374     31.17  2.29\r\n2  NA                            260     21.67  1.59\r\n3  NA                            120     20.00 10.39\r\n4  10.1145/3170358.3170421       118     19.67 10.21\r\n5  10.1016/j.compedu.2015.08.004 109     12.11  7.92\r\n6  10.1007/978-3-319-52977-6_1   102     14.57  8.71\r\n7  10.19173/irrodl.v12i3.883      83      6.38  1.00\r\n8  10.1145/3027385.3027400        75     10.71  6.40\r\n9  10.1007/978-3-319-66610-5_7    71     10.14  6.06\r\n10 10.1145/3027385.3027403        71     10.14  6.06\r\n\r\n\r\nCorresponding Author's Countries\r\n\r\n          Country Articles   Freq SCP MCP MCP_Ratio\r\n1  CHINA                52 0.1055  38  14     0.269\r\n2  USA                  48 0.0974  34  14     0.292\r\n3  JAPAN                36 0.0730  30   6     0.167\r\n4  AUSTRALIA            34 0.0690  18  16     0.471\r\n5  SPAIN                28 0.0568  25   3     0.107\r\n6  UNITED KINGDOM       26 0.0527  18   8     0.308\r\n7  GERMANY              23 0.0467  17   6     0.261\r\n8  NETHERLANDS          20 0.0406   9  11     0.550\r\n9  NORWAY               19 0.0385  11   8     0.421\r\n10 CANADA               14 0.0284   7   7     0.500\r\n\r\n\r\nSCP: Single Country Publications\r\n\r\nMCP: Multiple Country Publications\r\n\r\n\r\nTotal Citations per Country\r\n\r\n     Country      Total Citations Average Article Citations\r\n1  NETHERLANDS                853                    42.650\r\n2  UNITED KINGDOM             679                    26.115\r\n3  AUSTRALIA                  516                    15.176\r\n4  CHINA                      498                     9.577\r\n5  USA                        319                     6.646\r\n6  JAPAN                      243                     6.750\r\n7  SPAIN                      207                     7.393\r\n8  NORWAY                     174                     9.158\r\n9  AUSTRIA                    147                    18.375\r\n10 GERMANY                    109                     4.739\r\n\r\n\r\nMost Relevant Sources\r\n\r\n                                                                         Sources        Articles\r\n1  SEVENTH INTERNATIONAL LEARNING ANALYTICS & KNOWLEDGE CONFERENCE (LAK'17)                   18\r\n2  INTERACTIVE LEARNING ENVIRONMENTS                                                          16\r\n3  EDUCATIONAL TECHNOLOGY & SOCIETY                                                           15\r\n4  9TH INTERNATIONAL CONFERENCE ON EDUCATION AND NEW LEARNING TECHNOLOGIES (EDULEARN17)       12\r\n5  JOURNAL OF LEARNING ANALYTICS                                                              11\r\n6  IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES                                                  9\r\n7  EDULEARN18: 10TH INTERNATIONAL CONFERENCE ON EDUCATION AND NEW LEARNING TECHNOLOGIES        8\r\n8  ETR&D-EDUCATIONAL TECHNOLOGY RESEARCH AND DEVELOPMENT                                       8\r\n9  IEEE 21ST INTERNATIONAL CONFERENCE ON ADVANCED LEARNING TECHNOLOGIES (ICALT 2021)           8\r\n10 JOURNAL OF COMPUTER ASSISTED LEARNING                                                       8\r\n\r\n\r\nMost Relevant Keywords\r\n\r\n   Author Keywords (DE)      Articles Keywords-Plus (ID)     Articles\r\n1    LEARNING ANALYTICS           382            PERFORMANCE       41\r\n2    HIGHER EDUCATION              52            MODEL             31\r\n3    EDUCATIONAL DATA MINING       34            DESIGN            28\r\n4    LEARNING DESIGN               33            FRAMEWORK         25\r\n5    E-LEARNING                    28            EDUCATION         24\r\n6    SELF-REGULATED LEARNING       28            ONLINE            22\r\n7    BLENDED LEARNING              25            STUDENTS          21\r\n8    MACHINE LEARNING              24            ANALYTICS         17\r\n9    ONLINE LEARNING               23            IMPACT            16\r\n10   EDUCATION                     17            ACHIEVEMENT       14\r\n\r\nWe can check for most cited references with the code below.\r\n\r\n\r\nShow code\r\n\r\nCR <- citations(M, field = \"article\", sep = \";\")\r\ncbind(CR$Cited[1:20])\r\n\r\n                                                                                                    [,1]\r\nLONG PHIL, 2011, EDUCAUSE REVIEW, V46, P31                                                            58\r\nGASEVIC D, 2015, TECHTRENDS, V59, P64, DOI 10.1007/S11528-014-0822-X                                  57\r\nGRELLER W, 2012, EDUC TECHNOL SOC, V15, P42                                                           56\r\nFERGUSON R, 2012, INT J TECHNOL ENHANC, V4, P304, DOI 10.1504/IJTEL.2012.051816                       54\r\nSIEMENS G, 2013, AM BEHAV SCI, V57, P1380, DOI 10.1177/0002764213498851                               42\r\nARNOLD K.E., 2012, P 2 INT C LEARN AN K, DOI DOI 10.1145/2330601.2330666, 10.1145/2330601.2330666     40\r\nGASEVIC D, 2016, INTERNET HIGH EDUC, V28, P68, DOI 10.1016/J.IHEDUC.2015.10.002                       40\r\nCHATTI MA, 2012, INT J TECHNOL ENHANC, V4, P318, DOI 10.1504/IJTEL.2012.051815                        39\r\nLOCKYER L, 2013, AM BEHAV SCI, V57, P1439, DOI 10.1177/0002764213479367                               39\r\nTEMPELAAR DT, 2015, COMPUT HUM BEHAV, V47, P157, DOI 10.1016/J.CHB.2014.05.038                        37\r\nCLOW D., 2012, P 2 INT C LEARNING A, P134, DOI 10.1145/2330601.2330636, DOI 10.1145/2330601.2330636   31\r\nSLADE S, 2013, AM BEHAV SCI, V57, P1510, DOI 10.1177/0002764213479366                                 29\r\nPAPAMITSIOU Z, 2014, EDUC TECHNOL SOC, V17, P49                                                       28\r\nSCHWENDIMANN BA, 2017, IEEE T LEARN TECHNOL, V10, P30, DOI 10.1109/TLT.2016.2599522                   27\r\nSIEMENS G, 2012, P 2 INT C LEARNING A, DOI 10.1145/2330601.2330661, DOI 10.1145/2330601.2330661       27\r\nVIBERG O, 2018, COMPUT HUM BEHAV, V89, P98, DOI 10.1016/J.CHB.2018.07.027                             27\r\nMACFADYEN LP, 2010, COMPUT EDUC, V54, P588, DOI 10.1016/J.COMPEDU.2009.09.008                         26\r\nAGUDO-PEREGRINA AF, 2014, COMPUT HUM BEHAV, V31, P542, DOI 10.1016/J.CHB.2013.05.031                  24\r\nSHUM SB, 2012, EDUC TECHNOL SOC, V15, P3                                                              24\r\nWISE A. F., 2014, P 4 INT C LEARNING A, P203                                                          23\r\n\r\nWe can check for authors’ dominance ranking to see which author published the most, and the way that they work (single-authored, multi-authored).\r\n\r\n\r\nShow code\r\n\r\n#Authors’ Dominance ranking\r\nDF <- dominance(results, k = 10)\r\nDF\r\n\r\n                Author Dominance Factor Tot Articles Single-Authored\r\n1             KHALIL M       0.66666667            6               0\r\n2              TSAI YS       0.57142857            7               0\r\n3         IFENTHALER D       0.42857143            7               0\r\n4             NGUYEN Q       0.30000000           10               0\r\n5           RIENTIES B       0.16666667           12               0\r\n6             DAWSON S       0.16666667            6               0\r\n7              EBNER M       0.16666667            6               0\r\n8  RODRIGUEZ-TRIANA MJ       0.16666667            6               0\r\n9             YAMADA M       0.09090909           11               0\r\n10           GASEVIC D       0.06250000           16               0\r\n   Multi-Authored First-Authored Rank by Articles Rank by DF\r\n1               6              4                7          1\r\n2               7              4                5          2\r\n3               7              3                5          3\r\n4              10              3                4          4\r\n5              12              2                2          5\r\n6               6              1                7          5\r\n7               6              1                7          5\r\n8               6              1                7          5\r\n9              11              1                3          9\r\n10             16              1                1         10\r\n\r\nFinally, we can plot the output to make it understandable at a glance with the regular plot function.\r\n\r\n\r\nShow code\r\n\r\nplot(x=results, k=10, pause=FALSE)\r\n\r\n\r\n\r\nExamining relationships between authors with co-citation Analysis\r\nWe can check how the papers in our data set cited each other with co-citation analysis. Results will be displayed as co-citation network below. We can see that there are two main clusters, indicating that papers within a cluster cited each other the most.\r\n\r\n\r\nShow code\r\n\r\nNetMatrix <- biblioNetwork(M, analysis = \"co-citation\", network = \"references\", sep = \";\")\r\n\r\nnet=networkPlot(NetMatrix, weighted=NULL, n = 50, \r\n                Title = \"Co-Citation Network\", type = \"fruchterman\", \r\n                size=4, size.cex=TRUE, remove.multiple=FALSE, labelsize=1, label.n=10, label.cex=F, edgesize = 10)\r\n\r\n\r\n\r\nWe can also perform co-citation analysis at the source level (i.e., journals and conferences) to examine how sources in our data set cited each other.\r\n\r\n\r\nShow code\r\n\r\nSource=metaTagExtraction(M,\"CR_SO\",sep=\";\")\r\n\r\nNetMatrix <- biblioNetwork(Source, analysis = \"co-citation\", network = \"sources\", sep = \";\")\r\n\r\nnet=networkPlot(NetMatrix, n = 50, Title = \"Co-Citation Network-Journal\", type = \"auto\", size.cex=TRUE, size=3, remove.multiple=FALSE, labelsize=1,edgesize = 10, edges.min=5)\r\n\r\n\r\n\r\nWe can also examine top 5 keywords used by publication authors in our data set with the code below. We can see popular topics based on keyword with this result.\r\n\r\n\r\nShow code\r\n\r\n#Author keyword network\r\n\r\nA <- cocMatrix(M, Field = \"DE\", sep = \";\")\r\nsort(Matrix::colSums(A), decreasing = TRUE)[1:5]\r\n\r\n     LEARNING ANALYTICS        HIGHER EDUCATION \r\n                    382                      52 \r\nEDUCATIONAL DATA MINING         LEARNING DESIGN \r\n                     33                      33 \r\n             E-LEARNING \r\n                     28 \r\n\r\nWe can also examine author coupling cluster to see whose work is related to whose. Once we know the author, we can refer back to the list of our bibliographic records to examine their work.\r\n\r\n\r\nShow code\r\n\r\nres <- couplingMap(M, analysis = \"authors\", field = \"CR\", n = 250, impact.measure=\"local\",\r\nminfreq = 3, size = 0.5, repel = TRUE)\r\n\r\nplot(res$map)\r\n\r\n\r\n\r\nFinally, we can use a three fields plot to see the relationship between authors (AU), keywords that they used (DE), and journals/conferences that they submitted (SO).\r\n\r\n\r\nShow code\r\n\r\nthreeFieldsPlot(M, fields = c(\"AU\", \"DE\", \"SO\"), n = c(20, 20, 20))\r\n\r\n\r\n\r\nExamining conceptual structure with co-word analysis\r\nCo-word analysis is a useful tool to examine conceptual structure of the field. That is, we can see relationship between topics within the field of interest. The keyword co-occurrence network plot below shows the how frequent each keyword is presented in our bibliographic data from the size of its circle. The bigger they are, the more frequent they are used. Position of keywords on the plot also indicates which keywords are usually presented together.\r\n\r\n\r\nShow code\r\n\r\n#Co-occurrences network\r\n\r\n# keywords\r\nNetMatrix <- biblioNetwork(M, analysis = \"co-occurrences\", network = \"keywords\", sep = \";\")\r\n\r\n# Plot the network\r\nnet = networkPlot(NetMatrix, normalize=\"association\", weighted=T, n = 30, \r\n                  Title = \"Keyword Co-occurrences\", type = \"fruchterman\", \r\n                  size=TRUE, edgesize = 5, labelsize=0.7, remove.multiple=FALSE, label.cex=TRUE)\r\n\r\n\r\n\r\nWe can also perform correspondence analysis to see relationship between topics with multiple correspondence analysis (MCA) or hierarchical cluster plot as well. The factorial maps also show papers with the highest contribution within the keyword clusters in the output.\r\n\r\n\r\nShow code\r\n\r\n# Conceptual Structure using keywords (method=\"MCA\")\r\nCS <- conceptualStructure(M, field=\"ID\", method=\"MCA\", minDegree=4, clust=5, stemming=FALSE, labelsize=15, documents=20)\r\n\r\n\r\n\r\nExamining keyword usage across time with historiograph\r\nMost analyses we performed earlier did not use time point as a variable. We can plot a historiograph to see the evolution of keyword usage across time, as well as their relative popularity to each other. The plot below indicates that “learning analytics” is the post popular as it gained more interest since 2007 based on our data set.\r\n\r\n\r\nShow code\r\n\r\nlibrary(reshape2)\r\nlibrary(ggplot2)\r\n\r\nkword <- KeywordGrowth(M, Tag = \"DE\", sep = \";\", top = 15, cdf = TRUE)\r\n\r\nDF = melt(kword, id='Year')\r\n\r\n# Timeline keywords ggplot\r\nggplot(DF,aes(x=Year,y=value, group=variable, shape=variable, colour=variable))+\r\n  geom_point()+geom_line()+ \r\n  scale_shape_manual(values = 1:15)+\r\n  labs(color=\"Author Keywords\")+\r\n  scale_x_continuous(breaks = seq(min(DF$Year), max(DF$Year), by = 5))+\r\n  scale_y_continuous(breaks = seq(0, max(DF$value), by=10))+\r\n  guides(color=guide_legend(title = \"Author Keywords\"), shape=FALSE)+\r\n  labs(y=\"Count\", variable=\"Author Keywords\", title = \"Author's Keywords Usage Evolution Over Time\")+\r\n  theme(text = element_text(size = 10))+\r\n  facet_grid(variable ~ .)\r\n\r\n\r\n\r\nExamining trends of the field with thematic map\r\nAside from examining frequency of keyword usage, we can use thematic map to examine trend of the topics within the field. The map below places topics within four panes (Cobo et al., 2018):\r\nUpper right pane indicates motor theme that is important to the field as it is constantly used, hence the name motor;\r\nUpper left pane indicates niche theme that is very specialized to certain groups of research;\r\nLower left pane indicates emerging/declining theme that is still weakly developed, meaning that it could still growing or starting to disappear;\r\nLower right pane indicates basic theme that is important to the field but not well developed as motor theme, meaning that topics in this pane are usually studied for general knowledge of the field.\r\n\r\n\r\n\r\nShow code\r\n\r\nMap=thematicMap(M, field = \"ID\", n = 250, minfreq = 4,\r\n  stemming = FALSE, size = 0.7, n.labels=5, repel = TRUE)\r\nplot(Map$map)\r\n\r\n\r\n\r\nExamining collaboration with social structure analysis\r\nWe can examine collaboration pattern between authors from our data set with collaboration analysis. The author collaboration network below shows who collaborated with whom at the author level.\r\n\r\n\r\nShow code\r\n\r\nNetMatrix <- biblioNetwork(M, analysis = \"collaboration\",  network = \"authors\", sep = \";\")\r\n\r\nnet=networkPlot(NetMatrix,  n = 50, Title = \"Author collaboration\",type = \"auto\", size=5,size.cex=T,edgesize = 5,labelsize=1, community.repulsion = 0.1)\r\n\r\n\r\n\r\nWe can also plot an institution collaboration network to examine collaboration pattern between institutions as well.\r\n\r\n\r\nShow code\r\n\r\nNetMatrix <- biblioNetwork(M, analysis = \"collaboration\",  network = \"universities\", sep = \";\")\r\nnet=networkPlot(NetMatrix,  n = 50, Title = \"Institution collaboration\",type = \"auto\", size=4,size.cex=F,edgesize = 3,labelsize=1, community.repulsion = 0.05)\r\n\r\n\r\n\r\nLastly, we can examine collaboration pattern at international level with country collaboration network. This analysis shows countries that are productive within the field (indicated by size of the circle), as well as countries that they collaborate with as indicated by linkages in the plot.\r\n\r\n\r\nShow code\r\n\r\ncountry <- metaTagExtraction(M, Field = \"AU_CO\", sep = \";\")\r\nNetMatrix <- biblioNetwork(country, analysis = \"collaboration\",  network = \"countries\", sep = \";\")\r\n\r\nnet=networkPlot(NetMatrix,  n = dim(NetMatrix)[1], Title = \"Country collaboration\",type = \"circle\", size=10,size.cex=T,edgesize = 1,labelsize=0.6, cluster=\"none\")\r\n\r\n\r\n\r\nConcluding remarks\r\nWhat I like about bibliometric analysis is that it examines the field at the meta level, meaning that it is a research that is built on other research. Without the work of other researchers out there, this analysis would be impossible.\r\nThis analysis method is a useful tool to gain a bird-eye view on state of the field that we are interested in. Results can be used inform early career researchers or students in their topic selection. Basically, it could be helpful for us to know which topic is popular (and therefore has a lot of papers we could read) or which topic is declining.\r\nBy identifying authors that publish a lot in the field, we can decide whether we want to follow their work to keep up with research trend, or deviate our work off their path so that we can study something new to the field. Anyway, thank you so much for reading this! I hope you like it :)\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-03-20-bibanalysis/threefields.png",
    "last_modified": "2023-03-23T09:50:00-06:00",
    "input_file": "bibanalysis.knit.md",
    "preview_width": 2444,
    "preview_height": 1474
  },
  {
    "path": "posts/2022-12-31-ggstat/",
    "title": "Data Exploration with ggstatsplot",
    "description": "In this post, I will be performing and visualizing data exploration techniques such as Pearson's correlation test, Chi-square Goodness of Fit test, Chi-square Test of Independence, One-sample t-test, and Paired-sample t-test.\n\n(8 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2022-12-31",
    "categories": [
      "R",
      "Statistics"
    ],
    "contents": "\r\n\r\nContents\r\nIt’s been a while, eh?\r\nData Preprocessing and Assumption Check\r\nScatter plot - Pearson’s correlation test\r\nPie Chart - Chi-Square test\r\nHistogram - One Sample t-test\r\nViolin plot - Dependent Sample t-test\r\n\r\nConclusion\r\n\r\nIt’s been a while, eh?\r\nHi everyone. It has been a while since my last post. I have been busy with finishing my conference submissions and manuscripts. While working, one of my professors asked me and my colleague to perform a t-test for one of our projects. It was that time that I was introduced to a very helpful package called ggstatsplot. I usually do t-test with the t.test function from the base stats package. It still works, but I found that ggstatsplot is more user-friendly as it provides visual of the results instead of just texts and numbers. Plus, I like graphing. It is fun and beautiful.\r\nThe mentioned reason is why I decided to write this post as a way to introduce how useful ggstatsplot is, as well as brushing up the basic knowledge of data exploration techniques that maybe simple but foundational to a lot of existing quantitative analysis techniques out there. It is almost a mandatory for every researchers to learn about these techniques in their introduction to research course. That is why I hope this post would serve as a useful reference for you all. I also want to keep things simple as a change to discussing about advance techniques like genetic algorithm and machine learning.\r\nIn this post, I will be performing and visualizing data exploration techniques such as Pearson’s correlation test, Chi-square Goodness of Fit test, Chi-square Test of Independence, One-sample t-test, and Paired-sample t-test.\r\nAbout the data, we will be using the student alcohol consumption data set, which is a survey data of students’ mathematics score from Portuguese secondary schools in 2005-2006 school year. The data is available in the University of California-Irvine machine learning repository and Kaggle. The data was collected and used by Cortez and Silva (2008).\r\nData Preprocessing and Assumption Check\r\nThere are four packages that I used, ggplot2 for base data visualization, dplyr for data manipulation, DataExplorer for data summarization, psych for descriptive statistics, rstatix for preliminary analysis, and ggstatsplot, the main package that we will play around today, for data visualization with statistical details.\r\n\r\n\r\nShow code\r\n\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\nlibrary(DataExplorer)\r\nlibrary(psych)\r\nlibrary(rstatix)\r\nlibrary(tidyr)\r\nlibrary(ggstatsplot)\r\n\r\n\r\nFirst, we will read the data set with read.csv, convert the categorical variables into factors with as.factor, and subset relevant variables out with select. It makes the data much easier to work with when we filter the variables that we want into a smaller data set.\r\n\r\n\r\nShow code\r\n\r\ndf <- read.csv(\"student-mat.csv\", header = TRUE)\r\n\r\ndf <- df %>% \r\n  as.data.frame() %>%\r\n  mutate(across(c(sex, address, schoolsup, famsup), as.factor))\r\n\r\n#drop variable\r\n\r\ndf_clean <- df %>% select (c(sex, address, schoolsup, famsup, G1, G3))\r\n\r\n\r\nNext, we will explore variable type in the data set and missing value with plot_intro and proportion of categorical variables with plot_bar. Both packages are from the DataExplorer package.\r\n\r\n\r\nShow code\r\n\r\nplot_intro(df_clean)\r\n\r\n\r\n\r\nThe introduction bar plot shows no missing data. Majority of the variables are discrete, which are sex - students’ biological sex, address - student’s home address type (binary: ‘U’ - urban or ‘R’ - rural), schoolsup - whether that student enrolls in extra educational support, and famsup - whether that student gets educational support from their family. The remaining variables are continuous; those variables are G1 - students’ score in the first data collection period, and G3 - students’ final score.\r\n\r\n\r\nShow code\r\n\r\nplot_bar(df_clean)\r\n\r\n\r\n\r\nThe bar plots above show that the proportion between male and female students is rather balance. The proportion between students who receive educational support from family is skewed, and the proportion between students who receive extra educational support from school and students’ home address type are severely skewed.\r\nNext, we will perform preliminary analysis to check if our data follows assumptions of the test we will perform with the ggstatsplot package. We will use descriptive statistics with the describe function to examine characteristics of our variable of interest, and identify_outliers to check for extreme outliers.\r\n\r\n\r\nShow code\r\n\r\ndescribe(df_clean)\r\n\r\n           vars   n  mean   sd median trimmed  mad min max range\r\nsex*          1 395  1.47 0.50      1    1.47 0.00   1   2     1\r\naddress*      2 395  1.78 0.42      2    1.85 0.00   1   2     1\r\nschoolsup*    3 395  1.13 0.34      1    1.04 0.00   1   2     1\r\nfamsup*       4 395  1.61 0.49      2    1.64 0.00   1   2     1\r\nG1            5 395 10.91 3.32     11   10.80 4.45   3  19    16\r\nG3            6 395 10.42 4.58     11   10.84 4.45   0  20    20\r\n            skew kurtosis   se\r\nsex*        0.11    -1.99 0.03\r\naddress*   -1.33    -0.24 0.02\r\nschoolsup*  2.20     2.86 0.02\r\nfamsup*    -0.46    -1.79 0.02\r\nG1          0.24    -0.71 0.17\r\nG3         -0.73     0.37 0.23\r\n\r\nThe descriptive results shows that most variables except schoolsup have normal distribution, with skewness and kurtosis values within ± 2 (Lomax & Hahs-Vaughn, 2020). We can visually confirm normality with Quantile-quantile plot.\r\n\r\n\r\nShow code\r\n\r\nset.seed(123)\r\nplot_qq(df_clean)\r\n\r\n\r\n\r\nThe Q-Q plots show no departure from normality in our continuous variables (i.e., test scores from the first period and final score). Next, we will examine the presence of outliers.\r\n\r\n\r\nShow code\r\n\r\ndf_clean %>% \r\n  group_by(sex) %>%\r\n  identify_outliers(G1)\r\n\r\n[1] sex        address    schoolsup  famsup     G1         G3        \r\n[7] is.outlier is.extreme\r\n<0 rows> (or 0-length row.names)\r\n\r\nNo outlier detected. We are good to perform Pearson’s correlation test, Chi-square test, One-sample t-test, and Dependent-sample t-test with ggstatplot.\r\nScatter plot - Pearson’s correlation test\r\nThe first function I really like is ggscatterstat. It provides a scatter plot with statistical details to show association between two continuous variables. The plot can be used to check whether the association between two contuinuous variables is linear, as well as their distribution.\r\n\r\n\r\nShow code\r\n\r\nset.seed(123)\r\n\r\nggstatsplot::ggscatterstats(\r\n  data = df_clean, \r\n  x = G1, \r\n  y = G3, \r\n  type = \"parametric\",            # type of test that needs to be run\r\n  conf.level = 0.95,\r\n  \r\n  title = \"Correlation Test\",\r\n  xlab = \"First period grade\",       # label for x axis\r\n  ylab = \"Final grade\",              # label for y axis \r\n  line.color = \"blue\", \r\n  messages = FALSE,\r\n  \r\n  label.var = famsup,\r\n  label.expression = G3 >= 19)+\r\n  \r\n  ggplot2::theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\nThe scatter plot shows a strong positive correlation between first period scores and final scores among 395 pairs of students (p < 0.001). The meaning is that students’ score tend to go the same way; that is, if a student scores well early in their course, it is likely that their final score will be high as well. The distribution of both scores also seems normal as shown by the bar plot.\r\nWe can also flag cases with extreme value with label.expression and label.var as well for further investigation. In our case, we flag student whose final score equal to or greater than 19 and ask the function to identify whether that student gets educational support from their family.\r\nPie Chart - Chi-Square test\r\nThe second function, ggpiestats, can be used to examine “Goodness of fit” to see whether the proportions of our variable matches with what we hypothesized. We can first specify the proportion that we think our variable will have using ratio argument. Here, I hypothesized that the proportion between students who receive- and did not receive educational support from family are 50/50; therefore ,the value in ratio argument is c(0.5, 0.5).\r\n\r\n\r\nShow code\r\n\r\nset.seed(123)\r\n\r\nggstatsplot::ggpiestats(\r\n  data = df_clean,\r\n  x = famsup,\r\n  type = \"parametric\",\r\n  ratio = c(0.50,0.50),\r\n  messages = FALSE,\r\n  paired = FALSE, #Logical indicating whether data came from a within-subjects or repeated measures design study\r\n  conf.level = 0.95,\r\n  title = \"Chi-Square Goodness of Fit Test\"\r\n)+\r\n  ggplot2::theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\nThe pie chart shows that the proportion between students who receive- and did not receive educational support from family differ from what I hypothesized (p < 0.001). In fact, the pie chart shows that 61% of students from our sample receive educational support from family and 39% of students did not receive educational support from family.\r\nThe Pearson’s C value indicates effect size, which is magnitude of the result reported by the plot. In our case, the effect size is 0.22, indicating a small magnitude of difference between our hypothesized proportion and the actual proportion of our variable of interest. In plain language, it means that our hypothesized proportion (i.e., 50/50) is somewhat close to the actual proportion (i.e., 39/61).\r\nWe can also add another variable in the function to request for a Chi-Square test of independence that examines whether two variables are related. We will try examining if whether that student enrolls in extra educational support (schoolsup) is associated with whether that student gets educational support from their family (famsup).\r\n\r\n\r\nShow code\r\n\r\nset.seed(123)\r\n\r\nggstatsplot::ggpiestats(\r\n  data = df_clean,\r\n  x = famsup,\r\n  y = schoolsup,\r\n  type = \"parametric\",\r\n  ratio = c(0.50,0.50),\r\n  messages = FALSE,\r\n  paired = FALSE, #Logical indicating whether data came from a within-subjects or repeated measures design study\r\n  conf.level = 0.95,\r\n  title = \"Chi-Square Test of Independence\"\r\n)+\r\n  ggplot2::theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\nThe pie chart shows a small difference between schoolsup and famsup (p < 0.05). The Cramer’s V effect size of 0.09 indicating a small magnitude of difference between the two variables. The interpretation is that there is a weak association between whether a student enrolls in extra educational support and whether that a student gets educational support from their family.\r\nThe plot also shows that the proportion within the two variables differn from 50/50 as indicated by the provided Chi-square goodness of fit results.\r\nHistogram - One Sample t-test\r\nThe third function, gghistostats, can be used to examine distribution of a continuous variable and to test if the mean of a sample variable is different from a specified value (population parameter) with one-sample t-test. In the function, I hypothesized that the population mean for students’ final score (G3) is 10 as specified in test.value argument.\r\n\r\n\r\nShow code\r\n\r\nset.seed(123)\r\n\r\nggstatsplot::gghistostats(\r\n  data = df_clean,\r\n  x = G3,\r\n  title = \"Distribution of Final Grade\",\r\n  centrality.type = \"parametric\",    # one sample t-test\r\n  test.value = 10,                    \r\n  effsize.type = \"d\",         \r\n  xlab = \"Students' Final score\",\r\n  ylab = \"Number of Student\",\r\n  centrality.para = \"mean\",          # which measure of central tendency is to be plotted\r\n  normal.curve = TRUE,\r\n  binwidth = 1,                   # binwidth value (needs to be toyed around with until you find the best one)\r\n  messages = FALSE                   # turn off the messages\r\n)+\r\n  \r\n  ggplot2::theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\nThe histogram shows that the sample mean does not significantly differ from the hypothesized (or population) mean (p > 0.05), meaning that the actual mean in our sample (i.e., 10.42) is close to the mean that we thought students’ population would have (i.e., 10). The histogram also shows that our data is normally distributed as seen from the normality curve. We will not interpret effect size (i.e., Cohen’s d) as there is no statistical significance in the results.\r\nViolin plot - Dependent Sample t-test\r\nThe fourth and final function we will play around is ggwithinstats, that performs a paired-sample t-test or dependent sample t-test to determine whether the mean the two measurement time point is zero. In our case, we are examining students’ mathematics score between their first period and their final score. The function requires a long-format data set, so we will transform the data with pivot_longer from tidyr package.\r\n\r\n\r\nShow code\r\n\r\ndf_long <- df_clean %>% pivot_longer(cols=c('G1', 'G3'),\r\n                        names_to='Measure_point',\r\n                        values_to='Scores')\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# for reproducibility\r\nset.seed(123)\r\n\r\nggstatsplot::ggwithinstats(\r\n  data = df_long, \r\n  x = Measure_point, \r\n  y = Scores,\r\n  type = \"parametric\", # type of statistical test\r\n  xlab = \"Period of testing\",\r\n  ylab = \"Students' score\",\r\n  pairwise.comparisons = TRUE, \r\n  sphericity.correction = FALSE, ## don't display sphericity corrected dfs and p-values\r\n  \r\n  outlier.tagging = TRUE, ## whether outliers should be flagged\r\n  outlier.label = address, ## label to attach to outlier values\r\n  outlier.label.color = \"green\", ## outlier point label color\r\n  \r\n  mean.plotting = TRUE, ## whether the mean is to be displayed\r\n  mean.color = \"darkblue\", ## color for mean\r\n  \r\n  title = \"Comparison of students' score\")+\r\n  \r\n  ggplot2::theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\nResults of the paired sample t-test are displayed along with violin plot that shows both mean and distribution density of both conditions (i.e., G1 and G3). The results show that the mean of students’ score between two time points are different (p < 0.001). The Hedges’s g effect size of 0.18 indicating a small magnitude of difference between the two variables. The interpretation is that there is a small difference between students’ first period and final mathematics scores.\r\nThe dotted lines in the plot show the pair of students between the two time points. Note that there are some students who took the first exam but did not take the final exam as shown from outliers in the final exam scores (i.e., the orange part). There are some students who scored zero in the final exam.\r\nConclusion\r\nBoth descriptive and inferential statistics techniques that we used in this post are mainly for data exploration. Despite their simplicity, they serve as foundations for several research that underlies products such as data driven COVID-19 policy (Li et al., 2020), validation method for machine learning algorithm (Vabalas et al., 2019), and more complex statistical analysis such as structural equation modeling (Zhonglin et al., 2004).\r\nMy point for this post is that simpler techniques should not be overlooked. In some cases, they are enough to answer research questions without relying on complex techniques that are more challenging to perform, yet provide results that are harder to interpret. Basically, if it makes sense for your work to use just a simple t-test, then there is no need to overthink. In one of our articles, we used step-wise regression instead of random forest algorithm because it performs better with a smaller sample size and is easier to interpret. Staying simple when we can is the point that I think we, as researchers, should consider in our work. Thank you very much for reading this. Happy new year 2023!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-12-31-ggstat/ggstat_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2022-12-31T15:14:40-07:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-08-14-ga-aco/",
    "title": "Test Shortening with Genetic Algorithm and Ant Colony Optimization",
    "description": "In this post, I will use Genetic Algorithm and Ant Colony Optimization Algorithm to automatically shorten the length of a test.\n\n (8 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2022-08-14",
    "categories": [
      "R",
      "Psychometric"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nGenetic\r\nAlgorithm\r\nAnt\r\nColony Algorithm\r\nConclusion\r\n\r\nIntroduction\r\nHi, Everyone. It’s been really hot here in Summer. I have some\r\nmore time before my next school year (2023) begins. As I learned more\r\nabout Psychometric, I came to know about techniques that can be used to\r\nreduce the number of questions in a questionnaire for efficiency. This\r\npost was inspired by my\r\nsupervisor’s post of the same technique. He applied the data to the\r\nExperiences in Close Relationships (ECR) scale of Adult Romantic\r\nAttachment Measure (Brennan\r\net al., 1998). I want to try following his footstep and replicate\r\nthe same techniques to a different data set for my practice. In this\r\npost, I will use Genetic Algorithm and Ant Colony Optimization Algorithm\r\nto automatically shorten the length of a test.\r\nWe will be using the International Personality Item Pool data set\r\n(Goldberg\r\net al., 2006). The data set has polytomous test items; that is,\r\nanswers from test takers can be more than two values. Instead of “right”\r\nand “wrong”, the answer can be “strongly disagree”, “disagree”,\r\n“neutral”, “agree”, and “strongly agree”. The test measures person’s\r\ncharacteristics of extraversion, emotional stability, agreeableness,\r\nconscientiousness, and openness to experience.\r\nFirst, we will import the data set and subset a portion of the\r\ndata for feasibility. The full data set has 176,380 cases. We will\r\nsubset only 5000 of them.\r\n\r\n\r\nShow code\r\n\r\nsetwd('D:/Class_and_Work/U_Alberta/Research projects/Response time')\r\nlibrary(tidyverse)\r\nlibrary(data.table)\r\nRANDOM_STATE = 123\r\n\r\ndf <- fread(\"ipip.csv\", header = TRUE, fill = TRUE)\r\n\r\n\r\n\r\nThe data set contains responses of individuals to 50 personality\r\ntest questions. Each response is on ordinal scale, meaning that the\r\nresponse can be categorized and ranked (i.e., disagree - neutral -\r\nagree). The correlation plot below indicates relationships between items\r\nmeasuring the same dimension (e.g., extraversion). Items with negative\r\ncorrelation are reverse-coded, meaning that the items are rephrased to\r\nhave an opposite meaning. For example, a question measuring extraversion\r\nmay ask “I see myself as someone who is talkative”. The reverse-coded\r\nversion maybe “I see myself as someone who tends to be quiet”.\r\n\r\n\r\nShow code\r\n\r\ndf_survey <- df[1:5000, 1:50]\r\n\r\nhead(df_survey)\r\n\r\n\r\n   EXT1 EXT2 EXT3 EXT4 EXT5 EXT6 EXT7 EXT8 EXT9 EXT10 EST1 EST2 EST3\r\n1:    4    1    5    2    5    1    5    2    4     1    1    4    4\r\n2:    3    5    3    4    3    3    2    5    1     5    2    3    4\r\n3:    2    3    4    4    3    2    1    3    2     5    4    4    4\r\n4:    2    2    2    3    4    2    2    4    1     4    3    3    3\r\n5:    3    3    3    3    5    3    3    5    3     4    1    5    5\r\n6:    3    3    4    2    4    2    2    3    3     4    3    4    3\r\n   EST4 EST5 EST6 EST7 EST8 EST9 EST10 AGR1 AGR2 AGR3 AGR4 AGR5 AGR6\r\n1:    2    2    2    2    2    3     2    2    5    2    4    2    3\r\n2:    1    3    1    2    1    3     1    1    4    1    5    1    5\r\n3:    2    2    2    2    2    1     3    1    4    1    4    2    4\r\n4:    2    3    2    2    2    4     3    2    4    3    4    2    4\r\n5:    3    1    1    1    1    3     2    1    5    1    5    1    3\r\n6:    2    2    1    2    1    2     2    2    3    1    4    2    3\r\n   AGR7 AGR8 AGR9 AGR10 CSN1 CSN2 CSN3 CSN4 CSN5 CSN6 CSN7 CSN8 CSN9\r\n1:    2    4    3     4    3    4    3    2    2    4    4    2    4\r\n2:    3    4    5     3    3    2    5    3    3    1    3    3    5\r\n3:    1    4    4     3    4    2    2    2    3    3    4    2    4\r\n4:    2    4    3     4    2    4    4    4    1    2    2    3    1\r\n5:    1    5    5     3    5    1    5    1    3    1    5    1    5\r\n6:    2    3    4     4    3    2    4    1    3    2    4    3    4\r\n   CSN10 OPN1 OPN2 OPN3 OPN4 OPN5 OPN6 OPN7 OPN8 OPN9 OPN10\r\n1:     4    5    1    4    1    4    1    5    3    4     5\r\n2:     3    1    2    4    2    3    1    4    2    5     3\r\n3:     2    5    1    2    1    4    2    5    3    4     4\r\n4:     4    4    2    5    2    3    1    4    4    3     3\r\n5:     5    5    1    5    1    5    1    5    3    5     5\r\n6:     3    5    1    5    1    3    1    5    4    5     2\r\n\r\nShow code\r\n\r\nDataExplorer::plot_correlation(df_survey)\r\n\r\n\r\n\r\n\r\nThe test has 10 items (or questions) in each domain, totalling 50\r\ntest items. This number seems reasonable, but we could further reduce\r\nthe number of items to avoid fatiguing our examinee. One way we can\r\nreduce the number is through the use of coefficient alpha reliability.\r\nThe alpha function in psych package can be\r\nused to examine how much reliability the test will have after dropping\r\nsome items.\r\n\r\n\r\nShow code\r\n\r\nalpha <- psych::alpha(df_survey, check.keys=TRUE)\r\nalpha$alpha.drop\r\n\r\n\r\n       raw_alpha std.alpha   G6(smc) average_r      S/N    alpha se\r\nEXT1-  0.8696207 0.8679394 0.9137837 0.1182655 6.572282 0.002611411\r\nEXT2   0.8691535 0.8675726 0.9134115 0.1179326 6.551309 0.002621739\r\nEXT3-  0.8668282 0.8651111 0.9121684 0.1157390 6.413506 0.002671484\r\nEXT4   0.8683708 0.8667878 0.9129267 0.1172256 6.506819 0.002638125\r\nEXT5-  0.8673617 0.8656936 0.9123513 0.1162518 6.445660 0.002659426\r\nEXT6   0.8685602 0.8667696 0.9132939 0.1172093 6.505797 0.002632815\r\nEXT7-  0.8682855 0.8668348 0.9129689 0.1172677 6.509470 0.002640926\r\nEXT8   0.8712934 0.8695949 0.9150056 0.1197881 6.668412 0.002577510\r\nEXT9-  0.8704641 0.8687061 0.9143138 0.1189665 6.616502 0.002593862\r\nEXT10  0.8683898 0.8669061 0.9133637 0.1173317 6.513491 0.002638721\r\nEST1   0.8703855 0.8687731 0.9141121 0.1190281 6.620390 0.002594302\r\nEST2-  0.8715179 0.8698555 0.9152856 0.1200308 6.683770 0.002572223\r\nEST3   0.8713890 0.8698611 0.9149559 0.1200360 6.684096 0.002575067\r\nEST4-  0.8722369 0.8705305 0.9161060 0.1206634 6.723826 0.002558568\r\nEST5   0.8712487 0.8695326 0.9155456 0.1197302 6.664750 0.002576908\r\nEST6   0.8697921 0.8682577 0.9140230 0.1185557 6.590577 0.002606440\r\nEST7   0.8698114 0.8682822 0.9134175 0.1185780 6.591988 0.002605601\r\nEST8   0.8695089 0.8680298 0.9130377 0.1183477 6.577465 0.002612008\r\nEST9   0.8692737 0.8677100 0.9136737 0.1180571 6.559153 0.002617047\r\nEST10  0.8680675 0.8667208 0.9130501 0.1171656 6.503046 0.002644521\r\nAGR1   0.8729617 0.8708241 0.9162004 0.1209403 6.741382 0.002540781\r\nAGR2-  0.8696206 0.8674465 0.9136713 0.1178184 6.544122 0.002609287\r\nAGR3   0.8720020 0.8701186 0.9156558 0.1202767 6.699333 0.002559864\r\nAGR4-  0.8718810 0.8699519 0.9145632 0.1201208 6.689461 0.002561684\r\nAGR5   0.8713817 0.8694433 0.9147959 0.1196472 6.659506 0.002572622\r\nAGR6-  0.8740795 0.8722052 0.9166081 0.1222577 6.825042 0.002520054\r\nAGR7   0.8695310 0.8674391 0.9132795 0.1178118 6.543703 0.002610839\r\nAGR8-  0.8712084 0.8692263 0.9151725 0.1194462 6.646796 0.002576807\r\nAGR9-  0.8719947 0.8700279 0.9149316 0.1201919 6.693960 0.002559508\r\nAGR10- 0.8692295 0.8669770 0.9138922 0.1173954 6.517496 0.002617962\r\nCSN1-  0.8715010 0.8696873 0.9151554 0.1198740 6.673849 0.002569307\r\nCSN2   0.8743915 0.8722266 0.9165440 0.1222783 6.826354 0.002512017\r\nCSN3-  0.8729250 0.8712122 0.9165431 0.1213081 6.764712 0.002542491\r\nCSN4   0.8695425 0.8679183 0.9139521 0.1182463 6.571073 0.002609660\r\nCSN5-  0.8713024 0.8695137 0.9151010 0.1197126 6.663639 0.002573481\r\nCSN6   0.8723390 0.8703796 0.9153758 0.1205215 6.714834 0.002551467\r\nCSN7-  0.8739940 0.8724664 0.9170480 0.1225097 6.841070 0.002522233\r\nCSN8   0.8704663 0.8686382 0.9148756 0.1189042 6.612566 0.002591175\r\nCSN9-  0.8724726 0.8705899 0.9156435 0.1207194 6.727375 0.002550163\r\nCSN10- 0.8718644 0.8699681 0.9157181 0.1201359 6.690420 0.002563072\r\nOPN1-  0.8731489 0.8713227 0.9152167 0.1214131 6.771378 0.002536994\r\nOPN2   0.8723907 0.8704576 0.9152106 0.1205948 6.719479 0.002551733\r\nOPN3-  0.8742745 0.8725471 0.9161203 0.1225876 6.846034 0.002516526\r\nOPN4   0.8733534 0.8715435 0.9160396 0.1216236 6.784738 0.002533974\r\nOPN5-  0.8710467 0.8687933 0.9141883 0.1190467 6.621561 0.002579241\r\nOPN6   0.8723086 0.8703180 0.9150081 0.1204637 6.711173 0.002553227\r\nOPN7-  0.8718728 0.8699217 0.9156417 0.1200926 6.687679 0.002563265\r\nOPN8-  0.8752372 0.8730645 0.9163104 0.1230898 6.878014 0.002497109\r\nOPN9   0.8760793 0.8755095 0.9191497 0.1255113 7.032742 0.002491151\r\nOPN10- 0.8716998 0.8695968 0.9144305 0.1197899 6.668526 0.002565750\r\n            var.r      med.r\r\nEXT1-  0.02247896 0.08151262\r\nEXT2   0.02242356 0.08166635\r\nEXT3-  0.02239961 0.07907977\r\nEXT4   0.02233050 0.08041782\r\nEXT5-  0.02230091 0.07914325\r\nEXT6   0.02288756 0.07930301\r\nEXT7-  0.02235567 0.08146699\r\nEXT8   0.02281218 0.08201026\r\nEXT9-  0.02271017 0.08153477\r\nEXT10  0.02258192 0.08041782\r\nEST1   0.02257745 0.08146699\r\nEST2-  0.02293955 0.08210536\r\nEST3   0.02261406 0.08151262\r\nEST4-  0.02326988 0.08180150\r\nEST5   0.02327337 0.08157208\r\nEST6   0.02262279 0.08082590\r\nEST7   0.02258286 0.08146699\r\nEST8   0.02248911 0.08132409\r\nEST9   0.02280470 0.08041782\r\nEST10  0.02268410 0.08002237\r\nAGR1   0.02320503 0.08201026\r\nAGR2-  0.02287127 0.08108114\r\nAGR3   0.02323923 0.08151262\r\nAGR4-  0.02249784 0.08225522\r\nAGR5   0.02279661 0.08201026\r\nAGR6-  0.02261039 0.08321513\r\nAGR7   0.02275238 0.08092384\r\nAGR8-  0.02309551 0.08162526\r\nAGR9-  0.02265465 0.08180150\r\nAGR10- 0.02322872 0.07874079\r\nCSN1-  0.02314059 0.08166635\r\nCSN2   0.02285444 0.08249577\r\nCSN3-  0.02337612 0.08195375\r\nCSN4   0.02299208 0.08082590\r\nCSN5-  0.02315868 0.08157208\r\nCSN6   0.02302597 0.08225522\r\nCSN7-  0.02294173 0.08385676\r\nCSN8   0.02335693 0.07998131\r\nCSN9-  0.02305508 0.08195375\r\nCSN10- 0.02344565 0.08162526\r\nOPN1-  0.02302069 0.08295971\r\nOPN2   0.02315918 0.08201026\r\nOPN3-  0.02266237 0.08249577\r\nOPN4   0.02310353 0.08225522\r\nOPN5-  0.02321234 0.08082590\r\nOPN6   0.02315973 0.08201026\r\nOPN7-  0.02343876 0.08146699\r\nOPN8-  0.02269568 0.08336518\r\nOPN9   0.02248538 0.08407870\r\nOPN10- 0.02292938 0.08162526\r\n\r\n\r\n\r\nShow code\r\n\r\nsummary(alpha)\r\n\r\n\r\n\r\nReliability analysis   \r\n raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r\r\n      0.87      0.87    0.92      0.12 6.8 0.0025  2.7 0.44    0.082\r\n\r\nGenetic Algorithm\r\nAnother way we can reduce the number of items is through the use\r\nof Genetic Algorithm, which is an optimization method inspired from the\r\nnatural selection theory (Schroeders\r\net al., 2016). This is how it works:\r\nFirst, the algorithm randomly selects several item sets from the\r\nitem pool. These item sets act as parents.\r\nSecond, the algorithm picks items from each parent to form\r\nseveral sets of items as children (or an offspring).\r\nThird, some items in the children set were exchanged with items\r\nfrom the item pool as a mutation.\r\nFourth, the children were evaluated. Children who produce better\r\nresults were kept while those that did not do well are discarded. The\r\nprocess continues until a certain criterion is met.\r\n\r\nFigure 1 below from visualizes how the process is done.\r\nIllustration of the Genetic Algorithm.\r\nFigure from Schroeders\r\net al. (2016) .No copyright infringement is intendedWe will use the GAabbreviate package to perform test\r\nabbreviation with the Genetic Algorithm (Sahdra\r\net al., 2016). To prepare the data, we will first create summations\r\nof scores in each domain for every examinee as the scale score. We will\r\nalso transform the data frame into a matrix.\r\n\r\n\r\nShow code\r\n\r\nlibrary(GAabbreviate)\r\n\r\nscales = cbind(rowSums(df_survey[, 1:10]), \r\n               rowSums(df_survey[, 11:20]),\r\n               rowSums(df_survey[, 21:30]),\r\n               rowSums(df_survey[, 31:40]),\r\n               rowSums(df_survey[, 41:50]))\r\n\r\ndf_survey <- as.data.frame(sapply(df_survey, as.integer))\r\n\r\ndf_survey <- matrix(as.integer(unlist(df_survey)), nrow=nrow(df_survey))\r\n\r\n\r\n\r\nWe will create the Genetic Algorithm object with the\r\nGAabbreviate function (Sahdra\r\net al, 2016). We will set the cost of each item to 0.001 so that the\r\nalgorithm can produce results that explains the most variance (Yarkoni,\r\n2010).\r\n\r\n\r\nShow code\r\n\r\nipip_ga <- GAabbreviate(items = df_survey, # Matrix of item responses\r\n                      verbose = FALSE,\r\n                      scales = scales, # Scale scores\r\n                      itemCost = 0.001, # The cost of each item\r\n                      maxItems = 5, # Max number of items per dimension\r\n                      maxiter = 1000, # Max number of iterations\r\n                      run = 100, # Number of runs\r\n                      crossVal = TRUE, # Cross-validation\r\n                      seed = RANDOM_STATE) # Seed for reproducibility\r\n\r\n\r\n\r\nWe can request for a summary of the algorithm below. The algorithm\r\nran 301 times before achieving the best results. The number of items in\r\nthe final set is 25, meaning that we reduced the test in half.\r\n\r\n\r\nShow code\r\n\r\nsummary(ipip_ga)\r\n\r\n\r\n── Genetic Algorithm ─────────────────── \r\n\r\nGA settings: \r\nType                  =  binary \r\nPopulation size       =  50 \r\nNumber of generations =  1000 \r\nElitism               =  2 \r\nCrossover probability =  0.8 \r\nMutation probability  =  0.1 \r\n\r\nGA results: \r\nIterations             = 301 \r\nTotal cost             = 1.74582\r\nNumber of items in initial set = 50\r\nNumber of items in final set = 25\r\nMean coefficient alpha = 0.3212\r\nMean convergent correlation (training) = 0.8069\r\nMean convergent correlation (validation) = 0.8029\r\n\r\nWe can call for coefficient alpha reliability of the test dimensions\r\nbelow. Reliability of the emotional stability, agreeableness, and\r\nopenness to experience dimension are satisfactory (0.80 for emotional\r\nstability, 0.73 for agreeableness), meaning that the subtest we created\r\nfor the mentioned two dimensions are usable (Tavakol\r\n& Dennick, 2011). However, items in the extraversion,\r\nconscientiousness, openness to experience dimension should be revised as\r\nindicated by their reliability (-0.21 for extraversion, -0.39 for\r\nconscientiousness, and 0.67 for openness) (Tavakol\r\n& Dennick, 2011).\r\n\r\n\r\nShow code\r\n\r\nipip_ga$measure$alpha\r\n\r\n\r\n              A1        A2        A3         A4        A5\r\nalpha -0.2131873 0.8071732 0.7320936 -0.3923633 0.6721945\r\n\r\nWe can also request for the list of items in each dimension as\r\ndemonstrated below.\r\n\r\n\r\nShow code\r\n\r\next <- which(ipip_ga$measure$key[,1]==1)\r\nest <- which(ipip_ga$measure$key[,2]==1)\r\nagr <- which(ipip_ga$measure$key[,3]==1)\r\ncsn <- which(ipip_ga$measure$key[,4]==1)\r\nopn <- which(ipip_ga$measure$key[,5]==1)\r\n\r\nipip_ga$measure$items[ext]\r\n\r\n\r\nx1 x2 x3 x6 x7 \r\n 1  2  3  6  7 \r\n\r\nShow code\r\n\r\nipip_ga$measure$items[est]\r\n\r\n\r\nx11 x15 x16 x17 x18 \r\n 11  15  16  17  18 \r\n\r\nShow code\r\n\r\nipip_ga$measure$items[agr]\r\n\r\n\r\nx22 x26 x28 x29 x30 \r\n 22  26  28  29  30 \r\n\r\nShow code\r\n\r\nipip_ga$measure$items[csn]\r\n\r\n\r\nx32 x33 x35 x36 x37 \r\n 32  33  35  36  37 \r\n\r\nShow code\r\n\r\nipip_ga$measure$items[opn]\r\n\r\n\r\nx41 x45 x48 x49 x50 \r\n 41  45  48  49  50 \r\n\r\nWe can also request for a summary plot of the Genetic Algorithm\r\nbelow. The plot has total cost, test length, mean of the overall\r\nexplained variance, explained variance in each dimension, and pattern of\r\nitems included in each iteration.\r\n\r\n\r\nShow code\r\n\r\nplot(ipip_ga)\r\n\r\n\r\n\r\n\r\nAnt Colony Algorithm\r\nOne more way we can shorten our test is using the Ant Colony\r\nOptimization (ACO) algorithm. ACO is an optimization method inspired\r\nfrom the foraging behavior of Argentine ants by using virtual ants to\r\nfind the shortest path to a destination, which is the optimal set of\r\ntest items for our case (Doringo et\r\nal., 2006).\r\nSee Figure 2 below for the visual illustration of ACO. There are\r\ntwo routes that lead to the same food source. As ants travel randomly to\r\nthe food source, they all leave pheromone for others to follow\r\nthem.\r\nHowever, given that the upper route is shorter, it receives more\r\npheromone as ants travel back and forth from their nest to the food\r\nsource more often. The route with more pheromone (i.e., the shorter\r\nroute) is chosen more by the ants while the longer route gets chosen\r\nless as the pheromone evaporates from having less ant.\r\nIllustration of the Any Colony\r\nOptimization Algorithm. Figure from Doringo et al. (2006). No copyright\r\ninfringement is intendedWe will use the Shortform package to perform test\r\nabbreviation with ACO (Raborn\r\n& Leite, 2020). We will load the package and subset the data for\r\nfeasibility. ACO is quite computationally expensive with a large data\r\nset. To save time, I subsetted only the first 100 cases of IPIP response\r\ndata.\r\n\r\n\r\nShow code\r\n\r\nlibrary(ShortForm)\r\n\r\n\r\n  #####                             #######                      \r\n #     # #    #  ####  #####  ##### #        ####  #####  #    # \r\n #       #    # #    # #    #   #   #       #    # #    # ##  ## \r\n  #####  ###### #    # #    #   #   #####   #    # #    # # ## # \r\n       # #    # #    # #####    #   #       #    # #####  #    # \r\n #     # #    # #    # #   #    #   #       #    # #   #  #    # \r\n  #####  #    #  ####  #    #   #   #        ####  #    # #    # \r\n \r\n         Version 0.4.6\r\n             (o<\r\n             //\\\r\n             V_/_ \r\n\r\nShow code\r\n\r\ndf_survey_aco <- df[1:100, 1:50] #ACO uses dataframe format\r\n\r\n\r\n\r\nWe will also define model structure of the measurement with\r\nlavaan syntax. Here, I defined five dimensions that are\r\nmeasured by 10 items each. In addition to the measurement model, I will\r\nalso indicate the list of items that measure each dimension.\r\n\r\n\r\nShow code\r\n\r\nmodel <- \" ext =~ EXT1+EXT2+EXT3+EXT4+EXT5+EXT6+EXT7+EXT8+EXT9+EXT10\r\n           est =~ EST1+EST2+EST3+EST4+EST5+EST6+EST7+EST8+EST9+EST10\r\n           agr =~ AGR1+AGR2+AGR3+AGR4+AGR5+AGR6+AGR7+AGR8+AGR9+AGR10\r\n           csn =~ CSN1+CSN2+CSN3+CSN4+CSN5+CSN6+CSN7+CSN8+CSN9+CSN10\r\n           opn =~ OPN1+OPN2+OPN3+OPN4+OPN5+OPN6+OPN7+OPN8+OPN9+OPN10\r\n         \"\r\n\r\nitems <- list(c(paste0(\"EXT\", seq(1, 10))),\r\n              c(paste0(\"EST\", seq(1, 10))),\r\n              c(paste0(\"AGR\", seq(1, 10))),\r\n              c(paste0(\"CSN\", seq(1, 10))),\r\n              c(paste0(\"OPN\", seq(1, 10)))\r\n              )\r\n\r\nitems\r\n\r\n\r\n[[1]]\r\n [1] \"EXT1\"  \"EXT2\"  \"EXT3\"  \"EXT4\"  \"EXT5\"  \"EXT6\"  \"EXT7\"  \"EXT8\" \r\n [9] \"EXT9\"  \"EXT10\"\r\n\r\n[[2]]\r\n [1] \"EST1\"  \"EST2\"  \"EST3\"  \"EST4\"  \"EST5\"  \"EST6\"  \"EST7\"  \"EST8\" \r\n [9] \"EST9\"  \"EST10\"\r\n\r\n[[3]]\r\n [1] \"AGR1\"  \"AGR2\"  \"AGR3\"  \"AGR4\"  \"AGR5\"  \"AGR6\"  \"AGR7\"  \"AGR8\" \r\n [9] \"AGR9\"  \"AGR10\"\r\n\r\n[[4]]\r\n [1] \"CSN1\"  \"CSN2\"  \"CSN3\"  \"CSN4\"  \"CSN5\"  \"CSN6\"  \"CSN7\"  \"CSN8\" \r\n [9] \"CSN9\"  \"CSN10\"\r\n\r\n[[5]]\r\n [1] \"OPN1\"  \"OPN2\"  \"OPN3\"  \"OPN4\"  \"OPN5\"  \"OPN6\"  \"OPN7\"  \"OPN8\" \r\n [9] \"OPN9\"  \"OPN10\"\r\n\r\nWe will create the ACO object with the antcolony.lavaan\r\nfunction. We will set the number of ants to 20, pheromone evaporation\r\nrate to 0.5, and fit indices of comparative fit index (CFI),\r\nTucker-Lewis index (TLI), and root mean square error of approximation\r\n(RMSEA).\r\n\r\n\r\nShow code\r\n\r\nipip_ACO <- antcolony.lavaan(data = df_survey_aco, # Response data set\r\n                            ants = 20, # Number of ants\r\n                            evaporation = 0.5, #  % of the pheromone retained after evaporation\r\n                            antModel = model, # Factor model for IPIP\r\n                            list.items = items, # Items for each dimension\r\n                            full = 50, # The total number of unique items in the IPIP scale\r\n                            i.per.f = c(5, 5, 5, 5, 5), # The desired number of items per dimension. The number has to match the number of dimension\r\n                            factors = c('EXT','EST','AGR','CSN','OPN'), # Names of dimensions\r\n                            \r\n                            # lavaan settings - Change estimator to WLSMV\r\n                            lavaan.model.specs = list(model.type = \"cfa\", auto.var = T, estimator = \"WLSMV\",\r\n                                                      ordered = NULL, int.ov.free = TRUE, int.lv.free = FALSE, \r\n                                                      auto.fix.first = TRUE, auto.fix.single = TRUE, \r\n                                                      auto.cov.lv.x = TRUE, auto.th = TRUE, auto.delta = TRUE,\r\n                                                      auto.cov.y = TRUE, std.lv = F),\r\n                            \r\n                            steps = 50, # The number of ants in a row for which the model does not change\r\n                            fit.indices = c('cfi', 'tli', 'rmsea'), # Fit statistics to use\r\n                            fit.statistics.test = \"(cfi > 0.95)&(tli > 0.95)&(rmsea < 0.06)\",\r\n                            max.run = 300) # The maximum number of ants to run before the algorithm stops\r\n\r\n\r\n\r\n Run number 1.           \r\n Run number 2.           \r\n Run number 3.           \r\n Run number 4.           \r\n Run number 5.           \r\n Run number 6.           \r\n Run number 7.           \r\n Run number 8.           \r\n Run number 9.           \r\n Run number 10.           \r\n Run number 11.           \r\n Run number 12.           \r\n Run number 13.           \r\n Run number 14.           \r\n Run number 15.           \r\n Run number 16.           \r\n Run number 17.           \r\n Run number 18.           \r\n Run number 19.           \r\n Run number 20.           \r\n Run number 21.           [1] \"Compiling results.\"\r\n\r\nAfter getting the result, we can check our fit indices and the list\r\nof items retained by ACO. Item that has “1” indicates that it is\r\nselected by the algorithm.\r\n\r\n\r\nShow code\r\n\r\nipip_ACO[[1]]\r\n\r\n\r\n          cfi       tli      rmsea mean_gamma EXT1 EXT2 EXT3 EXT4\r\n[1,] 0.968031 0.9638086 0.03537734      0.529    1    0    1    0\r\n     EXT5 EXT6 EXT7 EXT8 EXT9 EXT10 EST1 EST2 EST3 EST4 EST5 EST6\r\n[1,]    1    0    1    0    1     0    0    0    1    0    1    1\r\n     EST7 EST8 EST9 EST10 AGR1 AGR2 AGR3 AGR4 AGR5 AGR6 AGR7 AGR8\r\n[1,]    0    1    0     1    0    1    1    1    1    0    0    1\r\n     AGR9 AGR10 CSN1 CSN2 CSN3 CSN4 CSN5 CSN6 CSN7 CSN8 CSN9 CSN10\r\n[1,]    0     0    1    1    0    0    1    0    0    0    1     1\r\n     OPN1 OPN2 OPN3 OPN4 OPN5 OPN6 OPN7 OPN8 OPN9 OPN10\r\n[1,]    1    0    0    0    1    0    0    1    1     1\r\n\r\nWe can also request for the summary of lavaan model from the subtest\r\nextracted by ACO.\r\n\r\n\r\nShow code\r\n\r\nipip_ACO$best.model\r\n\r\n\r\nlavaan 0.6-11 ended normally after 71 iterations\r\n\r\n  Estimator                                       DWLS\r\n  Optimization method                           NLMINB\r\n  Number of model parameters                        60\r\n                                                      \r\n  Number of observations                           100\r\n                                                      \r\nModel Test User Model:\r\n                                              Standard      Robust\r\n  Test Statistic                               283.962     324.326\r\n  Degrees of freedom                               265         265\r\n  P-value (Chi-square)                           0.202       0.007\r\n  Scaling correction factor                                  1.883\r\n  Shift parameter                                          173.490\r\n       simple second-order correction                             \r\n\r\nNext, we can also request for an output of which item is loaded onto\r\nwhich dimension in the new model.\r\n\r\n\r\nShow code\r\n\r\ncat(ipip_ACO$best.syntax)\r\n\r\n\r\nEXT =~ EXT7 + EXT3 + EXT5 + EXT1 + EXT9\r\nEST =~ EST3 + EST10 + EST6 + EST8 + EST5\r\nAGR =~ AGR8 + AGR4 + AGR3 + AGR2 + AGR7\r\nCSN =~ CSN2 + CSN5 + CSN9 + CSN1 + CSN10\r\nOPN =~ OPN5 + OPN8 + OPN1 + OPN9 + OPN10\r\n         \r\n\r\nWhen checking coefficient alpha reliability of the items chosen by\r\nACO, we can see below that the dimension of extraversion, emotional\r\nstability, agreeableness, and conscientiousness have satisfactory\r\nreliability values.\r\n\r\n\r\nShow code\r\n\r\n#check reliability of each subscale\r\n\r\n#EXT\r\npsych::alpha(df_survey_aco[, c(\"EXT1\", \"EXT3\", \"EXT5\", \"EXT4\", \"EXT7\")], check.keys=TRUE)$total\r\n\r\n\r\n raw_alpha std.alpha   G6(smc) average_r      S/N        ase  mean\r\n 0.8854019  0.886472 0.8729437 0.6096311 7.808396 0.01810261 3.038\r\n       sd  median_r\r\n 1.032285 0.5956059\r\n\r\nShow code\r\n\r\n#EST\r\npsych::alpha(df_survey_aco[, c(\"EST1\", \"EST5\", \"EST6\", \"EST7\", \"EST8\")], check.keys=TRUE)$total\r\n\r\n\r\n raw_alpha std.alpha  G6(smc) average_r      S/N        ase  mean\r\n  0.819672 0.8179554 0.816637 0.4733049 4.493158 0.02866376 2.962\r\n        sd  median_r\r\n 0.9705544 0.4724666\r\n\r\nShow code\r\n\r\n#AGR\r\npsych::alpha(df_survey_aco[, c(\"AGR2\", \"AGR4\", \"AGR5\", \"AGR9\", \"AGR10\")], check.keys=TRUE)$total\r\n\r\n\r\n raw_alpha std.alpha G6(smc) average_r      S/N        ase  mean\r\n 0.7890755  0.788927 0.76396 0.4277669 3.737698 0.03327975 3.794\r\n        sd  median_r\r\n 0.7776967 0.4230731\r\n\r\nShow code\r\n\r\n#CSN\r\npsych::alpha(df_survey_aco[, c(\"CSN3\", \"CSN5\", \"CSN7\", \"CSN9\", \"CSN10\")], check.keys=TRUE)$total\r\n\r\n\r\n raw_alpha std.alpha   G6(smc) average_r      S/N        ase  mean\r\n 0.7009179 0.6928163 0.6664406 0.3108564 2.255382 0.04580886 3.284\r\n        sd  median_r\r\n 0.7914595 0.2909734\r\n\r\nShow code\r\n\r\n#OPN\r\npsych::alpha(df_survey_aco[, c(\"OPN5\", \"OPN7\", \"OPN8\", \"OPN9\", \"OPN10\")], check.keys=TRUE)$total\r\n\r\n\r\n raw_alpha std.alpha   G6(smc) average_r      S/N        ase  mean\r\n 0.5971511 0.6095035 0.5874082 0.2379028 1.560843 0.06264639 3.802\r\n        sd  median_r\r\n 0.6006697 0.2483103\r\n\r\nThe explained variance plot below shows that ACO was able to\r\nincrease the amount of variance explained by the model.\r\n\r\n\r\nShow code\r\n\r\nplot(ipip_ACO, type = \"variance\")\r\n\r\n\r\n$Pheromone\r\nNULL\r\n\r\n$Gamma\r\nNULL\r\n\r\n$Beta\r\nNULL\r\n\r\n$Variance\r\n\r\n\r\nWe can also request for the pheromone plot with the argument\r\ntype=\"\"pheromone. This will allow us to see the amount of\r\npheromone used by our virtual ants.\r\n\r\n\r\nShow code\r\n\r\nplot(ipip_ACO, type = \"pheromone\")\r\n\r\n\r\n$Pheromone\r\n\r\n\r\n$Gamma\r\nNULL\r\n\r\n$Beta\r\nNULL\r\n\r\n$Variance\r\nNULL\r\n\r\nConclusion\r\nThis post demonstrated several ways we can reduce the number of\r\ntest items whether through the traditional method of reliability\r\nexamination, Genetic Algorithm, or ACO. Many psychological assessment\r\nprovides helpful insights to clinicians, educators, and the examinee\r\nthemselves with helpful information. However, some tests are quite\r\nlengthy that they fatigue the examinee out before; for example, the Minnesota\r\nMultiphasic Personality Inventory-2 Restructured Form (MMPI-2-RF)\r\nhas 338 items. Genetic Algorithm and ACO may be useful to reduce the\r\nnumber of test items to the optimal level while retaining accuracy and\r\nrepresentativeness of the test.\r\nHowever, note that the two algorithms are not silver bullets. We\r\ncannot just apply them to every test without consulting the literature,\r\ntest developers, test users, and other relevant stakeholders first.\r\nAfter reducing the number of test items, researchers should check if the\r\nremaining items are representative to the measured construct and perform\r\na pilot testing to examine if the test serves its intended purpose. As\r\nalways, thank you very much for reading this!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-14-ga-aco/ga-aco_files/figure-html5/unnamed-chunk-11-1.png",
    "last_modified": "2022-08-14T23:55:30-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-08-06-edm/",
    "title": "Leveraging a Large-Scale Educational Data Set with Educational Data Mining",
    "description": "In this post, I will be predicting students' high school dropout rate through a large-scale educational data set.\n\n (10 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2022-08-06",
    "categories": [
      "R",
      "Python",
      "Supervised Machine Learning"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nData\r\nPreprocessing\r\nData\r\nAugmentation\r\nClassification\r\nConclusion\r\n\r\nIntroduction\r\nHi Everyone. It’s been awhile since my last blog post. I have\r\nbeen occupied with writing and research meeting, among other things. I\r\nhave had the opportunity to work with several large-scale data sets from\r\nstart to finish (i.e., planning research ideas, data cleaning,\r\ninterpreting patterns, and translating insights for the audience. That\r\nis why I want to post some of my ideas to this blog to share with you\r\nwith it is like to work with data from one end to another. In this post,\r\nI will be predicting students’ high school dropout rate through the\r\nusage of a large-scale educational data set.\r\nMy work is largely in the field of educational data mining (EDM),\r\nwhich is the method of knowledge discovery from educational databases\r\n(Elatia\r\net al., 2016). Such data is usually extracted from sources such as\r\nstudents’ interactive learning environment, computerized testing, and\r\nlarge-scale assessment data repository (International Educational Data\r\nMining Society, 2022). The data set I use in this posting is the\r\nHigh School Longitudinal Study of 2009, which is a longitudinal data set\r\nthat tracks the transition of American youth from secondary schooling to\r\nsubsequent education and work roles.\r\nThe original data set has 4014 variables and 23,503 cases that\r\nwere collected from students’ base year (2009), first follow-up (2012),\r\n2013 update collection (2013), high school transcripts (2013–2014), and\r\nsecond follow-up (2016). First, I chose a handful of variables based on\r\ntheories that are relevant to the prediction of students’ school\r\ndropout.\r\nAfter the initial screening, we have 67 variables left. Then, I\r\nfurther removed responses that were not answered by students or their\r\nparents to preserve data representation. I use dataexplorer\r\npackage to examine types and missingness of the variables. Figure 1\r\nbelow shows that the data set largely consists of categorical variables\r\nthen continuous variables. The data also has a bit of missing\r\ndata.\r\nFigure 1. Variable Type and Missing\r\nDataData Preprocessing\r\nTo further clean up the data, variables with more than 30%\r\nmissingness were removed, and the rest missing data was imputed with\r\nRandom Forest algorithm based on the multivariate\r\nimputation by chained equation method. I have done everything in\r\nadvance to save time. Here is the cleaned data. I will also load the\r\nfollowing packages for data preprocessing.\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse)\r\nlibrary(readxl)\r\nlibrary(Hmisc)\r\nlibrary(corrplot)\r\n\r\nhsls_30_rf <-read_csv(\"hsls_30percent_imputed_rf.csv\", col_names = TRUE)\r\n\r\n\r\n\r\nAt this point, the data set has 51 variables and 16137 cases. I\r\nconverted some variables into factors to reflect their nature with\r\nas.factor function. I also mapped correlation matrix of the\r\ndata set to examine variables that are not related to one another.\r\n\r\n\r\nShow code\r\n\r\nhsls_30_rf <- hsls_30_rf %>% \r\n  as.data.frame() %>%\r\n  mutate(across(c(X1SEX, X1RACE, X1MOMRESP, \r\n                  X1MOMEDU, X1MOMRACE, X1DADRESP, \r\n                  X1DADEDU, X1DADRACE, X1HHNUMBER, \r\n                  X1STUEDEXPCT, X1PAREDEXPCT, X1TMRACE, \r\n                  X1TMCERT, \r\n                  X1LOCALE, X1REGION, S1NOHWDN, \r\n                  S1NOPAPER, S1NOBOOKS, S1LATE, \r\n                  S1PAYOFF, S1GETINTOCLG, S1AFFORD, \r\n                  S1WORKING, S1FRNDGRADES, S1FRNDSCHOOL, \r\n                  S1FRNDCLASS, S1FRNDCLG, S1HRMHOMEWK, \r\n                  S1HRSHOMEWK, S1SUREHSGRAD, P1BEHAVE, \r\n                  P1ATTEND, P1PERFORM, P1HWOFTEN, \r\n                  X4EVERDROP, X4PSENRSTLV), as.factor))\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\ncorrelation_30_rf <-rcorr(as.matrix(hsls_30_rf))\r\n\r\ncorrplot(correlation_30_rf$r, type = \"upper\", order = \"hclust\", \r\n         p.mat = correlation_30_rf$P, insig = \"pch\", pch = 4, pch.cex = 1,\r\n         tl.col = \"black\", tl.cex = 0.5, tl.srt = 90)\r\n\r\n\r\n\r\n\r\nBased on the above correlation matrix and theoretical relevance,\r\nvariables that are recommended for removal are: X1RACE,\r\nX1MOMRACE, X1DADRACE, X1LOCALE,\r\nP1HWOFTEN, X1HHNUMBER, X1TMCERT,\r\nX1REGION, X1MOMRESP, X1DADRESP,\r\nX1SEX, X1TMRACE, X1TSRACE,\r\nX1MTHUTI. They are removed because 1) they are not\r\ntheoretically related to the prediction of high school dropout and 2)\r\nthey have insignificant correlation that might negatively impact the\r\nprediction result.\r\n\r\n\r\nShow code\r\n\r\nhsls_30_rf_final <- hsls_30_rf %>% select(!c(X1RACE, X1MOMRACE, X1DADRACE, X1LOCALE, P1HWOFTEN, X1HHNUMBER, X1TMCERT, X1REGION, X1MOMRESP, X1DADRESP, X1SEX, X1TMRACE, X1MTHUTI))\r\n\r\n\r\n\r\nAfter uncorrelated variables were removed, we have 38 variables\r\nleft. The new correlation matrix is as follows:\r\n\r\n\r\nShow code\r\n\r\ncorrelation_30_rf_final <-rcorr(as.matrix(hsls_30_rf_final))\r\n\r\ncorrplot(correlation_30_rf_final$r, type = \"upper\", order = \"hclust\", \r\n         p.mat = correlation_30_rf_final$P, insig = \"pch\", pch = 4, pch.cex = 1,\r\n         tl.col = \"black\", tl.cex = 0.5, tl.srt = 90)\r\n\r\n\r\n\r\n\r\nData Augmentation\r\nAfter the initial data preprocessing in R, I will use Python to\r\nperform machine learning. I personally use R for data\r\nexploration/statistical analysis and Python for machine learning. First,\r\nI will initiate Python environment and import necessary modules.\r\n\r\n\r\nShow code\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nfrom collections import Counter\r\n\r\nfrom sklearn.manifold import TSNE\r\n\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\n\r\nRANDOM_STATE = 123\r\n\r\nThen, I will transfer the data set to Python environment because the\r\ntwo languages run in parallel instead of on the same ground.\r\n\r\n\r\nShow code\r\ndf = r.hsls_30_rf_final\r\n\r\ndf['X4EVERDROP'] = np.where(df['X4EVERDROP'] == \"0\", 0, 1)\r\n\r\ndf.head()\r\n  X1MOMEDU X1DADEDU   X1SES  ...  P1PERFORM  X4EVERDROP  X4PSENRSTLV\r\n0        5        5  1.5644  ...          1           0            1\r\n1        3        2 -0.3699  ...          1           0            0\r\n2        7        0  1.2741  ...          1           0            1\r\n3        4        0  0.1495  ...          1           1            2\r\n4        3        3  1.0639  ...          1           0            1\r\n\r\n[5 rows x 38 columns]\r\n\r\nFrom the data set, I will extract the predictors (X) and the\r\ntargeted variable (y). I will also check class proportion of the\r\ntargeted variable to see if they are balanced.\r\n\r\n\r\nShow code\r\nX_extreme = df.drop('X4EVERDROP', axis=1)\r\ny_extreme = df['X4EVERDROP']\r\n\r\nprint(\"The proportion of target variable's class :\", Counter(y_extreme))\r\nThe proportion of target variable's class : Counter({0: 14133, 1: 2004})\r\n\r\nWe can see that the data is imbalanced. We have 14133 cases of\r\nstudent who did not and 2004 student who dropped out of their high\r\nschool. Class imbalance problem in educational data sets could hamper\r\nthe accuracy of predictive models as many of them are designed on the\r\nassumption that the predicted class is balanced (He\r\n& Ma, 2013). This problem is especially prevalent in the\r\nprediction of high-stakes educational issues such as such as school\r\ndropout or grade repetition, where discrepancy between two classes is\r\nhigh due to its rare occurrence (Barros et al.,\r\n2019).\r\nI will visualize the imbalance with a t-Distributed Stochastic\r\nNeighbor Embedding (tSNE) plot and a count plot.\r\n\r\n\r\nShow code\r\ntsne = TSNE(n_components=2, random_state=RANDOM_STATE)\r\n\r\nTSNE_result = tsne.fit_transform(X_extreme)\r\n\r\nplt.figure(figsize=(12,8))\r\nsns.scatterplot(TSNE_result[:,0], TSNE_result[:,1], hue=y_extreme, legend='full', palette=\"hls\")\r\n\r\nplt.show()\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nsns.set_theme(style=\"darkgrid\")\r\nsns.countplot(x=\"X4EVERDROP\", data = df)\r\nplt.show()\r\n\r\n\r\nIn my\r\nprevious post, I used the combination of Synthetic Minority\r\nOversampling TEchnique (SMOTE) and Edited Nearest Neighbor (ENN). The\r\nthing is, SMOTE+ENN only works with numerical variables. We have a lot\r\nof categorical variables in this data set, so we need to find a\r\nworkaround for that. I will use SMOTE for nominal and continuous\r\nvariable (SMOTE-NC) and random undersampling instead instead.\r\n\r\n\r\nShow code\r\nfrom imblearn.over_sampling import SMOTENC\r\nfrom imblearn.under_sampling import RandomUnderSampler \r\nfrom sklearn.model_selection import train_test_split\r\n\r\nsmote_nc = SMOTENC(random_state=RANDOM_STATE, sampling_strategy=0.8,\r\n                    categorical_features=[0, 1, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36])\r\n\r\nrus_hybrid = RandomUnderSampler(random_state=RANDOM_STATE, sampling_strategy='not minority')\r\n\r\nX_smote_extreme, y_smote_extreme = smote_nc.fit_resample(X_extreme, y_extreme)\r\n\r\nX_hybrid_extreme, y_hybrid_extreme = rus_hybrid.fit_resample(X_smote_extreme, y_smote_extreme)\r\n\r\nprint(\"For Y extreme :\", Counter(y_extreme))\r\nFor Y extreme : Counter({0: 14133, 1: 2004})\r\n\r\nShow code\r\nprint(\"For Y smote extreme :\", Counter(y_smote_extreme))\r\nFor Y smote extreme : Counter({0: 14133, 1: 11306})\r\n\r\nShow code\r\nprint(\"For Y hybrid extreme :\", Counter(y_hybrid_extreme))\r\nFor Y hybrid extreme : Counter({0: 11306, 1: 11306})\r\n\r\nShow code\r\nX_train_hybrid_ext, X_test_hybrid_ext, y_train_hybrid_ext, y_test_hybrid_ext = train_test_split(X_hybrid_extreme, y_hybrid_extreme, test_size = 0.30, random_state = RANDOM_STATE)\r\n\r\nAfter we finished augmenting the data, below is the result. We have\r\nmuch more instances of student who dropped out of their high school as\r\nseen from the tSNE plot and the count plot below.\r\n\r\n\r\nShow code\r\nTSNE_result = tsne.fit_transform(X_hybrid_extreme)\r\n\r\nplt.figure(figsize=(12,8))\r\nsns.scatterplot(TSNE_result[:,0], TSNE_result[:,1], hue=y_hybrid_extreme, legend='full', palette=\"hls\")\r\n\r\nplt.show()\r\n\r\n\r\n\r\n\r\nShow code\r\nsns.set_theme(style=\"darkgrid\")\r\nsns.countplot(y_hybrid_extreme)\r\nplt.show()\r\n\r\n\r\nNext, we will proceed to the classification stage. For many\r\nclassification algorithms such as XGBoost or Random Forest, you need to\r\ntransform categorical variables into numerical variables with label\r\nencoding or one-hot encoding first. However, we have a lot of\r\ncategorical variables that may hamper the process. To circumvent this,\r\nwe will use CatBoost, which is a\r\ngradient boosting decision tree that supports categorical variables\r\nwithout the need for data transformation.\r\nClassification\r\n\r\n\r\nShow code\r\nfrom sklearn.feature_selection import RFECV\r\nfrom catboost import CatBoostClassifier\r\nfrom sklearn.model_selection import RandomizedSearchCV\r\n\r\nFirst, we will create a CatBoost object as well as a list of\r\nhyperparameters to tune. We can use the default mode of CatBoost, but\r\ntuning the algorithm makes the algorithm perform better. I will tune\r\ntree depth, learning rate, and the number of iteration that the machine\r\nlearns. I use randomized grid search to tune the algorithm to save\r\ntime.\r\n\r\n\r\nShow code\r\nCBC = CatBoostClassifier(random_state=RANDOM_STATE)\r\n\r\nparameters = {'depth'         : [4,5,6,7,8,9,10],\r\n              'learning_rate' : [0.01,0.02,0.03,0.04,0.05],\r\n              'iterations'    : [10,20,30,40,50,60,70,80,90,100]\r\n             }\r\n\r\ncat_features = [0, 1, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36]\r\n\r\nCat_random = RandomizedSearchCV(estimator = CBC, \r\n                                param_distributions = parameters, \r\n                                n_iter = 10, cv = 3, verbose=0, \r\n                                random_state = RANDOM_STATE, error_score='raise')\r\n\r\nCat_random.fit(X_train_hybrid_ext, y_train_hybrid_ext, cat_features = cat_features)\r\nRandomizedSearchCV(cv=3, error_score='raise',\r\n                   estimator=<catboost.core.CatBoostClassifier object at 0x0000028365650880>,\r\n                   param_distributions={'depth': [4, 5, 6, 7, 8, 9, 10],\r\n                                        'iterations': [10, 20, 30, 40, 50, 60,\r\n                                                       70, 80, 90, 100],\r\n                                        'learning_rate': [0.01, 0.02, 0.03,\r\n                                                          0.04, 0.05]},\r\n                   random_state=123)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=3, error_score='raise',\r\n                   estimator=<catboost.core.CatBoostClassifier object at 0x0000028365650880>,\r\n                   param_distributions={'depth': [4, 5, 6, 7, 8, 9, 10],\r\n                                        'iterations': [10, 20, 30, 40, 50, 60,\r\n                                                       70, 80, 90, 100],\r\n                                        'learning_rate': [0.01, 0.02, 0.03,\r\n                                                          0.04, 0.05]},\r\n                   random_state=123)estimator: CatBoostClassifier<catboost.core.CatBoostClassifier object at 0x0000028365650880>CatBoostClassifier<catboost.core.CatBoostClassifier object at 0x0000028365650880>\r\n\r\nAfter the tuning, I will print out the grid search result. The best\r\nhyperparameter values we have are learning_rate = 0.05, iterations = 80,\r\nand depth = 8. I will then let the machine learn from the data by\r\nfitting the model.\r\n\r\n\r\nShow code\r\nprint(\" Results from Grid Search \" )\r\n Results from Grid Search \r\n\r\nShow code\r\nprint(\"\\n The best estimator across ALL searched params:\\n\",Cat_random.best_estimator_)\r\n\r\n The best estimator across ALL searched params:\r\n <catboost.core.CatBoostClassifier object at 0x000002835DA96BE0>\r\n\r\nShow code\r\nprint(\"\\n The best score across ALL searched params:\\n\",Cat_random.best_score_)\r\n\r\n The best score across ALL searched params:\r\n 0.8718094516047511\r\n\r\nShow code\r\nprint(\"\\n The best parameters across ALL searched params:\\n\",Cat_random.best_params_)\r\n\r\n The best parameters across ALL searched params:\r\n {'learning_rate': 0.05, 'iterations': 80, 'depth': 8}\r\n\r\n\r\n\r\nShow code\r\nCBC_tuned = CatBoostClassifier(learning_rate = 0.05, iterations = 80, depth = 8, random_state=RANDOM_STATE)\r\n\r\nCBC_tuned.fit(X_train_hybrid_ext, y_train_hybrid_ext, cat_features = cat_features)\r\n0:  learn: 0.6665327    total: 79ms remaining: 6.24s\r\n1:  learn: 0.6464428    total: 157ms    remaining: 6.11s\r\n2:  learn: 0.6274255    total: 236ms    remaining: 6.07s\r\n3:  learn: 0.6038900    total: 327ms    remaining: 6.21s\r\n4:  learn: 0.5853056    total: 421ms    remaining: 6.31s\r\n5:  learn: 0.5711453    total: 519ms    remaining: 6.4s\r\n6:  learn: 0.5518932    total: 612ms    remaining: 6.38s\r\n7:  learn: 0.5371312    total: 715ms    remaining: 6.44s\r\n8:  learn: 0.5246082    total: 814ms    remaining: 6.42s\r\n9:  learn: 0.5108871    total: 916ms    remaining: 6.41s\r\n10: learn: 0.5017540    total: 1.03s    remaining: 6.46s\r\n11: learn: 0.4914030    total: 1.13s    remaining: 6.39s\r\n12: learn: 0.4829017    total: 1.23s    remaining: 6.34s\r\n13: learn: 0.4753740    total: 1.34s    remaining: 6.31s\r\n14: learn: 0.4684517    total: 1.45s    remaining: 6.28s\r\n15: learn: 0.4595894    total: 1.56s    remaining: 6.25s\r\n16: learn: 0.4516042    total: 1.67s    remaining: 6.17s\r\n17: learn: 0.4452736    total: 1.77s    remaining: 6.1s\r\n18: learn: 0.4382162    total: 1.88s    remaining: 6.02s\r\n19: learn: 0.4326054    total: 1.97s    remaining: 5.92s\r\n20: learn: 0.4268372    total: 2.08s    remaining: 5.86s\r\n21: learn: 0.4202041    total: 2.19s    remaining: 5.78s\r\n22: learn: 0.4167642    total: 2.31s    remaining: 5.73s\r\n23: learn: 0.4122437    total: 2.41s    remaining: 5.63s\r\n24: learn: 0.4093793    total: 2.51s    remaining: 5.53s\r\n25: learn: 0.4039919    total: 2.62s    remaining: 5.44s\r\n26: learn: 0.4004563    total: 2.73s    remaining: 5.37s\r\n27: learn: 0.3975197    total: 2.85s    remaining: 5.29s\r\n28: learn: 0.3932144    total: 2.98s    remaining: 5.24s\r\n29: learn: 0.3904852    total: 3.1s remaining: 5.16s\r\n30: learn: 0.3880708    total: 3.21s    remaining: 5.08s\r\n31: learn: 0.3859222    total: 3.31s    remaining: 4.96s\r\n32: learn: 0.3831005    total: 3.41s    remaining: 4.86s\r\n33: learn: 0.3810144    total: 3.53s    remaining: 4.77s\r\n34: learn: 0.3785504    total: 3.66s    remaining: 4.71s\r\n35: learn: 0.3761624    total: 3.77s    remaining: 4.61s\r\n36: learn: 0.3737088    total: 3.88s    remaining: 4.51s\r\n37: learn: 0.3712488    total: 4s   remaining: 4.42s\r\n38: learn: 0.3685311    total: 4.1s remaining: 4.31s\r\n39: learn: 0.3662991    total: 4.2s remaining: 4.2s\r\n40: learn: 0.3634880    total: 4.33s    remaining: 4.12s\r\n41: learn: 0.3606838    total: 4.43s    remaining: 4.01s\r\n42: learn: 0.3587348    total: 4.56s    remaining: 3.92s\r\n43: learn: 0.3567398    total: 4.66s    remaining: 3.81s\r\n44: learn: 0.3548707    total: 4.78s    remaining: 3.72s\r\n45: learn: 0.3520839    total: 4.88s    remaining: 3.61s\r\n46: learn: 0.3492787    total: 4.98s    remaining: 3.5s\r\n47: learn: 0.3467724    total: 5.08s    remaining: 3.39s\r\n48: learn: 0.3430803    total: 5.18s    remaining: 3.28s\r\n49: learn: 0.3411632    total: 5.3s remaining: 3.18s\r\n50: learn: 0.3396453    total: 5.41s    remaining: 3.07s\r\n51: learn: 0.3377685    total: 5.51s    remaining: 2.97s\r\n52: learn: 0.3362602    total: 5.66s    remaining: 2.88s\r\n53: learn: 0.3349568    total: 5.77s    remaining: 2.78s\r\n54: learn: 0.3336092    total: 5.88s    remaining: 2.67s\r\n55: learn: 0.3318186    total: 5.98s    remaining: 2.56s\r\n56: learn: 0.3287766    total: 6.13s    remaining: 2.47s\r\n57: learn: 0.3268753    total: 6.23s    remaining: 2.36s\r\n58: learn: 0.3251920    total: 6.36s    remaining: 2.26s\r\n59: learn: 0.3227292    total: 6.47s    remaining: 2.16s\r\n60: learn: 0.3213152    total: 6.58s    remaining: 2.05s\r\n61: learn: 0.3192716    total: 6.68s    remaining: 1.94s\r\n62: learn: 0.3183261    total: 6.8s remaining: 1.83s\r\n63: learn: 0.3171622    total: 6.9s remaining: 1.73s\r\n64: learn: 0.3159563    total: 7s   remaining: 1.62s\r\n65: learn: 0.3143367    total: 7.11s    remaining: 1.51s\r\n66: learn: 0.3133019    total: 7.21s    remaining: 1.4s\r\n67: learn: 0.3122973    total: 7.32s    remaining: 1.29s\r\n68: learn: 0.3103480    total: 7.42s    remaining: 1.18s\r\n69: learn: 0.3093429    total: 7.54s    remaining: 1.08s\r\n70: learn: 0.3081943    total: 7.65s    remaining: 969ms\r\n71: learn: 0.3067472    total: 7.76s    remaining: 862ms\r\n72: learn: 0.3058445    total: 7.86s    remaining: 754ms\r\n73: learn: 0.3047133    total: 7.98s    remaining: 647ms\r\n74: learn: 0.3035925    total: 8.08s    remaining: 539ms\r\n75: learn: 0.3022633    total: 8.19s    remaining: 431ms\r\n76: learn: 0.3011552    total: 8.29s    remaining: 323ms\r\n77: learn: 0.2998250    total: 8.41s    remaining: 216ms\r\n78: learn: 0.2985347    total: 8.51s    remaining: 108ms\r\n79: learn: 0.2977599    total: 8.62s    remaining: 0us\r\n<catboost.core.CatBoostClassifier object at 0x0000028365597730>\r\n\r\nWe have 38 variables. We can use all of them, but we can also\r\nfurther reduce them for to look for the most relevant variables to the\r\nmodel. We can trim the variable with recursive feature elimination\r\n(RFE), which is a feature selection method that fits the model and\r\nremove the weakest feature (or predictor) iteratively until the optimal\r\nnumber of features is found (Guyon\r\net al., 2022). Note that this process is entirely data-driven,\r\nmeaning that the machine decides which variable solely based on the\r\ndata, not the theory. In this post, I use a variant of RFE called RFE\r\nwith cross validation (RFECV) that selects the best subset of features\r\nbased on the cross-validation score of the model. RFECV is a\r\nbit\r\nI have computed RFECV in advance to save time. Below is the\r\nresult. Performance of the model jumped at 20 features and fluctuated\r\nafter that, meaning that the optimal number of features is 20.\r\n\r\n\r\nShow code\r\nrfecv_model = RFECV(estimator=CBC_tuned, step=1, cv=5 ,scoring='accuracy')\r\nrfecv = rfecv_model.fit(X_train_hybrid_ext, y_train_hybrid_ext)\r\n\r\nprint('Optimal number of features :', rfecv.n_features_)\r\nprint('Best features :', X_train_hybrid_ext.columns[rfecv.support_])\r\nprint('Original features :', X_train_hybrid_ext.columns)\r\n\r\nplt.figure(figsize=(10, 15), dpi=800)\r\nplt.xlabel(\"Number of features selected\")\r\nplt.ylabel(\"Cross validation score \\n of number of selected features\")\r\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\r\nplt.show()\r\n\r\n\r\nThe optimal set of features are X1MOMEDU,\r\nX1DADEDU, X1MTHEFF, X1SCIUTI,\r\nX1SCIEFF, X1SCHOOLBEL,\r\nX1SCHOOLENG, X1STUEDEXPCT,\r\nX1SCHOOLCLI, X1COUPERCOU,\r\nX1COUPERPRI, X3TGPA9TH, S1NOHWDN,\r\nS1NOPAPER, S1GETINTOCLG,\r\nS1WORKING, S1HRMHOMEWK,\r\nS1HRSHOMEWK, S1HROTHHOMWK,\r\nX4PSENRSTLV. I will reduce the number of variable based on\r\nthe RFECV result and create a training and a testing data set.\r\n\r\n\r\nShow code\r\nX_hybrid_extreme_trim = X_hybrid_extreme[['X1MOMEDU', 'X1DADEDU', 'X1MTHEFF', 'X1SCIUTI', 'X1SCIEFF','X1SCHOOLBEL', 'X1SCHOOLENG', 'X1STUEDEXPCT', 'X1SCHOOLCLI',\r\n'X1COUPERCOU', 'X1COUPERPRI', 'X3TGPA9TH', 'S1NOHWDN', 'S1NOPAPER',\r\n'S1GETINTOCLG', 'S1WORKING', 'S1HRMHOMEWK', 'S1HRSHOMEWK', 'S1HROTHHOMWK', 'X4PSENRSTLV']]\r\n\r\nX_train_hybrid_ext, X_test_hybrid_ext, y_train_hybrid_ext, y_test_hybrid_ext = train_test_split(X_hybrid_extreme_trim, y_hybrid_extreme, test_size = 0.30, random_state = RANDOM_STATE)\r\n\r\nThen, I will fit the CatBoost model I created earlier with this new\r\ndata set and use it to predict the testing data set.\r\n\r\n\r\nShow code\r\ncat_features_post_trim = [0, 1, 7, 12, 13, 14, 15,16, 17, 19]\r\n\r\nCBC_tuned.fit(X_train_hybrid_ext, y_train_hybrid_ext, cat_features = cat_features_post_trim)\r\n0:  learn: 0.6630968    total: 65.4ms   remaining: 5.17s\r\n1:  learn: 0.6347421    total: 130ms    remaining: 5.08s\r\n2:  learn: 0.6135360    total: 226ms    remaining: 5.8s\r\n3:  learn: 0.5958557    total: 308ms    remaining: 5.86s\r\n4:  learn: 0.5789939    total: 377ms    remaining: 5.66s\r\n5:  learn: 0.5568907    total: 450ms    remaining: 5.55s\r\n6:  learn: 0.5378680    total: 528ms    remaining: 5.5s\r\n7:  learn: 0.5226018    total: 605ms    remaining: 5.44s\r\n8:  learn: 0.5087498    total: 678ms    remaining: 5.35s\r\n9:  learn: 0.5009270    total: 763ms    remaining: 5.34s\r\n10: learn: 0.4841922    total: 838ms    remaining: 5.25s\r\n11: learn: 0.4746953    total: 921ms    remaining: 5.22s\r\n12: learn: 0.4681778    total: 1s   remaining: 5.17s\r\n13: learn: 0.4612541    total: 1.1s remaining: 5.17s\r\n14: learn: 0.4532992    total: 1.2s remaining: 5.19s\r\n15: learn: 0.4450863    total: 1.28s    remaining: 5.13s\r\n16: learn: 0.4375271    total: 1.38s    remaining: 5.1s\r\n17: learn: 0.4308885    total: 1.47s    remaining: 5.07s\r\n18: learn: 0.4254128    total: 1.56s    remaining: 5s\r\n19: learn: 0.4213425    total: 1.65s    remaining: 4.96s\r\n20: learn: 0.4177958    total: 1.74s    remaining: 4.89s\r\n21: learn: 0.4131347    total: 1.83s    remaining: 4.82s\r\n22: learn: 0.4089653    total: 1.91s    remaining: 4.73s\r\n23: learn: 0.4055672    total: 2s   remaining: 4.67s\r\n24: learn: 0.4030118    total: 2.1s remaining: 4.62s\r\n25: learn: 0.3997708    total: 2.19s    remaining: 4.55s\r\n26: learn: 0.3945504    total: 2.31s    remaining: 4.53s\r\n27: learn: 0.3909571    total: 2.39s    remaining: 4.44s\r\n28: learn: 0.3874681    total: 2.48s    remaining: 4.35s\r\n29: learn: 0.3842159    total: 2.56s    remaining: 4.26s\r\n30: learn: 0.3817295    total: 2.66s    remaining: 4.2s\r\n31: learn: 0.3793464    total: 2.75s    remaining: 4.13s\r\n32: learn: 0.3760956    total: 2.84s    remaining: 4.05s\r\n33: learn: 0.3744325    total: 2.94s    remaining: 3.97s\r\n34: learn: 0.3711698    total: 3.03s    remaining: 3.9s\r\n35: learn: 0.3690638    total: 3.13s    remaining: 3.82s\r\n36: learn: 0.3659461    total: 3.23s    remaining: 3.75s\r\n37: learn: 0.3637044    total: 3.31s    remaining: 3.66s\r\n38: learn: 0.3621688    total: 3.4s remaining: 3.58s\r\n39: learn: 0.3601269    total: 3.49s    remaining: 3.49s\r\n40: learn: 0.3580019    total: 3.59s    remaining: 3.42s\r\n41: learn: 0.3567636    total: 3.68s    remaining: 3.33s\r\n42: learn: 0.3544761    total: 3.76s    remaining: 3.23s\r\n43: learn: 0.3505973    total: 3.85s    remaining: 3.15s\r\n44: learn: 0.3495149    total: 3.95s    remaining: 3.07s\r\n45: learn: 0.3476525    total: 4.04s    remaining: 2.99s\r\n46: learn: 0.3462577    total: 4.13s    remaining: 2.9s\r\n47: learn: 0.3445400    total: 4.21s    remaining: 2.81s\r\n48: learn: 0.3427600    total: 4.33s    remaining: 2.74s\r\n49: learn: 0.3408402    total: 4.43s    remaining: 2.66s\r\n50: learn: 0.3381710    total: 4.52s    remaining: 2.57s\r\n51: learn: 0.3364197    total: 4.63s    remaining: 2.49s\r\n52: learn: 0.3339518    total: 4.72s    remaining: 2.4s\r\n53: learn: 0.3311018    total: 4.82s    remaining: 2.32s\r\n54: learn: 0.3294152    total: 4.93s    remaining: 2.24s\r\n55: learn: 0.3280687    total: 5.03s    remaining: 2.15s\r\n56: learn: 0.3269127    total: 5.11s    remaining: 2.06s\r\n57: learn: 0.3252681    total: 5.21s    remaining: 1.97s\r\n58: learn: 0.3243089    total: 5.3s remaining: 1.89s\r\n59: learn: 0.3212165    total: 5.39s    remaining: 1.8s\r\n60: learn: 0.3201323    total: 5.48s    remaining: 1.71s\r\n61: learn: 0.3189120    total: 5.56s    remaining: 1.61s\r\n62: learn: 0.3178599    total: 5.64s    remaining: 1.52s\r\n63: learn: 0.3169985    total: 5.75s    remaining: 1.44s\r\n64: learn: 0.3154376    total: 5.87s    remaining: 1.35s\r\n65: learn: 0.3142858    total: 5.96s    remaining: 1.26s\r\n66: learn: 0.3130732    total: 6.06s    remaining: 1.18s\r\n67: learn: 0.3115498    total: 6.14s    remaining: 1.08s\r\n68: learn: 0.3106751    total: 6.23s    remaining: 993ms\r\n69: learn: 0.3104327    total: 6.27s    remaining: 896ms\r\n70: learn: 0.3094918    total: 6.3s remaining: 799ms\r\n71: learn: 0.3084260    total: 6.42s    remaining: 714ms\r\n72: learn: 0.3070031    total: 6.51s    remaining: 624ms\r\n73: learn: 0.3052639    total: 6.59s    remaining: 534ms\r\n74: learn: 0.3043254    total: 6.67s    remaining: 445ms\r\n75: learn: 0.3038159    total: 6.73s    remaining: 354ms\r\n76: learn: 0.3029093    total: 6.82s    remaining: 266ms\r\n77: learn: 0.3014591    total: 6.91s    remaining: 177ms\r\n78: learn: 0.2992314    total: 7s   remaining: 88.6ms\r\n79: learn: 0.2984094    total: 7.1s remaining: 0us\r\n<catboost.core.CatBoostClassifier object at 0x0000028365597730>\r\n\r\nResults from classification report are satisfactory as seen from the\r\nmacro average of precision, recall, and f1-score. I also show the\r\nreceiver operating characteristic curve below.\r\n\r\n\r\nShow code\r\nfrom sklearn.metrics import roc_auc_score\r\nfrom sklearn.metrics import classification_report\r\n\r\npred_ext = CBC_tuned.predict(X_test_hybrid_ext)\r\n\r\nprint(classification_report(y_test_hybrid_ext, pred_ext))\r\n              precision    recall  f1-score   support\r\n\r\n           0       0.86      0.92      0.88      3351\r\n           1       0.91      0.85      0.88      3433\r\n\r\n    accuracy                           0.88      6784\r\n   macro avg       0.88      0.88      0.88      6784\r\nweighted avg       0.88      0.88      0.88      6784\r\n\r\nShow code\r\nroc_auc_score(y_test_hybrid_ext, pred_ext)\r\n0.8827700805886101\r\n\r\n\r\n\r\nShow code\r\nfrom sklearn import metrics\r\n\r\ny_pred_proba_cat = CBC_tuned.predict_proba(X_test_hybrid_ext)[::,1]\r\nfpr_cat, tpr_cat, _ = metrics.roc_curve(y_test_hybrid_ext,  y_pred_proba_cat)\r\n\r\nauc_cat = metrics.roc_auc_score(y_test_hybrid_ext, y_pred_proba_cat)\r\n\r\n#create ROC curve\r\nplt.plot(fpr_cat,tpr_cat, label=\"ROC_AUC=\"+str(auc_cat.round(3)))\r\n[<matplotlib.lines.Line2D object at 0x000002835E221820>]\r\n\r\nShow code\r\nplt.legend(loc=\"lower right\")\r\n<matplotlib.legend.Legend object at 0x00000283656576A0>\r\n\r\nShow code\r\nplt.ylabel('True Positive Rate')\r\nText(0, 0.5, 'True Positive Rate')\r\n\r\nShow code\r\nplt.xlabel('False Positive Rate')\r\n\r\n# displaying the title\r\nText(0.5, 0, 'False Positive Rate')\r\n\r\nShow code\r\nplt.title(\"Area Under Curve\")\r\nText(0.5, 1.0, 'Area Under Curve')\r\n\r\nShow code\r\nplt.show()\r\n\r\n\r\nI also visualize feature importance of the model below. The most\r\nimpactful predictor to students’ high school dropout is their last year\r\nGPA, followed by hours spent doing homework on typical school days, and\r\ntheir self-efficacy in mathematics.\r\n\r\n\r\nShow code\r\nfrom matplotlib.pyplot import figure\r\n\r\nimportances_cat = pd.Series(CBC_tuned.feature_importances_, index = X_hybrid_extreme_trim.columns)\r\n\r\nsorted_importance_cat = importances_cat.sort_values()\r\n\r\n#Horizontal bar plot\r\nsorted_importance_cat.plot(kind='barh', color='lightgreen'); \r\nplt.xlabel('Feature Importance Score')\r\nText(0.5, 0, 'Feature Importance Score')\r\n\r\nShow code\r\nplt.ylabel('Features')\r\nText(0, 0.5, 'Features')\r\n\r\nShow code\r\nplt.title(\"Visualizing Important Features\")\r\nText(0.5, 1.0, 'Visualizing Important Features')\r\n\r\nShow code\r\nplt.rcParams[\"figure.figsize\"] = (8, 4)\r\nplt.show()\r\n\r\n\r\nConclusion\r\nThe point of this post is to demonstrate how EDM can be used with\r\nlarge-scale educational data to derive insights and potentially apply it\r\nto practice. We started out with a lot of variables (4014), then we\r\nreduce it based on the relevant theory to 67, based on missing data to\r\n51, based on correlation coefficient to 38, and based on RFECV to\r\n20.\r\nWe might want to select variables that are actionable for the\r\nmodel to be meaningful. For example, saying that a student is likely to\r\ndropout of their high school because of their socio-economic status\r\nmight not be as helpful because you cannot change their family income in\r\na matter of days or months. However, saying that their GPA and hours\r\nspent on home work are influencing factors might allow students to\r\nadjust their learning behavior.\r\nWith a meaningful model, an early warning system can be developed\r\nto alert teachers of potential under-performing students for an early\r\nintervention. However, I do not mean that results from the model is\r\nperfect. It should be used in conjunction with other indicators such as\r\nstudent record, parents’ observation, and behavior note. As education\r\ngoes online or semi-online, records of student data can be leveraged to\r\nbetter understand them and ultimately benefit the teaching\r\npractice.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-06-edm/edm_files/figure-html5/unnamed-chunk-24-11.png",
    "last_modified": "2022-08-07T12:12:58-06:00",
    "input_file": {},
    "preview_width": 2304,
    "preview_height": 1536
  },
  {
    "path": "posts/2022-05-15-irt/",
    "title": "Item Response Theory",
    "description": "In this post, I will be examining characteristics of test items based on the Item Response Theory framework.\n\n (18 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2022-05-15",
    "categories": [
      "R",
      "Psychometric"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction to Item\r\nResponse Theory\r\nDichotomous Item\r\nPolytomous Item\r\nConclusion\r\n\r\nIntroduction to Item\r\nResponse Theory\r\nMy\r\nprevious post on Classical Test Theory (CTT) discussed how it has\r\nseveral disadvantages that limit its interpretation to a certain group\r\nof population and therefore reduces its utility to test development.\r\nSpecifically, generalizability of the test scores from CTT is quite\r\nlimited due to item/test dependency; item parameters such as item\r\ndifficulty, item discrimination, and reliability estimates are dependent\r\nupon test scores, which are derived from a group of sample (DeVellis,\r\n2006). If we change our sample, those parameters might change. Also,\r\nIf we administer two forms of the same test (i.e., form A and form B) to\r\nthe same examinee, we still cannot guarantee that they will obtain the\r\nsame score on both tests. Raw scores of a CTT-based test do not reflect\r\nlearning progress of an examinee as CTT-based scores are not comparable\r\nacross time (DeVellis,\r\n2006).\r\nSuch limitations can be addressed by Item Response Theory (IRT).\r\nIRT is able to link information from test items to examinee performance\r\non the same scale to provide information on the specific domain of\r\ninterest and ability of the examinee (θ) (Hambleton\r\n& Zenisky, 2018). The relationship between observable items and\r\nexaminee performance is explained through Item Characteristic Curve\r\n(ICC), which explains the probability of getting an item(s)\r\ncorrectly given the current ability level and parameter (Hambleton\r\n& Jones, 1993). Therefore, IRT allows researchers to predict\r\nexaminees’ expected test score given their ability level. In this post,\r\nI will be examining characteristics of test items based on the IRT\r\nframework. The R packages I will be using are ltm\r\nand mirt.\r\n\r\n\r\nShow code\r\n\r\nlibrary(ltm)\r\nlibrary(mirt)\r\n\r\n\r\n\r\nFirst, let’s load in a data set. I will be using the data from Law\r\nSchool Admission Test (LSAT), N = 1000, 5 items. The data can be called\r\nwith data(LSAT).As an initial step, we can use\r\nltm::descript for descriptive statistics.\r\n\r\n\r\nShow code\r\n\r\ndata(LSAT)\r\nltm::descript(LSAT)\r\n\r\n\r\n\r\nDescriptive statistics for the 'LSAT' data-set\r\n\r\nSample:\r\n 5 items and 1000 sample units; 0 missing values\r\n\r\nProportions for each level of response:\r\n           0     1  logit\r\nItem 1 0.076 0.924 2.4980\r\nItem 2 0.291 0.709 0.8905\r\nItem 3 0.447 0.553 0.2128\r\nItem 4 0.237 0.763 1.1692\r\nItem 5 0.130 0.870 1.9010\r\n\r\n\r\nFrequencies of total scores:\r\n     0  1  2   3   4   5\r\nFreq 3 20 85 237 357 298\r\n\r\n\r\nPoint Biserial correlation with Total Score:\r\n       Included Excluded\r\nItem 1   0.3620   0.1128\r\nItem 2   0.5668   0.1532\r\nItem 3   0.6184   0.1728\r\nItem 4   0.5344   0.1444\r\nItem 5   0.4354   0.1216\r\n\r\n\r\nCronbach's alpha:\r\n                  value\r\nAll Items        0.2950\r\nExcluding Item 1 0.2754\r\nExcluding Item 2 0.2376\r\nExcluding Item 3 0.2168\r\nExcluding Item 4 0.2459\r\nExcluding Item 5 0.2663\r\n\r\n\r\nPairwise Associations:\r\n   Item i Item j p.value\r\n1       1      5   0.565\r\n2       1      4   0.208\r\n3       3      5   0.113\r\n4       2      4   0.059\r\n5       1      2   0.028\r\n6       2      5   0.009\r\n7       1      3   0.003\r\n8       4      5   0.002\r\n9       3      4   7e-04\r\n10      2      3   4e-04\r\n\r\nFrom the output above, inspection of non significant results can be\r\nused to reveal ‘problematic’ items in pairwise association. Latent\r\nvariable models assume that the high associations between items can be\r\nexplained by a set of latent variables, so any pair of items that is not\r\nrelated to each other violates this assumption. Additionally, Item 1\r\nseems to be the easiest item as seen from its highest proportion of\r\ncorrect response.\r\nDichotomous Item\r\nWe will be performing IRT analyses on dichotomous items, which are\r\nitems that only have two possible scores of incorrect (0) and correct\r\n(1). The three most common dichotomous IRT models are Rasch/1-parameter\r\nlogistics model (1PL), 2-parameter logistics model (2PL), and\r\n3-parameter logistics model(3PL).\r\nRasch Model (1PL)\r\nWe will fit the original Rasch model, which fixes the item\r\ndiscrimination (aka a parameter) of all items to 1 to the data.\r\nThe 1PL (also called Rasch model) model describes test items in\r\nterms of only one parameter, item difficulty (aka b\r\nparameter). Item difficulty is simply how hard an item is (how high does\r\none’s latent ability level need to be in order to have a 50% chance of\r\ngetting the item right?). b-parameter is estimated for each\r\nitem of the test.\r\nThe ltm::rasch() assumes equal a-parameter across\r\nitems with an estimated value. In order to impose the constraint = 1,\r\nthe constraint argument is used. This argument accepts a\r\ntwo-column matrix where the first column denotes the parameter and the\r\nsecond column indicates the value at which the corresponding parameter\r\nshould be fixed.\r\n\r\n\r\nShow code\r\n\r\nmod_rasch <- rasch(LSAT, constraint = cbind(length(LSAT) + 1, 1))\r\n\r\nsummary(mod_rasch)\r\n\r\n\r\n\r\nCall:\r\nrasch(data = LSAT, constraint = cbind(length(LSAT) + 1, 1))\r\n\r\nModel Summary:\r\n   log.Lik      AIC      BIC\r\n -2473.054 4956.108 4980.646\r\n\r\nCoefficients:\r\n                value std.err   z.vals\r\nDffclt.Item 1 -2.8720  0.1287 -22.3066\r\nDffclt.Item 2 -1.0630  0.0821 -12.9458\r\nDffclt.Item 3 -0.2576  0.0766  -3.3635\r\nDffclt.Item 4 -1.3881  0.0865 -16.0478\r\nDffclt.Item 5 -2.2188  0.1048 -21.1660\r\nDscrmn         1.0000      NA       NA\r\n\r\nIntegration:\r\nmethod: Gauss-Hermite\r\nquadrature points: 21 \r\n\r\nOptimization:\r\nConvergence: 0 \r\nmax(|grad|): 6.3e-05 \r\nquasi-Newton: BFGS \r\n\r\nThe results of the descriptive analysis are also validated by the\r\nmodel fit, where items 3 and 1 are the most difficult and the easiest\r\nrespectively (the lower the b-parameter value, the easier). The\r\nparameter estimates can be transformed to probability estimates using\r\nthe coef() method\r\n\r\n\r\nShow code\r\n\r\ncoef(mod_rasch, prob = TRUE, order = TRUE)\r\n\r\n\r\n           Dffclt Dscrmn P(x=1|z=0)\r\nItem 1 -2.8719712      1  0.9464434\r\nItem 5 -2.2187785      1  0.9019232\r\nItem 4 -1.3880588      1  0.8002822\r\nItem 2 -1.0630294      1  0.7432690\r\nItem 3 -0.2576109      1  0.5640489\r\n\r\nThe last column denotes the probability of a positive response\r\n(getting the item correctly) to the ith item for the average\r\nindividual. The argument order = TRUE indicates the output\r\nto sort the items according to the difficulty estimates. In order to\r\ncheck the fit of the model to the data, the argument\r\nGoF.rasch() and margins() can be used. The\r\nformer argument performs a parametric Bootstrap goodness-of-fit test\r\nusing Pearson’s Chi-square statistics, while the latter examines the\r\ntwo- and three-way chi-square residual analysis.\r\n\r\n\r\nShow code\r\n\r\nGoF.rasch(mod_rasch, B = 199) # B = Bootstrap sample\r\n\r\n\r\n\r\nBootstrap Goodness-of-Fit using Pearson chi-squared\r\n\r\nCall:\r\nrasch(data = LSAT, constraint = cbind(length(LSAT) + 1, 1))\r\n\r\nTobs: 30.6 \r\n# data-sets: 200 \r\np-value: 0.225 \r\n\r\nBased on 200 data-points, the non significant p-value suggests an\r\nacceptable fit of the model. The null hypothesis states that the\r\nobserved data and the model fit with each other (no differences). Now,\r\nfor two-way margin analysis.\r\n\r\n\r\nShow code\r\n\r\nmargins(mod_rasch)\r\n\r\n\r\n\r\nCall:\r\nrasch(data = LSAT, constraint = cbind(length(LSAT) + 1, 1))\r\n\r\nFit on the Two-Way Margins\r\n\r\nResponse: (0,0)\r\n  Item i Item j Obs   Exp (O-E)^2/E  \r\n1      2      4  81 98.69      3.17  \r\n2      1      5  12 18.45      2.25  \r\n3      3      5  67 80.04      2.12  \r\n\r\nResponse: (1,0)\r\n  Item i Item j Obs    Exp (O-E)^2/E  \r\n1      3      5  63  51.62      2.51  \r\n2      2      4 156 139.78      1.88  \r\n3      3      4 108  99.42      0.74  \r\n\r\nResponse: (0,1)\r\n  Item i Item j Obs    Exp (O-E)^2/E  \r\n1      2      4 210 193.47      1.41  \r\n2      2      3 135 125.07      0.79  \r\n3      1      4  53  47.24      0.70  \r\n\r\nResponse: (1,1)\r\n  Item i Item j Obs    Exp (O-E)^2/E  \r\n1      2      4 553 568.06      0.40  \r\n2      3      5 490 501.43      0.26  \r\n3      2      3 418 427.98      0.23  \r\n\r\nFrom the above output, using the 3.5 rule of thumb, the value of all\r\ntwo-way combinations are below the cut-off (same way of how statistical\r\nhypothesis works) and therefore indicate a good fit to the two-way\r\nmargins. Next, we will examine the fit to the three-way margins.\r\n\r\n\r\nShow code\r\n\r\nmargins(mod_rasch, type = \"three-way\", nprint = 2) #nprint returns 2 highest residual values for each combinations\r\n\r\n\r\n\r\nCall:\r\nrasch(data = LSAT, constraint = cbind(length(LSAT) + 1, 1))\r\n\r\nFit on the Three-Way Margins\r\n\r\nResponse: (0,0,0)\r\n  Item i Item j Item k Obs   Exp (O-E)^2/E    \r\n1      2      3      4  48 66.07      4.94 ***\r\n2      1      3      5   6 13.58      4.23 ***\r\n\r\nResponse: (1,0,0)\r\n  Item i Item j Item k Obs   Exp (O-E)^2/E  \r\n1      1      2      4  70 82.01      1.76  \r\n2      2      4      5  28 22.75      1.21  \r\n\r\nResponse: (0,1,0)\r\n  Item i Item j Item k Obs   Exp (O-E)^2/E  \r\n1      1      2      5   3  7.73      2.90  \r\n2      3      4      5  37 45.58      1.61  \r\n\r\nResponse: (1,1,0)\r\n  Item i Item j Item k Obs    Exp (O-E)^2/E  \r\n1      3      4      5  48  36.91      3.33  \r\n2      1      2      4 144 126.35      2.47  \r\n\r\nResponse: (0,0,1)\r\n  Item i Item j Item k Obs   Exp (O-E)^2/E  \r\n1      1      3      5  41 34.58      1.19  \r\n2      2      4      5  64 72.26      0.94  \r\n\r\nResponse: (1,0,1)\r\n  Item i Item j Item k Obs    Exp (O-E)^2/E  \r\n1      1      2      4 190 174.87      1.31  \r\n2      1      2      3 126 114.66      1.12  \r\n\r\nResponse: (0,1,1)\r\n  Item i Item j Item k Obs   Exp (O-E)^2/E  \r\n1      1      2      5  42 34.35      1.70  \r\n2      1      4      5  46 38.23      1.58  \r\n\r\nResponse: (1,1,1)\r\n  Item i Item j Item k Obs    Exp (O-E)^2/E  \r\n1      3      4      5 397 416.73      0.93  \r\n2      2      3      4 343 361.18      0.91  \r\n\r\n'***' denotes a chi-squared residual greater than 3.5 \r\n\r\nThe three-way margins suggest a problematic fit for two triplets of\r\nitems, both containing item 3. We can try fitting the unconstrained\r\nversion of Rasch model (not fixing the a-parameter to 1) to see\r\nthe difference. This time, no need for the constraint\r\nargument.\r\n\r\n\r\nShow code\r\n\r\nmod_1pl <- rasch(LSAT, constraint = NULL)\r\n\r\n\r\n\r\nAfter fitting the 1PL model, we will request for Item\r\nCharacteristics Curve (ICC), Item Information Curve (IIC), Test\r\nInformation Function (TIF), Latent Ability Curve of the examinees, and\r\nUni-dimensionality Plot of the model.\r\n\r\n\r\nShow code\r\n\r\npar(mfrow = c(2, 3))\r\n\r\nplot(mod_1pl, type=c(\"ICC\"), \r\n     legend = TRUE, cx = \"bottomright\", lwd = 2, \r\n     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = \"1PL Item Characteristics Curve\")\r\n\r\nplot(mod_1pl,type=c(\"IIC\"),\r\n     legend = TRUE, cx = \"topright\", lwd = 2,\r\n     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = \"1PL Item Information Curve\")\r\n\r\nplot(mod_1pl,type=c(\"IIC\"),items=c(0), lwd = 2,\r\n     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = \"Test Information Function\")\r\n\r\nplot(0:1, 0:1, type = \"n\", ann = FALSE, axes = FALSE)\r\ninfo1 <- information(mod_1pl, c(-4, 0)) #Item information on the negative area of theta (-4 to 0)\r\ninfo2 <- information(mod_1pl, c(0, 4)) #Item information on the positive area of theta (0 to 4)\r\ntext(0.5, 0.5, labels = paste(\"Total Information:\", round(info1$InfoTotal, 3),\r\n                               \"\\n\\nInformation in (-4, 0):\", round(info1$InfoRange, 3),\r\n                               paste(\"(\", round(100 * info1$PropRange, 2), \"%)\", sep = \"\"),\r\n                               \"\\n\\nInformation in (0, 4):\", round(info2$InfoRange, 3),\r\n                               paste(\"(\", round(100 * info2$PropRange, 2), \"%)\", sep = \"\")))\r\n\r\ntheta.rasch<-ltm::factor.scores(mod_1pl)\r\nsummary(theta.rasch$score.dat$z1)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n-1.9104 -0.9594 -0.4660 -0.6867 -0.4660  0.5930 \r\n\r\nShow code\r\n\r\nplot(theta.rasch, main = \"Latent Ability of the Examinee\")\r\n\r\nunitest_1pl <- unidimTest(mod_1pl,LSAT)\r\nplot(unitest_1pl, type = \"b\", pch = 1:2, main = \"Modified Parallel Analysis Plot\")\r\nlegend(\"topright\", c(\"Real Data\", \"Average Simulated Data\"), lty = 1, \r\n    pch = 1:2, col = 1:2, bty = \"n\")\r\n\r\n\r\n\r\n\r\nThe first plot is ICC of out 1PL model. ICC shows the\r\nrelationship between examinee ability (θ) and the probability of\r\nexaminees answering an item correctly based on their ability. On ICC,\r\nitem discrimination is represented by the steepness of the curve, and\r\nitem difficulty is represented by the position where the probability of\r\ngetting the item correct is 0.5.\r\nTry comparing item 1 and item 3. For item 1, the point where the\r\nprobability of getting the item correctly is 0.5 matches with the\r\nability point -4 on the X-axis. However, for item 3, the point where the\r\nprobability of getting the item correctly is 0.5 matches with the\r\nability point 0 on the X-axis. In other words, you need more ability to\r\nget item 3 correct than item 1.\r\nThe second plot is IIC. IIC shows how much “information” about\r\nthe latent trait ability an item can provide. Item information curves\r\npeak at the point of difficulty value, where the item has the highest\r\ndiscrimination and the probability of answering the item correctly is\r\n0.5. To put it in plain language, a very difficult item will provide\r\nvery little information about persons with low ability (because the item\r\nis already too hard), and very easy items will provide little\r\ninformation about persons with high ability levels (because it is too\r\neasy).\r\nThe third plot is TIF of the whole test. This is simply the sum\r\nof the individual IICs above. The curves shows how much information this\r\ntest offers in terms of ability level of examinees. Ideally, we want a\r\ntest which provides fairly good coverage of a wide range of latent\r\nability levels. Otherwise, the test is only good at identifying a\r\nlimited range of examinees. The current TIF shows that more information\r\nis yielded around examinees with -2 ability level. The test could use\r\nmore items for people with high ability (more difficult item is needed).\r\nIn particular, the amount of Test Information for ability levels in the\r\ninterval (-4 - 0) is almost 60%, and the item that seems to distinguish\r\nbetween respondents with higher ability levels is item 3 as it is the\r\nmost difficult item.\r\nThe Latent Ability Curve shows the distribution of examinee’s\r\nlatent ability level. The plot shows that most examinees are located\r\naround 0 to 1 ability level.\r\n\r\n\r\nShow code\r\n\r\nunitest_1pl\r\n\r\n\r\n\r\nUnidimensionality Check using Modified Parallel Analysis\r\n\r\nCall:\r\nrasch(data = LSAT, constraint = NULL)\r\n\r\nMatrix of tertachoric correlations\r\n       Item 1 Item 2 Item 3 Item 4 Item 5\r\nItem 1 1.0000 0.1703 0.2275 0.1072 0.0665\r\nItem 2 0.1703 1.0000 0.1891 0.1111 0.1724\r\nItem 3 0.2275 0.1891 1.0000 0.1867 0.1055\r\nItem 4 0.1072 0.1111 0.1867 1.0000 0.2009\r\nItem 5 0.0665 0.1724 0.1055 0.2009 1.0000\r\n\r\nAlternative hypothesis: the second eigenvalue of the observed data is substantially larger \r\n            than the second eigenvalue of data under the assumed IRT model\r\n\r\nSecond eigenvalue in the observed data: 0.2254\r\nAverage of second eigenvalues in Monte Carlo samples: 0.252\r\nMonte Carlo samples: 100\r\np-value: 0.6139\r\n\r\nThe last plot is a test for unidimensionality of the test with\r\nmodified parallel analysis (Drasgpw &\r\nLissak, 1983). The output above shows that the result is\r\nnon-significant, meaning that the 1PL model fits the data well and we\r\nare actually measuring a single trait here. The data is a law school\r\ntest, so it should be measuring contents about law, not maths or\r\nEnglish. The unidimensionality analysis shows that the test is measuring\r\nwhat it is intended to measure.\r\n\r\n\r\nShow code\r\n\r\nsummary(mod_1pl)\r\n\r\n\r\n\r\nCall:\r\nrasch(data = LSAT, constraint = NULL)\r\n\r\nModel Summary:\r\n   log.Lik      AIC      BIC\r\n -2466.938 4945.875 4975.322\r\n\r\nCoefficients:\r\n                value std.err   z.vals\r\nDffclt.Item 1 -3.6153  0.3266 -11.0680\r\nDffclt.Item 2 -1.3224  0.1422  -9.3009\r\nDffclt.Item 3 -0.3176  0.0977  -3.2518\r\nDffclt.Item 4 -1.7301  0.1691 -10.2290\r\nDffclt.Item 5 -2.7802  0.2510 -11.0743\r\nDscrmn         0.7551  0.0694  10.8757\r\n\r\nIntegration:\r\nmethod: Gauss-Hermite\r\nquadrature points: 21 \r\n\r\nOptimization:\r\nConvergence: 0 \r\nmax(|grad|): 2.9e-05 \r\nquasi-Newton: BFGS \r\n\r\nShow code\r\n\r\ncoef(mod_1pl, prob = TRUE, order = TRUE)\r\n\r\n\r\n           Dffclt    Dscrmn P(x=1|z=0)\r\nItem 1 -3.6152665 0.7551347  0.9387746\r\nItem 5 -2.7801716 0.7551347  0.8908453\r\nItem 4 -1.7300903 0.7551347  0.7869187\r\nItem 2 -1.3224208 0.7551347  0.7307844\r\nItem 3 -0.3176306 0.7551347  0.5596777\r\n\r\nThe output above suggests that the discrimination parameter of our\r\nunconstrained model is different from 1, meaning that our constrained\r\nand unconstrained Rasch models are different. The difference can be\r\ntested with a likelihood ratio test using anova()..\r\n\r\n\r\nShow code\r\n\r\nanova(mod_rasch, mod_1pl)\r\n\r\n\r\n\r\n Likelihood Ratio Table\r\n              AIC     BIC  log.Lik   LRT df p.value\r\nmod_rasch 4956.11 4980.65 -2473.05                 \r\nmod_1pl   4945.88 4975.32 -2466.94 12.23  1  <0.001\r\n\r\nBy comparing model summary of the constrained and unconstrained\r\nversion, the latter is more suitable for the LSAT data due to its\r\nsmaller Akaike’s Information Criterion (AIC) and Bayesian Information\r\nCriterion (BIC) values. AIC and BIC are measures of model performance\r\nthat account for model complexity. AIC is a measure that determines\r\nwhich model fits the data better. The lower the score, the better fit\r\nthe model is. Similarly for BIC, the score measures complexity of the\r\nmodel. BIC penalizes the model more for its complexity, meaning that\r\nmore complex models will have a worse (larger) score and will, in turn,\r\nbe less likely to be selected.\r\nWe can double check the result by testing the unconstrained model\r\nwith the three-way margins, which yields a problematic fit with the\r\nconstrained model.\r\n\r\n\r\nShow code\r\n\r\nmargins(mod_1pl, type = \"three-way\", nprint = 2)\r\n\r\n\r\n\r\nCall:\r\nrasch(data = LSAT, constraint = NULL)\r\n\r\nFit on the Three-Way Margins\r\n\r\nResponse: (0,0,0)\r\n  Item i Item j Item k Obs   Exp (O-E)^2/E  \r\n1      1      3      5   6  9.40      1.23  \r\n2      3      4      5  30 25.85      0.67  \r\n\r\nResponse: (1,0,0)\r\n  Item i Item j Item k Obs   Exp (O-E)^2/E  \r\n1      2      4      5  28 22.75      1.21  \r\n2      2      3      4  81 74.44      0.58  \r\n\r\nResponse: (0,1,0)\r\n  Item i Item j Item k Obs  Exp (O-E)^2/E  \r\n1      1      2      5   3 7.58      2.76  \r\n2      1      3      4   5 9.21      1.92  \r\n\r\nResponse: (1,1,0)\r\n  Item i Item j Item k Obs   Exp (O-E)^2/E  \r\n1      2      4      5  51 57.49      0.73  \r\n2      3      4      5  48 42.75      0.64  \r\n\r\nResponse: (0,0,1)\r\n  Item i Item j Item k Obs    Exp (O-E)^2/E  \r\n1      1      3      5  41  33.07      1.90  \r\n2      2      3      4 108 101.28      0.45  \r\n\r\nResponse: (1,0,1)\r\n  Item i Item j Item k Obs    Exp (O-E)^2/E  \r\n1      2      3      4 210 218.91      0.36  \r\n2      1      2      4 190 185.56      0.11  \r\n\r\nResponse: (0,1,1)\r\n  Item i Item j Item k Obs   Exp (O-E)^2/E  \r\n1      1      3      5  23 28.38      1.02  \r\n2      1      4      5  46 42.51      0.29  \r\n\r\nResponse: (1,1,1)\r\n  Item i Item j Item k Obs    Exp (O-E)^2/E  \r\n1      1      2      4 520 526.36      0.08  \r\n2      1      2      3 398 393.30      0.06  \r\n\r\nThe new three-way margins suggests a good fit with the unconstrained\r\nRasch model. Finally, we investigate two more possible extensions of the\r\nunconstrained Rasch model, the two-parameter logistic (2PL) model that\r\nassumes a different discrimination parameter per item, and Rasch model\r\nthat incorporates a guessing parameter (3PL).\r\n2 Parameter Logistics Model\r\n(2PL)\r\nThe 2PL model has the same equation as the 1PL model, but unlike\r\n1PL, 2PL allows item discrimination and item difficulty to vary across\r\nitems instead of fixing it to a constant value.\r\nThe 2PL model can also be fitted with ltm().The\r\nformula of ltm() is two-sided, where its left is either a\r\ndata frame or a matrix, and its right allows only z1 and/or\r\nz2. Latent variables with z2 serves in the\r\ncase of interaction.\r\n\r\n\r\nShow code\r\n\r\nmod_2pl <- ltm(LSAT ~ z1)\r\nsummary(mod_2pl)\r\n\r\n\r\n\r\nCall:\r\nltm(formula = LSAT ~ z1)\r\n\r\nModel Summary:\r\n   log.Lik      AIC      BIC\r\n -2466.653 4953.307 5002.384\r\n\r\nCoefficients:\r\n                value std.err  z.vals\r\nDffclt.Item 1 -3.3597  0.8669 -3.8754\r\nDffclt.Item 2 -1.3696  0.3073 -4.4565\r\nDffclt.Item 3 -0.2799  0.0997 -2.8083\r\nDffclt.Item 4 -1.8659  0.4341 -4.2982\r\nDffclt.Item 5 -3.1236  0.8700 -3.5904\r\nDscrmn.Item 1  0.8254  0.2581  3.1983\r\nDscrmn.Item 2  0.7229  0.1867  3.8721\r\nDscrmn.Item 3  0.8905  0.2326  3.8281\r\nDscrmn.Item 4  0.6886  0.1852  3.7186\r\nDscrmn.Item 5  0.6575  0.2100  3.1306\r\n\r\nIntegration:\r\nmethod: Gauss-Hermite\r\nquadrature points: 21 \r\n\r\nOptimization:\r\nConvergence: 0 \r\nmax(|grad|): 0.024 \r\nquasi-Newton: BFGS \r\n\r\nThe output above shows that the model estimated both item difficulty\r\nand item discrimination. Next, we can try comparing our 1PL model with\r\nthe newly fitted 2PL model.\r\n\r\n\r\nShow code\r\n\r\n#compare\r\nanova(mod_1pl, mod_2pl)\r\n\r\n\r\n\r\n Likelihood Ratio Table\r\n            AIC     BIC  log.Lik  LRT df p.value\r\nmod_1pl 4945.88 4975.32 -2466.94                \r\nmod_2pl 4953.31 5002.38 -2466.65 0.57  4   0.967\r\n\r\nThe comparison suggested no significant difference between both\r\nmodels. Next, we ca nrequest for ICC, IIC, TIF, Latent Ability\r\nDistribution, and Unidimensionality Plot like we did with the 1PL\r\nmodel.\r\n\r\n\r\nShow code\r\n\r\npar(mfrow = c(2, 3))\r\n\r\nplot(mod_2pl, type=c(\"ICC\"), \r\n     legend = TRUE, cx = \"bottomright\", lwd = 2, \r\n     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = \"2PL Item Characteristics Curve\")\r\n\r\nplot(mod_2pl,type=c(\"IIC\"),\r\n     legend = TRUE, cx = \"topright\", lwd = 2,\r\n     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = \"2PL Item Information Curve\")\r\n\r\nplot(mod_2pl,type=c(\"IIC\"),items=c(0), lwd = 2,\r\n     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = \"Test Information Function\")\r\n\r\nplot(0:1, 0:1, type = \"n\", ann = FALSE, axes = FALSE)\r\ninfo1_2pl <- information(mod_2pl, c(-4, 0)) #Item information on the negative area of theta (-4 to 0)\r\ninfo2_2pl <- information(mod_2pl, c(0, 4)) #Item information on the positive area of theta (0 to 4)\r\ntext(0.5, 0.5, labels = paste(\"Total Information:\", round(info1_2pl$InfoTotal, 3),\r\n                               \"\\n\\nInformation in (-4, 0):\", round(info1_2pl$InfoRange, 3),\r\n                               paste(\"(\", round(100 * info1_2pl$PropRange, 2), \"%)\", sep = \"\"),\r\n                               \"\\n\\nInformation in (0, 4):\", round(info2_2pl$InfoRange, 3),\r\n                               paste(\"(\", round(100 * info2_2pl$PropRange, 2), \"%)\", sep = \"\")))\r\n\r\ntheta.2pl<-ltm::factor.scores(mod_2pl)\r\nsummary(theta.2pl$score.dat$z1)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n-1.8953 -1.0026 -0.5397 -0.6629 -0.3572  0.6064 \r\n\r\nShow code\r\n\r\nplot(theta.2pl, main = \"Latent ability scores of the participants 2PL\")\r\n\r\nunitest_2pl <- unidimTest(mod_2pl,LSAT)\r\nplot(unitest_2pl, type = \"b\", pch = 1:2)\r\nlegend(\"topright\", c(\"Real Data\", \"Average Simulated Data\"), lty = 1, \r\n    pch = 1:2, col = 1:2, bty = \"n\")\r\n\r\n\r\n\r\n\r\nICC and IIC of the two models are different, meaning that when we\r\nallow item discrimination to vary, characteristics and yielded\r\ninformation of each item also changed accordingly.\r\n\r\n\r\nShow code\r\n\r\nunitest_2pl\r\n\r\n\r\n\r\nUnidimensionality Check using Modified Parallel Analysis\r\n\r\nCall:\r\nltm(formula = LSAT ~ z1)\r\n\r\nMatrix of tertachoric correlations\r\n       Item 1 Item 2 Item 3 Item 4 Item 5\r\nItem 1 1.0000 0.1703 0.2275 0.1072 0.0665\r\nItem 2 0.1703 1.0000 0.1891 0.1111 0.1724\r\nItem 3 0.2275 0.1891 1.0000 0.1867 0.1055\r\nItem 4 0.1072 0.1111 0.1867 1.0000 0.2009\r\nItem 5 0.0665 0.1724 0.1055 0.2009 1.0000\r\n\r\nAlternative hypothesis: the second eigenvalue of the observed data is substantially larger \r\n            than the second eigenvalue of data under the assumed IRT model\r\n\r\nSecond eigenvalue in the observed data: 0.2254\r\nAverage of second eigenvalues in Monte Carlo samples: 0.2576\r\nMonte Carlo samples: 100\r\np-value: 0.6337\r\n\r\nThe above results of unidimensionality testing also suggests that\r\nthe test measures only one construct. When considering two models\r\ntogether, it might be more preferable for us to choose the 1PL model as\r\nthere is no difference between both 1PL and 2PL; however, by nature, 1PL\r\nmodel is more simple and easier to explain comparing to 2PL. Let us try\r\nfitting the data to a 3PL model just in case.\r\n3 Parameter Logistics Model\r\n(3PL)\r\nThe 3PL model is very similar to the 2PL model; however, the model\r\nincludes an additional parameter: lower asymptote (also known as the\r\nguessing parameter). Under this model, individuals with zero ability\r\nhave a nonzero chance of correctly answering any item just by guessing\r\nrandomly. A 3PL model can be fitted with tpm()\r\n\r\n\r\nShow code\r\n\r\nmod_3pl <- tpm(LSAT, type = \"latent.trait\", max.guessing = 1)\r\n\r\nsummary(mod_3pl)\r\n\r\n\r\n\r\nCall:\r\ntpm(data = LSAT, type = \"latent.trait\", max.guessing = 1)\r\n\r\nModel Summary:\r\n  log.Lik      AIC      BIC\r\n -2466.66 4963.319 5036.935\r\n\r\nCoefficients:\r\n                value std.err  z.vals\r\nGussng.Item 1  0.0374  0.8650  0.0432\r\nGussng.Item 2  0.0777  2.5282  0.0307\r\nGussng.Item 3  0.0118  0.2815  0.0419\r\nGussng.Item 4  0.0353  0.5769  0.0612\r\nGussng.Item 5  0.0532  1.5596  0.0341\r\nDffclt.Item 1 -3.2965  1.7788 -1.8532\r\nDffclt.Item 2 -1.1451  7.5166 -0.1523\r\nDffclt.Item 3 -0.2490  0.7527 -0.3308\r\nDffclt.Item 4 -1.7658  1.6162 -1.0925\r\nDffclt.Item 5 -2.9902  4.0606 -0.7364\r\nDscrmn.Item 1  0.8286  0.2877  2.8797\r\nDscrmn.Item 2  0.7604  1.3774  0.5520\r\nDscrmn.Item 3  0.9016  0.4190  2.1516\r\nDscrmn.Item 4  0.7007  0.2574  2.7219\r\nDscrmn.Item 5  0.6658  0.3282  2.0284\r\n\r\nIntegration:\r\nmethod: Gauss-Hermite\r\nquadrature points: 21 \r\n\r\nOptimization:\r\nOptimizer: optim (BFGS)\r\nConvergence: 0 \r\nmax(|grad|): 0.028 \r\n\r\nThe model summary above suggested that all three item parameters of\r\nitem discrimination (a), item difficulty (b), and item\r\nguessing (c) are allowed to vary. Like what we did, we can try\r\nrequesting for ICC, IIC, TIF, latent ability distribution, and\r\nunidimensionality plot.\r\n\r\n\r\nShow code\r\n\r\npar(mfrow = c(2, 3))\r\n\r\nplot(mod_3pl, type=c(\"ICC\"), \r\n     legend = TRUE, cx = \"bottomright\", lwd = 2, \r\n     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = \"3PL Item Characteristics Curve\")\r\n\r\nplot(mod_3pl,type=c(\"IIC\"),\r\n     legend = TRUE, cx = \"topright\", lwd = 2,\r\n     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = \"3PL Item Information Curve\")\r\n\r\nplot(mod_3pl,type=c(\"IIC\"),items=c(0))\r\n\r\nplot(0:1, 0:1, type = \"n\", ann = FALSE, axes = FALSE)\r\ninfo1_3pl <- information(mod_3pl, c(-4, 0)) #Item information on the negative area of theta (-4 to 0)\r\ninfo2_3pl <- information(mod_3pl, c(0, 4)) #Item information on the positive area of theta (0 to 4)\r\ntext(0.5, 0.5, labels = paste(\"Total Information:\", round(info1_3pl$InfoTotal, 3),\r\n                               \"\\n\\nInformation in (-4, 0):\", round(info1_3pl$InfoRange, 3),\r\n                               paste(\"(\", round(100 * info1_3pl$PropRange, 2), \"%)\", sep = \"\"),\r\n                               \"\\n\\nInformation in (0, 4):\", round(info2_3pl$InfoRange, 3),\r\n                               paste(\"(\", round(100 * info2_3pl$PropRange, 2), \"%)\", sep = \"\")))\r\n\r\ntheta.3pl<-ltm::factor.scores(mod_3pl)\r\nsummary(theta.3pl$score.dat$z1)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n-1.8706 -0.9992 -0.5368 -0.6584 -0.3590  0.6116 \r\n\r\nShow code\r\n\r\nplot(theta.3pl, main = \"Latent ability scores of the participants 3PL\")\r\n\r\nunitest_3pl <- unidimTest(mod_3pl,LSAT)\r\nplot(unitest_3pl, type = \"b\", pch = 1:2)\r\nlegend(\"topright\", c(\"Real Data\", \"Average Simulated Data\"), lty = 1, \r\n    pch = 1:2, col = 1:2, bty = \"n\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nunitest_3pl\r\n\r\n\r\n\r\nUnidimensionality Check using Modified Parallel Analysis\r\n\r\nCall:\r\ntpm(data = LSAT, type = \"latent.trait\", max.guessing = 1)\r\n\r\nMatrix of tertachoric correlations\r\n       Item 1 Item 2 Item 3 Item 4 Item 5\r\nItem 1 1.0000 0.1703 0.2275 0.1072 0.0665\r\nItem 2 0.1703 1.0000 0.1891 0.1111 0.1724\r\nItem 3 0.2275 0.1891 1.0000 0.1867 0.1055\r\nItem 4 0.1072 0.1111 0.1867 1.0000 0.2009\r\nItem 5 0.0665 0.1724 0.1055 0.2009 1.0000\r\n\r\nAlternative hypothesis: the second eigenvalue of the observed data is substantially larger \r\n            than the second eigenvalue of data under the assumed IRT model\r\n\r\nSecond eigenvalue in the observed data: 0.2254\r\nAverage of second eigenvalues in Monte Carlo samples: 0.2465\r\nMonte Carlo samples: 100\r\np-value: 0.5743\r\n\r\nThe unidimensionality testing above suggested non-significant\r\nresult, meaning that the test measures one construct as intended.\r\n\r\n\r\nShow code\r\n\r\nanova(mod_1pl, mod_3pl)\r\n\r\n\r\n\r\n Likelihood Ratio Table\r\n            AIC     BIC  log.Lik  LRT df p.value\r\nmod_1pl 4945.88 4975.32 -2466.94                \r\nmod_3pl 4963.32 5036.94 -2466.66 0.56  9       1\r\n\r\nThe comparison between the 1PL and the 3PL model also suggests no\r\nstatistical differences. The 3PL model also has larger AIC and BIC;\r\ntherefore, it is more preferable for us to use 1PL model with this data.\r\nFinally, we can request for ability estimates for all response pattern\r\nwith the factor.scores() function.\r\n\r\n\r\nShow code\r\n\r\nfactor.scores(mod_1pl)\r\n\r\n\r\n\r\nCall:\r\nrasch(data = LSAT, constraint = NULL)\r\n\r\nScoring Method: Empirical Bayes\r\n\r\nFactor-Scores for observed response patterns:\r\n   Item 1 Item 2 Item 3 Item 4 Item 5 Obs     Exp     z1 se.z1\r\n1       0      0      0      0      0   3   2.364 -1.910 0.790\r\n2       0      0      0      0      1   6   5.468 -1.439 0.793\r\n3       0      0      0      1      0   2   2.474 -1.439 0.793\r\n4       0      0      0      1      1  11   8.249 -0.959 0.801\r\n5       0      0      1      0      0   1   0.852 -1.439 0.793\r\n6       0      0      1      0      1   1   2.839 -0.959 0.801\r\n7       0      0      1      1      0   3   1.285 -0.959 0.801\r\n8       0      0      1      1      1   4   6.222 -0.466 0.816\r\n9       0      1      0      0      0   1   1.819 -1.439 0.793\r\n10      0      1      0      0      1   8   6.063 -0.959 0.801\r\n11      0      1      0      1      1  16  13.288 -0.466 0.816\r\n12      0      1      1      0      1   3   4.574 -0.466 0.816\r\n13      0      1      1      1      0   2   2.070 -0.466 0.816\r\n14      0      1      1      1      1  15  14.749  0.049 0.836\r\n15      1      0      0      0      0  10  10.273 -1.439 0.793\r\n16      1      0      0      0      1  29  34.249 -0.959 0.801\r\n17      1      0      0      1      0  14  15.498 -0.959 0.801\r\n18      1      0      0      1      1  81  75.060 -0.466 0.816\r\n19      1      0      1      0      0   3   5.334 -0.959 0.801\r\n20      1      0      1      0      1  28  25.834 -0.466 0.816\r\n21      1      0      1      1      0  15  11.690 -0.466 0.816\r\n22      1      0      1      1      1  80  83.310  0.049 0.836\r\n23      1      1      0      0      0  16  11.391 -0.959 0.801\r\n24      1      1      0      0      1  56  55.171 -0.466 0.816\r\n25      1      1      0      1      0  21  24.965 -0.466 0.816\r\n26      1      1      0      1      1 173 177.918  0.049 0.836\r\n27      1      1      1      0      0  11   8.592 -0.466 0.816\r\n28      1      1      1      0      1  61  61.235  0.049 0.836\r\n29      1      1      1      1      0  28  27.709  0.049 0.836\r\n30      1      1      1      1      1 298 295.767  0.593 0.862\r\n\r\nBy default, factor.scores() produces ability estimates\r\nfor the observed response patterns (every combination available); if\r\nability estimates are required for non observed or specific response\r\npatterns, these could be specified using the resp.patterns\r\nargument.\r\n\r\n\r\nShow code\r\n\r\nfactor.scores(mod_1pl, resp.patterns = rbind(c(1,1,1,1,1), c(0,0,0,0,0)))\r\n\r\n\r\n\r\nCall:\r\nrasch(data = LSAT, constraint = NULL)\r\n\r\nScoring Method: Empirical Bayes\r\n\r\nFactor-Scores for specified response patterns:\r\n  Item 1 Item 2 Item 3 Item 4 Item 5 Obs     Exp     z1 se.z1\r\n1      1      1      1      1      1 298 295.767  0.593 0.862\r\n2      0      0      0      0      0   3   2.364 -1.910 0.790\r\n\r\nThe specified response patterns above are for examinees who got\r\nall items correctly and incorrectly respectively. The results suggested\r\nthat the examinee who got all item correctly has ability level of 0.50\r\nand the examinee who got all item incorrectly has ability level of\r\n-1.91.\r\nThe model we discussed so far, Rasch, 1PL,2PL, 3PL are all\r\nsuitable for dichotomous test items (True/False), but what if item\r\nresponses have more than 2 categories like in a survey (i.e., 1 =\r\nStrongly disagree 2 = Disagree 3 = Agree 4 = Strongly Agree)? This is\r\nwhen we use polytomous IRT models.\r\nPolytomous Item\r\nThe data we consider here comes from the Environment section of the\r\n1990 British Social Attitudes Survey, N = 291, 6 items, 3 ordinal\r\nresponse options. The data can be loaded with\r\ndata(Environment).\r\n\r\n\r\nShow code\r\n\r\ndata(Environment)\r\ndescript(Environment)\r\n\r\n\r\n\r\nDescriptive statistics for the 'Environment' data-set\r\n\r\nSample:\r\n 6 items and 291 sample units; 0 missing values\r\n\r\nProportions for each level of response:\r\n             very concerned slightly concerned not very concerned\r\nLeadPetrol           0.6151             0.3265             0.0584\r\nRiverSea             0.8007             0.1753             0.0241\r\nRadioWaste           0.7457             0.1924             0.0619\r\nAirPollution         0.6495             0.3196             0.0309\r\nChemicals            0.7491             0.1924             0.0584\r\nNuclear              0.5155             0.3265             0.1581\r\n\r\n\r\nFrequencies of total scores:\r\n      6  7  8  9 10 11 12 13 14 15 16 17 18\r\nFreq 96 51 37 27 26 18 13  7  6  6  1  1  2\r\n\r\n\r\nCronbach's alpha:\r\n                        value\r\nAll Items              0.8215\r\nExcluding LeadPetrol   0.8218\r\nExcluding RiverSea     0.7990\r\nExcluding RadioWaste   0.7767\r\nExcluding AirPollution 0.7751\r\nExcluding Chemicals    0.7790\r\nExcluding Nuclear      0.8058\r\n\r\n\r\nPairwise Associations:\r\n   Item i Item j p.value\r\n1       1      2   0.001\r\n2       1      3   0.001\r\n3       1      4   0.001\r\n4       1      5   0.001\r\n5       1      6   0.001\r\n6       2      3   0.001\r\n7       2      4   0.001\r\n8       2      5   0.001\r\n9       2      6   0.001\r\n10      3      4   0.001\r\n\r\nFrom the descriptive output, the first response, “very\r\nconcerned”, has the highest frequency. The p-values for the\r\npairwise associations indicate significant associations between all\r\nitems. An alternative method to explore the degree of association\r\nbetween pairs of items can be done with rcor.test() for\r\nnon-parametric correlation coefficients.\r\n\r\n\r\nShow code\r\n\r\nrcor.test(Environment, method = \"kendall\")\r\n\r\n\r\n\r\n             LeadPetrol RiverSea RadioWaste AirPollution Chemicals\r\nLeadPetrol    *****      0.385    0.260      0.457        0.305   \r\nRiverSea     <0.001      *****    0.399      0.548        0.403   \r\nRadioWaste   <0.001     <0.001    *****      0.506        0.623   \r\nAirPollution <0.001     <0.001   <0.001      *****        0.504   \r\nChemicals    <0.001     <0.001   <0.001     <0.001        *****   \r\nNuclear      <0.001     <0.001   <0.001     <0.001       <0.001   \r\n             Nuclear\r\nLeadPetrol    0.279 \r\nRiverSea      0.320 \r\nRadioWaste    0.484 \r\nAirPollution  0.382 \r\nChemicals     0.463 \r\nNuclear       ***** \r\n\r\nupper diagonal part contains correlation coefficient estimates \r\nlower diagonal part contains corresponding p-values\r\n\r\nThe rcor.test() provides two options for nonparametric\r\ncalculation, Kendall’s Tau and Spearman’s rho in\r\nmethod argument. Initially, we will fit the partial credit\r\nmodel (PCM), which is a Polytomous version of the Rasch model.\r\nPartial Credit Model\r\nPCM fixes the discrimination parameter of all item as 1 in the\r\nsame way as what Rasch’s model does. The threshold (or the parameter\r\nthat represents the trait level necessary for an examinee to have 50% to\r\npick a response category) of PCM is allowed to vary.\r\nFor the gpcm() function, If\r\nconstraint = \"rasch\", then the discrimination parameter is\r\nassumed equal for all items and fixed at one. If\r\nconstraint = \"1PL\", then the discrimination parameter βi is\r\nassumed equal for all items but is estimated. Here, we are fixing all\r\ndiscrimination parameter to 1.\r\n\r\n\r\nShow code\r\n\r\nmod_pcm_rasch <- gpcm(Environment, constraint = \"rasch\")\r\nsummary(mod_pcm_rasch)\r\n\r\n\r\n\r\nCall:\r\ngpcm(data = Environment, constraint = \"rasch\")\r\n\r\nModel Summary:\r\n   log.Lik      AIC      BIC\r\n -1147.176 2318.351 2362.431\r\n\r\nCoefficients:\r\n$LeadPetrol\r\n        value std.err z.value\r\nCatgr.1 0.680   0.153   4.450\r\nCatgr.2 2.785   0.292   9.541\r\nDscrmn  1.000      NA      NA\r\n\r\n$RiverSea\r\n        value std.err z.value\r\nCatgr.1 1.822   0.180  10.149\r\nCatgr.2 3.385   0.435   7.781\r\nDscrmn  1.000      NA      NA\r\n\r\n$RadioWaste\r\n        value std.err z.value\r\nCatgr.1 1.542   0.174   8.879\r\nCatgr.2 2.328   0.302   7.709\r\nDscrmn  1.000      NA      NA\r\n\r\n$AirPollution\r\n        value std.err z.value\r\nCatgr.1 0.822   0.153   5.363\r\nCatgr.2 3.517   0.376   9.343\r\nDscrmn  1.000      NA      NA\r\n\r\n$Chemicals\r\n        value std.err z.value\r\nCatgr.1 1.555   0.174   8.949\r\nCatgr.2 2.399   0.308   7.788\r\nDscrmn  1.000      NA      NA\r\n\r\n$Nuclear\r\n        value std.err z.value\r\nCatgr.1 0.316   0.156   2.029\r\nCatgr.2 1.498   0.208   7.218\r\nDscrmn  1.000      NA      NA\r\n\r\n\r\nIntegration:\r\nmethod: Gauss-Hermite\r\nquadrature points: 21 \r\n\r\nOptimization:\r\nConvergence: 0 \r\nmax(|grad|): 0.0079 \r\noptimizer: nlminb \r\n\r\nThe model summary provides AIC and BIC of the model. Same as what we\r\ndid with our dichotomous data, we can request for parameter estimates of\r\neach item.\r\n\r\n\r\nShow code\r\n\r\ncoef(mod_pcm_rasch, prob = TRUE, order = TRUE)\r\n\r\n\r\n             Catgr.1 Catgr.2 Dscrmn\r\nLeadPetrol     0.680   2.785      1\r\nRiverSea       1.822   3.385      1\r\nRadioWaste     1.542   2.328      1\r\nAirPollution   0.822   3.517      1\r\nChemicals      1.555   2.399      1\r\nNuclear        0.316   1.498      1\r\n\r\nIn order to check the fit of the model to the data, the argument\r\nGoF.gpcm and margins() can be used.\r\n\r\n\r\nShow code\r\n\r\nGoF.gpcm(mod_pcm_rasch, B = 199) # B = Bootstrap sample\r\n\r\n\r\n\r\nParametric Bootstrap Approximation to Pearson chi-squared Goodness-of-Fit Measure\r\n\r\nCall:\r\ngpcm(data = Environment, constraint = \"rasch\")\r\n\r\nTobs: 1001.41 \r\n# data-sets: 200 \r\np-value: 0.07 \r\n\r\nBased on 200 data-points, the non significant p-value suggests an\r\nacceptable fit of the model. The null hypothesis states that the\r\nobserved data and the model fit with each other (no differences). We can\r\nmove on to the two-way margin analysis.\r\n\r\n\r\nShow code\r\n\r\nmargins(mod_pcm_rasch)\r\n\r\n\r\n\r\nCall:\r\ngpcm(data = Environment, constraint = \"rasch\")\r\n\r\nFit on the Two-Way Margins\r\n\r\n             LeadPetrol RiverSea RadioWaste AirPollution Chemicals\r\nLeadPetrol   <NA>       35.47     3.04      34.43         7.41    \r\nRiverSea     ***        <NA>     20.07      77.50        18.75    \r\nRadioWaste                       <NA>       44.64        68.12    \r\nAirPollution ***        ***      ***        <NA>         33.29    \r\nChemicals                        ***        ***          <NA>     \r\nNuclear                          ***                              \r\n             Nuclear\r\nLeadPetrol    4.71  \r\nRiverSea     10.36  \r\nRadioWaste   43.35  \r\nAirPollution 16.56  \r\nChemicals    27.68  \r\nNuclear      <NA>   \r\n\r\n'***' denotes pairs of items with lack-of-fit\r\n\r\nThe upper diagonal part of the output contains the residuals, and\r\nthe lower diagonal part indicates the pairs for which the residuals\r\nexceed the threshold value. The two-way margin analysis above suggests\r\nproblematic fit of the data with the PCM model, meaning that PCM might\r\nnot be suitable for this data. We can try using the next model, the\r\nGraded Response Model (GRM).\r\nGraded Response Model\r\nGRM is the polytomous version of the 2PL model (Dai\r\net al., 2021). Despite able to constrain the discrimination\r\nparameter, GRM works differently than PCM. PCM estimates separate\r\ncategory response parameters for each item, while the GRM model further\r\nassumes that the thresholds for category response are also equal across\r\nitems (Nguyen\r\net al., 2014). Initially, we can try fitting the constrained version\r\nof Graded Response Model (GRM) that assumes equal a parameter\r\nacross items (similar to Rasch model). The model is fitted by\r\ngrm() as follows\r\n\r\n\r\nShow code\r\n\r\nmod_grm <- grm(Environment, constrained = TRUE)\r\nsummary(mod_grm)\r\n\r\n\r\n\r\nCall:\r\ngrm(data = Environment, constrained = TRUE)\r\n\r\nModel Summary:\r\n   log.Lik      AIC      BIC\r\n -1106.193 2238.386 2286.139\r\n\r\nCoefficients:\r\n$LeadPetrol\r\n        value\r\nExtrmt1 0.395\r\nExtrmt2 1.988\r\nDscrmn  2.218\r\n\r\n$RiverSea\r\n        value\r\nExtrmt1 1.060\r\nExtrmt2 2.560\r\nDscrmn  2.218\r\n\r\n$RadioWaste\r\n        value\r\nExtrmt1 0.832\r\nExtrmt2 1.997\r\nDscrmn  2.218\r\n\r\n$AirPollution\r\n        value\r\nExtrmt1 0.483\r\nExtrmt2 2.448\r\nDscrmn  2.218\r\n\r\n$Chemicals\r\n        value\r\nExtrmt1 0.855\r\nExtrmt2 2.048\r\nDscrmn  2.218\r\n\r\n$Nuclear\r\n        value\r\nExtrmt1 0.062\r\nExtrmt2 1.266\r\nDscrmn  2.218\r\n\r\n\r\nIntegration:\r\nmethod: Gauss-Hermite\r\nquadrature points: 21 \r\n\r\nOptimization:\r\nConvergence: 0 \r\nmax(|grad|): 0.0049 \r\nquasi-Newton: BFGS \r\n\r\nIf standard errors for the parameter estimates are required, you can\r\nadd the argument Hessian = T to the function\r\ngrm(). Similarly to our dichotomous case, the fit of the\r\nmodel can be checked using margins() for two-way\r\nmargins.\r\n\r\n\r\nShow code\r\n\r\nmargins(mod_grm)\r\n\r\n\r\n\r\nCall:\r\ngrm(data = Environment, constrained = TRUE)\r\n\r\nFit on the Two-Way Margins\r\n\r\n             LeadPetrol RiverSea RadioWaste AirPollution Chemicals\r\nLeadPetrol   -          10.03     9.98       5.19         7.85    \r\nRiverSea                -         5.06      17.12         2.56    \r\nRadioWaste                       -           6.78        20.60    \r\nAirPollution                                -             4.49    \r\nChemicals                                                -        \r\nNuclear                                                           \r\n             Nuclear\r\nLeadPetrol   16.93  \r\nRiverSea      7.14  \r\nRadioWaste   12.09  \r\nAirPollution  4.57  \r\nChemicals     3.85  \r\nNuclear      -      \r\n\r\nThe output above looks good as there is no indication of poor fit.\r\nNext, we will try with the three-way margins.\r\n\r\n\r\nShow code\r\n\r\nmargins(mod_grm, type = \"three\")\r\n\r\n\r\n\r\nCall:\r\ngrm(data = Environment, constrained = TRUE)\r\n\r\nFit on the Three-Way Margins\r\n\r\n   Item i Item j Item k (O-E)^2/E  \r\n1       1      2      3     28.52  \r\n2       1      2      4     34.26  \r\n3       1      2      5     29.91  \r\n4       1      2      6     42.74  \r\n5       1      3      4     33.03  \r\n6       1      3      5     66.72  \r\n7       1      3      6     65.31  \r\n8       1      4      5     25.48  \r\n9       1      4      6     34.46  \r\n10      1      5      6     39.49  \r\n11      2      3      4     29.63  \r\n12      2      3      5     37.74  \r\n13      2      3      6     32.50  \r\n14      2      4      5     27.08  \r\n15      2      4      6     36.77  \r\n16      2      5      6     19.49  \r\n17      3      4      5     38.99  \r\n18      3      4      6     26.91  \r\n19      3      5      6     39.62  \r\n20      4      5      6     22.25  \r\n\r\nBoth the two- and three-way residuals show a good fit of the\r\nconstrained model to the data, but checking the fit of the model in the\r\nmargins does not correspond to an overall goodness-of-fit test. As a\r\nresult, we will fit the unconstrained version of the GRM as well.\r\n\r\n\r\nShow code\r\n\r\nmod_grm_unconstrained <- grm(Environment) #unconstrained GRM\r\n\r\nsummary(mod_grm_unconstrained)\r\n\r\n\r\n\r\nCall:\r\ngrm(data = Environment)\r\n\r\nModel Summary:\r\n   log.Lik      AIC      BIC\r\n -1090.404 2216.807 2282.927\r\n\r\nCoefficients:\r\n$LeadPetrol\r\n        value\r\nExtrmt1 0.487\r\nExtrmt2 2.584\r\nDscrmn  1.378\r\n\r\n$RiverSea\r\n        value\r\nExtrmt1 1.058\r\nExtrmt2 2.499\r\nDscrmn  2.341\r\n\r\n$RadioWaste\r\n        value\r\nExtrmt1 0.779\r\nExtrmt2 1.793\r\nDscrmn  3.123\r\n\r\n$AirPollution\r\n        value\r\nExtrmt1 0.457\r\nExtrmt2 2.157\r\nDscrmn  3.283\r\n\r\n$Chemicals\r\n        value\r\nExtrmt1 0.809\r\nExtrmt2 1.868\r\nDscrmn  2.947\r\n\r\n$Nuclear\r\n        value\r\nExtrmt1 0.073\r\nExtrmt2 1.427\r\nDscrmn  1.761\r\n\r\n\r\nIntegration:\r\nmethod: Gauss-Hermite\r\nquadrature points: 21 \r\n\r\nOptimization:\r\nConvergence: 0 \r\nmax(|grad|): 0.003 \r\nquasi-Newton: BFGS \r\n\r\nWe can use a likelihood ratio test to check if the unconstrained\r\nversion GRM is better than its constrained one.\r\n\r\n\r\nShow code\r\n\r\nanova(mod_grm, mod_grm_unconstrained)\r\n\r\n\r\n\r\n Likelihood Ratio Table\r\n                          AIC     BIC  log.Lik   LRT df p.value\r\nmod_grm               2238.39 2286.14 -1106.19                 \r\nmod_grm_unconstrained 2216.81 2282.93 -1090.40 31.58  5  <0.001\r\n\r\nThe likelihood ratio test indicates that the unconstrained GRM is\r\npreferable for the Environment data. We can plot the Item Characteristic\r\nCurve (ICC) of all 6 items, Item Information Curve (IIC), and Test\r\nInformation Curve (TIC) below.\r\n\r\n\r\nShow code\r\n\r\npar(mar=c(2,6,2,2), mfrow = c(3, 3))\r\n\r\nplot(mod_grm_unconstrained, lwd = 2, cex = 0.6, legend = TRUE, cx = \"left\", \r\n     xlab = \"Latent Trait\", cex.main = 0.7, cex.lab = 0.7, cex.axis = 1)\r\n\r\n##############################################################\r\n\r\nplot(mod_grm_unconstrained, type = \"IIC\", lwd = 2, cex = 0.6, legend = TRUE, cx = \"topleft\",\r\n     xlab = \"Latent Trait\", cex.main = 0.7, cex.lab = 0.7, cex.axis = 1)\r\n\r\nplot(mod_grm_unconstrained, type = \"IIC\", items = 0, lwd = 2, xlab = \"Latent Trait\",cex.main = 0.7, cex.lab = 0.7, cex.axis = 1)\r\n\r\ninfo3 <- information(mod_grm_unconstrained, c(-4, 0))\r\ninfo4 <- information(mod_grm_unconstrained, c(0, 4))\r\n\r\ntext(-1.9, 8, labels = paste(\"Information in (-4, 0):\",\r\n                             paste(round(100 * info3$PropRange, 1), \"%\", sep = \"\"),\r\n                             \"\\n\\nInformation in (0, 4):\",\r\n                             paste(round(100 * info4$PropRange, 1), \"%\", sep = \"\")), cex = 0.5)\r\n\r\n\r\n\r\n\r\nFrom the Item Characteristic Curve, we observe that there is low\r\nprobability of endorsing the the first option, “very concerned”, for\r\nrelatively high latent trait levels, which means that the questions\r\nasked are not considered as major environmental issues by the\r\nrespondent.\r\nThe Test Information Curve also tells us that the test provides\r\n89% of the total information for high latent trait levels.\r\nFinally, the Item Information Curve indicates that items in\r\nLeadPetrol and Nuclear provide little information in the whole latent\r\ntrait continuum. We can check this in detail using\r\ninformation().\r\n\r\n\r\nShow code\r\n\r\ninformation(mod_grm_unconstrained, c(-4, 4))\r\n\r\n\r\n\r\nCall:\r\ngrm(data = Environment)\r\n\r\nTotal Information = 26.97\r\nInformation in (-4, 4) = 26.7 (98.97%)\r\nBased on all the items\r\n\r\nFor item 1 and item 6\r\n\r\n\r\nShow code\r\n\r\ninformation(mod_grm_unconstrained, c(-4, 4), items = c(1, 6)) #for item 1 and 6\r\n\r\n\r\n\r\nCall:\r\ngrm(data = Environment)\r\n\r\nTotal Information = 5.36\r\nInformation in (-4, 4) = 5.17 (96.38%)\r\nBased on items 1, 6\r\n\r\nWe observe that item 1 and 6 provide only the 5.36% of the total\r\ninformation (from the total of 26.97); Thus, they could probably be\r\nexcluded from a similar future study. Finally, a useful comparison is to\r\nplot the ICC of each response option separately.\r\n\r\n\r\nShow code\r\n\r\npar(mar=c(2,6,2,2), mfrow = c(2, 2))\r\n\r\nplot(mod_grm_unconstrained, category = 1, lwd = 2, cex = 0.7, legend = TRUE, cx = -4.5,\r\n     cy = 0.85, xlab = \"Latent Trait\", cex.main = 0.7, cex.lab = 0.7,\r\n     cex.axis = 1)\r\n\r\nfor (ctg in 2:3) \r\n  {\r\n  plot(mod_grm_unconstrained, category = ctg, lwd = 2, cex = 0.5, annot = FALSE,\r\n      xlab = \"Latent Trait\", cex.main = 0.7, cex.lab = 0.7,\r\n      cex.axis = 1)\r\n  }\r\n\r\n\r\n\r\n\r\nFrom the plot, the response option for RadioWaste and Chemicals have\r\nnearly identical characteristic curves for all categories, indicating\r\nthat these two items are probably regarded to have the same effect on\r\nthe environment.\r\nConclusion\r\nSo far, we have discussed what IRT is, including its model for\r\ndichotomous and polytomous test items. There are several models for us\r\nto choose from, and each model has its parameter we can adjust to suit\r\nour needs as well depending on how complex we want our model to be.\r\nThere is no right or wrong answer model selection. For example, if we\r\nwant the model to be as simple as possible with a dichotomous test (say,\r\na math test), we could go for Rasch model. If we want our model to be\r\nmore realistic, we may want to use 2PL or 3PL, which comes with expenses\r\nsuch as the need for more sample size or more difficulty to fit the\r\nmodel.\r\nHowever, given how useful IRT is in analyzing test items with\r\nseveral unique parameters, it doesn’t mean that we need to disregard the\r\nconcept of CTT in our practice. Both theories have its own contribution\r\nand usefulness. For example, CTT is more practical to implement in the\r\nclassroom setting where there is small amount of students and there is\r\nno need to investigate items at a deeper level. For example, if we want\r\nto use a classroom assessment for formative purposes (i.e., a practice\r\nquiz to help students prepare for the final exam), using CTT might be\r\nsufficient. If we want to develop a large-scale test to measure students\r\nat a national level, maybe IRT might be more appropriate to improve the\r\ntest with a more realistic model.\r\nHambleton\r\nand Jones (1993) did a really great job in comparing the difference\r\nbetween CTT and IRT in their work. I have presented the table below for\r\nyour information.\r\nThe Comparison between CTT and IRT Models (Hambleton\r\n& Jones, 1993, p.43)\r\nArea\r\nClassical Test Theory\r\nItem Response Theory\r\nModel\r\nLinear\r\nNon-linear\r\nLevel\r\nTest\r\nItem\r\nAssumptions\r\nWeak (i.e., easy to fit with test data)\r\nStrong (i.e., more difficult to meet with test data)\r\nItem-Ability Relationship\r\nNot Specified\r\nItem Characteristics Function\r\nExaminee Ability\r\nRepresented by test scores or estimated true scores\r\nRepresented by latent ability (Theta/θ)\r\nInvariance of item and person statistics\r\nUnavailable / item and person parameters are sample dependent\r\nItem and person parameters are sample dependent if the model fits\r\nthe data\r\nItem Statistics\r\np = item difficulty\r\nr = item discrimination\r\nItem discrimination (a), Item difficulty (b),\r\nguessing parameter (c)\r\nSample Size (for item parameter estimation)\r\n200-500\r\nMore than 500\r\nSimilar to CTT, IRT can be used to develop tests, scales,\r\nsurveys, or other measurement tools. The model can analyze item-level\r\ndata of both dichotomous (i.e., exams with true/false) and polytomous\r\n(i.e., surveys with no right/wrong answers) tests to provide information\r\non sensitivity of measurement across a range of latent trait. Knowing\r\ninformation like item difficulty, item discrimination, and item guessing\r\nis useful when building tests as we can examine which item is a good\r\nitem and which item is a not-so-useful one. For example, a test item\r\nthat is easy to guess might not be appropriate because everyone can do\r\nit, so it doesn’t really measure anything.\r\nIRT also allows us to put ability levels and difficulties of\r\nitems into the same scale to match the trait levels of a target\r\npopulation. If we want a test to measure someone with “just enough”\r\nknowledge to pass (say, a driver license exam), we can build a test to\r\nmeasure people with low to medium knowledge. I mean, a taxi driver or a\r\nracing driver know how to drive a car, and that is enough. However, if\r\nwe want to develop a test to select the best-of-the-best candidates for\r\nscholarship selection, we might want to build a difficult test to\r\nseparate low-to-mid tier students to high performance students. Anyway,\r\nthat is all for this post. Thank you so much for reading this! Have a\r\ngood day!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-15-irt/IIC.png",
    "last_modified": "2022-05-15T17:56:34-06:00",
    "input_file": {},
    "preview_width": 656,
    "preview_height": 551
  },
  {
    "path": "posts/2022-04-28-xai/",
    "title": "Making Sense of Machine Learning with Explanable Artificial Intelligence",
    "description": "I will be applying the methods of Explanable Artificial Intelligence (XAI) to extract interpretable insights from a classification model that predicts students' grade repetition.\n\n(14 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2022-05-05",
    "categories": [
      "Python",
      "Supervised Machine Learning"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nAddressing Sample\r\nImbalance\r\nRandom Forest Ensemble\r\nExplaining AI\r\nConclusion\r\n\r\nIntroduction\r\nIn my previous\r\npost on ensemble machine learning models, I mentioned that one major\r\ndrawback in the artificial intelligence (AI) field is the\r\nblack box problem, which hampers interpretability of the results\r\nfrom complex algorithms such as Random Forest or Extreme Gradient\r\nBoosting. Not knowing how the algorithm works behind the prediction\r\ncould reduce applicability of the method itself as the audience can’t\r\nfully comprehend the result and therefore unable to use it to inform\r\ntheir decisions; this problem could therefore damage trust from the\r\nstakeholders (users, policy makers, general audience) to the field as\r\nwell (McGovern et\r\nal., 2019).\r\nOn the developer’s side, fully understanding the machine learning\r\nmodels through the explanable approach (aka the white-box approach)\r\nallows developers to identify potential problems such as data\r\nleakage in the algorithm and fix (or debug) it with relative ease\r\n(Loyola-Gonzalez,\r\n2019). Further, knowing which variable affects the prediction the\r\nmost can inform feature engineering to reduce model complexity and\r\ndirect future data collection as well by focusing on collecting the\r\nvariables that matter (Becker\r\n& Cook, 2021).\r\nOn stakeholder’s side, it is important to emphasize model\r\nexplanability especially in industries such as healthcare, finances, and\r\nmilitary to foster trust between the people inside and outside of the\r\nfield that could lead to the extent that the result is used to inform\r\ndecisions made by humans such as financial credit approval (Becker\r\n& Cook, 2021; Loyola-Gonzalez,\r\n2019). Clearly Understanding how, where, and why the model also\r\nbenefits the model itself as users are able to identify potential\r\nproblems in its performance and provide the develoeprs with their\r\nfeedback (Velez\r\net al., 2021).\r\nThe above examples knowing how to extract human-understandable\r\ninsights from a complex machine learning model is important, especially\r\nin social science data where the theoretical part is as important as the\r\nmethodological and the practical part. For that reason, I will be\r\napplying the methods of Explanable Artificial Intelligence (XAI) to\r\nextract interpretable insights from a classification model that predicts\r\nstudents’ grade repetition. We will begin by setting up the environment\r\nas usual.\r\n\r\n\r\nShow code\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nfrom collections import Counter\r\n\r\nfrom imblearn.combine import SMOTEENN\r\n\r\nfrom sklearn.manifold import TSNE\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.model_selection import RepeatedStratifiedKFold\r\nfrom sklearn.model_selection import cross_val_score\r\nfrom sklearn.ensemble import RandomForestClassifier\r\n\r\nfrom sklearn.metrics import accuracy_score\r\n\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\n\r\nRANDOM_STATE = 123\r\n\r\nI will be using the same data set as my previous\r\npost about statistical learning, namely the Programme for\r\nInternational Student Assessment (PISA) 2018 (OECD,\r\n2019). However, the set of variables that I am examining will be\r\ndifferent as PISA contains several school-related variables that can be\r\nshifted as the researcher sees fit. For this post, I will predict\r\nstudents’ class repetition from 25 predictors (or features as called in\r\nthe field of machine learning) such as students’ socio-economic status,\r\nhistory of bullying involvement, and their learning motivation. The data\r\nis collected from Thai student in 2018.\r\n\r\n\r\nShow code\r\ndf = pd.read_csv(\"PISA_TH.csv\")\r\n\r\nX = df.drop('REPEAT', axis=1)\r\ny = df['REPEAT']\r\n\r\ndf.head()\r\n   REPEAT    ESCS  DAYSKIP  ...  Invest_effort  WEALTH  Home_resource\r\n0       0 -0.7914        1  ...              6  0.0721        -1.4469\r\n1       0  0.8188        1  ...              8 -0.3429         1.1793\r\n2       0  0.4509        1  ...             10  0.3031         1.1793\r\n3       0  0.7086        1  ...             10 -0.5893        -0.1357\r\n4       0  0.8361        1  ...             10  0.5406         1.1793\r\n\r\n[5 rows x 25 columns]\r\n\r\nAddressing Sample Imbalance\r\nThe problem is that our targeted variable is imbalance; that is,\r\nthe number of students who repeated a class is smaller than the number\r\nof students who did not. This situation makes sense in the real-world\r\ndata as normal samples are usually more prevalent than the abnormal\r\nones, but it is undesirable in the machine learning scenario as the\r\nmodel could recognize minority samples as unimportant and therefore\r\ndisregard them as noises (Chawla et al., 2022). As\r\na result, the model could give misleadingly optimistic performance on\r\nclassification datasets as it classifies only students who did not\r\nrepeat a class.\r\nSee the t-Distributed Stochastic Neighbor Embedding (tSNE) plot\r\nbelow for the visualization. There isn’t much samples of repeaters in\r\ncontrary to non-repeater students. Plus, the pattern is not prominent\r\nenough as the blut dots (repeaters) stay very close to the red dots\r\n(non-repeaters). This could make the pattern difficult to be learned by\r\nthe machine due to its ambiguity. One way we can mitigate this problem\r\nis to perform data augmentation via oversampling and undersampling,\r\nwhich synthesizes more minority samples and deletes or merges majority\r\nsamples to improve performance of the machine (Budhiman\r\net al., 2019; Wong et\r\nal;., 2016).\r\n\r\n\r\nShow code\r\nCounter(y)\r\nCounter({0: 8044, 1: 589})\r\n\r\nShow code\r\ntsne = TSNE(n_components=2, random_state=RANDOM_STATE)\r\n\r\nTSNE_result = tsne.fit_transform(X)\r\n\r\nplt.figure(figsize=(12,8))\r\nsns.scatterplot(TSNE_result[:,0], TSNE_result[:,1], hue=y, legend='full', palette=\"hls\")\r\n\r\n\r\nTo balance the data, I will use both oversampling and\r\nundersampling. Normal oversampling methods duplicates minority samples\r\nfor more sample size; however, this approach does not add any more\r\ninformation to the model (more of the same, basically). Instead, we can\r\nsynthesize minority samples by creating samples that are\r\nsimilar to the existing minority samples; this technique is\r\nnamed as Synthetic Minority Oversampling TEchnique\r\n(SMOTE) (He\r\nand Ma, 2013).\r\nAlso, we can further enhance the effectiveness of SMOTE by adding\r\nundersampling into the process (Chawla et al., 2022).\r\nInstead of randomly delete our majority samples, we will use the\r\nEdited Nearest Neighbor (ENN) method, which deletes\r\ndata points based on their neighbors to make the difference between\r\nmajority and minority samples (Ludera,\r\n2021). The combination of these two techniques is called SMOTEENN\r\nSee Figure 1 for the example of ENN from Guan et al.,\r\n(2009).\r\nENN Editing with 1-NN Classifier. No\r\ncopyright infringement is intended\r\n\r\nShow code\r\nsmote_enn = SMOTEENN(random_state=RANDOM_STATE, sampling_strategy = 'minority', n_jobs=-1)\r\n\r\nX_resampled, y_resampled = smote_enn.fit_resample(X, y)\r\n\r\nCounter(y_resampled)\r\nCounter({1: 8040, 0: 4794})\r\n\r\n\r\n\r\nShow code\r\ntsne = TSNE(n_components=2, random_state=RANDOM_STATE)\r\n\r\nTSNE_result = tsne.fit_transform(X_resampled)\r\n\r\nplt.figure(figsize=(12,8))\r\nsns.scatterplot(TSNE_result[:,0], TSNE_result[:,1], hue=y_resampled, legend='full', palette=\"hls\")\r\n\r\n\r\nThe second tSNE plot shows a more noticable pattern between student\r\nrepeaters and non-repeaters. The number of repeaters is increased while\r\nthe number of non-repeaters is decreased. Next, we can put our augmented\r\ndata into the Random Forest model for prediction.\r\nRandom Forest Ensemble\r\nWe will be splitting the data set into a training and a testing set\r\nas usual. For a quick recap, Random Forest is a machine learning model\r\nthat consists of several unique and uncorrelated decision trees; hence\r\nthe word Random in its name. Those trees work together to improve the\r\npredictive accuracy of that dataset than a single decision tree (Kubat,\r\n2017). The model will be evaluated with the repeated stratified\r\n10-folds technique to test our model prediction on different sets of\r\nunseen data to ensure its accuracy, especially in the case of imbalanced\r\ndata set.\r\n\r\n\r\nShow code\r\nCV = RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=RANDOM_STATE)\r\n\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size = 0.30, \r\n                                                    random_state = RANDOM_STATE)\r\n\r\n\r\n\r\nShow code\r\n# random forest model creation\r\nclf_rfc = RandomForestClassifier(random_state=RANDOM_STATE)\r\nclf_rfc.fit(X_train, y_train)\r\n\r\n# predictions\r\nRandomForestClassifier(random_state=123)\r\n\r\nShow code\r\nrfc_predict = clf_rfc.predict(X_test)\r\n\r\nrfc_cv_score = cross_val_score(clf_rfc, X_resampled, y_resampled, cv=CV, scoring='roc_auc')\r\n\r\nprint(\"=== All AUC Scores ===\")\r\n=== All AUC Scores ===\r\n\r\nShow code\r\nprint(rfc_cv_score)\r\n[0.98907675 0.99357898 0.99228597 0.99276793 0.99409399 0.99378369\r\n 0.9931644  0.9901627  0.98967714 0.99287747 0.99384328 0.99252177\r\n 0.99452348 0.99187137 0.99040419 0.99201929 0.9896265  0.99140778\r\n 0.99115331 0.99457436]\r\n\r\nShow code\r\nprint('\\n')\r\n\r\nShow code\r\nprint(\"=== Mean AUC Score ===\")\r\n=== Mean AUC Score ===\r\n\r\nShow code\r\nprint(\"Mean AUC Score - RandForest: \", rfc_cv_score.mean())\r\nMean AUC Score - RandForest:  0.9921707177188173\r\n\r\n\r\n\r\nShow code\r\n#define metrics for normal RF\r\nfrom sklearn import metrics\r\n\r\ny_pred_proba_rf = clf_rfc.predict_proba(X_test)[::,1]\r\nfpr_rf, tpr_rf, _ = metrics.roc_curve(y_test,  y_pred_proba_rf)\r\n\r\nauc_rf = metrics.roc_auc_score(y_test, y_pred_proba_rf)\r\nplt.plot(fpr_rf,tpr_rf, label=\"AUC for Random Forest Classifier = \"+str(auc_rf.round(3)))\r\n[<matplotlib.lines.Line2D object at 0x000001EA9D9FA340>]\r\n\r\nShow code\r\nplt.legend(loc=\"lower right\")\r\n<matplotlib.legend.Legend object at 0x000001EA9D9FA220>\r\n\r\nShow code\r\nplt.ylabel('True Positive Rate')\r\nText(0, 0.5, 'True Positive Rate')\r\n\r\nShow code\r\nplt.xlabel('False Positive Rate')\r\n           \r\nText(0.5, 0, 'False Positive Rate')\r\n\r\nShow code\r\nplt.title(\"Receiver-Operator Curve (ROC)\")\r\nText(0.5, 1.0, 'Receiver-Operator Curve (ROC)')\r\n\r\nShow code\r\nplt.show()\r\n\r\n\r\nThe results will be evaluated with the Receiver Operator\r\nCharacteristic (ROC) curve, which shows the diagnostic ability of binary\r\nclassifiers. One approach to use ROC is to evaluate its Area Under Curve\r\n(AUC), which measures of the ability of a classifier to distinguish\r\nbetween classes and is used as a summary of the ROC curve. The higher\r\nthe AUC, the better the performance of the model at distinguishing\r\nbetween the positive and negative classes. The mean of 20 rounds of\r\ntesting (randomly splitting the data into 10 stratified parts, repeated\r\nit for 2 times) looks good is around 0.99, meaning that there is a 99%\r\nchance that the model is able to correctly predict which student is a\r\nreapeater and which is not based on the data used to train the machine.\r\nNow we know that the model works well with our data, let us move on to\r\ninterpreting it with XAI techniques.\r\nExplaining AI\r\nXAI is a set of methods that allows a machine learning model and its\r\nresults understandable to human in terms of how it works in terms of\r\nprediction, including the impace of variables to the prediction results\r\n(Gianfagna\r\n& Di Cecco, 2021). The XAI methods that we will extract insights\r\nare permutation importance, partial dependence plot, and Shapley\r\nAdditive explanations (SHAP) values.\r\nPermutation Importance\r\nOne of the most basic questions we might ask of a model is: What\r\nfeatures have the biggest impact on predictions? This quention could be\r\nanswered through the examination of feature importance.\r\nThere are multiple ways to measure feature importance. One way is to\r\nextract the feature importance plot from the model itself as\r\ndemonstrated below.\r\n\r\n\r\nShow code\r\n# Create a pd.Series of features importances\r\nimportances_rf = pd.Series(clf_rfc.feature_importances_, index = X_resampled.columns)\r\n\r\n# Sort importances_rf\r\nsorted_importance_rf = importances_rf.sort_values()\r\n\r\n#Horizontal bar plot\r\nsorted_importance_rf.plot(kind='barh', color='lightgreen'); \r\nplt.xlabel('Feature Importance Score')\r\nText(0.5, 0, 'Feature Importance Score')\r\n\r\nShow code\r\nplt.ylabel('Features')\r\nText(0, 0.5, 'Features')\r\n\r\nShow code\r\nplt.title(\"Visualizing Important Features\")\r\nText(0.5, 1.0, 'Visualizing Important Features')\r\n\r\nShow code\r\nplt.show()\r\n\r\n\r\nAnother way, which we will focus on in this post, is to use the\r\npermutation importance score from the area of XAI. Permutation\r\nimportance is calculated by asking the following question: “If I\r\nrandomly shuffle a single column of the validation data, leaving the\r\ntarget and all other columns in place, how would that affect the\r\naccuracy of predictions in that now-shuffled data?”. Randomly\r\nre-ordering a single column should cause less accurate predictions,\r\nsince the resulting data no longer corresponds to anything observed in\r\nthe real world. Model accuracy especially suffers if we shuffle a column\r\nthat the model relied on heavily for predictions. In our case, if we\r\nmess with the “BEINGBULLIED” variable, the model would be severely\r\naffected by the reduced prediction accuracy. The same would happen to\r\nthe variable “Parent_emosup”, “Positive_feel” and so forth as well.\r\n\r\n\r\nShow code\r\nimport eli5\r\nfrom eli5.sklearn import PermutationImportance\r\n\r\nFEATURES = X_test.columns.tolist()\r\n\r\nperm = PermutationImportance(clf_rfc, random_state=RANDOM_STATE).fit(X_test, y_test)\r\neli5.show_weights(perm, feature_names = FEATURES, top = 10)\r\n<IPython.core.display.HTML object>\r\n\r\nPermutation ImportanceThe permutation importance results are consistent with the\r\nfeature importance score we extracted from the model. The values towards\r\nthe top are the most important features, and those towards the bottom\r\nmatter least. The first number in each row shows how much model\r\nperformance decreased with a random shuffling (in this case, using\r\n“accuracy” as the performance metric). Like most things in data science,\r\nthere is some randomness to the exact performance change from a\r\nshuffling a column. We measure the amount of randomness in our\r\npermutation importance calculation by repeating the process with\r\nmultiple shuffles. The number after the ± measures how performance\r\nvaried from one-reshuffling to the next.\r\nIn our example, the most important feature was “BEINGBULLIED”,\r\nwhich is the index of exposure to bullying. The index was constructed\r\nfrom questions that ask if students have experienced bullying in the\r\npast 12 months from statements such as “Other students left me out of\r\nthings on purpose”; “Other students made fun of me”; “I was threatened\r\nby other students”. Positive values on this scale indicate that the\r\nstudent was more exposed to bullying at school than the average student\r\nin OECD countries; negative values on this scale indicate that the\r\nstudent was less exposed to bullying at school than the average student\r\nacross OECD countries. This result is consistent with the literature\r\nthat students’ grade repetition is associated with the likelihood of\r\nbeing bullied (Lian\r\net al., 2021; Ozada\r\nNazim & Duyan, 2019)\r\nPartial Dependence Plots\r\nWhile feature importance shows what variables most affect\r\npredictions, partial dependence plots show how a feature affects\r\npredictions. For our case, partial dependence plots can be used to\r\nanswer questions such as “Controlling for all variables, what impact\r\ndoes the index of exposure to bullying have on the prediction of grade\r\nrepetition?”. The interpretation of partial dependence plot is somewhat\r\nsimilar to the interpretation of linear or logistic regression. On this\r\nplot, The y axis is interpreted as change in the prediction from what it\r\nwould be predicted at the baseline or leftmost value. A blue shaded area\r\nindicates level of confidence.\r\nThe plot below indicates that being subjected to bullying (as\r\nreflected by having positive value of the variable) increases the\r\nlikelihood of students to repeat a grade. Positive values in this index\r\nindicate that the student is more exposed to bullying at school than the\r\naverage student in OECD countries. Negative values in this index\r\nindicate that the student is less exposed to bullying at school than the\r\naverage student in OECD countries; therefore, having zero does not mean\r\nstudents did not experience any form of bullying, but rather\r\nexperiencing bullying to some degree (i.e., being bullied a bit).\r\nHowever, the predicting power does not change much after 0, meaning that\r\nthe amount of exposure to bullying does not matter in predicting\r\nstudents’ grade repetition.\r\n\r\n\r\nShow code\r\nfrom pdpbox import pdp\r\n\r\npdp_bullied = pdp.pdp_isolate(model=clf_rfc, dataset=X_test, model_features=FEATURES, feature='BEINGBULLIED')\r\n\r\npdp.pdp_plot(pdp_bullied, 'BEINGBULLIED')\r\n(<Figure size 1500x950 with 2 Axes>, {'title_ax': <AxesSubplot:>, 'pdp_ax': <AxesSubplot:xlabel='BEINGBULLIED'>})\r\n\r\nShow code\r\nplt.show()\r\n\r\n\r\nPartial Dependence Plots can also be used to examine interactions\r\nbetween variables as well. The graph below shows predictions for any\r\ncombination of students’ exposure to bullying and the amount of\r\nemotional support from parents. The prediction power is highest when\r\nstudents score 0 in the index of exposure to bullying (i.e., being\r\nbullied a bit) and having scores on the index of parents’ emotional\r\nsupport between -1.7 to +0.5. Positive values on this scale mean that\r\nstudents perceived greater levels of emotional support from their\r\nparents than did the average student across OECD countries while\r\nnegative value means otherwise. Having higher exposure to bullying\r\nreduces prediction power of the model as indicated by the changing color\r\nfrom yellow to green, and when the score in the index of emotional\r\nsupport reaches 1, the score of the exposure to bullying index becomes\r\nless matter as the prediction power reduces.\r\n\r\n\r\nShow code\r\nfeatures_to_plot = ['BEINGBULLIED', 'Parent_emosup']\r\n\r\ninter1  =  pdp.pdp_interact(model=clf_rfc, dataset=X_test, model_features=FEATURES, features=features_to_plot)\r\n\r\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\r\n(<Figure size 750x950 with 3 Axes>, {'title_ax': <AxesSubplot:>, 'pdp_inter_ax': <AxesSubplot:xlabel='BEINGBULLIED', ylabel='Parent_emosup'>})\r\n\r\nShow code\r\nplt.show()\r\n\r\n\r\nSHAP Values\r\nFinally, SHAP value allows us to interpret the prediction at a\r\nfine-grained level to the components of individual predictions to show\r\nthe impact of each feature. For our case, CHAP value can be used to\r\nanswer questions like “On what basis did the model predict that student\r\nA is likely to repeat a grade?”. The plot is quite straightforward to\r\ninterpret. The red part shows what increases the likelihood of repeating\r\na grade, and the blue part shows what decreases the likelihood of\r\nrepeating a grade.\r\nOn the plot below, the prediction is at the base alue of 0.60,\r\nmeaning that it is the average of the model output. For this particular\r\nstudent, their likelihood to repeat a grade is increased by being\r\nexposed to bullying (BEINGBULLIED), having mediocre emotional support\r\nfrom parents (Parent_emosup), and having poor overall social standing as\r\nindicated by -0.9 the variable the index of socio-economic, social and\r\ncultural status (ESCS). However, the likelihood is decreased by their\r\neducational resources at home (Home_resource), having cooperative class\r\n(Stu_coop), their parents’ occupational status (Parent_occupation), and\r\nhaving low record of class skipping (CLASSKIP).\r\nSHAP Value for a predictionSHAP Summary Plot\r\nIn addition to the breakdown of each individual prediction, we\r\ncan also visualize groups of SHAP values with SHAP summary plot and SHAP\r\ndependence contribution plot. SHAP summary plots give us an overview of\r\nfeature importance and what is driving the prediction. This plot is made\r\nof many dots. Each dot has three characteristics as follows: a)\r\nhorizontal location (the x-axis) that indicates whether the effect of\r\nthat value caused a higher or lower prediction; b) vertical location\r\n(the y-axis) that indicates the variable name, in order of importance\r\nfrom top to bottom. c) Gradient color indicates the original value for\r\nthat variable. In booleans (i.e., yes/no variable), it will take two\r\ncolors, but in number it can contain the whole spectrum.\r\nFor example, the left most point in the ‘Parent_emosup’ row is\r\nred in color, meaning that for that particular student, having greater\r\nlevels of emotional support from their parents reduces their likelihood\r\nof repeating a grade by roughly 0.3. Seeing variables have a wide spread\r\nin range can be inferred that permutation importance is high; however,\r\nit is best to use permutation importance to measure which variable is\r\nimportant to the prediction.\r\nSome features such as Home_resource (educational\r\nresource at home) have reasonably clear separation between the blue and\r\npink dots, which implies a straightforward meaning that the increase in\r\nthe variable value lower (i.e., more resource) the likelihood of\r\nrepeating a grade while the decrease in educational resource impacts the\r\nvariable in the other direction (higher chance to repeat a\r\ngrade).\r\nHowever, some variables such as Stu_coop (the degree\r\nof cooperativeness within classrooms) have blue and pink dots jumbled\r\ntogether, suggesting that the increase in this variable leads to higher\r\npredictions, and other times it leads to a lower prediction. In other\r\nwords, both high and low values of the variable can have both positive\r\nand negative effects on the prediction. The most likely explanation for\r\nthis “jumbling” of effects is that the variable (in this case\r\nStu_coop) has an interaction effect with other variables.\r\nFor example, there may be some situations where cooperating with other\r\nstudents lead to social\r\nloafing - when stduents contribute less effort when working as a\r\ngroup, and therefore learns less. This interaction needs further\r\ninvestigation.\r\n\r\n\r\nShow code\r\nshap_values_summary = explainer.shap_values(X_test)\r\nshap.summary_plot(shap_values[1], X_test)\r\n\r\nSHAP Summary PlotSHAP Dependence Contribution\r\nPlot\r\nThe earlier Partial Dependence Plots to show how a single feature\r\nimpacts predictions. This is insightful and relevant for many real-world\r\nuse cases. The interpretation is also friendly to non-technical audience\r\nas well. However, there is a lot that we still don’t know; for example,\r\nwhat is the distribution of effects? Is the effect of having a certain\r\nvalue pretty constant, or does it vary a lot depending on the values of\r\nother feaures. SHAP dependence contribution plots provide a similar\r\ninsight to the partial dependence plot, but they add a lot more detail.\r\nThe plot shows scatter dots that explain how the effect a single feature\r\nhas on the predictions made by the model. The plot can be read as\r\nfollows: a) The x-axis is the value of the feature; b) The y-axis is the\r\nSHAP value for that feature, which represents how much knowing that\r\nfeature’s value changes the output of the model prediction; c) The color\r\ncorresponds to a second feature that may have an interaction effect with\r\nthe feature we are plotting.\r\nThe plot below shows the relatively flat trend of the\r\nBEINGBULLIED feature, meaning that this variable does\r\nimpact the prediction regardless of the value; this trend is consistent\r\nwith the partial dependence plot shown earlier in the post. However,\r\nthere is a sign of interaction as there are points with similar value\r\nthat produce different outcome. See the left of the 2D pane, for\r\nexample. For some students, being less exposed to bullying gives them\r\nmore chance to repeat a grade while some students got less chance. There\r\nmight be other features that interact with this variable.\r\nWhile the primary trend is that being bullied increases the\r\nchance of repeating a grade , there are some variations that can be\r\nexplained by the interaction of features as well. For a concrete\r\nexplanation, see the right of the 2D pane. Being positioned overthere\r\nmeans that those students experience a lot of bullying, but their chance\r\nof repeating a grade is relatively lower than those who experience less\r\nbullying. One explanation is that some of those students have positive\r\nfeelings for themselves (indicated by the red color), which could make\r\nthem more resilient toward being bullied.\r\n\r\n\r\nShow code\r\nshap.dependence_plot('BEINGBULLIED', shap_values_summary[1], X_test, interaction_index=\"Positive_feel\")\r\n\r\nSHAP Dependence Contribution\r\nPlotConclusion\r\nWhat we have done so far is making a prediction with a Random Forest\r\nEnsemble model, which has high predictive power at the price of being\r\nchallenging to explain due to its complexity (Zhang &\r\nWang, 2009). XAI tools such as permutation importance, partial\r\ndependence plot, and SHAP values, allow us to understand outputs of the\r\nmodel at various levels from the overall picture to fine-grained\r\nindividual cases. Knowing how predictions are made also allow\r\nestablishes venues for future studies as well. XAI results are important\r\nto bridge the knowledge gap between technical (e.g., developers) and\r\nnon-technical (e.g., customers, users) audiences, which could build\r\ntrust and confidence when putting the AI models into the actual use. XAI\r\nalso helps an organization develop a responsible approach to AI\r\ndevelopment by avoiding the reliance on results that we do not\r\nunderstand to inform our decisions.\r\nHowever, note that XAI is not perfect. Its results are\r\ncontext-dependent, meaning that if the context changes, so does the\r\nresult (de\r\nBrujin et al., 2021). The prediction and how it happens can only be\r\nused as a factor to be considered along with other lines of evidence\r\nsuch as expert opinion, counter explanations, and potential\r\nconsequences. Regardless, XAI is still a useful too to have in expanding\r\nthe knowledge we get from machine learning. Thank you very much for\r\nreading!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-28-xai/xai_files/figure-html5/unnamed-chunk-14-11.png",
    "last_modified": "2022-04-29T11:06:10-06:00",
    "input_file": {},
    "preview_width": 1440,
    "preview_height": 1824
  },
  {
    "path": "posts/2022-04-28-semisupervised/",
    "title": "Addressing Data Imbalance with Semi-Supervised Learning",
    "description": "For this post, I will use semi-supervised learning approach to perform a classification task with a highly imbalance data.  \n\n(7 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2022-04-28",
    "categories": [
      "Python",
      "Unsupervised Machine Learning",
      "Supervised Machine Learning"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nData Set Preparation\r\nVisualize Fraud\r\nand NonFraud Transactions\r\nAutoEncoders\r\nObtain the Generated\r\nData\r\nLinear Classifier\r\nConclusion\r\n\r\nIntroduction\r\nMachine\r\nLearning (ML) is a process of learning the best possible and most\r\nrelevant patterns, relationships, or associations from a data set to\r\npredict the outcomes on unseen data. Broadly, there are three types of\r\nML process:\r\nSupervised Learning, which is a process that trains a ML model on\r\na labelled data set. The model aims to find the relationships among the\r\nindependent and dependent variable to predict unseen data we may receive\r\nin the future.\r\nUnsupervised Learning, which is a process of training a ML model\r\non a dataset in which the target variable is not known. The model aims\r\nto find the most relevant patterns in the data or the segments of\r\ndata.\r\nSemi-Supervised Learning, which is a combination of supervised\r\nand unsupervised learning processes in which the unlabeled data is used\r\nto train a model as well. In this approach, the properties of\r\nunsupervised learning are used to learn the best possible representation\r\nof data, and the properties of supervised learning are used to learn the\r\nrelationships to make predictions.\r\nEach ML algorithm has its own use under the consideration of a)\r\nthe size, quality, and nature of data, b) The available computational\r\ntime, c) The urgency of the task, and d) the expected result. Among the\r\nvast availability of algorithms, each ML type has its own role in\r\ntackling different types of data science problem depending on the\r\nmentioned considerations. The ML algorithm cheat sheet below is a good\r\nstarting point to choose algorithms that are appropriate for your\r\nspecific problems.\r\nImage from https://blogs.sas.com/content/subconsciousmusings/2020/12/09/machine-learning-algorithm-use/.\r\nNo copyright infringement is intendedWe have previously discussed about the use supervised and\r\nunsupervised ML separately in other posts. However, the problem we will\r\ntackle in this post requires the ability of both unsupervised and\r\nsupervised learning, which is why I will use semi-supervised learning in\r\nthis post to solve data imbalance problem by generating a pseudo-labeled\r\ndata and use it to train an ML model to perform a classification.\r\nImage from https://teksands.ai/blog/semi-supervised-learning. No\r\ncopyright infringement is intendedData Set Preparation\r\nSemi-supervised learning is a restatement of the missing data\r\nimputation problem, which is specific to small data set with\r\nmissing-label cases. This problem is commonly encountered in data set\r\ngeneration contexts as retrieving clean data can be costly and time\r\nconsuming. Applying supervised machine learning techniques to small data\r\nset might yield poor results that cannot be further used; thus, it would\r\nbe more useful to address this problem with a combination of both\r\nmachine learning approaches (i.e., unsupervised and supervised) for\r\noptimal results.\r\nFor this post, I will be using the Credit\r\nCard Fraud Detection data set from the Université Libre de Bruxelles (Brussels,\r\nBelgium) machine learning group The dataset contains transactions\r\nmade by credit cards in September 2013 by European cardholders. This\r\ndataset presents transactions that occurred in two days, where we have\r\n492 fraud transactions out of 284,807 transactions. We will first\r\nimporting in Python modules and the data set as usual.\r\n\r\n\r\nShow code\r\nfrom keras.layers import Input, Dense\r\nfrom keras.models import Model, Sequential\r\nfrom keras import regularizers\r\nfrom sklearn.model_selection import train_test_split \r\nfrom sklearn import metrics\r\nfrom sklearn.metrics import classification_report, accuracy_score\r\nfrom sklearn.manifold import TSNE\r\nfrom sklearn import preprocessing \r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.metrics import confusion_matrix\r\n\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd \r\nimport numpy as np\r\nimport seaborn as sns\r\n\r\n\r\n\r\nShow code\r\nsns.set(style=\"whitegrid\")\r\nnp.random.seed(203)\r\n\r\ndata = pd.read_csv(\"creditcard.csv\")\r\ndata[\"Time\"] = data[\"Time\"].apply(lambda x : x / 3600 % 24)\r\ndata.head()\r\n       Time        V1        V2        V3  ...       V27       V28  Amount  Class\r\n0  0.000000 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0\r\n1  0.000000  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69      0\r\n2  0.000278 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66      0\r\n3  0.000278 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50      0\r\n4  0.000556 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99      0\r\n\r\n[5 rows x 31 columns]\r\n\r\nTo check if the data is imbalance, we will count the target variable\r\nby its value (i.e., 0, 1). The result below indicates that the data is\r\nhighly imbalance as only 0.17 % cases are fraud transactions. This\r\nproblem could lower performance of a supervised ML model as the machine\r\nlearns only one class (0) and discard the other class as noises.\r\n\r\n\r\nShow code\r\nvc = data['Class'].value_counts().to_frame().reset_index()\r\nvc['percent'] = vc[\"Class\"].apply(lambda x : round(100*float(x) / len(data), 2))\r\nvc = vc.rename(columns = {\"index\" : \"Target\", \"Class\" : \"Count\"})\r\nvc\r\n   Target   Count  percent\r\n0       0  284315    99.83\r\n1       1     492     0.17\r\n\r\nWe will also sample 1000 non-fraud data and merge it with fraud data\r\nto form a data set for this task.\r\n\r\n\r\nShow code\r\nnon_fraud = data[data['Class'] == 0].sample(1000)\r\nfraud = data[data['Class'] == 1]\r\n\r\ndf = non_fraud.append(fraud).sample(frac=1).reset_index(drop=True)\r\nX = df.drop(['Class'], axis = 1).values\r\nY = df[\"Class\"].values  \r\n\r\nVisualize Fraud and\r\nNonFraud Transactions\r\nLet’s visualize the nature of fraud and non-fraud transactions using\r\nT-SNE. T-SNE (t-Distributed Stochastic Neighbor Embedding) is an\r\nunsupervised technique primarily used for data exploration and\r\nvisualizing high-dimensional data. The technique reduces dimensionality\r\nof the data set and produces only components with maximum information.\r\nEvery dot in the following represents a transaction. Non Fraud\r\ntransactions are coded as “0” - the red dot, while Fraud transactions\r\nare coded as “1” - the blue dot. The two axis are the components\r\nextracted by T-SNE.\r\n\r\nShow code\r\ntsne = TSNE(n_components=2, random_state=0)\r\n\r\nTSNE_result = tsne.fit_transform(X)\r\n\r\nsns.scatterplot(TSNE_result[:,0], TSNE_result[:,1], hue=Y, legend='full', palette=\"hls\")\r\n\r\n\r\nThe graph above shoes that there are many non_fraud transactions\r\nwhich are very close to fraud transactions. This pattern makes the data\r\nchallenging to classify as they are ambiguous. This problem can be\r\nmitigated by using AutoEncoders, which is another unsupervised learning\r\nalgorithm.\r\nAutoEncoders\r\nAutoencoders are a special type of neural network architectures\r\nin which the output is same as the input. In other words, an\r\nautoencoder, once trained on appropriate training data, can be used to\r\ngenerate compressed copies of input data point while preserve most of\r\nthe information (features) from the input. For our case, the model will\r\ntry to learn the best representation of non-fraud cases and generate the\r\nrepresentations of fraud cases.\r\nAs for how it works, Autoencoders are designed to have a\r\nbottle-neck architecture with a few neurons to comprehensively compress\r\nthe knowledge about representation of the original input for the machine\r\nto reproduce it by using decoders. The picture below shows architecture\r\nof the machine in how it learns from the input data.\r\n\r\n- First, we will create a network with one input layer and one output\r\nlayer. Both of them will have identical dimensions.\r\n\r\n\r\nShow code\r\n## input layer \r\ninput_layer = Input(shape=(X.shape[1],))\r\n\r\n## encoding part\r\nencoded = Dense(100, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(input_layer)\r\nencoded = Dense(50, activation='relu')(encoded)\r\n\r\n## decoding part\r\ndecoded = Dense(50, activation='tanh')(encoded)\r\ndecoded = Dense(100, activation='tanh')(decoded)\r\n\r\n## output layer\r\noutput_layer = Dense(X.shape[1], activation='relu')(decoded)\r\n\r\nNext, we will create the model architecture by compiling the above\r\ninput layer and output layers. We will also add the optimizer and loss\r\nfunction. I am using “adadelta” as the optimizer and “mse” as the loss\r\nfunction. I will also transform the data with min-max scaling to make it\r\neasier for the model to process.\r\n\r\n\r\nShow code\r\nautoencoder = Model(input_layer, output_layer)\r\nautoencoder.compile(optimizer=\"adadelta\", loss=\"mse\")\r\n\r\n\r\n\r\nShow code\r\nx = data.drop([\"Class\"], axis=1)\r\ny = data[\"Class\"].values\r\n\r\nx_scale = preprocessing.MinMaxScaler().fit_transform(x.values)\r\nx_norm, x_fraud = x_scale[y == 0], x_scale[y == 1]\r\n\r\nOne advantage of using autoencoders is that we do not need a large\r\namount of data to teach the machine. We will use only 2000 rows of non\r\nfraud cases to train the autoencoder. The choice of small samples from\r\nthe original data set is based on the intuition that one class\r\ncharacteristics (non fraud) will differ from that of the other (fraud).\r\nTo distinguish these characteristics, we need to show the autoencoders\r\nonly one class of data. This is because the autoencoder will try to\r\nlearn only one class and automatically distinguish the other class.\r\n\r\n\r\nShow code\r\nautoencoder.fit(x_norm[0:2000], x_norm[0:2000], \r\n                batch_size = 256, epochs = 10, \r\n                shuffle = True, validation_split = 0.20)\r\n\r\nObtain the Generated Data\r\nAfter training the model, we can obtain representation of the data\r\n(aka its latent pattern) to learn how the data is distributed by\r\naccessing the model. A latent variable is a random variable that cannot\r\nbe observed directly, but it lays the foundation of how the data is\r\ndistributed. Latent variables also give us a low-level representation of\r\nhigh-dimensional data. They give us an abstract representation of how\r\nthe data is distributed. We will also extract the generated “fraud” and\r\n“non-fraud” data to create a new data set to be used. We will also\r\nvisualize the new data for comparison.\r\n\r\n\r\nShow code\r\nhidden_representation = Sequential()\r\nhidden_representation.add(autoencoder.layers[0])\r\nhidden_representation.add(autoencoder.layers[1])\r\nhidden_representation.add(autoencoder.layers[2])\r\n\r\n\r\n\r\nShow code\r\nnorm_hid_rep = hidden_representation.predict(x_norm[:3000])\r\nfraud_hid_rep = hidden_representation.predict(x_fraud)\r\n\r\n\r\n\r\nShow code\r\nnew_x = np.append(norm_hid_rep, fraud_hid_rep, axis = 0)\r\nnew_y_not_fraud = np.zeros(norm_hid_rep.shape[0])\r\nnew_y_fraud = np.ones(fraud_hid_rep.shape[0])\r\nnew_y = np.append(new_y_not_fraud, new_y_fraud)\r\n\r\n\r\n\r\nShow code\r\nTSNE_result_new = tsne.fit_transform(new_x)\r\nsns.scatterplot(TSNE_result_new[:,0], TSNE_result_new[:,1], hue=new_y, legend='full', palette=\"hls\")\r\n\r\n\r\nThe new TSNE plot shows that both fraud and non-fraud transactions\r\nare pretty visible and linearly separable. We can use simple linear\r\nmodels such as logistic regression to classify this data without the\r\nneeds for complex models like Random Forest that could be harder to\r\ninterpret.\r\nLinear Classifier\r\nAs usual, we will divide the new data set into a training and a\r\ntesting asset before passing them to a logistic regression model. We\r\nwill also request for a confusion matrix to map out a summary of\r\nprediction results.\r\n\r\n\r\nShow code\r\nX_train, X_test, y_train, y_test = train_test_split(new_x, new_y, test_size=0.3)\r\n\r\nclf_lr = LogisticRegression(max_iter = 450, random_state = 123)\r\nclf_lr.fit(X_train,y_train)  \r\nLogisticRegression(max_iter=450, random_state=123)\r\n\r\nShow code\r\npred_lr = clf_lr.predict(X_test)\r\nprint(\"Accuracy: {:0.4f}\".format(accuracy_score(y_test, pred_lr)))  \r\nAccuracy: 0.9781\r\n\r\nShow code\r\nreport_lr = classification_report(y_test, pred_lr)\r\nprint(report_lr)  \r\n              precision    recall  f1-score   support\r\n\r\n         0.0       0.97      1.00      0.99       894\r\n         1.0       1.00      0.85      0.92       154\r\n\r\n    accuracy                           0.98      1048\r\n   macro avg       0.99      0.93      0.95      1048\r\nweighted avg       0.98      0.98      0.98      1048\r\n\r\n\r\n\r\nShow code\r\nconf_matrix = confusion_matrix(y_test, pred_lr)\r\nsns.heatmap(conf_matrix.T, square=True, annot=True, fmt='d', cbar=False)\r\nplt.xlabel('true label')\r\nplt.ylabel('predicted label')\r\nplt.title(\"Confusion matrix of Logistic Regression\")\r\nplt.show()\r\n\r\n\r\nConclusion\r\nOur logistic regression model shows a satisfaction result with the\r\naccuracy of 99.52%. The application of autoencoders as the unsupervised\r\npart in this task shows that semi-supervised learning can be effective\r\nat handling the case where there is a lack of labeled data to train our\r\nmachine. Manual data labeling by human is an expensive and\r\ntime-consuming process, which is why this approach to machine learning\r\ncould be very helpful in lessening the task on our end. Semi-supervised\r\nlearning can be used for the classification of web pages, audio, images,\r\nor even textual and numerical data where human labeling is not possible.\r\nAs always, thank you for reading this!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-28-semisupervised/semi-ml.png",
    "last_modified": "2022-04-28T22:26:40-06:00",
    "input_file": {},
    "preview_width": 900,
    "preview_height": 450
  },
  {
    "path": "posts/2022-04-18-unsupervisedml/",
    "title": "Examining Customer Cluster with Unsupervised Machine Learning",
    "description": "In this post, I will be using two unsupervised learning techniques with a data set, namely K-means clustering and Hierarchical clustering, to determine groups of customers from their age, income, and spending behavior data.  \n\n(8 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2022-04-18",
    "categories": [
      "Python",
      "Unsupervised Machine Learning"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nAbout the Data Set\r\nK-Means Clustering\r\nHierarchical Clustering\r\nConclusion\r\n\r\nIntroduction\r\nA lot of my recent posts are about supervised machine learning problems, which is defined by its use of labeled datasets to train algorithms to classify or predict outcomes of the unseen data set. In other words, the model was trained (or supervised) by humans to achieve the best possible predictive capability. However, unsupervised machine learning is defined in opposition to supervised learning. Unsupervised learning, in contrast, is learning without labels. It is pure pattern discovery, unguided by a prediction task. The model learns from raw data without any prior knowledge or human training.\r\nFor example, you have a group of customers with a variety of characteristics such as age, location, and financial history. You wish to discover patterns and sort them into natural “clusters” without tampering with them in any ways. Or perhaps you have a set of texts, such as Wikipedia pages, and you wish to segment them into categories based on their content. These are examples of unsupervised learning techniques called “clustering” and “dimension reduction”.\r\nUnsupervised learning is called as such because you are not guiding the pattern discovery by some prediction task, but instead uncovering hidden structure from unlabeled data. In this post, I will be using two unsupervised learning techniques with a data set, namely K-means clustering and Hierarchical clustering, to determine groups of customers from their age, income, and spending behavior data. As usual, we will import the necessary modules first to set up the environment.\r\n\r\n\r\nShow code\r\nimport numpy as np \r\nimport pandas as pd \r\nimport matplotlib.pyplot as plt \r\nimport seaborn as sns\r\n\r\nfrom sklearn.cluster import KMeans\r\nimport scipy.cluster.hierarchy as sch\r\nfrom sklearn.cluster import AgglomerativeClustering \r\n\r\nimport warnings\r\nwarnings.filterwarnings('ignore')\r\n\r\nAbout the Data Set\r\nThis data set is created by Kaggle for the learning purpose of the data segmentation concepts. The scenario is as follows: you are owning a supermarket mall. Through membership cards , you have some basic data about your customers like Customer ID, age, gender, annual income and spending score. Spending Score is something you assign to the customer based on your defined parameters like customer behavior and purchasing data. Specifically put, if you buy a lot, you get a lot of scores to be posted on a top spender board. Below is the information about the data set we will be using in this task.\r\n\r\n\r\nShow code\r\ndf = pd.read_csv(\"customers.csv\")\r\n\r\n# data set shape\r\nprint(\"The data set has\", df.shape[0], \"cases and\", df.shape[1], \"variables\")\r\n\r\n# print head of data set\r\nThe data set has 200 cases and 5 variables\r\n\r\nShow code\r\nprint(df.head(10))\r\n   CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)\r\n0           1       1   19                  15                      39\r\n1           2       1   21                  15                      81\r\n2           3       0   20                  16                       6\r\n3           4       0   23                  16                      77\r\n4           5       0   31                  17                      40\r\n5           6       0   22                  17                      76\r\n6           7       0   35                  18                       6\r\n7           8       0   23                  18                      94\r\n8           9       1   64                  19                       3\r\n9          10       0   30                  19                      72\r\n\r\nShow code\r\ndf.info()\r\n<class 'pandas.core.frame.DataFrame'>\r\nRangeIndex: 200 entries, 0 to 199\r\nData columns (total 5 columns):\r\n #   Column                  Non-Null Count  Dtype\r\n---  ------                  --------------  -----\r\n 0   CustomerID              200 non-null    int64\r\n 1   Gender                  200 non-null    int64\r\n 2   Age                     200 non-null    int64\r\n 3   Annual Income (k$)      200 non-null    int64\r\n 4   Spending Score (1-100)  200 non-null    int64\r\ndtypes: int64(5)\r\nmemory usage: 7.9 KB\r\n\r\nShow code\r\ndf.describe()\r\n\r\n#Check for missing data\r\n       CustomerID      Gender  ...  Annual Income (k$)  Spending Score (1-100)\r\ncount  200.000000  200.000000  ...          200.000000              200.000000\r\nmean   100.500000    0.440000  ...           60.560000               50.200000\r\nstd     57.879185    0.497633  ...           26.264721               25.823522\r\nmin      1.000000    0.000000  ...           15.000000                1.000000\r\n25%     50.750000    0.000000  ...           41.500000               34.750000\r\n50%    100.500000    0.000000  ...           61.500000               50.000000\r\n75%    150.250000    1.000000  ...           78.000000               73.000000\r\nmax    200.000000    1.000000  ...          137.000000               99.000000\r\n\r\n[8 rows x 5 columns]\r\n\r\nShow code\r\ndf.isnull().sum()\r\nCustomerID                0\r\nGender                    0\r\nAge                       0\r\nAnnual Income (k$)        0\r\nSpending Score (1-100)    0\r\ndtype: int64\r\n\r\nWe will first begin by exploring distribution of the data set to examine if their distribution is normal. Then, we can examine the distribution of each variable pair as indicated by density plots and scatter plots. We will also examine correlation coefficients between all variables with a heatmap and potential clusters with a cluster map as well.\r\n\r\n\r\nShow code\r\nplt.figure(1 , figsize = (15 , 6))\r\nn = 0 \r\nfor x in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\r\n    n += 1\r\n    plt.subplot(1 , 3 , n)\r\n    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\r\n    sns.distplot(df[x] , bins = 15)\r\n    plt.title('Distplot of {}'.format(x))\r\n    \r\nplt.show()\r\n\r\n\r\n\r\n\r\nShow code\r\nsns.heatmap(df.corr(), annot=True, cmap=\"YlGnBu\")\r\nplt.show()\r\n\r\n\r\n\r\n\r\nShow code\r\nplt.figure(1, figsize = (16 ,8))\r\nsns.clustermap(df)\r\n\r\n\r\n\r\n\r\nShow code\r\nsns.pairplot(df, vars = ['Spending Score (1-100)', 'Annual Income (k$)', 'Age'], hue = \"Gender\")\r\n\r\n\r\nK-Means Clustering\r\nK-means clustering is one of the simplest and popular unsupervised machine learning algorithms. For this method, we define a target number k, which refers to the number of centroids you need in the dataset. A centroid is the imaginary or real location representing the center of the cluster; then, the algorithm allocates every data point to the nearest cluster, while keeping the centroids as small as possible. The means in the K-means refers to averaging of the data in finding their corresponding centroids.\r\nInstead of using equations, this short animation by Allison Horst explains k-means clustering in a very cute and comprehensive way.\r\nThe Process of K-means Clustering by Allison Horst. No copyright infringement intended\r\n\r\nShow code\r\nplt.figure(1 , figsize = (15 , 7))\r\nplt.title('Scatter plot of Age v/s Spending Score', fontsize = 20)\r\nplt.xlabel('Age')\r\nplt.ylabel('Spending Score')\r\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, s = 100)\r\nplt.show()\r\n\r\n\r\nClustering is basically the process of mapping data points into classes of similar objects. The scatter plot above maps the relationship between customers’ age and their spending score. It could be challenging at this point to determine how many clusters can be seen in this 2D pane. This is where clustering comes into play. We will first examine how many clusters should we group the data points into with the elbow method. To find the optimal K for a dataset, we need to find the point where the decrease in inertia begins to slow. K=3 is the “elbow” of this graph, so the points after that are K=4 or K=5.\r\n\r\n\r\nShow code\r\nX1 = df[['Age' , 'Spending Score (1-100)']].iloc[: , :].values\r\ninertia = []\r\n\r\nfor n in range(1 , 15):\r\n    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \r\n                        tol=0.0001,  random_state= 111  , algorithm='full') )\r\n    algorithm.fit(X1)\r\n    inertia.append(algorithm.inertia_)\r\n    \r\nKMeans(algorithm='full', n_clusters=1, random_state=111)\r\nKMeans(algorithm='full', n_clusters=2, random_state=111)\r\nKMeans(algorithm='full', n_clusters=3, random_state=111)\r\nKMeans(algorithm='full', n_clusters=4, random_state=111)\r\nKMeans(algorithm='full', n_clusters=5, random_state=111)\r\nKMeans(algorithm='full', n_clusters=6, random_state=111)\r\nKMeans(algorithm='full', n_clusters=7, random_state=111)\r\nKMeans(algorithm='full', random_state=111)\r\nKMeans(algorithm='full', n_clusters=9, random_state=111)\r\nKMeans(algorithm='full', n_clusters=10, random_state=111)\r\nKMeans(algorithm='full', n_clusters=11, random_state=111)\r\nKMeans(algorithm='full', n_clusters=12, random_state=111)\r\nKMeans(algorithm='full', n_clusters=13, random_state=111)\r\nKMeans(algorithm='full', n_clusters=14, random_state=111)\r\n\r\nShow code\r\nplt.figure(1 , figsize = (15 ,6))\r\nplt.plot(np.arange(1 , 15) , inertia , 'o')\r\nplt.plot(np.arange(1 , 15) , inertia , '-' , alpha = 0.5)\r\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\r\n(Text(0.5, 0, 'Number of Clusters'), Text(0, 0.5, 'Inertia'))\r\n\r\nShow code\r\nplt.show()\r\n\r\n\r\nThe elbow plot shows that grouping the data into 4 or 5 clusters seems to be the best as the inertia, which measures how well a dataset was clustered by K-Means, seems to reach the optimal point at 4 and 5 clusters. A good model is the one with low inertia AND a low number of clusters (K). However, this is a tradeoff because as K increases, inertia decreases. We want the inertia to be as low as possible, but we don’t want to have 12+ clusters as well (I mean, why would you group your data anyway if you want it to have a lot of clusters). We can try clustering the data into 4 clusters first, then we can go with 5 later.\r\n\r\n\r\nShow code\r\nalgorithm = (KMeans(n_clusters = 4 ,init='k-means++', n_init = 10 ,max_iter=300, \r\n                        tol=0.0001,  random_state= 111  , algorithm='full') )\r\nalgorithm.fit(X1)\r\nKMeans(algorithm='full', n_clusters=4, random_state=111)\r\n\r\nShow code\r\nlabels1 = algorithm.labels_\r\ncentroids1 = algorithm.cluster_centers_\r\n\r\nh = 0.02\r\nx_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\r\ny_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\r\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\r\nZ = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \r\n\r\nplt.figure(1 , figsize = (15 , 7) )\r\nplt.clf()\r\nZ = Z.reshape(xx.shape)\r\nplt.imshow(Z , interpolation='nearest', \r\n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\r\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\r\n\r\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, c = labels1, s = 100)\r\nplt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\r\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Age')\r\n(Text(0, 0.5, 'Spending Score (1-100)'), Text(0.5, 0, 'Age'))\r\n\r\nShow code\r\nplt.title(\"Four Clusters\", loc='center')\r\nplt.show()  \r\n\r\n\r\n\r\n\r\nShow code\r\n#%%Applying KMeans for k=5\r\n\r\nalgorithm = (KMeans(n_clusters = 5, init='k-means++', n_init = 10, max_iter=300, \r\n                        tol=0.0001, random_state= 111 , algorithm='elkan'))\r\nalgorithm.fit(X1)\r\nKMeans(algorithm='elkan', n_clusters=5, random_state=111)\r\n\r\nShow code\r\nlabels1 = algorithm.labels_\r\ncentroids1 = algorithm.cluster_centers_\r\n\r\nh = 0.02\r\nx_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\r\ny_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\r\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\r\nZ = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \r\n\r\nplt.figure(1 , figsize = (15 , 7) )\r\nplt.clf()\r\nZ = Z.reshape(xx.shape)\r\nplt.imshow(Z , interpolation='nearest', \r\n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\r\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\r\n\r\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = df, c = labels1, s = 100)\r\nplt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\r\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Age')\r\n(Text(0, 0.5, 'Spending Score (1-100)'), Text(0.5, 0, 'Age'))\r\n\r\nShow code\r\nplt.title(\"Five Clusters\", loc='center')\r\nplt.show()\r\n\r\n\r\nThe two diagrams above show what it is like when we group the data into 4 and 5 clusters. It is important that the clusters are stable. Even though the algorithm begins by randomly initializing the cluster centers, if the k-means algorithm is the right choice for the data, then different runs of the algorithm will result in similar clusters in terms of size and variable distribution. If there is a lot of change in clusters between the different iterations of the algorithm, then k-means clustering may not be the right choice for the data. However, it is not possible to validate that the clusters obtained from the algorithm are accurate because there is no patient labeling; thus, it is necessary to examine how the clusters change between different iterations of the algorithm and check if the number of clusters makes sense in both theoretical and practical sense. We can also have domain experts give their opinions about if the clusters of customer make practical sense.\r\nFor one more practice, we can try making a cluster based on annual income and spending score.\r\n\r\n\r\nShow code\r\nX2 = df[['Annual Income (k$)' , 'Spending Score (1-100)']].iloc[: , :].values\r\ninertia = []\r\nfor n in range(1 , 11):\r\n    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \r\n                        tol=0.0001,  random_state= 111  , algorithm='full') )\r\n    algorithm.fit(X2)\r\n    inertia.append(algorithm.inertia_)\r\n    \r\nKMeans(algorithm='full', n_clusters=1, random_state=111)\r\nKMeans(algorithm='full', n_clusters=2, random_state=111)\r\nKMeans(algorithm='full', n_clusters=3, random_state=111)\r\nKMeans(algorithm='full', n_clusters=4, random_state=111)\r\nKMeans(algorithm='full', n_clusters=5, random_state=111)\r\nKMeans(algorithm='full', n_clusters=6, random_state=111)\r\nKMeans(algorithm='full', n_clusters=7, random_state=111)\r\nKMeans(algorithm='full', random_state=111)\r\nKMeans(algorithm='full', n_clusters=9, random_state=111)\r\nKMeans(algorithm='full', n_clusters=10, random_state=111)\r\n\r\nShow code\r\nplt.figure(1 , figsize = (15 ,6))\r\nplt.plot(np.arange(1 , 11) , inertia , 'o')\r\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\r\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\r\n(Text(0.5, 0, 'Number of Clusters'), Text(0, 0.5, 'Inertia'))\r\n\r\nShow code\r\nplt.show()\r\n\r\n\r\n\r\n\r\nShow code\r\nalgorithm = (KMeans(n_clusters = 5 ,init='k-means++', n_init = 10 ,max_iter=300, \r\n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\r\nalgorithm.fit(X2)\r\nKMeans(algorithm='elkan', n_clusters=5, random_state=111)\r\n\r\nShow code\r\nlabels2 = algorithm.labels_\r\ncentroids2 = algorithm.cluster_centers_\r\n\r\n#%%\r\nh = 0.02\r\nx_min, x_max = X2[:, 0].min() - 1, X2[:, 0].max() + 1\r\ny_min, y_max = X2[:, 1].min() - 1, X2[:, 1].max() + 1\r\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\r\nZ2 = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \r\n\r\n\r\n#%%\r\n\r\nplt.figure(1 , figsize = (15 , 7) )\r\nplt.clf()\r\nZ2 = Z2.reshape(xx.shape)\r\nplt.imshow(Z2 , interpolation='nearest', \r\n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\r\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\r\n\r\nplt.scatter( x = 'Annual Income (k$)' ,y = 'Spending Score (1-100)' , data = df , c = labels2 , \r\n            s = 100 )\r\nplt.scatter(x = centroids2[: , 0] , y =  centroids2[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\r\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Annual Income (k$)')\r\n(Text(0, 0.5, 'Spending Score (1-100)'), Text(0.5, 0, 'Annual Income (k$)'))\r\n\r\nShow code\r\nplt.title(\"Five Clusters\", loc='center')\r\nplt.show()\r\n\r\n\r\nHierarchical Clustering\r\nAn alternative to k-means clustering is hierarchical clustering (also known as hierarchical cluster analysis), which groups similar objects into hierarchies (or levels) of clusters. The end product is a set of clusters, where each cluster is distinct on its own, and the objects within each cluster are broadly similar to each other. This method works well when data have a nested structure - meaning that one characteristic is related to another (e.g., spending habit of a certain age group).\r\nAgain, Allison Horst did a really good job explaining how hierarchical clustering works with her visuals below. Note that the visual is for the “single” method, but we will be using the “ward” method. However, they are similar in terms of how they build a hierarchical dendrogram. A dendrogram is a diagram representing a tree that, in this context, illustrates the arrangement of the clusters produced by the corresponding analyses.\r\nWe will try performing divisive hierarchical clustering first. This method is known as a top-down approach that splits a cluster that contains the whole data into smaller clusters recursively until each single data point have been splitted into singleton clusters or the termination condition holds. This method is rigid. Once a merging or splitting is done, it can never be undone.\r\nThe Process of Hierarchical Clustering by Allison Horst. No copyright infringement intended\r\n\r\nShow code\r\nplt.figure(1, figsize = (16 ,8))\r\ndendrogram = sch.dendrogram(sch.linkage(df, method  = \"ward\"))\r\n\r\nplt.title('Dendrogram')\r\nplt.xlabel('Customers')\r\nplt.ylabel('Euclidean distances')\r\nplt.show()\r\n\r\n\r\nNext, we will try agglomerative clustering. This method is known as a “bottom-up” approach; that is, each observation starts in its own cluster, and pairs of clusters are merged as one amd moved up the hierarchy. We start with each object forming a separate group. It keeps on merging the objects or groups that are close to one another. The iteration keeps on doing so until all of the groups are merged into one or until the termination condition holds.\r\n\r\n\r\nShow code\r\nhc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage ='average')\r\n\r\ny_hc = hc.fit_predict(df)\r\ny_hc\r\narray([3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4,\r\n       3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 2,\r\n       3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\r\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\r\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\r\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 1, 2, 1, 0, 1, 0, 1,\r\n       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\r\n       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\r\n       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\r\n       0, 1], dtype=int64)\r\n\r\nShow code\r\nX = df.iloc[:, [3,4]].values\r\nplt.scatter(X[y_hc==0, 0], X[y_hc==0, 1], s=100, c='red', label ='Cluster 1')\r\nplt.scatter(X[y_hc==1, 0], X[y_hc==1, 1], s=100, c='blue', label ='Cluster 2')\r\nplt.scatter(X[y_hc==2, 0], X[y_hc==2, 1], s=100, c='green', label ='Cluster 3')\r\nplt.scatter(X[y_hc==3, 0], X[y_hc==3, 1], s=100, c='purple', label ='Cluster 4')\r\nplt.scatter(X[y_hc==4, 0], X[y_hc==4, 1], s=100, c='orange', label ='Cluster 5')\r\nplt.title('Clusters of Customers (Hierarchical Clustering Model)')\r\nplt.xlabel('Annual Income(k$)')\r\nplt.ylabel('Spending Score(1-100)')\r\nplt.show()\r\n\r\n\r\nConclusion\r\nThe main advantage of clustering over classification is that it is adaptable to changes and helps single out useful features that distinguish different groups. This is a useful tool for data scientists to understand raw data with unsupervised learning. The methods is best used for exploratory purposes for applications such as the investigation of a group that stands out the most and identify useful features that affect them. Specifically, The method can be used to discover distinct groups in their customer base, categorize genes with similar functionality, gain insight into structures inherent to populations in Biology, or even classify documents on the web for information discovery by combining clustering with natural language processing techniques. As always, thank you very much for reading this!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-18-unsupervisedml/unsupervisedml_files/figure-html5/unnamed-chunk-14-19.png",
    "last_modified": "2022-04-28T20:11:54-06:00",
    "input_file": {},
    "preview_width": 2880,
    "preview_height": 1152
  },
  {
    "path": "posts/2022-04-09-ensemble/",
    "title": "Combining Multiple Machine Learning Models with the Ensemble Methods",
    "description": "This entry explores different ways to combine supervised machine learning models to maximize their predictive capability.  \n\n(13 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2022-04-09",
    "categories": [
      "Python",
      "Supervised Machine Learning"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is ensemble methods?\r\nSetting up the Environment and Data set\r\nData Preprocessing\r\nPerformance of a Single Model\r\nVoting and Averaging\r\nBagging\r\nBoosting\r\nStacking\r\nConclusion\r\n\r\nWhat is ensemble methods?\r\nWhen you’re building a machine learning model, people generally choose the one that performs the best according to some evaluation metric such as accuracy score or F1 score. However, choosing one model over another, say decision tree over k-nearest Neighbor (Knn) or logistic Regression, may result in us discarding strengths of the remaining models which were able to learn different patterns that might have additional useful properties.\r\nWhen you conduct a survey, you don’t accept only one “best” answer. You consider a combined response of all the participants, and use statistics like the mode or the mean to represent the responses. The combined responses will likely lead to a better decision than relying on a single response. The same principle applies to ensemble methods, where we could form a new model by combining the existing ones. The combined model will have better performance than any of the individual models, or at least, be as good as the best individual model. In other words, ensemble learning methods is the usage of multiple machine learning models to maximize their predictive capability.\r\nIn this post, I will be exploring the usage of ensemble machine learning models to predict which mushrooms are edible based on their properties (e.g., cap size, color, odor). The data set is from the UC-Irvine Machine Learning repository and is currently distributed for practice on Kaggle. I will explore the usage of one model, model voting, bootstrap aggregating (aka bagging), model boosting, and model stacking via Python.\r\nImage from https://www.irasutoya.com. No copyright infringement is intendedSetting up the Environment and Data set\r\nImporting the necessary modules.\r\n\r\n\r\nShow code\r\nimport pandas as pd\r\n\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.model_selection import GridSearchCV\r\n\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.naive_bayes import GaussianNB \r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom mlxtend.classifier import StackingClassifier\r\nimport xgboost as xgb\r\n\r\nfrom sklearn.metrics import accuracy_score\r\nfrom sklearn.metrics import f1_score\r\nfrom sklearn.metrics import classification_report\r\n\r\nImporting the data set and examine the distribution of our targeted variable - the mushroom class. E stands for edible and P stands for poisonous.\r\n\r\n\r\nShow code\r\ndf = pd.read_csv(\"mushrooms.csv\")\r\n\r\n# data set shape\r\nprint(\"There are {} rows and {} columns in this dataset\".format(df.shape[0], df.shape[1]))\r\nThere are 8124 rows and 23 columns in this dataset\r\n\r\nShow code\r\ndf.info()\r\n<class 'pandas.core.frame.DataFrame'>\r\nRangeIndex: 8124 entries, 0 to 8123\r\nData columns (total 23 columns):\r\n #   Column                    Non-Null Count  Dtype \r\n---  ------                    --------------  ----- \r\n 0   class                     8124 non-null   object\r\n 1   cap-shape                 8124 non-null   object\r\n 2   cap-surface               8124 non-null   object\r\n 3   cap-color                 8124 non-null   object\r\n 4   bruises                   8124 non-null   object\r\n 5   odor                      8124 non-null   object\r\n 6   gill-attachment           8124 non-null   object\r\n 7   gill-spacing              8124 non-null   object\r\n 8   gill-size                 8124 non-null   object\r\n 9   gill-color                8124 non-null   object\r\n 10  stalk-shape               8124 non-null   object\r\n 11  stalk-root                8124 non-null   object\r\n 12  stalk-surface-above-ring  8124 non-null   object\r\n 13  stalk-surface-below-ring  8124 non-null   object\r\n 14  stalk-color-above-ring    8124 non-null   object\r\n 15  stalk-color-below-ring    8124 non-null   object\r\n 16  veil-type                 8124 non-null   object\r\n 17  veil-color                8124 non-null   object\r\n 18  ring-number               8124 non-null   object\r\n 19  ring-type                 8124 non-null   object\r\n 20  spore-print-color         8124 non-null   object\r\n 21  population                8124 non-null   object\r\n 22  habitat                   8124 non-null   object\r\ndtypes: object(23)\r\nmemory usage: 1.4+ MB\r\n\r\nShow code\r\nclass_dict= {'e': 'edible' , 'p':'poisonous'}\r\ndf['class'] = df['class'].map(class_dict)\r\n\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nsns.set_theme(style=\"darkgrid\")\r\nax = sns.countplot(x=\"class\", data = df)\r\nplt.show()\r\n\r\n\r\nData Preprocessing\r\nThe thing is, the data point is still coded in strings (i.e., alphabets, words or other characters). We need to re-code them into the numerical format for the machine to recognize patterns of the data.\r\n\r\n\r\nShow code\r\n\r\nfrom sklearn.preprocessing import LabelEncoder\r\nlabelencoder=LabelEncoder()\r\n\r\nfor col in df.columns:\r\n    df[col] = labelencoder.fit_transform(df[col])\r\n\r\n#The machine does not know about mushrooms. It only knows pattern of the data as reflected by the type of occurrence.\r\n\r\n#Checking the encoded values\r\ndf['stalk-color-above-ring'].unique()\r\narray([7, 3, 6, 4, 0, 2, 5, 1, 8])\r\n\r\nShow code\r\nprint(df.groupby('class').size())\r\nclass\r\n0    4208\r\n1    3916\r\ndtype: int64\r\n\r\nI have re-coded labels of the data into numbers, with 0 means edible, 1 means poisonous.\r\n\r\n\r\nShow code\r\nsns.set_theme(style=\"darkgrid\")\r\nax = sns.countplot(x=\"class\", data = df)\r\nplt.show()\r\n\r\n\r\nAs always, I will create a training and a testing set for the models to learn from.\r\n\r\n\r\nShow code\r\n\r\nRANDOM_STATE = 123\r\n\r\nX = df.drop('class', axis=1) #features\r\ny = df['class'] #label\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, \r\n                                                    random_state = RANDOM_STATE)\r\n\r\nPerformance of a Single Model\r\nNaive Bayes\r\nFirst, I will test out the performance of Naive Bayes Classifier, which is one of the simplest machine learning algorithms in the field. The algorithm assumes that all predictor variables are independent to each other;this is not usually possible in the practical context; hence the name naive. However, Naive Bayes is fast, does not require much data, and can achieve great results if the assumption holds.\r\n\r\n\r\nShow code\r\n#Instantiate a Naive Bayes classifier\r\nclf_nb = GaussianNB()\r\n\r\n# Fit the model to the training set\r\nclf_nb.fit(X_train,y_train)\r\n\r\n# Calculate the predictions on the test set\r\nGaussianNB()\r\n\r\nShow code\r\npred_nb = clf_nb.predict(X_test)\r\n\r\n# Evaluate the performance using the accuracy score\r\nprint(\"Accuracy: {:0.4f}\".format(accuracy_score(y_test, pred_nb)))\r\nAccuracy: 0.9130\r\n\r\nShow code\r\nprint(\"F1: {:0.4f}\".format(f1_score(y_test, pred_nb)))\r\nF1: 0.9091\r\n\r\nLogistic Regression\r\nNext, I will try performing Logistic Regression to with the same task to see if the result if going to be different. Logistic Regression is a regression model that predicts the probability of which category an input data point belongs to. The algorithm is one of the simplest and most commonly used models for for various classification problems such as spam detection or Diabetes prediction.\r\n\r\n\r\nShow code\r\nclf_lr = LogisticRegression(max_iter = 450, random_state = RANDOM_STATE)\r\n\r\nclf_lr.fit(X_train,y_train)\r\nLogisticRegression(max_iter=450, random_state=123)\r\n\r\nShow code\r\npred_lr = clf_lr.predict(X_test)\r\n\r\nprint(\"Accuracy: {:0.4f}\".format(accuracy_score(y_test, pred_lr)))\r\nAccuracy: 0.9471\r\n\r\nShow code\r\nprint(\"F1: {:0.4f}\".format(f1_score(y_test, pred_lr)))\r\nF1: 0.9444\r\n\r\nThe results show that logistic regression performs better than Naive Bayes as seen from higher accuracy and F-1 score (94 vs 91). This is just the performance of a single model. We can actually improve the prediction result if we pool multiple models together to tackle the same task. Let us try diving into model voting and averaging.\r\nVoting and Averaging\r\nOne type of ensemble methods is majority voting, which combines the output of many classifiers by using the mode of the individual predictions. It is recommended to use an odd number of classifiers. For example, if we use four classifiers, the predictions for positive and negative classes could be tied. Therefore, we need at least three classifiers, and when problem constraints allow it, use five or more.\r\nThere are some characteristics you need in your “crowd” for a voting ensemble to be effective. First, the ensemble needs to be diverse: you can do this by using different algorithms or different data sets. Second, each prediction needs to be independent and uncorrelated from the rest. Third, each model should be able to make its own prediction without relying on the other predictions. Finally, the ensemble model should aggregate individual predictions into a collective one. Keep in mind that Majority Voting is a technique which can only be applied to classification problems.\r\nIn addition to the Naive Bayes model we called for earlier, I will call for two additional models to participate in this majority voting method, namely k-Nearest Neighbors (Knn) and Decision Tree (DT). Knn is a classification algorithm in which a new data point is classified based on similarity in the specific group of neighboring data points. DR is another algorithm that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility to predict outcomes of a data point.\r\n\r\n\r\nShow code\r\nclf_knn = KNeighborsClassifier(n_neighbors = 5)\r\n\r\nclf_dt = DecisionTreeClassifier(min_samples_leaf = 3, min_samples_split = 9, random_state = RANDOM_STATE)\r\n\r\nfrom mlxtend.classifier import EnsembleVoteClassifier\r\n\r\nclf_vote = EnsembleVoteClassifier(clfs=[clf_nb, clf_knn, clf_dt], voting = \"hard\")\r\n\r\nclf_vote.fit(X_train, y_train)\r\nEnsembleVoteClassifier(clfs=[GaussianNB(), KNeighborsClassifier(),\r\n                             DecisionTreeClassifier(min_samples_leaf=3,\r\n                                                    min_samples_split=9,\r\n                                                    random_state=123)])\r\n\r\nShow code\r\npred_vote = clf_vote.predict(X_test)\r\n\r\nscore_vote = f1_score(pred_vote, y_test)\r\n\r\nprint(\"Accuracy: {:0.4f}\".format(accuracy_score(y_test, pred_vote)))\r\nAccuracy: 0.9996\r\n\r\nShow code\r\nprint('F1-Score: {:.3f}'.format(score_vote))\r\nF1-Score: 1.000\r\n\r\nThe accuracy and F1 scores increase to almost 1.00, which means that the combination of the three models works well in improving their performance. The voting we did above is called “hard voting”, where every individual classifier votes for a class, and the majority wins. In statistical terms, the predicted target label of the ensemble is the mode of the distribution of individually predicted labels. There is also “soft voting”, where every individual classifier provides a probability value that a specific data point belongs to a particular target class.\r\nIn this technique, the combined prediction is the mean of the individual predictions. For Regression, we use the predicted values. And for Classification, we use the predicted probabilities. As the mean doesn’t have ambiguous cases like the mode, we can use any number of estimators, as long as we have at least two of them.\r\n\r\n\r\nShow code\r\nclf_ave = EnsembleVoteClassifier(clfs=[clf_nb, clf_lr, clf_knn, clf_dt], voting = \"soft\")\r\n\r\nclf_ave.fit(X_train, y_train)\r\nEnsembleVoteClassifier(clfs=[GaussianNB(),\r\n                             LogisticRegression(max_iter=450, random_state=123),\r\n                             KNeighborsClassifier(),\r\n                             DecisionTreeClassifier(min_samples_leaf=3,\r\n                                                    min_samples_split=9,\r\n                                                    random_state=123)],\r\n                       voting='soft')\r\n\r\nShow code\r\npred_ave = clf_ave.predict(X_test)\r\n\r\nscore_ave = f1_score(pred_ave, y_test)\r\n\r\nprint(\"Accuracy: {:0.4f}\".format(accuracy_score(y_test, pred_ave)))\r\nAccuracy: 0.9979\r\n\r\nShow code\r\nprint('F1-Score: {:.3f}'.format(score_ave))\r\nF1-Score: 0.998\r\n\r\nBagging\r\nModel Voting and averaging work by combining the predictions of already trained models. These estimators that we included in the voting process are so well trained that, in some cases, they produce decent results on their own. Voting is appropriate when you already have optimized models and want to improve performance further by combining them. But what happens when you don’t have these estimators trained beforehand? Well, that’s when “weak” estimators come into play.\r\nThe idea of “weak” doesn’t mean that it is a bad model, just that it is not as strong as a highly optimized, fine-tuned model. Bootstrap Aggregating (aka Bagging) is the ensemble method behind powerful machine learning algorithms such as random forests that works by combining several weak models together to work on the same task. To clarify, a weak model (e.g., a single DT) is the model which works just slightly better than random guessing (approximately 50%). Therefore, the error rate is less than 50% but close to it. However, they are light in terms of space and computational requirements, and fast during training and evaluation.\r\nYou might be wondering how it is possible for a large group of “weak” models to be able to achieve good performance? This is the work of the “wisdom of the crowd”. Do ensemble methods with the same model have that potential? We have to refer to “Condorcet’s Jury Theorem”. If a jury (or a model, in our case) has more than 50% probability of getting the right answer, adding more voters increases the probability that the majority decision is correct up to 100% (not exactly 100%, but close enough).\r\nThe requirements for this theorem are the following: First, all the models must be independent. Secondly, each model performs better than random guessing. And finally, all individual models have similar performance. If these three conditions are met, then adding more models increases the probability of the ensemble to be correct, and makes this probability tend to 1, equivalent to 100%! The second and third requirements can be fulfilled by using the same “weak” model for all the estimators, as then all will have a similar performance and be better than random guessing. Several jury theorems carry the optimistic message that, in suitable circumstances, “crowds are wise”: many individuals together (using, for instance, majority voting) tend to make good decisions, outperforming fewer or just one individual.\r\nLet’s try using the Random Forest classifier, which is an ensemble of a large number of individual decision trees that are designed to be uncorrelated through randomization. Each tree is unique and has its own errors. A group of unique trees are able to produce results that are moving toward the right direction should the requirements mentioned above are fulfilled.\r\n\r\n\r\nShow code\r\n# random forest model creation\r\nclf_rf = RandomForestClassifier(random_state = RANDOM_STATE)\r\nclf_rf.fit(X_train, y_train)\r\n\r\n# predictions\r\nRandomForestClassifier(random_state=123)\r\n\r\nShow code\r\npred_rfc = clf_rf.predict(X_test)\r\n\r\nprint(\"Accuracy: {:0.4f}\".format(accuracy_score(y_test, pred_rfc)))\r\nAccuracy: 1.0000\r\n\r\nShow code\r\nprint(\"F1: {:0.4f}\".format(f1_score(y_test, pred_rfc)))\r\nF1: 1.0000\r\n\r\nThe codes below visualize the first five trees from the forest and export it as a png file.\r\n\r\n\r\nShow code\r\nfrom sklearn import tree\r\n\r\nfn = X.columns #for features\r\ncn = df.columns[0] #for class\r\n\r\nfig, axes = plt.subplots(nrows = 1,ncols = 5,figsize = (10,2), dpi=900)\r\nfor index in range(0, 5):\r\n    tree.plot_tree(clf_rf.estimators_[index],\r\n                   feature_names = fn, \r\n                   class_names = cn,\r\n                   filled = True,\r\n                   ax = axes[index]);\r\n\r\n    axes[index].set_title('Estimator: ' + str(index), fontsize = 11)\r\nfig.savefig('rf_5trees.png')\r\n\r\nFive Samples of Trees from the Random Forest ModelBoosting\r\nThe ensemble methods you’ve seen so far are based on an idea known as collective learning - that is, the wisdom of the crowd. For collective learning to be efficient, the estimators need to be independent and uncorrelated. In addition, all the estimators are learning the same task, for the same goal: to predict the target variable given the features. Gradual learning methods, on the other hand, are based on the principle of iterative learning. In this approach, each subsequent model tries to fix the errors of the previous model. Gradual learning creates dependent estimators, as each model takes advantage of the knowledge from the previous estimator.\r\nIn gradual learning, instead of the same model being corrected in every iteration, a new model is built that tries to fix the errors of the previous model. While this learning approach sounds promising, you should remain vigilant. It’s possible that some incorrect predictions may be made due to noise in the data, not because those data points are hard to predict. Gradient Boosting is another popular and powerful gradual learning ensemble model.\r\nTo explain how it works, let’s say that you want to predict values of a variable (e.g., whether a mushroom is edible, in our case). On the first iteration, our initial model is a weak estimator that is fit to the dataset. Then, on each subsequent iteration, a new model is built based pn error from the previous iteration. We repeat this process until the error is small enough such that the difference in performance is negligible. This is a peculiarity of Gradient Boosting, as the individual estimators are not combined through voting or averaging, but by addition. We had a model learn from its previous mistake.\r\n\r\n\r\nShow code\r\n#Call the model\r\nclf_gbm = GradientBoostingClassifier(n_estimators = 100, learning_rate = 0.1, random_state=RANDOM_STATE)\r\n\r\n#Fit the model\r\nclf_gbm.fit(X_train, y_train)\r\nGradientBoostingClassifier(random_state=123)\r\n\r\nShow code\r\npred_gbm = clf_gbm.predict(X_test)\r\n\r\nprint(\"Accuracy: {:0.4f}\".format(accuracy_score(y_test, pred_gbm)))\r\nAccuracy: 1.0000\r\n\r\nShow code\r\nprint(\"F1: {:0.4f}\".format(f1_score(y_test, pred_gbm)))\r\nF1: 1.0000\r\n\r\nAnother example of boosting model is Extreme Gradient Boosting (aka XGBoost). XGBoost regularly wins online data science competitions and is widely used across different industries. The model is known in its speed and performance. Because the core XGBoost algorithm is parallelizable (as opposed to sequential ensemble of normal Gradient boosting), it can harness all of the processing power of modern multi-core computers. The model also consistently outperforms almost all other single-algorithm methods in machine learning competitions and has been shown to achieve state-of-the-art performance on a variety of benchmark machine learning data sets.\r\n\r\n\r\nShow code\r\nclf_xgb = xgb.XGBClassifier(objective='binary:logistic', n_estimators = 10, seed = RANDOM_STATE, use_label_encoder = False)\r\nclf_xgb.fit(X_train, y_train)\r\n\r\n#n_estimator is the number of boosting round\r\n\r\n# predictions\r\n[23:57:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\r\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\r\n              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\r\n              gamma=0, gpu_id=-1, importance_type=None,\r\n              interaction_constraints='', learning_rate=0.300000012,\r\n              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\r\n              monotone_constraints='()', n_estimators=10, n_jobs=8,\r\n              num_parallel_tree=1, predictor='auto', random_state=123,\r\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=123,\r\n              subsample=1, tree_method='exact', use_label_encoder=False,\r\n              validate_parameters=1, verbosity=None)\r\n\r\nShow code\r\npred_xgb = clf_xgb.predict(X_test)\r\n\r\nprint(\"Accuracy: {:0.4f}\".format(accuracy_score(y_test, pred_xgb)))\r\nAccuracy: 1.0000\r\n\r\nShow code\r\nprint(\"F1: {:0.4f}\".format(f1_score(y_test, pred_xgb)))\r\nF1: 1.0000\r\n\r\nIt is worth mentioning that you should consider using XGBoost for any supervised machine learning task that fits the following criteria: first, having at least 1000 examples; second, you have a mixture of categorical and numeric features, or when you have just numeric features. XGBoost is not ideally suited for image recognition, computer vision, or natural language processing as such problems can be much better tackled using deep learning approaches.XGBoost is also not suitable when you have very small training sets (less than 100 training examples) or when the number of training examples is significantly smaller than the number of features being used for training as it could lead to model overfitting.\r\nStacking\r\nThe final type of ensemble model we will be discussing here is model stacking. Consider a relay race, in which sprinters run until they pass the baton over to the next on track. This is a good example of teamwork. While all team members must be strong competitors, each individual has a special role to play based on their abilities. In this case, we will have the lead runner (aka anchor model) that knows individual strengths and weaknesses of each team member. Second, they should have clearly define tasks to do. Each team member must know their responsibilities and focus on them. Finally, the anchor must participate in the race to carry out the final run.\r\nHere’s a diagram depicting the architecture of stacking ensembles. Each individual model uses the same data set and input features. These are the first-layer estimators. Then, estimators pass their predictions as additional input features to the second-layer estimator.\r\nModel stacking Flow chart. Image from https://towardsai.net/p/l/machine-learning-model-stacking-in-python. No copyright infringement is intendedSo far, we have seen ensemble methods that use simple arithmetic operations like the mean or the mode as combiners. However, in Stacking, the combiner is itself a trainable model. In addition, this combiner model has not only the predictions as input features, but also the original dataset. This allows it to determine which estimator is more accurate depending on the input features. In other words, it brings out the best ability in its team members to complete the task. The combiner model (aka meta learner) plays similar roles to the anchor in the relay race. It is also the last team member and the one which provides the final predictions.\r\nIn this post, I will stack Knn, DT, Naive Bayes, and Random Forest classifiers together with logistic regression as the meta model (team leader).\r\n\r\n\r\nShow code\r\nclf_stack = StackingClassifier(classifiers=[clf_knn, clf_dt, clf_nb, clf_rf], meta_classifier=clf_lr)\r\nclf_stack.fit(X_train, y_train)\r\nStackingClassifier(classifiers=[KNeighborsClassifier(),\r\n                                DecisionTreeClassifier(min_samples_leaf=3,\r\n                                                       min_samples_split=9,\r\n                                                       random_state=123),\r\n                                GaussianNB(),\r\n                                RandomForestClassifier(random_state=123)],\r\n                   meta_classifier=LogisticRegression(max_iter=450,\r\n                                                      random_state=123))\r\n\r\nShow code\r\npred_stack = clf_stack.predict(X_test)\r\n\r\nprint(\"Accuracy: {:0.4f}\".format(accuracy_score(y_test, pred_stack)))\r\nAccuracy: 1.0000\r\n\r\nShow code\r\nprint(\"F1: {:0.4f}\".format(f1_score(y_test, pred_stack)))\r\nF1: 1.0000\r\n\r\nShow code\r\nreport_stack = classification_report(y_test, pred_stack)\r\nprint(report_stack)\r\n              precision    recall  f1-score   support\r\n\r\n           0       1.00      1.00      1.00      1271\r\n           1       1.00      1.00      1.00      1167\r\n\r\n    accuracy                           1.00      2438\r\n   macro avg       1.00      1.00      1.00      2438\r\nweighted avg       1.00      1.00      1.00      2438\r\n\r\nShow code\r\nfrom sklearn.metrics import confusion_matrix\r\nconfusion_matrix(y_test, pred_stack)\r\narray([[1271,    0],\r\n       [   0, 1167]], dtype=int64)\r\n\r\nTo lessen our burden in model optimization, we can also tune hyperparameters of our models (both base model and meta model) all at the same time as well. This way, we will know which combination of hyperparameter values yield the best result. We can use it in our final model development.\r\n\r\n\r\nShow code\r\nparams = {'kneighborsclassifier__n_neighbors': [1, 5],\r\n          'randomforestclassifier__n_estimators': [10, 50],\r\n          'meta_classifier__C': [0.1, 10.0]}\r\n\r\ngrid = GridSearchCV(estimator=clf_stack, \r\n                    param_grid=params, \r\n                    cv=5,\r\n                    refit=True)\r\n\r\ngrid.fit(X_train, y_train)\r\nGridSearchCV(cv=5,\r\n             estimator=StackingClassifier(classifiers=[KNeighborsClassifier(),\r\n                                                       DecisionTreeClassifier(min_samples_leaf=3,\r\n                                                                              min_samples_split=9,\r\n                                                                              random_state=123),\r\n                                                       GaussianNB(),\r\n                                                       RandomForestClassifier(random_state=123)],\r\n                                          meta_classifier=LogisticRegression(max_iter=450,\r\n                                                                             random_state=123)),\r\n             param_grid={'kneighborsclassifier__n_neighbors': [1, 5],\r\n                         'meta_classifier__C': [0.1, 10.0],\r\n                         'randomforestclassifier__n_estimators': [10, 50]})\r\n\r\nShow code\r\ncv_keys = ('mean_test_score', 'std_test_score', 'params')\r\n\r\nfor r, _ in enumerate(grid.cv_results_['mean_test_score']):\r\n    print(\"%0.3f +/- %0.2f %r\"\r\n          % (grid.cv_results_[cv_keys[0]][r],\r\n             grid.cv_results_[cv_keys[1]][r] / 2.0,\r\n             grid.cv_results_[cv_keys[2]][r]))\r\n1.000 +/- 0.00 {'kneighborsclassifier__n_neighbors': 1, 'meta_classifier__C': 0.1, 'randomforestclassifier__n_estimators': 10}\r\n1.000 +/- 0.00 {'kneighborsclassifier__n_neighbors': 1, 'meta_classifier__C': 0.1, 'randomforestclassifier__n_estimators': 50}\r\n1.000 +/- 0.00 {'kneighborsclassifier__n_neighbors': 1, 'meta_classifier__C': 10.0, 'randomforestclassifier__n_estimators': 10}\r\n1.000 +/- 0.00 {'kneighborsclassifier__n_neighbors': 1, 'meta_classifier__C': 10.0, 'randomforestclassifier__n_estimators': 50}\r\n1.000 +/- 0.00 {'kneighborsclassifier__n_neighbors': 5, 'meta_classifier__C': 0.1, 'randomforestclassifier__n_estimators': 10}\r\n1.000 +/- 0.00 {'kneighborsclassifier__n_neighbors': 5, 'meta_classifier__C': 0.1, 'randomforestclassifier__n_estimators': 50}\r\n1.000 +/- 0.00 {'kneighborsclassifier__n_neighbors': 5, 'meta_classifier__C': 10.0, 'randomforestclassifier__n_estimators': 10}\r\n1.000 +/- 0.00 {'kneighborsclassifier__n_neighbors': 5, 'meta_classifier__C': 10.0, 'randomforestclassifier__n_estimators': 50}\r\n\r\nShow code\r\nprint('Best parameters: %s' % grid.best_params_)\r\nBest parameters: {'kneighborsclassifier__n_neighbors': 1, 'meta_classifier__C': 0.1, 'randomforestclassifier__n_estimators': 10}\r\n\r\nShow code\r\nprint('Accuracy: %.2f' % grid.best_score_)\r\nAccuracy: 1.00\r\n\r\nI will be plotting the decision region below to visualize how each model perform compared to their collective performance from model stacking. The decision region of each model changes in accordance with its performance.\r\n\r\n\r\nShow code\r\nimport matplotlib.pyplot as plt\r\nfrom mlxtend.plotting import plot_decision_regions\r\nimport matplotlib.gridspec as gridspec\r\nimport itertools\r\nfrom sklearn.decomposition import PCA\r\n\r\npca = PCA(n_components = 2)\r\n\r\nX_np = X_train.to_numpy()\r\ny_np = y_train.to_numpy()\r\n\r\nX_np_reduced = pca.fit_transform(X_np)\r\n\r\ngs = gridspec.GridSpec(2, 2)\r\n\r\nfig = plt.figure(figsize=(10,8))\r\n\r\nfor clf, lab, grd in zip([clf_knn, clf_dt, clf_nb, clf_stack], \r\n                         ['KNN', \r\n                          'Decision Tree', \r\n                          'Naive Bayes',\r\n                          'StackingClassifier'],\r\n                          itertools.product([0, 1], repeat=2)):\r\n\r\n    clf.fit(X_np_reduced, y_np)\r\n    ax = plt.subplot(gs[grd[0], grd[1]])\r\n    fig = plot_decision_regions(X=X_np_reduced, y=y_np, clf=clf)\r\n    plt.title(lab)\r\n    \r\nKNeighborsClassifier()\r\nText(0.5, 1.0, 'KNN')\r\nDecisionTreeClassifier(min_samples_leaf=3, min_samples_split=9,\r\n                       random_state=123)\r\nText(0.5, 1.0, 'Decision Tree')\r\nGaussianNB()\r\nText(0.5, 1.0, 'Naive Bayes')\r\nStackingClassifier(classifiers=[KNeighborsClassifier(),\r\n                                DecisionTreeClassifier(min_samples_leaf=3,\r\n                                                       min_samples_split=9,\r\n                                                       random_state=123),\r\n                                GaussianNB(),\r\n                                RandomForestClassifier(random_state=123)],\r\n                   meta_classifier=LogisticRegression(max_iter=450,\r\n                                                      random_state=123))\r\nText(0.5, 1.0, 'StackingClassifier')\r\n\r\nShow code\r\nplt.show()\r\n\r\n\r\nConclusion\r\nTo summarize, Voting combines the predictions of individual models using the mode. As the mode is a categorical measure, Voting can only be applied to Classification. The Averaging method combines the individual predictions using the mean. In contrast to Voting, Averaging can be applied on both classification and regression. Bagging uses a large amount of “weak” estimators. Their predictions are then aggregated by Voting or Averaging. Boosting is based on the iterative learning principle, in which each model attempts to fix the errors from the previous one. Therefore, this approach uses a sequential model building. Finally, model stacking works by combining individual estimators, but the combiner is an estimator itself, instead of just an operation.\r\nUsing ensemble techniques is an effective way to maximize the performance of our predictive model. However, a major drawback of this approach is model interpretability. While its performance is good, explaining how features interact with each other to form the result could be challenging. This is the black box problem that happens when the model is too complex that we don’t know how it actually works. If a single model performs relatively well, going for it would be a good compromise. The simpler the model is, the easier for us to explain the result to our audience. Rather than going full technical, we should consider perspectives of the audience as well to make our work accessible. As always, thank you very much for reading! Have a good one!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-04-09-ensemble/robot.png",
    "last_modified": "2022-04-11T00:07:34-06:00",
    "input_file": {},
    "preview_width": 626,
    "preview_height": 528
  },
  {
    "path": "posts/2022-02-27-statlearning/",
    "title": "Examining PISA 2018 Data Set with Statistical Learning Approach",
    "description": "In this post, I will be developing two statistical learning models, namely Linear Regression and Polynomial Regression, and apply it to Thai student data from the Programme for International Student Assessment (PISA) 2018 data set to examine the impact of classroom competition and cooperation to students' academic performance.  \n\n(14 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2022-02-27",
    "categories": [
      "R",
      "Statistics",
      "Supervised Machine Learning"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nSetting up R environment\r\nMissing Data Handling\r\nExploratory Data Analysis and Assumption Check\r\nData Preprocessing\r\nLinear Regression Model\r\nPolynomial Regression Model\r\nModel Tuning with Cross Validation\r\nFinal Model\r\nFinal Remarks and Conclusion\r\n\r\nIntroduction\r\nIt has been a while since my last post as I am swamped with manuscript writings, but a good thing is I got more ideas of what to write for my blog as well. As I learned more about data mining, I became aware about the overlapping space between Statistics and Machine Learning in how they involve data as a primary part in their work. Statistics, which is the study that concerns the understanding, analyzing, and interpreting empirical data, is known as the underlying mechanism of machine learning as algorithms such as Linear Regression or Logistic Regression usually operate under a set of equations; for that, it is reasonable to say that statistics is a prerequisite for researchers to learn before dicing into machine learning (Lomax and Hahs-Vaughn, 2012)\r\nStatistical Learning (SL) is a sub-field of Machine Learning (ML) research that seeks to explain relationship between variables with statistical models before extending their capability to predict the outcome of unseen data points (Vapnik, 1999). Some may say that SL and ML are the same thing; that is true to an extent as statistics is the science that works behind the development of ML, but there are also differences if you really look into the technical part of it.\r\nThe traditional statistical approach focuses on inferring relationships between variables with explicitly laid out instructions or equations such as Linear Regression, while the ML approach to research focuses on developing algorithms that can recognize patterns in the data set to accurately predict the unseen data without much emphasis on assumptions behind the data set and interpretability of the model (e.g., Random Forest, Neural Network) (Glen, 2019). Statistical Learning positions itself in the intersection of the two fields by focusing on understanding relationships between variables while at the same time seeking to develop a meaningful model that can be used to predict unseen data.\r\nIn other words, Statistics focuses on learning the meaning of data, typically the low-dimensionality one, via statistical inferences while ML focuses on the application aspect by developing a complex model that is accurate and usable in the real application. Statistical Learning aims to cover both purposes by focusing on understanding meaning of the data with a meaningful model while also aiming to use that model in the real-world application (Iniesta et al., 2016; University of Delaware, 2021).\r\nIn this post, I will be developing two statistical learning models, namely Linear Regression and Polynomial Regression, and apply it to Thai student data from the Programme for International Student Assessment (PISA) 2018 data set to examine the impact of classroom competition and cooperation to students’ academic performance. PISA is a large-scale educational survey that is collected internationally every three years on school-related topics such as student achievement, student school well-being, and teachers’ instructional support on students (OECD, 2019).\r\nImage from https://marketbusinessnews.com/financial-glossary/statistical-learning. No copyright infringement is intendedSetting up R environment\r\nAs always, we will first set up the working directory, loading in the packages, and importing the data set.\r\n\r\n\r\nShow code\r\n\r\nsetwd(\"D:/Program/Private_project/DistillSite/_posts/2022-02-27-statlearning\")\r\n\r\nlibrary(foreign) #To read SPSS data\r\nlibrary(psych) #For descriptive stat\r\nlibrary(tidyverse) #for data manipulation\r\nlibrary(DataExplorer)\r\nlibrary(ggcorrplot) #for correlation matrix\r\nlibrary(tidymodels) #for model building\r\nlibrary(kableExtra) #for kable table\r\nlibrary(visdat) #for overall missingness visualization\r\n\r\n\r\n\r\nWe will Import the data set with read_csv and subset our variables of interest with select. We will also recode factor variables such as gender to make it dichotomous. That is, 1 as an indicator of that variable and 0 as anything that is not. See the Coding Systems for Categorical Variables page of University of California, Los Angeles.\r\n\r\n\r\nShow code\r\n\r\n#Import the data set\r\nPISA_TH <-read_csv(\"PISA2018TH.csv\", col_names = TRUE)\r\n\r\nPISA_Subsetted <-  PISA_TH %>% \r\n  select(FEMALE = ST004D01T, READING_SCORE = PV1READ, CLASSCOMP = PERCOMP,\r\n         CLASSCOOP = PERCOOP, ATTCOMP = COMPETE)\r\n\r\nPISA_Subsetted$FEMALE <- recode_factor(PISA_Subsetted$FEMALE, \r\n                                       \"1\" = \"1\", \"2\" = \"0\")\r\nkbl(head(PISA_Subsetted)) %>%\r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\r\n                full_width = TRUE, position = \"left\")\r\n\r\n\r\n\r\nFEMALE\r\n\r\n\r\nREADING_SCORE\r\n\r\n\r\nCLASSCOMP\r\n\r\n\r\nCLASSCOOP\r\n\r\n\r\nATTCOMP\r\n\r\n\r\n1\r\n\r\n\r\n480.756\r\n\r\n\r\n1.5630\r\n\r\n\r\n1.6762\r\n\r\n\r\n0.4352\r\n\r\n\r\n1\r\n\r\n\r\n502.610\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n1\r\n\r\n\r\n476.744\r\n\r\n\r\n0.0866\r\n\r\n\r\n-0.5358\r\n\r\n\r\n0.4352\r\n\r\n\r\n1\r\n\r\n\r\n489.858\r\n\r\n\r\n0.6912\r\n\r\n\r\n0.6012\r\n\r\n\r\n-0.2661\r\n\r\n\r\n0\r\n\r\n\r\n536.178\r\n\r\n\r\n1.2903\r\n\r\n\r\n1.6762\r\n\r\n\r\n0.4352\r\n\r\n\r\n0\r\n\r\n\r\n566.755\r\n\r\n\r\n0.2020\r\n\r\n\r\n0.6012\r\n\r\n\r\n0.3709\r\n\r\n\r\nMissing Data Handling\r\nBefore we dive in further, we will be examining the data set for missing data ratio and patterns that could interfere with our analysis.\r\n\r\n\r\nShow code\r\n\r\nvisdat::vis_miss(PISA_Subsetted, sort_miss = TRUE, cluster = TRUE)\r\n\r\n\r\n\r\nShow code\r\n\r\nnaniar::gg_miss_upset(PISA_Subsetted, nsets = 5, nintersects = 10)\r\n\r\n\r\n\r\n\r\nIt turns out we only have 1.8% of missing data. The missingness pattern indicates that the missing pair of perceived classroom cooperation and perceived classroom competition has the most frequency among all missing variables. There are also 91 cases that do not have any variable values. Given the discovered pattern, it makes more sense to remove them with listwise deletion instead of performing an imputation (aka plugging in possible numbers with an educated guess). Note that there is no rule of thumb in handling missing data. It depends on judgement of the researcher themselves. I tried imputing the data set as well and it gives no noticeable change, but I just want to try something different this time.\r\n\r\n\r\nShow code\r\n\r\nPISA_Subsetted <- na.omit(PISA_Subsetted)\r\n\r\n\r\n\r\nExploratory Data Analysis and Assumption Check\r\nTo perform analyses using statistical learning, it is important to confirm that all statistical assumptions of the models were met to ensure that the results are meaningful. The assumptions that we will check in this section are normality distribution, multicollinearity, and influential outliers. There are other assumptions we need to check as well such as variable independence andhomoscedasticity (for the equality of variance throughout the data set), but we would have to create a linear regression model with the whole data set without teaching the machine. We won’t be doing that here because I aim to automate the process with the tidymodel approach instead of performing regression manually every time.\r\nWe will first examine structure of the data set with plot_str to plot data structure and plot_intro to plot basic infornmation of the data set.\r\n\r\n\r\nShow code\r\n\r\nplot_intro(PISA_Subsetted, title = \"Types of Variables\")+\r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nThis plot shows us how many discrete and continuous variables that we have in the data set. Mext, we will plot structure of the data to check which variable is numerical and which variable is factor.\r\n\r\n\r\nShow code\r\n\r\nplot_str(PISA_Subsetted)\r\n\r\n\r\n\r\nNext, we can check for normality distribution with histogram, skewness/kurtosis with descriptive statistics, and Quantile-Quantile Plot (Q-Q Plot).\r\n\r\n\r\nShow code\r\n\r\ndescribe(PISA_Subsetted)\r\n\r\n\r\n              vars    n   mean    sd median trimmed   mad    min\r\nFEMALE*          1 8158   1.46  0.50   1.00    1.45  0.00   1.00\r\nREADING_SCORE    2 8158 411.26 88.42 402.43  408.00 91.53 146.33\r\nCLASSCOMP        3 8158   0.20  0.87   0.20    0.19  0.73  -1.99\r\nCLASSCOOP        4 8158   0.20  0.92   0.60    0.22  1.07  -2.14\r\nATTCOMP          5 8158   0.03  0.79   0.20    0.00  0.65  -2.35\r\n                 max  range  skew kurtosis   se\r\nFEMALE*         2.00   1.00  0.18    -1.97 0.01\r\nREADING_SCORE 720.09 573.76  0.32    -0.37 0.98\r\nCLASSCOMP       2.04   4.03  0.02    -0.04 0.01\r\nCLASSCOOP       1.68   3.82 -0.39    -0.47 0.01\r\nATTCOMP         2.01   4.35  0.13     1.11 0.01\r\n\r\nShow code\r\n\r\nplot_histogram(PISA_Subsetted)\r\n\r\n\r\n\r\nShow code\r\n\r\nset.seed(123)\r\nplot_qq(PISA_Subsetted, sampled_rows = 1000L)\r\n\r\n\r\n\r\n\r\nThe results of skewness and kurtosis indicate no departure from normality; that is, the data is normally distributed with all skewness stays between the range of -0.5 and 0.5, and all kurtosis stays between the range of -3 and 3 (Lomax and Hahs-Vaughn, 2012). The normality assumption is further confirmed with bell shape of the histogram and visualization from the Q-Q plots. Next, we will examine bivariate correlation between variables to investigate their relationships and confirm the absence of multicollinearity assumption.\r\nMulticollinearity refers to the condition where relationships between our variables of interest is high; this which may influence the result of our analysis because we are having two variables that investigate the same thing (Alin, 2010). For example, when you investigate the influence of person’s weight to their height and included both weight in pound and weight in kilogram, the results can be messed up because you include variables that measure to much of the same thing.\r\n\r\n\r\nShow code\r\n\r\nplot_correlation(PISA_Subsetted, type = \"continuous\", title = \"Correlation Matrix\") +\r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nAll variables exhibit positive correlation to each other. The matrix shows no multicollinearity as the highest correlation coefficient between variables is 0.26, which is far from extreme in terms of magnitude (.6 to .7 coefficients should be flagged). For your reference, the report of data exploration can be generated with a simple function of create_report(PISA_Subsetted).\r\nData Preprocessing\r\nTo test out our models, we need to split the data set into a training and a testing set to make sure that the testing is fair. It is similar to the situation where we do not want our students to take a peek at the exam before their test.\r\n\r\n\r\nShow code\r\n\r\nset.seed(456)\r\n\r\n# Put 3/4 of the data into the training set \r\ndata_split <- initial_split(PISA_Subsetted, prop = 3/4)\r\n\r\n# Create data frames for the two sets:\r\ntrain_data <- training(data_split) #3/4\r\ntest_data  <- testing(data_split)  #1/4\r\n\r\n\r\n\r\nLinear Regression Model\r\nNow we have a training set that has 75% of the whole data and a testing set that has 25% of the whole data. Next, we will create a Linear Regression Model to predict the value of students’ reading score (READING_SCORE) with the degree of classroom cooperation, the degree of classroom competition, and students’ attitude to competition. First, we will request for a linear regression model with linear_reg() set the mode to regression and set the engine to “lm”, which stands for linear model.\r\n\r\n\r\nShow code\r\n\r\nlm_mod <- \r\n  linear_reg() %>% \r\n  set_mode(\"regression\") %>%\r\n  set_engine(\"lm\")\r\n\r\nlm_mod\r\n\r\n\r\nLinear Regression Model Specification (regression)\r\n\r\nComputational engine: lm \r\n\r\nNext, we will do some data preprocessing by removing variables that we do not need in the analysis with recipe package. We will specify the formula as READING_SCORE ~ ., which means we will be predicting students’ reading score with all other variables. The step_rm() argument removes the variable we specified; for our case, it is students’ gender. For your reference, there are functions that assist us in other ways of data preprocessing as well such as step_dummy() to dummy code the variable or step_string2factor that converts string variables into factor variables.\r\n\r\n\r\nShow code\r\n\r\nPISA_linear_recipe <- \r\n  recipe(READING_SCORE ~ ., data = train_data) %>%\r\n  step_rm(FEMALE)\r\n\r\nPISA_linear_recipe\r\n\r\n\r\nRecipe\r\n\r\nInputs:\r\n\r\n      role #variables\r\n   outcome          1\r\n predictor          4\r\n\r\nOperations:\r\n\r\nVariables removed FEMALE\r\n\r\nWe have the model. We have the formula. Now, we can combine them both into a workflow with workflow()\r\n\r\n\r\nShow code\r\n\r\nPISA_workflow <- \r\n  workflow() %>% \r\n  add_model(lm_mod) %>% \r\n  add_recipe(PISA_linear_recipe)\r\n\r\nPISA_workflow\r\n\r\n\r\n== Workflow ==========================================================\r\nPreprocessor: Recipe\r\nModel: linear_reg()\r\n\r\n-- Preprocessor ------------------------------------------------------\r\n1 Recipe Step\r\n\r\n* step_rm()\r\n\r\n-- Model -------------------------------------------------------------\r\nLinear Regression Model Specification (regression)\r\n\r\nComputational engine: lm \r\n\r\nFit the Linear Regression Model to the Training/Testing Dataset\r\nWe will train (or teach) our Linear Regression model to learn characteristics of our data with the training set. This way, the machine will be able to predict values of the testing data with what it learned.\r\n\r\n\r\nShow code\r\n\r\nset.seed(678)\r\n\r\nPISA_linear_fit <- \r\n  PISA_workflow %>% \r\n  fit(data = train_data)\r\n\r\n\r\n\r\nNow that we have taught the model, we can examine its performance by comparing its predicted value with the actual value of our targeted variable (i.e., students’ reading score). We will try extracting coefficients of the model first.\r\n\r\n\r\nShow code\r\n\r\nPISA_linear_fit %>% \r\n  extract_fit_parsnip() %>% \r\n  broom::tidy()\r\n\r\n\r\n# A tibble: 4 x 5\r\n  term        estimate std.error statistic  p.value\r\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\r\n1 (Intercept)   406.        1.15    355.   0       \r\n2 CLASSCOMP       3.77      1.33      2.84 4.50e- 3\r\n3 CLASSCOOP      19.7       1.23     16.0  1.91e-56\r\n4 ATTCOMP         3.08      1.45      2.12 3.38e- 2\r\n\r\nCoefficient interpretation is actually the same as interpreting linear regression results. First, we look at the p-value to see if any of these predictors are significant. Then, we interpret the coefficient of each prtedictor to the targeted variable. For example, the coefficient of 19.77 for the classroom cooperation variable means that every one-unit increase in the degree of classroom cooperation, students’ score will increase by 19.7. Next, we can evaluate performance of the model by comparing the predicted value with the actual value, as well as extracting values that can summarize overall performance.\r\n\r\n\r\nShow code\r\n\r\nPISA_linear_train_pred <- \r\n  predict(PISA_linear_fit, train_data) %>% \r\n  bind_cols(train_data %>% select(READING_SCORE)) \r\n\r\nPISA_linear_train_pred\r\n\r\n\r\n# A tibble: 6,118 x 2\r\n   .pred READING_SCORE\r\n   <dbl>         <dbl>\r\n 1  427.          471.\r\n 2  421.          239.\r\n 3  451.          546.\r\n 4  405.          248.\r\n 5  407.          457.\r\n 6  398.          491.\r\n 7  389.          308.\r\n 8  421.          320.\r\n 9  394.          473.\r\n10  404.          507.\r\n# ... with 6,108 more rows\r\n\r\nThe .pred column is the predicted value and the READING_SCORE column is the actual value. We can see that some predicted numbers are off from the actual number. Let us see overall performance of the model with Mean-Squared Error (MSE), Root Mean-Squared Error (RMSE), and R-Squared (R^2). The first two values indicate how poor our model performs, and the third value indicates how many percent of the variation of our targeted variable is explained by the model.\r\n\r\n\r\nShow code\r\n\r\nmse_vec <- function(truth, estimate, na_rm = TRUE, ...) {\r\n  \r\n  mse_impl <- function(truth, estimate) {\r\n    mean((truth - estimate) ^ 2)\r\n  }\r\n  \r\n  metric_vec_template(\r\n    metric_impl = mse_impl,\r\n    truth = truth, \r\n    estimate = estimate,\r\n    na_rm = na_rm,\r\n    cls = \"numeric\",\r\n    ...\r\n  )\r\n  \r\n}\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nmse_vec(\r\n  truth = PISA_linear_train_pred$READING_SCORE, \r\n  estimate = PISA_linear_train_pred$.pred\r\n)\r\n\r\n\r\n[1] 7389.577\r\n\r\n\r\n\r\nShow code\r\n\r\nrmse(PISA_linear_train_pred, \r\n     truth = READING_SCORE,\r\n     estimate = .pred)\r\n\r\n\r\n# A tibble: 1 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 rmse    standard        86.0\r\n\r\n\r\n\r\nShow code\r\n\r\nrsq(PISA_linear_train_pred, \r\n     truth = READING_SCORE,\r\n     estimate = .pred)\r\n\r\n\r\n# A tibble: 1 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 rsq     standard      0.0499\r\n\r\nWe have MSE = 7389.577, RMSE = 86, and R-Squared = 0.0499 (or 4% of the targeted variable). Let us to the same for the testing round to ‘test’ out how well our model can predict the targeted variable.\r\n\r\n\r\nShow code\r\n\r\nPISA_linear_test_pred <- \r\n  predict(PISA_linear_fit, test_data) %>% \r\n  bind_cols(test_data %>% select(READING_SCORE)) \r\n\r\nPISA_linear_test_pred\r\n\r\n\r\n# A tibble: 2,040 x 2\r\n   .pred READING_SCORE\r\n   <dbl>         <dbl>\r\n 1  398.          477.\r\n 2  427.          542.\r\n 3  415.          519.\r\n 4  422.          597.\r\n 5  421.          467.\r\n 6  434.          573.\r\n 7  397.          522.\r\n 8  425.          436.\r\n 9  423.          440.\r\n10  399.          543.\r\n# ... with 2,030 more rows\r\n\r\n\r\n\r\nShow code\r\n\r\nmse_vec(\r\n  truth = PISA_linear_test_pred$READING_SCORE, \r\n  estimate = PISA_linear_test_pred$.pred\r\n)\r\n\r\n\r\n[1] 7472.221\r\n\r\n\r\n\r\nShow code\r\n\r\nrmse(PISA_linear_test_pred, \r\n     truth = READING_SCORE,\r\n     estimate = .pred)\r\n\r\n\r\n# A tibble: 1 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 rmse    standard        86.4\r\n\r\n\r\n\r\nShow code\r\n\r\nrsq(PISA_linear_test_pred, \r\n     truth = READING_SCORE,\r\n     estimate = .pred)\r\n\r\n\r\n# A tibble: 1 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 rsq     standard      0.0584\r\n\r\nIn the testing round, We have MSE = 7472.221, RMSE = 86.4, and R-Squared = 0.0584 (or 5% of the targeted variable). The performance of both training and testing round are similar, meaning that there is no overfit (the machine memorized its lesson too much / too strict) or underfit (the machine did not learn as much as it is supposed to and therefore unable to know the relationship between variables). Now that we know what Linear Regression can do, let us see what can we accomplish with Polynomial Regression.\r\nPolynomial Regression Model\r\nPolynomial Regression is similar to Linear Regression, with the difference in its ability to capture non-linear relationship like the picture below. This ability allows the model to drop its assumption that the relationship between the independent and dependent variables has to be in linear shape.\r\nImage from https://towardsdatascience.com/polynomial-regression-an-alternative-for-neural-networks-c4bd30fa6cf6. No copyright infringement is intendedWe can customize ability to which our model can capture the pattern of our data by increasing its degree as demonstrated by the picture above. We can see that the red line (3rd degree) misses to capture a lot of patterns while the blue line (5th degree) captures a fair amount of patterns while the green line travels to almost every possible patterns. However, it is not necessarily true that the model with the highest degree is the best since it could capture noises that will not appear in the new data we will test the machine on (as well as the real-world data).\r\nSo, after all we have discussed so far, let’s try creating a 3rd degree polynomial regression to see if we can capture the relationship any better than our linear regression. We will include step_poly with degree = 3 into the recipe function to make our model able to capture a degree of non-linearity.\r\n\r\n\r\nShow code\r\n\r\nPISA_poly_recipe <- \r\n  recipe(READING_SCORE ~ CLASSCOMP + CLASSCOOP + ATTCOMP, data = train_data) %>%\r\n  step_poly(CLASSCOMP, degree = 3) %>%\r\n  step_poly(CLASSCOOP, degree = 3)\r\n\r\nPISA_poly_recipe\r\n\r\n\r\nRecipe\r\n\r\nInputs:\r\n\r\n      role #variables\r\n   outcome          1\r\n predictor          3\r\n\r\nOperations:\r\n\r\nOrthogonal polynomials on CLASSCOMP\r\nOrthogonal polynomials on CLASSCOOP\r\n\r\nThen, we will add the 3rd degree polynomial recipe to a workflow using the same model as our linear regression part (lm_mod).\r\n\r\n\r\nShow code\r\n\r\nPISA_poly_wf <- workflow() %>%\r\n  add_model(lm_mod) %>%\r\n  add_recipe(PISA_poly_recipe)\r\n\r\nPISA_poly_wf\r\n\r\n\r\n== Workflow ==========================================================\r\nPreprocessor: Recipe\r\nModel: linear_reg()\r\n\r\n-- Preprocessor ------------------------------------------------------\r\n2 Recipe Steps\r\n\r\n* step_poly()\r\n* step_poly()\r\n\r\n-- Model -------------------------------------------------------------\r\nLinear Regression Model Specification (regression)\r\n\r\nComputational engine: lm \r\n\r\nFinally, we can fit our model to the training data to teach it to recognize patterns of our training data set before using it to predict the testing data. While we are at it, we can use extract_fit_parsnip() to extract coefficients of our model as well to see which polynomial degree of which variable is significant in capturing patterns of the training data.\r\n\r\n\r\nShow code\r\n\r\nset.seed(999)\r\n\r\nPISA_poly_fit <- \r\n  PISA_poly_wf %>%\r\n  fit(data = train_data)\r\n\r\nPISA_poly_fit\r\n\r\n\r\n== Workflow [trained] ================================================\r\nPreprocessor: Recipe\r\nModel: linear_reg()\r\n\r\n-- Preprocessor ------------------------------------------------------\r\n2 Recipe Steps\r\n\r\n* step_poly()\r\n* step_poly()\r\n\r\n-- Model -------------------------------------------------------------\r\n\r\nCall:\r\nstats::lm(formula = ..y ~ ., data = data)\r\n\r\nCoefficients:\r\n     (Intercept)           ATTCOMP  CLASSCOMP_poly_1  \r\n         411.053             2.729           271.369  \r\nCLASSCOMP_poly_2  CLASSCOMP_poly_3  CLASSCOOP_poly_1  \r\n         295.526            19.960          1400.633  \r\nCLASSCOOP_poly_2  CLASSCOOP_poly_3  \r\n         -55.306           304.765  \r\n\r\n\r\n\r\nShow code\r\n\r\nPISA_poly_fit %>% \r\n  extract_fit_parsnip() %>% \r\n  broom::tidy()\r\n\r\n\r\n# A tibble: 8 x 5\r\n  term             estimate std.error statistic  p.value\r\n  <chr>               <dbl>     <dbl>     <dbl>    <dbl>\r\n1 (Intercept)        411.        1.10   374.    0       \r\n2 ATTCOMP              2.73      1.45     1.88  6.00e- 2\r\n3 CLASSCOMP_poly_1   271.       90.4      3.00  2.69e- 3\r\n4 CLASSCOMP_poly_2   296.       88.4      3.34  8.31e- 4\r\n5 CLASSCOMP_poly_3    20.0      87.2      0.229 8.19e- 1\r\n6 CLASSCOOP_poly_1  1401.       88.7     15.8   4.97e-55\r\n7 CLASSCOOP_poly_2   -55.3      88.6     -0.625 5.32e- 1\r\n8 CLASSCOOP_poly_3   305.       87.2      3.50  4.75e- 4\r\n\r\nAfter we trained our model, we can use it to predict the unseen test data and compare it with our actual data to assess its accuracy.\r\n\r\n\r\nShow code\r\n\r\nPISA_poly_pred <- \r\n  predict(PISA_poly_fit, test_data) %>% \r\n  bind_cols(test_data %>% select(READING_SCORE)) \r\n\r\nkbl(head(PISA_poly_pred)) %>%\r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\r\n                full_width = TRUE, position = \"left\")  \r\n\r\n\r\n\r\n.pred\r\n\r\n\r\nREADING_SCORE\r\n\r\n\r\n398.7327\r\n\r\n\r\n476.744\r\n\r\n\r\n426.4006\r\n\r\n\r\n542.238\r\n\r\n\r\n410.2957\r\n\r\n\r\n518.744\r\n\r\n\r\n417.5398\r\n\r\n\r\n597.441\r\n\r\n\r\n415.7663\r\n\r\n\r\n466.964\r\n\r\n\r\n431.5449\r\n\r\n\r\n572.923\r\n\r\n\r\nTo check if our 3rd degree polynomial model performed well, we can extract MSE, RMSE, and R^2 as our performance metrics.\r\n\r\n\r\nShow code\r\n\r\nmse_vec(\r\n  truth = PISA_poly_pred$READING_SCORE, \r\n  estimate = PISA_poly_pred$.pred)\r\n\r\n\r\n[1] 7470.233\r\n\r\n\r\n\r\nShow code\r\n\r\nrmse(PISA_poly_pred, \r\n     truth = READING_SCORE,\r\n     estimate = .pred)\r\n\r\n\r\n# A tibble: 1 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 rmse    standard        86.4\r\n\r\n\r\n\r\nShow code\r\n\r\nrsq(PISA_poly_pred, \r\n     truth = READING_SCORE,\r\n     estimate = .pred)\r\n\r\n\r\n# A tibble: 1 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 rsq     standard      0.0584\r\n\r\nThe metrics show that our polynomial slightly outperform our linear regression model, with higher R-squared and lower MSE. The problem is, how do we know if 3rd degree is the most appropriate degree we can make to predict our data. For polynomial regression, we can look for the most appropriate polynomial degree via cross-validation.\r\nModel Tuning with Cross Validation\r\nCross-Validation is a way to evaluate performance of our machine learning models by dividing our data into smaller groups and use them to estimate how the model will perform in when used to make predictions on data that are not in our training set. Basically, we test our machines with smaller tests to see which of them qualifies for the final test. Another way we can see this is when we tune an old ratio to make it able to receive the clearest frequency of the broadcast, but we use the machines to automatically do it instead to save our time and effort.\r\nWe will begin by setting up our recipe with tune() and add it to a workflow.\r\n\r\n\r\nShow code\r\n\r\nPISA_poly_tuned_recipe <- \r\n  recipe(READING_SCORE ~ CLASSCOMP + CLASSCOOP + ATTCOMP, data = train_data) %>%\r\n  step_poly(CLASSCOMP, CLASSCOOP, degree = tune())\r\n\r\nPISA_poly_wf <- workflow() %>%\r\n  add_model(lm_mod) %>%\r\n  add_recipe(PISA_poly_tuned_recipe)\r\n\r\nPISA_poly_wf\r\n\r\n\r\n== Workflow ==========================================================\r\nPreprocessor: Recipe\r\nModel: linear_reg()\r\n\r\n-- Preprocessor ------------------------------------------------------\r\n1 Recipe Step\r\n\r\n* step_poly()\r\n\r\n-- Model -------------------------------------------------------------\r\nLinear Regression Model Specification (regression)\r\n\r\nComputational engine: lm \r\n\r\nThen, we will create smaller data sets from our training set. We will create 10 sets (or 10 folds) to test 10 candidates of our model, which are models with polynomial degree from 1 (equivalent to linear regression), 2 (quadratic regression), 3 (cubic regression),…, to 10 (decic regression). In other words, we are looking for the model that performs the best among 10 candidates using RMSE and R-Squared as our performance metrics.\r\n\r\n\r\nShow code\r\n\r\ntrain_data_subsetted = subset(train_data, select = c('READING_SCORE','CLASSCOMP','CLASSCOOP','ATTCOMP'))\r\n\r\nPISA_folds <- vfold_cv(train_data, v = 10)\r\n\r\ndegree_grid <- grid_regular(degree(range = c(1, 10)), levels = 10)\r\n\r\ntune_resample <- tune_grid(\r\n  object = PISA_poly_wf, \r\n  resamples = PISA_folds, \r\n  grid = degree_grid)\r\n\r\nautoplot(tune_resample)\r\n\r\n\r\n\r\n\r\nWe want RMSE to be as low as possible and R-Squared to be as high as possible. However, our models can only do so much. Their performance will remain somewhat steady when it reaches a certain point. In our case, it is from the 6th polynomial degree onward. While the plot suggests that the 10th polynomial degree performed the best, we do not want our model to be too complex as it can be harder to interpret. We want the model that performs the best and is easiest to interpret. For that, we might want to pick the model that comes right after a significant performance boost to not make it too complex while able to bring out its maximum performance, which is the 6th degree polynomial model (Sextic Regression).\r\nData Tuning Results with 10-Folds Cross ValidationTo dive into the numbers, we can have the software shows and selects the best model candidate to include it in our final model.\r\n\r\n\r\nShow code\r\n\r\nshow_best(tune_resample, metric = \"rmse\")\r\n\r\n\r\n# A tibble: 5 x 7\r\n  degree .metric .estimator  mean     n std_err .config              \r\n   <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \r\n1     10 rmse    standard    85.4    10   0.852 Preprocessor10_Model1\r\n2      6 rmse    standard    85.5    10   0.845 Preprocessor06_Model1\r\n3      9 rmse    standard    85.5    10   0.860 Preprocessor09_Model1\r\n4      8 rmse    standard    85.5    10   0.857 Preprocessor08_Model1\r\n5      7 rmse    standard    85.5    10   0.843 Preprocessor07_Model1\r\n\r\nShow code\r\n\r\nselect_by_one_std_err(tune_resample, degree, metric = \"rsq\")\r\n\r\n\r\n# A tibble: 1 x 9\r\n  degree .metric .estimator   mean     n std_err .config  .best .bound\r\n   <dbl> <chr>   <chr>       <dbl> <int>   <dbl> <chr>    <dbl>  <dbl>\r\n1      6 rsq     standard   0.0616    10 0.00730 Prepro~ 0.0647 0.0563\r\n\r\nFinal Model\r\nWe will then assign the best model candidate as our best_degree to include it in the final workflow. Then, as usual, we teach (fit) it and test it with our test data.\r\n\r\n\r\nShow code\r\n\r\nbest_degree <- select_by_one_std_err(tune_resample, degree, metric = \"rsq\")\r\n\r\nfinal_wf <- finalize_workflow(PISA_poly_wf, best_degree)\r\n\r\nfinal_fit <- fit(final_wf, train_data)\r\n\r\nfinal_fit\r\n\r\n\r\n== Workflow [trained] ================================================\r\nPreprocessor: Recipe\r\nModel: linear_reg()\r\n\r\n-- Preprocessor ------------------------------------------------------\r\n1 Recipe Step\r\n\r\n* step_poly()\r\n\r\n-- Model -------------------------------------------------------------\r\n\r\nCall:\r\nstats::lm(formula = ..y ~ ., data = data)\r\n\r\nCoefficients:\r\n     (Intercept)           ATTCOMP  CLASSCOMP_poly_1  \r\n         411.050             2.836           279.070  \r\nCLASSCOMP_poly_2  CLASSCOMP_poly_3  CLASSCOMP_poly_4  \r\n         300.125            19.260          -167.827  \r\nCLASSCOMP_poly_5  CLASSCOMP_poly_6  CLASSCOOP_poly_1  \r\n        -197.978          -477.187          1381.076  \r\nCLASSCOOP_poly_2  CLASSCOOP_poly_3  CLASSCOOP_poly_4  \r\n         -56.933           281.590          -251.840  \r\nCLASSCOOP_poly_5  CLASSCOOP_poly_6  \r\n        -130.564           389.358  \r\n\r\nNow, we will test our model and extract its performance metrics to evaluate it.\r\n\r\n\r\nShow code\r\n\r\nPISA_poly_test_pred <- \r\n  predict(final_fit, test_data) %>% \r\n  bind_cols(test_data %>% select(READING_SCORE))\r\n\r\nmse_vec(\r\n  truth = PISA_poly_test_pred$READING_SCORE, \r\n  estimate = PISA_poly_test_pred$.pred)\r\n\r\n\r\n[1] 7382.151\r\n\r\nShow code\r\n\r\nrmse(PISA_poly_test_pred, \r\n     truth = READING_SCORE,\r\n     estimate = .pred)\r\n\r\n\r\n# A tibble: 1 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 rmse    standard        85.9\r\n\r\nShow code\r\n\r\nrsq(PISA_poly_test_pred, \r\n     truth = READING_SCORE,\r\n     estimate = .pred)\r\n\r\n\r\n# A tibble: 1 x 3\r\n  .metric .estimator .estimate\r\n  <chr>   <chr>          <dbl>\r\n1 rsq     standard      0.0697\r\n\r\nTo compare the performance of both Linear Regression and 6th Degree Polynomial Regression, we will create a comparison table to compare their performance for our discussion.\r\n\r\n\r\nShow code\r\n\r\ntabl <- \"\r\n| Model         | R-Square      | MSE   | RMSE |\r\n|---------------|:-------------:|------:|-----:|\r\n| Linear Reg    | 0.0584        | 7472.2| 86.4 |\r\n| Sextic Reg    | 0.0697        | 7382.1| 85.9 |\r\n\"\r\ncat(tabl)\r\n\r\n\r\nModel\r\nR-Square\r\nMSE\r\nRMSE\r\nLinear Reg\r\n0.0584\r\n7472.2\r\n86.4\r\nSextic Reg\r\n0.0697\r\n7382.1\r\n85.9\r\n\r\nThe table shows that our sextic regression slightly outperform our linear regression model as seen from its higher R-Squared and Lower Error Value (i.e., MSE, RMSE).\r\nFinal Remarks and Conclusion\r\nWhat we have done so far is developing two predictive models of linear regression and polynomial regression to predict students’ reading score from the degree of classroom cooperation, the degree of classroom competition, and students’ attitude to competition. While the traditional statistical approach examines the entire data set to make retrospective inferences, the statistical learning approach that we use both examines the relationship between the predictors and the targeted variables as well as testing predictive power of our models with the hold-out testing set, so that it can be used with new data we may receive in the future.\r\nHowever, considering the differences in performance metrics between the two algorithm, linear regression could be more appropriate with the variables examined in this study as its performance is slightly lower than sextic Regression at the trade-off of model interpretability. In other words, it is easier to interpret the model to non-technical audience and therefore easier to put the result into practice.\r\nWe can incorporate the model into an early warning system to inform teachers in their feedback provision to students in addition to other information such as students’ score and their class history. However, note that the models we developed in this post is far from perfect. The data set used in this study is relatively outdated as the new data of PISA 2022 cycle will be released in the future. Performance of the models leaves much to be desired as only 6% of the dependent variable was accounted for at most.\r\nThe thing is, we have accomplished what we are here for. We have tried our hands on developing a couple models with a large scale data set and user it to predict unseen data with statistical learning approach. Instead of going for complex models like Random Forest or Artificial Neural Networks, sticking to the ground with basic models could also be feasible in the actual practice as well. This is the point I am trying to make in this post. Thank you very much for reading. Have a good one!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-27-statlearning/statlearn.jpeg",
    "last_modified": "2022-03-27T18:36:46-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-15-ctt/",
    "title": "Classical Test Theory in R",
    "description": "For this post, I will be analyzing characteristics of test items based on the framework of Classical Test Theory (CTT).\n\n(13 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2022-01-15",
    "categories": [
      "R",
      "Psychometric"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nReliability Analysis\r\nStandard Error of Measurement\r\nConfidence Intervals for True Scores\r\nItem Analysis\r\nConcluding remark\r\n\r\nIntroduction\r\nClassical Test Theory (CTT - also known as weak theory or true-score theory) is a psychometric framework that analyzes psychological tests based on the three concepts of True score (T), Observed score (X), and Error score (E) (Fan, 1998).\r\nCTT assumes that each person has a true score that could be obtained if there were no errors in measurement. This approach to psychometric employs simple mathematical estimates such as averages, proportions, and correlation as methods to measure test takers’ level of construct regardless of their individual attribute.\r\nThe major advantage of CTT are its relatively weak theoretical assumptions, which make it easy to apply in many testing situations such as classroom assessment without much constraints. Relatively weak theoretical assumptions not only characterize CTT but also its extensions (e.g., generalizability theory) as well.\r\n\r\n\r\nShow code\r\n\r\nknitr::opts_chunk$set(error = TRUE)\r\n\r\n\r\n\r\nWe will load essential libraries as usual and import test item data and its answer key.\r\n\r\n\r\nShow code\r\n\r\nlibrary(psych) #for general psychometric functions\r\nlibrary(CTT) #for CTT-based item analysis\r\nlibrary(tidyverse) #data toolbox for R\r\nlibrary(kableExtra) #neat tables for R markdown\r\nlibrary(psychometric) #for item difficulty\r\n\r\n\r\n\r\nThe dataset we will use for this task is an assessment dataset for 241 students N = (241). The assessment comprises of 20 multiple-choice exam items. Each item has 4 response options.\r\nWe will begin by loading in the unscored answer data as seen below. i001 represents item 1, i002 represents item 2 and so forth until item 20.\r\n\r\n\r\nShow code\r\n\r\n#Importing test data\r\ndata <- read_csv(\"sample.score.csv\", col_names = T)\r\n\r\nkbl(head(data)) %>%\r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\r\n                full_width = TRUE, position = \"left\")\r\n\r\n\r\n\r\ni001\r\n\r\n\r\ni002\r\n\r\n\r\ni003\r\n\r\n\r\ni004\r\n\r\n\r\ni005\r\n\r\n\r\ni006\r\n\r\n\r\ni007\r\n\r\n\r\ni008\r\n\r\n\r\ni009\r\n\r\n\r\ni010\r\n\r\n\r\ni011\r\n\r\n\r\ni012\r\n\r\n\r\ni013\r\n\r\n\r\ni014\r\n\r\n\r\ni015\r\n\r\n\r\ni016\r\n\r\n\r\ni017\r\n\r\n\r\ni018\r\n\r\n\r\ni019\r\n\r\n\r\ni020\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nD\r\n\r\n\r\nD\r\n\r\n\r\nA\r\n\r\n\r\nB\r\n\r\n\r\nC\r\n\r\n\r\nA\r\n\r\n\r\nB\r\n\r\n\r\nA\r\n\r\n\r\nA\r\n\r\n\r\nB\r\n\r\n\r\nD\r\n\r\n\r\nC\r\n\r\n\r\nB\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nD\r\n\r\n\r\nC\r\n\r\n\r\nD\r\n\r\n\r\nB\r\n\r\n\r\nC\r\n\r\n\r\nB\r\n\r\n\r\nD\r\n\r\n\r\nC\r\n\r\n\r\nC\r\n\r\n\r\nA\r\n\r\n\r\nA\r\n\r\n\r\nB\r\n\r\n\r\nD\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nD\r\n\r\n\r\nB\r\n\r\n\r\nB\r\n\r\n\r\nB\r\n\r\n\r\nA\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nD\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nB\r\n\r\n\r\nD\r\n\r\n\r\nB\r\n\r\n\r\nA\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nB\r\n\r\n\r\nD\r\n\r\n\r\nC\r\n\r\n\r\nA\r\n\r\n\r\nD\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nB\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nC\r\n\r\n\r\nB\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nB\r\n\r\n\r\nD\r\n\r\n\r\nB\r\n\r\n\r\nA\r\n\r\n\r\nA\r\n\r\n\r\nA\r\n\r\n\r\nB\r\n\r\n\r\nD\r\n\r\n\r\nA\r\n\r\n\r\nD\r\n\r\n\r\nB\r\n\r\n\r\nD\r\n\r\n\r\nC\r\n\r\n\r\nD\r\n\r\n\r\nA\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nC\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nD\r\n\r\n\r\nD\r\n\r\n\r\nA\r\n\r\n\r\nB\r\n\r\n\r\nB\r\n\r\n\r\nA\r\n\r\n\r\nB\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nA\r\n\r\n\r\nD\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nD\r\n\r\n\r\nC\r\n\r\n\r\nC\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nB\r\n\r\n\r\nD\r\n\r\n\r\nC\r\n\r\n\r\nB\r\n\r\n\r\nC\r\n\r\n\r\nC\r\n\r\n\r\nB\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nB\r\n\r\n\r\nD\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nB\r\n\r\n\r\nC\r\n\r\n\r\nD\r\n\r\n\r\nNext, we will import answer key data for all 20 items. Note that the answer keys need to match with the response options in the unscored response data; that is, if the answer for i001 is A, the key has to be “A”, not “a” or “B” or other responses.\r\n\r\n\r\nShow code\r\n\r\n#Importing test key\r\nkey <- read_csv(\"sample.key.csv\", col_names = T) \r\nkey <- as.matrix(key)\r\n\r\n\r\nkbl(key) %>%\r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\r\n                full_width = TRUE, position = \"left\")\r\n\r\n\r\n\r\ni001\r\n\r\n\r\ni002\r\n\r\n\r\ni003\r\n\r\n\r\ni004\r\n\r\n\r\ni005\r\n\r\n\r\ni006\r\n\r\n\r\ni007\r\n\r\n\r\ni008\r\n\r\n\r\ni009\r\n\r\n\r\ni010\r\n\r\n\r\ni011\r\n\r\n\r\ni012\r\n\r\n\r\ni013\r\n\r\n\r\ni014\r\n\r\n\r\ni015\r\n\r\n\r\ni016\r\n\r\n\r\ni017\r\n\r\n\r\ni018\r\n\r\n\r\ni019\r\n\r\n\r\ni020\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nC\r\n\r\n\r\nD\r\n\r\n\r\nB\r\n\r\n\r\nB\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nB\r\n\r\n\r\nD\r\n\r\n\r\nC\r\n\r\n\r\nA\r\n\r\n\r\nD\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nA\r\n\r\n\r\nC\r\n\r\n\r\nD\r\n\r\n\r\nD\r\n\r\n\r\nC\r\n\r\n\r\nWe will then use the function score() function to score multiple choice responses by specifying our unscored response and answer key dataframe. What makes this function useful is that we can also request for reliability analysis with rel = TRUE.\r\nWhen scored, each item will be binarily coded, with 0 as incorrect and 1 as correct. We can also requested for item parameter such as mean of that item (itemMean), point-biserial and biserial correlation (we will get there soon), Coefficient Alpha reliability (or Cronbach’s Alpha) if that item is removed from the exam, and we can even requested for the function to flag if that item has low reliability value.\r\n\r\n\r\nShow code\r\n\r\n#scoring\r\nmyScore <- score(data, key, output.scored=TRUE, rel = TRUE)\r\n\r\nscored_item <- myScore$scored %>% as.data.frame()\r\n\r\nkbl(head(scored_item)) %>%\r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\r\n                full_width = TRUE, position = \"left\")\r\n\r\n\r\n\r\nV1\r\n\r\n\r\nV2\r\n\r\n\r\nV3\r\n\r\n\r\nV4\r\n\r\n\r\nV5\r\n\r\n\r\nV6\r\n\r\n\r\nV7\r\n\r\n\r\nV8\r\n\r\n\r\nV9\r\n\r\n\r\nV10\r\n\r\n\r\nV11\r\n\r\n\r\nV12\r\n\r\n\r\nV13\r\n\r\n\r\nV14\r\n\r\n\r\nV15\r\n\r\n\r\nV16\r\n\r\n\r\nV17\r\n\r\n\r\nV18\r\n\r\n\r\nV19\r\n\r\n\r\nV20\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nShow code\r\n\r\nitem_parameter <- itemAnalysis(scored_item, itemReport=TRUE, NA.Delete=TRUE, pBisFlag = T,  bisFlag = T, flagStyle = c(\"X\",\"\"))\r\n\r\nstr(item_parameter)\r\n\r\n\r\nList of 6\r\n $ nItem     : int 20\r\n $ nPerson   : int 241\r\n $ alpha     : num 0.54\r\n $ scaleMean : num 9.44\r\n $ scaleSD   : num 3\r\n $ itemReport:'data.frame': 20 obs. of  7 variables:\r\n  ..$ itemName      : chr [1:20] \"V1\" \"V2\" \"V3\" \"V4\" ...\r\n  ..$ itemMean      : num [1:20] 0.759 0.734 0.257 0.68 0.365 ...\r\n  ..$ pBis          : num [1:20] 0.1148 0.1036 0.0506 0.0591 0.1111 ...\r\n  ..$ bis           : num [1:20] 0.1576 0.1395 0.0686 0.0771 0.1423 ...\r\n  ..$ alphaIfDeleted: num [1:20] 0.535 0.537 0.545 0.545 0.537 ...\r\n  ..$ lowPBis       : chr [1:20] \"X\" \"X\" \"X\" \"X\" ...\r\n  ..$ lowBis        : chr [1:20] \"X\" \"X\" \"X\" \"X\" ...\r\n - attr(*, \"class\")= chr \"itemAnalysis\"\r\n\r\nThe output also comes with scores of each examinee on the exam, which we can perform a descriptive analysis with psych::describe() on to learn how students of this section perform on this test with mean, median, mode, standard deviation, and so forth.\r\n\r\n\r\nShow code\r\n\r\nkbl(head(myScore$score)) %>%\r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\r\n                full_width = TRUE, position = \"left\")\r\n\r\n\r\n\r\n\r\n\r\nx\r\n\r\n\r\nP1\r\n\r\n\r\n9\r\n\r\n\r\nP2\r\n\r\n\r\n6\r\n\r\n\r\nP3\r\n\r\n\r\n13\r\n\r\n\r\nP4\r\n\r\n\r\n9\r\n\r\n\r\nP5\r\n\r\n\r\n14\r\n\r\n\r\nP6\r\n\r\n\r\n12\r\n\r\n\r\nShow code\r\n\r\ndescribe(myScore$score)\r\n\r\n\r\n   vars   n mean sd median trimmed  mad min max range skew kurtosis\r\nX1    1 241 9.44  3      9    9.34 2.97   1  19    18 0.26    -0.14\r\n     se\r\nX1 0.19\r\n\r\nWe can also transform the score into a new scale by setting a new mean, SD, and normality with score.transform(). We will refer to the IQ score, with its mean = 100 and its SD = 15. We can even transform the score into a Z score (mean = 0, SD = 1), T score (mean = 50, SD = 10), or even Stanine score (mean = 5, SD = 2).\r\n\r\n\r\nShow code\r\n\r\n#Score transformation\r\nIQ <- score.transform(myScore$score, mu.new = 100, sd.new = 15, normalize = TRUE)\r\n\r\n#new.scores is the transformed score\r\nkbl(head(IQ$new.scores)) %>%\r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\r\n                full_width = FALSE, position = \"left\")\r\n\r\n\r\n\r\n\r\n\r\nx\r\n\r\n\r\nP1\r\n\r\n\r\n99.29768\r\n\r\n\r\nP2\r\n\r\n\r\n82.70691\r\n\r\n\r\nP3\r\n\r\n\r\n116.70018\r\n\r\n\r\nP4\r\n\r\n\r\n99.29768\r\n\r\n\r\nP5\r\n\r\n\r\n121.19395\r\n\r\n\r\nP6\r\n\r\n\r\n112.44734\r\n\r\n\r\nShow code\r\n\r\n#p.scores is the percentile rank of every examinee\r\nkbl(head(IQ$p.scores)) %>%\r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\r\n                full_width = FALSE, position = \"left\")\r\n\r\n\r\n\r\n\r\n\r\nx\r\n\r\n\r\nP1\r\n\r\n\r\n0.4813278\r\n\r\n\r\nP2\r\n\r\n\r\n0.1244813\r\n\r\n\r\nP3\r\n\r\n\r\n0.8672199\r\n\r\n\r\nP4\r\n\r\n\r\n0.4813278\r\n\r\n\r\nP5\r\n\r\n\r\n0.9211618\r\n\r\n\r\nP6\r\n\r\n\r\n0.7966805\r\n\r\n\r\nWe can also visualize Item Characteristic Curve (ICC) of each item from the scoring output. The x-axis indicates total test score of examinees while the y-axis indicates the proportion of examinee that gets that item right. For example, there are 70% (or 0.7) of examinees who have 4 as their total score got item 1 correctly. The curve goes up as total score of the examinee increases, with the proportion of examinee that gets the item correctly at 100% in examinees with the total score of 16 and above.\r\n\r\n\r\nShow code\r\n\r\ncttICC(score = myScore$score, itemVector = myScore$scored[,1], \r\n       xlab = \"Total test score\",\r\n       ylab = \"Proportion of the examinee\",\r\n       plotTitle = \"Item Characteristic Curve of item 1\",\r\n       colTheme=\"dukes\", cex=1.5)\r\n\r\n\r\n\r\n\r\nWe will convert our score data into matrix for for further implementation with item analysis.\r\n\r\n\r\nShow code\r\n\r\n#extract responses only as a matrix\r\nresponses <- as.matrix(myScore$scored) \r\n\r\n\r\n\r\nReliability Analysis\r\nReliability of a test is an extent to which a test can produce consistent scores for a particular sample of examinees; that is, a test should yield similar, if not the same, scores for an examinee when taken multiple times, given that there is minimal interference such as practice effect. Test reliability can be measured in different ways such as test-retest, internal consistency, or alternate forms.\r\nInternal Consistency\r\nFor starter, we will compute internal consistency of the test, which includes Coefficient Alpha (or Cronbach’s Alpha), Guttman’s Lambda-6, Kuder-Richardson 20, and Kuder-Richardson 21. The output also yields the proportion of students who got the item correctly to students who got the item incorrectly as well.\r\n\r\n\r\nShow code\r\n\r\npsych::alpha(myScore$scored, check.keys = T)\r\n\r\n\r\n\r\nReliability analysis   \r\nCall: psych::alpha(x = myScore$scored, check.keys = T)\r\n\r\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r\r\n      0.55      0.55    0.58     0.057 1.2 0.041 0.48 0.15    0.045\r\n\r\n lower alpha upper     95% confidence boundaries\r\n0.47 0.55 0.63 \r\n\r\n Reliability if an item is dropped:\r\n     raw_alpha std.alpha G6(smc) average_r  S/N alpha se  var.r med.r\r\nV1        0.55      0.54    0.58     0.059 1.18    0.042 0.0081 0.045\r\nV2        0.56      0.55    0.58     0.060 1.21    0.041 0.0079 0.048\r\nV3        0.56      0.55    0.59     0.060 1.22    0.041 0.0080 0.048\r\nV4        0.56      0.56    0.59     0.062 1.25    0.040 0.0080 0.052\r\nV5        0.55      0.54    0.58     0.059 1.20    0.041 0.0081 0.048\r\nV6        0.51      0.50    0.54     0.050 1.00    0.046 0.0071 0.041\r\nV7        0.56      0.55    0.59     0.062 1.25    0.040 0.0078 0.051\r\nV8        0.55      0.54    0.58     0.059 1.19    0.041 0.0080 0.048\r\nV9        0.55      0.54    0.58     0.058 1.17    0.042 0.0080 0.048\r\nV10-      0.56      0.56    0.59     0.062 1.25    0.040 0.0079 0.052\r\nV11       0.54      0.54    0.57     0.057 1.15    0.042 0.0081 0.045\r\nV12       0.55      0.54    0.58     0.058 1.17    0.042 0.0080 0.044\r\nV13       0.53      0.52    0.56     0.055 1.10    0.043 0.0074 0.044\r\nV14       0.53      0.52    0.56     0.055 1.10    0.043 0.0076 0.043\r\nV15       0.54      0.53    0.57     0.057 1.14    0.042 0.0076 0.044\r\nV16       0.51      0.51    0.55     0.051 1.03    0.045 0.0068 0.044\r\nV17       0.49      0.48    0.52     0.047 0.93    0.048 0.0056 0.039\r\nV18       0.51      0.50    0.53     0.050 1.00    0.046 0.0060 0.044\r\nV19       0.55      0.54    0.58     0.059 1.19    0.042 0.0079 0.045\r\nV20       0.55      0.55    0.58     0.060 1.20    0.041 0.0077 0.048\r\n\r\n Item statistics \r\n       n raw.r std.r r.cor r.drop mean   sd\r\nV1   241  0.26  0.28 0.181  0.125 0.76 0.43\r\nV2   241  0.23  0.24 0.127  0.083 0.73 0.44\r\nV3   241  0.22  0.23 0.115  0.082 0.26 0.44\r\nV4   241  0.20  0.20 0.066  0.043 0.68 0.47\r\nV5   241  0.26  0.26 0.152  0.106 0.37 0.48\r\nV6   241  0.51  0.50 0.497  0.370 0.54 0.50\r\nV7   241  0.19  0.20 0.074  0.045 0.28 0.45\r\nV8   241  0.27  0.27 0.163  0.113 0.48 0.50\r\nV9   241  0.26  0.29 0.199  0.143 0.83 0.38\r\nV10- 241  0.21  0.20 0.067  0.043 0.56 0.50\r\nV11  241  0.32  0.32 0.228  0.169 0.34 0.47\r\nV12  241  0.31  0.30 0.205  0.148 0.56 0.50\r\nV13  241  0.39  0.38 0.323  0.240 0.39 0.49\r\nV14  241  0.40  0.38 0.327  0.247 0.45 0.50\r\nV15  241  0.33  0.33 0.254  0.183 0.31 0.46\r\nV16  241  0.47  0.47 0.452  0.337 0.64 0.48\r\nV17  241  0.60  0.59 0.647  0.478 0.44 0.50\r\nV18  241  0.52  0.50 0.526  0.387 0.33 0.47\r\nV19  241  0.25  0.27 0.176  0.123 0.19 0.39\r\nV20  241  0.26  0.25 0.152  0.097 0.44 0.50\r\n\r\nNon missing response frequency for each item\r\n         0    1 miss\r\n [1,] 0.24 0.76    0\r\n [2,] 0.27 0.73    0\r\n [3,] 0.74 0.26    0\r\n [4,] 0.32 0.68    0\r\n [5,] 0.63 0.37    0\r\n [6,] 0.46 0.54    0\r\n [7,] 0.72 0.28    0\r\n [8,] 0.52 0.48    0\r\n [9,] 0.17 0.83    0\r\n[10,] 0.56 0.44    0\r\n[11,] 0.66 0.34    0\r\n[12,] 0.44 0.56    0\r\n[13,] 0.61 0.39    0\r\n[14,] 0.55 0.45    0\r\n[15,] 0.69 0.31    0\r\n[16,] 0.36 0.64    0\r\n[17,] 0.56 0.44    0\r\n[18,] 0.67 0.33    0\r\n[19,] 0.81 0.19    0\r\n[20,] 0.56 0.44    0\r\n\r\nKuder-Richardson\r\nFor Kuder-Richardson formula 20 (KR20) and Kuder-Richardson formula 21 (KR21), we can write functions that compute them as follows:\r\n\r\n\r\nShow code\r\n\r\nKR20 <-\r\n  function(X)\r\n    {\r\n    X <- data.matrix(X)\r\n    k <- ncol(X)\r\n    \r\n    # Person total score variances\r\n    SX <- var(rowSums(X))\r\n    \r\n    # item means\r\n    IM <- colMeans(X)\r\n    \r\n    return(((k/(k - 1))*((SX - sum(IM*(1 - IM)))/SX)))\r\n  }\r\n\r\nKR20(responses)\r\n\r\n\r\n[1] 0.541653\r\n\r\n\r\n\r\nShow code\r\n\r\nKR21 <-\r\n  function(X)\r\n    {\r\n    X <- data.matrix(X)\r\n    n <- ncol(X)\r\n    \r\n    return((n/(n-1))*((var(rowSums(X)) - n*(sum(colMeans(X))/n) * \r\n                         (1-(sum(colMeans(X))/n))))/var(rowSums(X)))\r\n  }\r\n\r\nKR21(responses)\r\n\r\n\r\n[1] 0.4700428\r\n\r\nSplit-half (Test-Retest) Reliability\r\nThe test-retest reliability is an estimation of reliability based on the correlation of two equivalent forms of the tests. It is not recommended to use the same test form twice to avoid practice effect, which could artifically increase reliability coefficient of the test.\r\n\r\n\r\nShow code\r\n\r\npsych::splitHalf(scored_item, raw = TRUE, check.keys = TRUE)\r\n\r\n\r\nSplit half reliabilities  \r\nCall: psych::splitHalf(r = scored_item, raw = TRUE, check.keys = TRUE)\r\n\r\nMaximum split half reliability (lambda 4) =  0.69\r\nGuttman lambda 6                          =  0.58\r\nAverage split half reliability            =  0.55\r\nGuttman lambda 3 (alpha)                  =  0.55\r\nGuttman lambda 2                          =  0.57\r\nMinimum split half reliability  (beta)    =  0.31\r\nAverage interitem r =  0.06  with median =  0.04\r\n                                             2.5% 50% 97.5%\r\n Quantiles of split half reliability      =  0.44 0.55 0.63\r\n\r\nSpearman-Brown Reliability\r\nTo compute Spearman-Brown reliability, we need to use Cronbach’s Alpha as a base computation; for that, we need to export the function as a separate file first.\r\n\r\n\r\nShow code\r\n\r\n#With a written function\r\ncronbachs.alpha <-\r\n  function(X)\r\n    {\r\n    \r\n    X <- data.matrix(X)\r\n    n <- ncol(X) # Number of items\r\n    k <- nrow(X) # Number of examinees\r\n    \r\n    # Cronbachs alpha\r\n    alpha <- (n/(n - 1))*(1 - sum(apply(X, 2, var))/var(rowSums(X)))\r\n    \r\n    return(list(\"Crombach's alpha\" = alpha,\r\n                \"Number of items\" = n,\r\n                \"Number of examinees\" = k))\r\n    }\r\n\r\n#Dump \"cronbach.alpha\" function for further use\r\ndump(\"cronbachs.alpha\", file = \"cronbachs.alpha.R\")\r\n\r\n\r\n\r\nThe Spearman-Brown prophecy formula is a formula that predicts reliability of the test through the modification of test length. This formula is one way can use to answer questions such as “how short can I make my assessment?” or “how many items should I write for my test to have adequate reliability?”\r\nReliability of a test usually decreases when we shorten the test length and increases when we add more test items as well. However, there is no magic number of how long a test should be, so one should consider their context such as nature of the test content, ability of the examinee, and average time for a student to complete the assessment.\r\n\r\n\r\nShow code\r\n\r\nSpearmanBrown <- \r\n  function(x, n1, n2)\r\n    {\r\n    \r\n    source(\"cronbachs.alpha.R\")\r\n    \r\n    x <- as.matrix(x)\r\n    N <- n2/n1\r\n    \r\n    # cronbach's alpha for the original test\r\n    alpha <- cronbachs.alpha(x)[[1]]\r\n    predicted.alpha <- N * alpha / (1 + (N - 1) * alpha)\r\n    \r\n    return(list(original.reliability = alpha,\r\n                original.sample.size = n1,\r\n                predicted.reliability = predicted.alpha,\r\n                predicted.sample.size = n2))\r\n  }\r\n\r\n# predict reliability by Spearman-Brown formula\r\n# if the number of items is reduced from 25 to 15\r\nSpearmanBrown(responses, n1 = 20, n2 = 15)\r\n\r\n\r\n$original.reliability\r\n[1] 0.5395239\r\n\r\n$original.sample.size\r\n[1] 20\r\n\r\n$predicted.reliability\r\n[1] 0.4677309\r\n\r\n$predicted.sample.size\r\n[1] 15\r\n\r\n\r\n\r\nShow code\r\n\r\n# predict reliability by Spearman-Brown formula\r\n# if the number of items is increased from 25 to 35\r\nSpearmanBrown(responses, n1 = 20, n2 = 35)\r\n\r\n\r\n$original.reliability\r\n[1] 0.5395239\r\n\r\n$original.sample.size\r\n[1] 20\r\n\r\n$predicted.reliability\r\n[1] 0.6721757\r\n\r\n$predicted.sample.size\r\n[1] 35\r\n\r\n\r\n\r\nShow code\r\n\r\n# predict reliability by Spearman-Brown formula\r\n# if the number of items is doubled\r\nSpearmanBrown(responses, n1 = 20, n2 = 40)\r\n\r\n\r\n$original.reliability\r\n[1] 0.5395239\r\n\r\n$original.sample.size\r\n[1] 20\r\n\r\n$predicted.reliability\r\n[1] 0.7008971\r\n\r\n$predicted.sample.size\r\n[1] 40\r\n\r\nGuttman’s Lambda\r\nGuttman’s Lambda is also another reliability coefficient we can use in similar way as Coefficient Alpha (or Cronbach’s Alpha).\r\n\r\n\r\nShow code\r\n\r\npsych::guttman(responses)\r\n\r\n\r\nCall: psych::guttman(r = responses)\r\n\r\nAlternative estimates of reliability\r\n\r\nGuttman bounds \r\nL1 =  0.51 \r\nL2 =  0.56 \r\nL3 (alpha) =  0.53 \r\nL4 (max) =  0.69 \r\nL5 =  0.55 \r\nL6 (smc) =  0.57 \r\nTenBerge bounds \r\nmu0 =  0.53 mu1 =  0.56 mu2 =  0.56 mu3 =  0.56 \r\n\r\nalpha of first PC =  0.64 \r\nestimated greatest lower bound based upon communalities=  0.69 \r\n\r\nbeta found by splitHalf  =  0.32 \r\n\r\nPearson product-moment correlation coefficient\r\nAnother way we can compute reliability of the test is via Pearson product-moment correlation between two halves of the test (aka Split-half method). However, comparing only a pair or two of the test half might not be enough. We can use Bootstrap resampling method to randomly split the examinees into two halves 1000 times, so that we can make sure that the computation is as exhaustive as possible.\r\n\r\n\r\nShow code\r\n\r\n#Split data (variables-item) into two equally and randomly.\r\n\r\nsplit.items <- \r\n  function(X, seed = NULL)\r\n    {\r\n    # optional fixed seed\r\n    if (!is.null(seed)) {set.seed(seed)} \r\n    \r\n    X <- as.matrix(X)\r\n    \r\n    # if n = 2x, then lengths Y1 = Y2\r\n    # if n = 2x+1, then lenths Y1 = Y2+1\r\n    n <- ncol(X)\r\n    index <- sample(1:n, ceiling(n/2))\r\n    Y1 <- X[, index ]\r\n    Y2 <- X[, -index]\r\n    return(list(Y1, Y2)) \r\n  }\r\n\r\ndump(\"split.items\", file = \"split.items.R\")\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\npearson <- \r\n  function(X, seed = NULL, n = NULL)\r\n    {\r\n    source(\"split.items.R\")\r\n    \r\n    # optional fixed seed\r\n    if (!is.null(seed)) {set.seed(seed)}\r\n    \r\n    # the number of bootstrap replicates. 1e3 = 1000\r\n    if (is.null(n)) {n <- 1e3}   \r\n    \r\n    X <- as.matrix(X)\r\n    r <- rep(NA, n)\r\n    \r\n    for (i in 1:n) {\r\n      # split items\r\n      Y <- split.items(X)\r\n      \r\n      # total scores\r\n      S1 <- as.matrix(rowSums(Y[[1]]))\r\n      S2 <- as.matrix(rowSums(Y[[2]]))\r\n      \r\n      # residual scores\r\n      R1 <- S1 - mean(S1)\r\n      R2 <- S2 - mean(S2)\r\n      \r\n      # Pearson product-moment correlation coefficient\r\n      r[i] <- (t(R1) %*% R2) / (sqrt((t(R1) %*% R1)) * sqrt((t(R2) %*% R2)))\r\n    }\r\n    \r\n    return(mean(r))\r\n  }\r\n\r\n# compute the Pearson product-moment correlation coefficient\r\npearson(responses, seed = 456, n = 1)\r\n\r\n\r\n[1] 0.3499066\r\n\r\nWe can also double check if the function is right by splitting response of the examinee into two halves and compute a correlation coefficient by ourselves.\r\n\r\n\r\nShow code\r\n\r\n# compare\r\n# split items\r\nset.seed(456)\r\nY <- split.items(responses)\r\n\r\n# total scores\r\nSet1 <- as.matrix(rowSums(Y[[1]]))\r\nSet2 <- as.matrix(rowSums(Y[[2]]))\r\n\r\ncor(Set1, Set2)\r\n\r\n\r\n          [,1]\r\n[1,] 0.3499066\r\n\r\nNote that the split-half method can both overestimate or underestimate ability of the examinees when two halves of the test are not parallel.\r\nStandard Error of Measurement\r\nStandard error of measurement is a measure of the spread of observed score around the “true” score. The standard error of measurement that uses Coefficient’s alpha as its reliability statistics can be computed as follows:\r\n\r\n\r\nShow code\r\n\r\nSEM <-\r\n  function(X){\r\n    source(\"cronbachs.alpha.R\")\r\n    X <- data.matrix(X)\r\n    \r\n    return(sd(rowSums(X)) * sqrt(1 - cronbachs.alpha(X)[[1]]))\r\n  }\r\n\r\nSEM(responses)\r\n\r\n\r\n[1] 2.036401\r\n\r\nAs a result, the range in which the true score of an examinee might stay in could be ± 2.036401.\r\nConfidence Intervals for True Scores\r\nWe can also compute confidence interval for true scores of each item via the code below. Note that one limitation of CTT is that it is population dependent, so the confidence interval may change when the test is administered to a different group of examinees.\r\n\r\n\r\nShow code\r\n\r\n# 90% confidence interval for the true score\r\nhead(cbind(lower_bound = round(rowSums(responses)-1.65* sd(rowSums(responses))*\r\n                                 sqrt(1-KR20(responses)), 2), observed = rowSums(responses),\r\n           upper_bound = round(rowSums(responses)+1.65* sd(rowSums(responses))*\r\n                                 sqrt(1-KR20(responses)), 2)), 20)\r\n\r\n\r\n      lower_bound observed upper_bound\r\n [1,]        5.65        9       12.35\r\n [2,]        2.65        6        9.35\r\n [3,]        9.65       13       16.35\r\n [4,]        5.65        9       12.35\r\n [5,]       10.65       14       17.35\r\n [6,]        8.65       12       15.35\r\n [7,]        3.65        7       10.35\r\n [8,]        5.65        9       12.35\r\n [9,]        4.65        8       11.35\r\n[10,]        7.65       11       14.35\r\n[11,]        2.65        6        9.35\r\n[12,]        4.65        8       11.35\r\n[13,]        1.65        5        8.35\r\n[14,]        9.65       13       16.35\r\n[15,]        4.65        8       11.35\r\n[16,]        1.65        5        8.35\r\n[17,]       11.65       15       18.35\r\n[18,]        4.65        8       11.35\r\n[19,]        6.65       10       13.35\r\n[20,]        9.65       13       16.35\r\n\r\nItem Analysis\r\nItem Analysis of the Classical Test Theory approach relies on two statistics to evaluate an item, P-value (not to be confused with p-value in statistical tests) and point-biserial correlation coefficient (p-Bis).\r\n\r\nP-value in this context represents the proportion of examinees responding in the keyed direction It is typically referred to as item difficulty. Point-biserial corrrelation coefficient is the correlation between a particular item and other items; this index is typically referred to as item discrimination that indicates the degree to which that item can distinguish high ability examinee or examinees who actually know that construct from examinees who do not possess adequate knowledge to get that item correctly.\r\nItem Discrimination.\r\nAs mentioned above, item discrimination refers to how well an item discriminates high-ability examinees from those with low-ability. Items that are very hard (i.e., p < 0.20) or very easy (p > 0.90) usually have lower item discrimination values than items with medium difficulty.\r\nWe usually examine point-biserial correlation coefficient (p-Bis) of the item. If p-Bis is lower than 0.20, the item can be flagged for low discrimination, while 0.20 to 0.39 indicates good discrimination, and 0.4 and above indicates excellent discrimination. If p-Bis is negative, then the item doesn’t seem to measure the same construct that the other items are measuring. It could also mean that the item is mis-keyed.\r\n\r\n\r\nShow code\r\n\r\nitem.analysis <- \r\n  function(responses)\r\n    {\r\n    # CRITICAL VALUES\r\n    cvpb = 0.20\r\n    cvdl = 0.15\r\n    cvdu = 0.85\r\n    \r\n    require(CTT, warn.conflicts = FALSE, quietly = TRUE)\r\n    (ctt.analysis <- CTT::reliability(responses, itemal = TRUE, NA.Delete = TRUE))\r\n    \r\n    # Mark items that are potentially problematic\r\n    item.analysis <- data.frame(item = seq(1:ctt.analysis$nItem),\r\n                                r.pbis = ctt.analysis$pBis,\r\n                                bis = ctt.analysis$bis,\r\n                                item.mean = ctt.analysis$itemMean,\r\n                                alpha.del = ctt.analysis$alphaIfDeleted)\r\n  \r\n    if (TRUE) {\r\n      item.analysis$check <- \r\n        ifelse(item.analysis$r.pbis < cvpb |\r\n                 item.analysis$item.mean < cvdl |\r\n                 item.analysis$item.mean > cvdu, \"X\", \"\")\r\n    }\r\n    \r\n    return(item.analysis)\r\n  }\r\n\r\nkbl(item.analysis(responses)) %>%\r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\r\n                full_width = FALSE, position = \"left\")\r\n\r\n\r\n\r\nitem\r\n\r\n\r\nr.pbis\r\n\r\n\r\nbis\r\n\r\n\r\nitem.mean\r\n\r\n\r\nalpha.del\r\n\r\n\r\ncheck\r\n\r\n\r\n1\r\n\r\n\r\n0.1147596\r\n\r\n\r\n0.1683232\r\n\r\n\r\n0.7593361\r\n\r\n\r\n0.5353385\r\n\r\n\r\nX\r\n\r\n\r\n2\r\n\r\n\r\n0.1035530\r\n\r\n\r\n0.1409292\r\n\r\n\r\n0.7344398\r\n\r\n\r\n0.5372914\r\n\r\n\r\nX\r\n\r\n\r\n3\r\n\r\n\r\n0.0506175\r\n\r\n\r\n0.0685887\r\n\r\n\r\n0.2572614\r\n\r\n\r\n0.5452346\r\n\r\n\r\nX\r\n\r\n\r\n4\r\n\r\n\r\n0.0591143\r\n\r\n\r\n0.0785356\r\n\r\n\r\n0.6804979\r\n\r\n\r\n0.5450389\r\n\r\n\r\nX\r\n\r\n\r\n5\r\n\r\n\r\n0.1111202\r\n\r\n\r\n0.1406669\r\n\r\n\r\n0.3651452\r\n\r\n\r\n0.5369402\r\n\r\n\r\nX\r\n\r\n\r\n6\r\n\r\n\r\n0.3656502\r\n\r\n\r\n0.4640379\r\n\r\n\r\n0.5394191\r\n\r\n\r\n0.4909063\r\n\r\n\r\n\r\n\r\n7\r\n\r\n\r\n0.0700444\r\n\r\n\r\n0.0920897\r\n\r\n\r\n0.2780083\r\n\r\n\r\n0.5426513\r\n\r\n\r\nX\r\n\r\n\r\n8\r\n\r\n\r\n0.1112357\r\n\r\n\r\n0.1394688\r\n\r\n\r\n0.4813278\r\n\r\n\r\n0.5373705\r\n\r\n\r\nX\r\n\r\n\r\n9\r\n\r\n\r\n0.1515853\r\n\r\n\r\n0.2230714\r\n\r\n\r\n0.8298755\r\n\r\n\r\n0.5299043\r\n\r\n\r\nX\r\n\r\n\r\n10\r\n\r\n\r\n-0.0434185\r\n\r\n\r\n-0.0549382\r\n\r\n\r\n0.4356846\r\n\r\n\r\n0.5634988\r\n\r\n\r\nX\r\n\r\n\r\n11\r\n\r\n\r\n0.1696824\r\n\r\n\r\n0.2157830\r\n\r\n\r\n0.3360996\r\n\r\n\r\n0.5269375\r\n\r\n\r\nX\r\n\r\n\r\n12\r\n\r\n\r\n0.1279600\r\n\r\n\r\n0.1627060\r\n\r\n\r\n0.5601660\r\n\r\n\r\n0.5343645\r\n\r\n\r\nX\r\n\r\n\r\n13\r\n\r\n\r\n0.2312726\r\n\r\n\r\n0.2933878\r\n\r\n\r\n0.3900415\r\n\r\n\r\n0.5161192\r\n\r\n\r\n\r\n\r\n14\r\n\r\n\r\n0.2134979\r\n\r\n\r\n0.2656683\r\n\r\n\r\n0.4481328\r\n\r\n\r\n0.5191567\r\n\r\n\r\n\r\n\r\n15\r\n\r\n\r\n0.1947141\r\n\r\n\r\n0.2547864\r\n\r\n\r\n0.3112033\r\n\r\n\r\n0.5227779\r\n\r\n\r\nX\r\n\r\n\r\n16\r\n\r\n\r\n0.3353941\r\n\r\n\r\n0.4428850\r\n\r\n\r\n0.6390041\r\n\r\n\r\n0.4977310\r\n\r\n\r\n\r\n\r\n17\r\n\r\n\r\n0.4609036\r\n\r\n\r\n0.5799529\r\n\r\n\r\n0.4356846\r\n\r\n\r\n0.4727918\r\n\r\n\r\n\r\n\r\n18\r\n\r\n\r\n0.4013261\r\n\r\n\r\n0.5080745\r\n\r\n\r\n0.3319502\r\n\r\n\r\n0.4865119\r\n\r\n\r\n\r\n\r\n19\r\n\r\n\r\n0.0956187\r\n\r\n\r\n0.1323704\r\n\r\n\r\n0.1908714\r\n\r\n\r\n0.5375545\r\n\r\n\r\nX\r\n\r\n\r\n20\r\n\r\n\r\n0.1054536\r\n\r\n\r\n0.1323520\r\n\r\n\r\n0.4356846\r\n\r\n\r\n0.5382775\r\n\r\n\r\nX\r\n\r\n\r\nItem Difficulty\r\nUnder CTT, item difficulty is simply the percentages of examinees obtaining the correct answer. Item difficulty ranges from 0 to 1, with higher values indicate easier items as more examinees are able to correctly answer this item. This index is useful in assessing whether it is appropriate for the level of the students taking the test.\r\nThe desired range of item difficulty index is between 0.3 to 0.9 (by approximate), while the number close to 0 or 1 offers little information on measuring students’ level of the construct. In plain words, we wouldn’t want to have test items that are too easy that everybody get it right, or items that are too hard that no one can answer it correctly. However, the extreme cut-off for item difficulty could apply to measurements that are designed for extreme groups.\r\n\r\n\r\nShow code\r\n\r\nItem_Difficulty <- item.exam(x = responses, y = NULL, discrim = T)\r\n\r\nkbl(Item_Difficulty) %>%\r\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\r\n                full_width = FALSE, position = \"left\")\r\n\r\n\r\n\r\nSample.SD\r\n\r\n\r\nItem.total\r\n\r\n\r\nItem.Tot.woi\r\n\r\n\r\nDifficulty\r\n\r\n\r\nDiscrimination\r\n\r\n\r\nItem.Criterion\r\n\r\n\r\nItem.Reliab\r\n\r\n\r\nItem.Rel.woi\r\n\r\n\r\nItem.Validity\r\n\r\n\r\n0.4283763\r\n\r\n\r\n0.2544665\r\n\r\n\r\n0.1147596\r\n\r\n\r\n0.7593361\r\n\r\n\r\n0.2125\r\n\r\n\r\nNA\r\n\r\n\r\n0.1087810\r\n\r\n\r\n0.0490582\r\n\r\n\r\nNA\r\n\r\n\r\n0.4425501\r\n\r\n\r\n0.2483214\r\n\r\n\r\n0.1035530\r\n\r\n\r\n0.7344398\r\n\r\n\r\n0.2875\r\n\r\n\r\nNA\r\n\r\n\r\n0.1096664\r\n\r\n\r\n0.0457322\r\n\r\n\r\nNA\r\n\r\n\r\n0.4380344\r\n\r\n\r\n0.1956677\r\n\r\n\r\n0.0506175\r\n\r\n\r\n0.2572614\r\n\r\n\r\n0.2375\r\n\r\n\r\nNA\r\n\r\n\r\n0.0855312\r\n\r\n\r\n0.0221262\r\n\r\n\r\nNA\r\n\r\n\r\n0.4672541\r\n\r\n\r\n0.2135535\r\n\r\n\r\n0.0591143\r\n\r\n\r\n0.6804979\r\n\r\n\r\n0.2125\r\n\r\n\r\nNA\r\n\r\n\r\n0.0995765\r\n\r\n\r\n0.0275640\r\n\r\n\r\nNA\r\n\r\n\r\n0.4824729\r\n\r\n\r\n0.2684805\r\n\r\n\r\n0.1111202\r\n\r\n\r\n0.3651452\r\n\r\n\r\n0.3125\r\n\r\n\r\nNA\r\n\r\n\r\n0.1292655\r\n\r\n\r\n0.0535011\r\n\r\n\r\nNA\r\n\r\n\r\n0.4994811\r\n\r\n\r\n0.5054235\r\n\r\n\r\n0.3656502\r\n\r\n\r\n0.5394191\r\n\r\n\r\n0.5500\r\n\r\n\r\nNA\r\n\r\n\r\n0.2519252\r\n\r\n\r\n0.1822561\r\n\r\n\r\nNA\r\n\r\n\r\n0.4489499\r\n\r\n\r\n0.2181283\r\n\r\n\r\n0.0700444\r\n\r\n\r\n0.2780083\r\n\r\n\r\n0.1625\r\n\r\n\r\nNA\r\n\r\n\r\n0.0977253\r\n\r\n\r\n0.0313811\r\n\r\n\r\nNA\r\n\r\n\r\n0.5006911\r\n\r\n\r\n0.2744754\r\n\r\n\r\n0.1112357\r\n\r\n\r\n0.4813278\r\n\r\n\r\n0.3125\r\n\r\n\r\nNA\r\n\r\n\r\n0.1371420\r\n\r\n\r\n0.0555790\r\n\r\n\r\nNA\r\n\r\n\r\n0.3765241\r\n\r\n\r\n0.2730001\r\n\r\n\r\n0.1515853\r\n\r\n\r\n0.8298755\r\n\r\n\r\n0.2375\r\n\r\n\r\nNA\r\n\r\n\r\n0.1025776\r\n\r\n\r\n0.0569570\r\n\r\n\r\nNA\r\n\r\n\r\n0.4968782\r\n\r\n\r\n0.1224407\r\n\r\n\r\n-0.0434185\r\n\r\n\r\n0.4356846\r\n\r\n\r\n0.1375\r\n\r\n\r\nNA\r\n\r\n\r\n0.0607118\r\n\r\n\r\n-0.0215289\r\n\r\n\r\nNA\r\n\r\n\r\n0.4733565\r\n\r\n\r\n0.3208135\r\n\r\n\r\n0.1696824\r\n\r\n\r\n0.3360996\r\n\r\n\r\n0.3125\r\n\r\n\r\nNA\r\n\r\n\r\n0.1515438\r\n\r\n\r\n0.0801535\r\n\r\n\r\nNA\r\n\r\n\r\n0.4973999\r\n\r\n\r\n0.2892525\r\n\r\n\r\n0.1279600\r\n\r\n\r\n0.5601660\r\n\r\n\r\n0.2625\r\n\r\n\r\nNA\r\n\r\n\r\n0.1435753\r\n\r\n\r\n0.0635151\r\n\r\n\r\nNA\r\n\r\n\r\n0.4887744\r\n\r\n\r\n0.3825120\r\n\r\n\r\n0.2312726\r\n\r\n\r\n0.3900415\r\n\r\n\r\n0.4000\r\n\r\n\r\nNA\r\n\r\n\r\n0.1865738\r\n\r\n\r\n0.1128054\r\n\r\n\r\nNA\r\n\r\n\r\n0.4983375\r\n\r\n\r\n0.3691600\r\n\r\n\r\n0.2134979\r\n\r\n\r\n0.4481328\r\n\r\n\r\n0.4750\r\n\r\n\r\nNA\r\n\r\n\r\n0.1835842\r\n\r\n\r\n0.1061730\r\n\r\n\r\nNA\r\n\r\n\r\n0.4639493\r\n\r\n\r\n0.3412014\r\n\r\n\r\n0.1947141\r\n\r\n\r\n0.3112033\r\n\r\n\r\n0.3625\r\n\r\n\r\nNA\r\n\r\n\r\n0.1579714\r\n\r\n\r\n0.0901499\r\n\r\n\r\nNA\r\n\r\n\r\n0.4812889\r\n\r\n\r\n0.4738815\r\n\r\n\r\n0.3353941\r\n\r\n\r\n0.6390041\r\n\r\n\r\n0.4750\r\n\r\n\r\nNA\r\n\r\n\r\n0.2276002\r\n\r\n\r\n0.1610862\r\n\r\n\r\nNA\r\n\r\n\r\n0.4968782\r\n\r\n\r\n0.5863010\r\n\r\n\r\n0.4609036\r\n\r\n\r\n0.4356846\r\n\r\n\r\n0.6125\r\n\r\n\r\nNA\r\n\r\n\r\n0.2907152\r\n\r\n\r\n0.2285373\r\n\r\n\r\nNA\r\n\r\n\r\n0.4718933\r\n\r\n\r\n0.5290626\r\n\r\n\r\n0.4013261\r\n\r\n\r\n0.3319502\r\n\r\n\r\n0.5000\r\n\r\n\r\nNA\r\n\r\n\r\n0.2491426\r\n\r\n\r\n0.1889898\r\n\r\n\r\nNA\r\n\r\n\r\n0.3938058\r\n\r\n\r\n0.2248264\r\n\r\n\r\n0.0956187\r\n\r\n\r\n0.1908714\r\n\r\n\r\n0.1500\r\n\r\n\r\nNA\r\n\r\n\r\n0.0883541\r\n\r\n\r\n0.0375770\r\n\r\n\r\nNA\r\n\r\n\r\n0.4968782\r\n\r\n\r\n0.2677464\r\n\r\n\r\n0.1054536\r\n\r\n\r\n0.4356846\r\n\r\n\r\n0.3125\r\n\r\n\r\nNA\r\n\r\n\r\n0.1327610\r\n\r\n\r\n0.0522888\r\n\r\n\r\nNA\r\n\r\n\r\nVisualizing Item Discrimination\r\nWe can use the functions below to visualize for items that should be revised for low discrimination power\r\n\r\n\r\nShow code\r\n\r\nitem.discrimination <-\r\n  function(responses)\r\n    {\r\n    # CRITICAL VALUES\r\n    cvpb = 0.20\r\n    cvdl = 0.15\r\n    cvdu = 0.85\r\n    \r\n    require(CTT, warn.conflicts = FALSE, quietly = TRUE)\r\n    ctt.analysis <- CTT::reliability(responses, itemal = TRUE, NA.Delete = TRUE)\r\n    \r\n    item.discrimination <- data.frame(item = 1:ctt.analysis$nItem , \r\n                                      discrimination = ctt.analysis$pBis)\r\n    \r\n    plot(item.discrimination,\r\n         type = \"p\",\r\n         pch = 1,\r\n         cex = 3,\r\n         col = \"purple\",\r\n         ylab = \"point-Biserial correlation\",\r\n         xlab = \"Item Number\",\r\n         ylim = c(0, 1),\r\n         main = \"Test Item Discriminations\")\r\n    \r\n    abline(h = cvpb, col = \"red\")\r\n    \r\n    outlier <- data.matrix(subset(item.discrimination,\r\n                                  subset = (item.discrimination[, 2] < cvpb)))\r\n    \r\n    text(outlier, paste(\"i\", outlier[,1], sep = \"\"), col = \"red\", cex = .7)\r\n    \r\n    return(item.discrimination[order(item.discrimination$discrimination),])\r\n  }\r\n\r\nitem.discrimination(responses)\r\n\r\n\r\n\r\n   item discrimination\r\n10   10    -0.04341855\r\n3     3     0.05061749\r\n4     4     0.05911432\r\n7     7     0.07004435\r\n19   19     0.09561872\r\n2     2     0.10355300\r\n20   20     0.10545357\r\n5     5     0.11112015\r\n8     8     0.11123568\r\n1     1     0.11475958\r\n12   12     0.12795997\r\n9     9     0.15158531\r\n11   11     0.16968243\r\n15   15     0.19471413\r\n14   14     0.21349786\r\n13   13     0.23127262\r\n16   16     0.33539410\r\n6     6     0.36565023\r\n18   18     0.40132606\r\n17   17     0.46090362\r\n\r\nVisualizing Item Total-Correlation (Bis)\r\nThe item total correlation is a correlation between the question score (e.g., 0 or 1 for multiple choice) and the overall assessment score. The assumption is that if an examinee gets a question correctly they should, in general, have higher overall assessment scores than participants who get that question wrong.\r\n\r\n\r\nShow code\r\n\r\ntest_item.total <-\r\n  function(responses)\r\n    {\r\n    # CRITICAL VALUES\r\n    cvpb = 0.20\r\n    cvdl = 0.15\r\n    cvdu = 0.85\r\n    \r\n    require(CTT, warn.conflicts = FALSE, quietly = TRUE)\r\n    ctt.analysis <- CTT::reliability(responses, itemal = TRUE, NA.Delete = TRUE)\r\n    \r\n    test_item.total <- data.frame(item = 1:ctt.analysis$nItem , \r\n                                  biserial = ctt.analysis$bis)\r\n    \r\n    plot(test_item.total,\r\n         main = \"Test Item-Total Correlation\",\r\n         type = \"p\",\r\n         pch = 1,\r\n         cex = 2.8,\r\n         col = \"purple\",\r\n         ylab = \"Biserial correlation\",\r\n         xlab = \"Item Number\",\r\n         ylim = c(0, 1),\r\n         xlim = c(0, ctt.analysis$nItem))\r\n    \r\n    abline(h = cvpb, col = \"red\")\r\n    \r\n    outlier <- data.matrix(subset(test_item.total,\r\n                                  subset = test_item.total[,2] < cvpb))\r\n    \r\n    text(outlier, paste(\"i\", outlier[,1], sep = \"\"), col = \"red\", cex = .7)\r\n    \r\n    return(test_item.total[order(test_item.total$biserial),])\r\n    }\r\n\r\ntest_item.total(responses)\r\n\r\n\r\n\r\n   item    biserial\r\n10   10 -0.05493825\r\n3     3  0.06858871\r\n4     4  0.07853557\r\n7     7  0.09208970\r\n20   20  0.13235198\r\n19   19  0.13237037\r\n8     8  0.13946879\r\n5     5  0.14066694\r\n2     2  0.14092920\r\n12   12  0.16270597\r\n1     1  0.16832321\r\n11   11  0.21578295\r\n9     9  0.22307140\r\n15   15  0.25478638\r\n14   14  0.26566826\r\n13   13  0.29338781\r\n16   16  0.44288497\r\n6     6  0.46403793\r\n18   18  0.50807452\r\n17   17  0.57995293\r\n\r\nDistractor/Option Analysis\r\nIn distractor analysis, examinees are divided into three ability levels (i.e., lower, middle and upper) based on their total test score. The proportions of examinees who mark each option in each of the three ability levels are compared. In the lower ability level, we would expect to see a smaller proportion of examinees choosing the correct option and a larger proportion of them choosing the incorrect options (known as distractors).\r\nIdeally, good distractors would attract about the same proportion of examinees. Distractors that don’t attract any or attract a very small proportion of examinees relative to other distractors should be considered for revision. We do not want response options that are too obvious.\r\nIn those with higher ability level, we would expect to see the majority of examinees choose the correct option. If distractors are more appealing than the correct option to high ability examineess, then it should be eliminated or revised.\r\n\r\n\r\nShow code\r\n\r\ndistractorAnalysis(items = data, key = key, nGroups = 4, pTable = T)\r\n\r\n\r\n$i001\r\n  correct key   n       rspP       pBis     discrim      lower\r\nA       *   A 183 0.75933610  0.1147596  0.22334218 0.70769231\r\nB           B  16 0.06639004 -0.2888067 -0.13846154 0.13846154\r\nC           C  22 0.09128631 -0.1975788 -0.05968170 0.07692308\r\nD           D  20 0.08298755 -0.1841578 -0.02519894 0.07692308\r\n       mid50      mid75      upper\r\nA 0.63636364 0.78846154 0.93103448\r\nB 0.06060606 0.05769231 0.00000000\r\nC 0.16666667 0.09615385 0.01724138\r\nD 0.13636364 0.05769231 0.05172414\r\n\r\n$i002\r\n  correct key   n       rspP        pBis     discrim      lower\r\nA           A   8 0.03319502 -0.01743924  0.01909814 0.01538462\r\nB           B  40 0.16597510 -0.28940212 -0.16180371 0.23076923\r\nC       *   C 177 0.73443983  0.10355300  0.26392573 0.61538462\r\nD           D  16 0.06639004 -0.28880672 -0.12122016 0.13846154\r\n       mid50      mid75      upper\r\nA 0.01515152 0.07692308 0.03448276\r\nB 0.16666667 0.19230769 0.06896552\r\nC 0.74242424 0.71153846 0.87931034\r\nD 0.07575758 0.01923077 0.01724138\r\n\r\n$i003\r\n  correct key  n      rspP        pBis     discrim     lower\r\nA           A 61 0.2531120 -0.21045253 -0.05278515 0.2769231\r\nB           B 74 0.3070539 -0.32875754 -0.22387268 0.4307692\r\nC       *   C 62 0.2572614  0.05061749  0.16021220 0.1846154\r\nD           D 44 0.1825726 -0.04787523  0.11644562 0.1076923\r\n      mid50     mid75     upper\r\nA 0.2878788 0.2115385 0.2241379\r\nB 0.2878788 0.2884615 0.2068966\r\nC 0.1818182 0.3461538 0.3448276\r\nD 0.2424242 0.1538462 0.2241379\r\n\r\n$i004\r\n  correct key   n       rspP        pBis     discrim      lower\r\nA           A  11 0.04564315 -0.16015191 -0.09045093 0.10769231\r\nB           B  24 0.09958506 -0.15640748 -0.04244032 0.07692308\r\nC           C  42 0.17427386 -0.28460760 -0.15437666 0.29230769\r\nD       *   D 164 0.68049793  0.05911432  0.28726790 0.52307692\r\n      mid50      mid75      upper\r\nA 0.0000000 0.05769231 0.01724138\r\nB 0.1515152 0.13461538 0.03448276\r\nC 0.1363636 0.11538462 0.13793103\r\nD 0.7121212 0.69230769 0.81034483\r\n\r\n$i005\r\n  correct key  n       rspP       pBis     discrim      lower\r\nA           A 84 0.34854772 -0.2097597 -0.02997347 0.32307692\r\nB       *   B 88 0.36514523  0.1111202  0.27294430 0.26153846\r\nC           C 61 0.25311203 -0.3237027 -0.19681698 0.36923077\r\nD           D  8 0.03319502 -0.1706663 -0.04615385 0.04615385\r\n       mid50      mid75     upper\r\nA 0.42424242 0.34615385 0.2931034\r\nB 0.22727273 0.48076923 0.5344828\r\nC 0.30303030 0.13461538 0.1724138\r\nD 0.04545455 0.03846154 0.0000000\r\n\r\n$i006\r\n  correct key   n       rspP       pBis    discrim     lower\r\nA           A  61 0.25311203 -0.3587149 -0.2503979 0.3538462\r\nB       *   B 130 0.53941909  0.3656502  0.6466844 0.2153846\r\nC           C  34 0.14107884 -0.3800902 -0.2615385 0.2615385\r\nD           D  16 0.06639004 -0.2994444 -0.1347480 0.1692308\r\n       mid50      mid75      upper\r\nA 0.34848485 0.17307692 0.10344828\r\nB 0.45454545 0.69230769 0.86206897\r\nC 0.16666667 0.11538462 0.00000000\r\nD 0.03030303 0.01923077 0.03448276\r\n\r\n$i007\r\n  correct key  n      rspP        pBis      discrim     lower\r\nA       *   A 67 0.2780083  0.07004435  0.211936340 0.1846154\r\nB           B 63 0.2614108 -0.17772394  0.027851459 0.2307692\r\nC           C 47 0.1950207 -0.13004489  0.003183024 0.1692308\r\nD           D 64 0.2655602 -0.32387947 -0.242970822 0.4153846\r\n      mid50     mid75     upper\r\nA 0.2424242 0.3076923 0.3965517\r\nB 0.3333333 0.2115385 0.2586207\r\nC 0.1818182 0.2692308 0.1724138\r\nD 0.2424242 0.2115385 0.1724138\r\n\r\n$i008\r\n  correct key   n       rspP        pBis     discrim      lower\r\nA           A  61 0.25311203 -0.45040855 -0.34641910 0.41538462\r\nB           B  53 0.21991701 -0.11191555  0.01061008 0.23076923\r\nC       *   C 116 0.48132780  0.11123568  0.36286472 0.29230769\r\nD           D  11 0.04564315 -0.08820493 -0.02705570 0.06153846\r\n       mid50      mid75      upper\r\nA 0.34848485 0.13461538 0.06896552\r\nB 0.16666667 0.25000000 0.24137931\r\nC 0.42424242 0.59615385 0.65517241\r\nD 0.06060606 0.01923077 0.03448276\r\n\r\n$i009\r\n  correct key   n       rspP       pBis     discrim      lower\r\nA           A   8 0.03319502 -0.1554692 -0.04244032 0.07692308\r\nB       *   B 200 0.82987552  0.1515853  0.28488064 0.64615385\r\nC           C  19 0.07883817 -0.2169620 -0.15013263 0.18461538\r\nD           D  14 0.05809129 -0.2866682 -0.09230769 0.09230769\r\n       mid50      mid75      upper\r\nA 0.01515152 0.00000000 0.03448276\r\nB 0.87878788 0.88461538 0.93103448\r\nC 0.01515152 0.07692308 0.03448276\r\nD 0.09090909 0.03846154 0.00000000\r\n\r\n$i010\r\n  correct key   n       rspP        pBis     discrim     lower\r\nA           A  94 0.39004149 -0.13702229  0.02360743 0.3384615\r\nB           B  23 0.09543568 -0.16743006 -0.05596817 0.1076923\r\nC           C  19 0.07883817 -0.27622610 -0.13474801 0.1692308\r\nD       *   D 105 0.43568465 -0.04341855  0.16710875 0.3846154\r\n       mid50      mid75      upper\r\nA 0.43939394 0.42307692 0.36206897\r\nB 0.07575758 0.15384615 0.05172414\r\nC 0.04545455 0.05769231 0.03448276\r\nD 0.43939394 0.36538462 0.55172414\r\n\r\n$i011\r\n  correct key   n       rspP       pBis     discrim     lower\r\nA           A 117 0.48547718 -0.3887571 -0.29151194 0.5846154\r\nB           B  23 0.09543568 -0.1398436 -0.02148541 0.1076923\r\nC       *   C  81 0.33609959  0.1696824  0.38435013 0.1846154\r\nD           D  20 0.08298755 -0.1548240 -0.07135279 0.1230769\r\n       mid50      mid75      upper\r\nA 0.59090909 0.44230769 0.29310345\r\nB 0.12121212 0.05769231 0.08620690\r\nC 0.21212121 0.42307692 0.56896552\r\nD 0.07575758 0.07692308 0.05172414\r\n\r\n$i012\r\n  correct key   n       rspP       pBis     discrim     lower\r\nA       *   A 135 0.56016598  0.1279600  0.32413793 0.4000000\r\nB           B  59 0.24481328 -0.2253473 -0.03925729 0.2461538\r\nC           C  24 0.09958506 -0.2766477 -0.16737401 0.1846154\r\nD           D  23 0.09543568 -0.2673278 -0.11750663 0.1692308\r\n       mid50      mid75      upper\r\nA 0.50000000 0.65384615 0.72413793\r\nB 0.31818182 0.19230769 0.20689655\r\nC 0.09090909 0.09615385 0.01724138\r\nD 0.09090909 0.05769231 0.05172414\r\n\r\n$i013\r\n  correct key  n      rspP       pBis    discrim     lower     mid50\r\nA           A 27 0.1120332 -0.2781623 -0.1501326 0.1846154 0.1363636\r\nB           B 46 0.1908714 -0.1753162 -0.1100796 0.2307692 0.1212121\r\nC           C 74 0.3070539 -0.3778479 -0.2564987 0.4461538 0.3333333\r\nD       *   D 94 0.3900415  0.2312726  0.5167109 0.1384615 0.4090909\r\n       mid75      upper\r\nA 0.07692308 0.03448276\r\nB 0.30769231 0.12068966\r\nC 0.23076923 0.18965517\r\nD 0.38461538 0.65517241\r\n\r\n$i014\r\n  correct key   n      rspP       pBis    discrim     lower     mid50\r\nA       *   A 108 0.4481328  0.2134979  0.5358090 0.1538462 0.4393939\r\nB           B  31 0.1286307 -0.2535706 -0.1445623 0.2307692 0.1060606\r\nC           C  58 0.2406639 -0.3415163 -0.2774536 0.4153846 0.2121212\r\nD           D  44 0.1825726 -0.2361190 -0.1137931 0.2000000 0.2424242\r\n       mid75     upper\r\nA 0.55769231 0.6896552\r\nB 0.07692308 0.0862069\r\nC 0.17307692 0.1379310\r\nD 0.19230769 0.0862069\r\n\r\n$i015\r\n  correct key  n       rspP       pBis     discrim      lower\r\nA           A 15 0.06224066 -0.1403108 -0.02148541 0.10769231\r\nB           B 74 0.30705394 -0.3315040 -0.24111406 0.43076923\r\nC       *   C 75 0.31120332  0.1947141  0.40769231 0.09230769\r\nD           D 77 0.31950207 -0.2661525 -0.14509284 0.36923077\r\n       mid50      mid75     upper\r\nA 0.03030303 0.01923077 0.0862069\r\nB 0.31818182 0.26923077 0.1896552\r\nC 0.30303030 0.38461538 0.5000000\r\nD 0.34848485 0.32692308 0.2241379\r\n\r\n$i016\r\n  correct key   n       rspP       pBis     discrim      lower\r\nA       *   A 154 0.63900415  0.3353941  0.59071618 0.32307692\r\nB           B  39 0.16182573 -0.4136149 -0.31750663 0.36923077\r\nC           C  11 0.04564315 -0.1861371 -0.06153846 0.06153846\r\nD           D  37 0.15352697 -0.3545356 -0.21167109 0.24615385\r\n       mid50      mid75      upper\r\nA 0.56060606 0.82692308 0.91379310\r\nB 0.13636364 0.05769231 0.05172414\r\nC 0.09090909 0.01923077 0.00000000\r\nD 0.21212121 0.09615385 0.03448276\r\n\r\n$i017\r\n  correct key   n      rspP       pBis    discrim     lower\r\nA           A  82 0.3402490 -0.4829386 -0.4350133 0.5384615\r\nB           B  26 0.1078838 -0.3113845 -0.1538462 0.1538462\r\nC       *   C 105 0.4356846  0.4609036  0.7716180 0.1076923\r\nD           D  28 0.1161826 -0.2706021 -0.1827586 0.2000000\r\n       mid50      mid75      upper\r\nA 0.36363636 0.32692308 0.10344828\r\nB 0.21212121 0.03846154 0.00000000\r\nC 0.33333333 0.48076923 0.87931034\r\nD 0.09090909 0.15384615 0.01724138\r\n\r\n$i018\r\n  correct key  n       rspP       pBis    discrim      lower\r\nA           A 96 0.39834025 -0.4250997 -0.3180371 0.50769231\r\nB           B 43 0.17842324 -0.2755281 -0.1925729 0.26153846\r\nC           C 22 0.09128631 -0.2623175 -0.1366048 0.15384615\r\nD       *   D 80 0.33195021  0.4013261  0.6472149 0.07692308\r\n       mid50      mid75      upper\r\nA 0.48484848 0.38461538 0.18965517\r\nB 0.18181818 0.19230769 0.06896552\r\nC 0.09090909 0.09615385 0.01724138\r\nD 0.24242424 0.32692308 0.72413793\r\n\r\n$i019\r\n  correct key   n       rspP        pBis     discrim     lower\r\nA           A  17 0.07053942 -0.14630133 -0.07135279 0.1230769\r\nB           B  48 0.19917012 -0.23586731 -0.09469496 0.2153846\r\nC           C 130 0.53941909 -0.22016521 -0.05570292 0.5384615\r\nD       *   D  46 0.19087137  0.09561872  0.22175066 0.1230769\r\n       mid50      mid75      upper\r\nA 0.06060606 0.03846154 0.05172414\r\nB 0.22727273 0.23076923 0.12068966\r\nC 0.57575758 0.55769231 0.48275862\r\nD 0.13636364 0.17307692 0.34482759\r\n\r\n$i020\r\n  correct key   n       rspP       pBis     discrim      lower\r\nA           A  21 0.08713693 -0.2194626 -0.07506631 0.09230769\r\nB           B  49 0.20331950 -0.1948577 -0.07374005 0.24615385\r\nC       *   C 105 0.43568465  0.1054536  0.32838196 0.29230769\r\nD           D  66 0.27385892 -0.2973162 -0.17957560 0.36923077\r\n      mid50      mid75      upper\r\nA 0.1515152 0.07692308 0.01724138\r\nB 0.1969697 0.19230769 0.17241379\r\nC 0.3939394 0.46153846 0.62068966\r\nD 0.2575758 0.26923077 0.18965517\r\n\r\nConcluding remark\r\nWhile we have discussed a lot about characteristics of test items such as Reliability, Item Difficulty, and Item Discrimination, those concepts are building blocks of a bigger concept known as test validity. Test Validity is the foundational concept in measurement that concerns the evidential support to the interpretation and use score of a test score in a particular context (AERA et al., 2014);\r\nFor example, in order to use WAIS-IV for diagnostic purposes in Thailand, an array of validity evidence needs to be established such as evidence based on test content (i.e., is content of the test known to the Thai population?), evidence based on internal structure of the test (i.e., how do we know if items of the test measure the same psychological attribute?), or even evidence based on consequences of the test (i.e., are we sure that claims made from the test such as learning disability diagnosis are for that purpose only and for no other unrelated claims such as stigmatization?).\r\nCTT is relatively weak in its theoretical assumption, which makes it applicable to many testing situations. This framework also does not require a complex theoretical model to assess psychological attributes of examinees. However, CTT falls short in its sample dependency, which makes it less preferable in test development scenarios that require associations with other population such as test equating and computerized adaptive testing. Item Response Theory is able to address this limitation.\r\nWith online assessment becoming more implemented, especially in this pandemic time where most activities take place in online environments, we can take advantage of the technology by having computers score the exam for us for improved efficiency. We can also improve property of our test by using information from item analysis as demonstrated above as well. While testing brings about anxiety in a lot of students (me included. I hate being tested), we can hardly deny that it is an important part of education and other settings (e.g., clinical, legal). For that, it is important that every decision made on the test needs to be supported by as much evidence as possible. As always, thank you very much for checking this post out. Good day, everyone!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-15-ctt/ctt_files/figure-html5/unnamed-chunk-28-1.png",
    "last_modified": "2022-01-16T00:09:10-07:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-01-01-efacfa/",
    "title": "Examining the Big 5 personality Dataset with factor analysis",
    "description": "For this entry, I will be examining the Big 5 personality Inventory data set with Exploratory Data Analysis to identify potential structures of personality trait and verify them with Confirmatory Factor Analysis.\n\n(8 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2022-01-01",
    "categories": [
      "R",
      "Statistics"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nDataset\r\nData preprocessing\r\nExploratory Factor Analysis\r\nConfirmatory Factor Analysis\r\nConclusion\r\n\r\nIntroduction\r\nThe concept of Big 5 personality traits represents tendency of individuals to possess five personality characteristics of extraversion, agreeableness, openness, conscientiousness, and neuroticism (Neal et al., 2012).\r\nHere, we will examine the data set with Exploratory Data Analysis (EFA) to see if we can identify any other structures aside from the original five groups as indicated by the concept of big 5 personality traits; then, we will verify those structures with Confirmatory Factor Analysis (CFA) to assess and compare their statistical characteristic (i.e., model fit).\r\nWe will begin by loading essential packages for data preprocessing and statistical modeling as indicated below.\r\n\r\n\r\nShow code\r\n\r\nlibrary(parameters) #for parameter processing\r\nlibrary(tidymodels) #for data splitting\r\nlibrary(tidyverse) #toolbox for R\r\nlibrary(psych) #for descriptive statistics and the data set\r\nlibrary(ggcorrplot) #for correlation matrix\r\nlibrary(see) #add-on for ggplot2\r\nlibrary(lavaan) #for SEM\r\nlibrary(performance) #assessment of Regression Models Performance\r\nlibrary(semPlot) #to plot path model for CFA\r\nlibrary(dlookr) #missing data diagnosis\r\nlibrary(mice) #missing data imputation\r\n\r\n\r\n\r\nDataset\r\nThe data set we use is a built-in data set from psych package, which was collected in the United States as a part of the Synthetic Aperture Personality Assessment (SAPA) project (Revelle et al., 2010). There are 2800 observations and 25 variables of all 5 personality traits.\r\nWe will begin by loading in the data set and check for its missing value.\r\n\r\n\r\nShow code\r\n\r\n# Load the data\r\ndata <- psych::bfi[, 1:25]\r\n\r\nhead(data)\r\n\r\n\r\n      A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1\r\n61617  2  4  3  4  4  2  3  3  4  4  3  3  3  4  4  3  4  2  2  3  3\r\n61618  2  4  5  2  5  5  4  4  3  4  1  1  6  4  3  3  3  3  5  5  4\r\n61620  5  4  5  4  4  4  5  4  2  5  2  4  4  4  5  4  5  4  2  3  4\r\n61621  4  4  6  5  5  4  4  3  5  5  5  3  4  4  4  2  5  2  4  1  3\r\n61622  2  3  3  4  5  4  4  5  3  2  2  2  5  4  5  2  3  4  4  3  3\r\n61623  6  6  5  6  5  6  6  6  1  3  2  1  6  5  6  3  5  2  2  3  4\r\n      O2 O3 O4 O5\r\n61617  6  3  4  3\r\n61618  2  4  3  3\r\n61620  2  5  5  2\r\n61621  3  4  3  5\r\n61622  3  4  3  3\r\n61623  3  5  6  1\r\n\r\n\r\n\r\nShow code\r\n\r\n#diagnose for missing value\r\ndlookr::diagnose(data)\r\n\r\n\r\n# A tibble: 25 x 6\r\n   variables types   missing_count missing_percent unique_count\r\n   <chr>     <chr>           <int>           <dbl>        <int>\r\n 1 A1        integer            16           0.571            7\r\n 2 A2        integer            27           0.964            7\r\n 3 A3        integer            26           0.929            7\r\n 4 A4        integer            19           0.679            7\r\n 5 A5        integer            16           0.571            7\r\n 6 C1        integer            21           0.75             7\r\n 7 C2        integer            24           0.857            7\r\n 8 C3        integer            20           0.714            7\r\n 9 C4        integer            26           0.929            7\r\n10 C5        integer            16           0.571            7\r\n# ... with 15 more rows, and 1 more variable: unique_rate <dbl>\r\n\r\nShow code\r\n\r\nvisdat::vis_miss(data, sort_miss = FALSE)\r\n\r\n\r\n\r\n\r\nThere are some degree of missingness in the dataset as indicated by the missingness map. For good measure, we will impute it with the predictive mean matchmaking method by the mice package.\r\nData preprocessing\r\n\r\n\r\nShow code\r\n\r\n#imputation\r\nmice_model <- mice(data, method='pmm', seed = 123)\r\n\r\n\r\n\r\n iter imp variable\r\n  1   1  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  1   2  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  1   3  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  1   4  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  1   5  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  2   1  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  2   2  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  2   3  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  2   4  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  2   5  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  3   1  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  3   2  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  3   3  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  3   4  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  3   5  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  4   1  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  4   2  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  4   3  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  4   4  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  4   5  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  5   1  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  5   2  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  5   3  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  5   4  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n  5   5  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5\r\n\r\nShow code\r\n\r\ndata_complete <- complete(mice_model)\r\n\r\nvisdat::vis_miss(data_complete, sort_miss = FALSE)\r\n\r\n\r\n\r\n\r\nThere is no missing value present in the dataset after the imputation. We can proceed with factor structure checking to assess whether the dataset is appropriate for factor analysis with check_factorstructure(). Two existing methods are the Bartlett’s Test of Sphericity and the Kaiser, Meyer, Olkin (KMO) Measure of Sampling Adequacy (MSA).\r\nThe former tests whether a matrix is significantly different from an identity matrix. This statistical test for the presence of correlations among variables, providing the statistical probability that the correlation matrix has significant correlations among at least some of variables. As for factor analysis to work, some relationships between variables are needed, thus, a significant Bartlett’s test of sphericity is required to be significant.\r\nThe latter method, ranging from 0 to 1, indicates the degree to which each variable in the dataset is predicted without error by the other variables. A value of 0 indicates that the sum of partial correlations is large relative to the sum correlations, indicating factor analysis is likely to be inappropriate. A KMO value close to 1 indicates that the sum of partial correlations is not large relative to the sum of correlations and so factor analysis should yield distinct and reliable factors.\r\n\r\n\r\nShow code\r\n\r\n#check for factor structure\r\ncheck_factorstructure(data_complete)\r\n\r\n\r\n# Is the data suitable for Factor Analysis?\r\n\r\n  - KMO: The Kaiser, Meyer, Olkin (KMO) measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.84).\r\n  - Sphericity: Bartlett's test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(300) = 20158.27, p < .001).\r\n\r\nThe Barlett’s test suggested that there is sufficient significant correlation in the data for factor analysis. Speaking of correlation, let us generate a correlation matrix to check for relationship between variables in the dataset as well.\r\n\r\n\r\nShow code\r\n\r\nvariables.to.use<-c(\"A1\", \"A2\", \"A3\", \"A4\", \"A5\",\r\n                    \"C1\", \"C2\", \"C3\", \"C4\", \"C5\",\r\n                    \"E1\", \"E2\", \"E3\", \"E4\", \"E5\",\r\n                    \"N1\", \"N2\", \"N3\", \"N4\", \"N5\",\r\n                    \"O1\", \"O2\", \"O3\", \"O4\", \"O5\")\r\n\r\ndata.corr<-cor(data_complete[variables.to.use],\r\n                 method = \"pearson\",\r\n                 use='all.obs')\r\nggcorrplot(data.corr,\r\n           p.mat=cor_pmat(data_complete[variables.to.use]),\r\n           hc.order=TRUE, \r\n           type='lower',\r\n           color=c('red3', 'white', 'green3'),\r\n           outline.color = 'darkgoldenrod1', \r\n           lab=FALSE, #omit the correlation coefficient\r\n           legend.title='Correlation',\r\n           pch=4, \r\n           pch.cex=4, #size of the cross mark for non-significant indicator\r\n           lab_size=6)+ \r\n  labs(title=\"Correlation Matrix\")+\r\n  theme(plot.title=element_text(face='bold',size=14,hjust=0.5,colour=\"darkred\"))+\r\n  theme(legend.position=c(0.10,0.80), legend.box.just = \"bottom\")\r\n\r\n\r\n\r\n\r\nThe green panel indicates positive relationships while the red panel indicates negative relationship. The cross symbol suggests that the relationship between two variables is not statistically significant.\r\nNext, we would need to split the dataset into two to perform EFA and CFA, so that we can make sure to test the model on an unseen set of data.\r\n\r\n\r\nShow code\r\n\r\n# Establish two sets of indices to split the dataset\r\nN <- nrow(data_complete)\r\nindices <- seq(1, N)\r\nindices_EFA <- sample(indices, floor((.5*N)))\r\nindices_CFA <- indices[!(indices %in% indices_EFA)]\r\n\r\n# Use those indices to split the dataset into halves for your EFA and CFA\r\nbfi_EFA <- data_complete[indices_EFA, ]\r\nbfi_CFA <- data_complete[indices_CFA, ]\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\ncheck_factorstructure(bfi_EFA)\r\n\r\n\r\n# Is the data suitable for Factor Analysis?\r\n\r\n  - KMO: The Kaiser, Meyer, Olkin (KMO) measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.84).\r\n  - Sphericity: Bartlett's test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(300) = 10244.89, p < .001).\r\n\r\n\r\n\r\nShow code\r\n\r\ncheck_factorstructure(bfi_CFA)\r\n\r\n\r\n# Is the data suitable for Factor Analysis?\r\n\r\n  - KMO: The Kaiser, Meyer, Olkin (KMO) measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.84).\r\n  - Sphericity: Bartlett's test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(300) = 10210.30, p < .001).\r\n\r\nThe two datasets that we splitted are appropriate for factor analysis, so we can proceed with EFA as the first analysis\r\nExploratory Factor Analysis\r\nExploratory Factor Analysis is a statistical technique in social science to explain the variance between several measured variables as a smaller set of latent variables. EFA is often used to consolidate survey data by revealing the groupings (factors) that underly individual questions.\r\nAn EFA provides information on each item’s relationship to a single factor that is hypothesized to be represented by each of the items. EFA results give you basic information about how well items relate to that hypothesized construct.\r\nThe common application of EFA is to investigate relationships between observed variable and latent variables (factor) such as measurement piloting.\r\nScree plot\r\nTo empirically determine the dimensionality of your data, a common strategy is to examine the eigenvalues and scree plot.\r\nThe scree plot is a visual representation of eigenvalues that determines potential dimensionality of the dataset. Eigenvalues can be generated from a principal component analysis or a factor analysis, and the scree() function calculates and plots both by default. Since eigen() finds eigenvalues via principal components analysis, we will use factors = FALSE so our scree plot will only display the values corresponding to those results.\r\n\r\n\r\nShow code\r\n\r\n# Calculate the correlation matrix first\r\nbfi_EFA_cor <- cor(bfi_EFA, use = \"pairwise.complete.obs\") \r\n\r\n# Then use that correlation matrix to calculate eigenvalues\r\neigenvals <- eigen(bfi_EFA_cor)\r\n\r\n# Look at the eigenvalues returned\r\neigenvals$values\r\n\r\n\r\n [1] 5.1697200 2.6134230 2.1562029 1.7907326 1.6067099 1.0566407\r\n [7] 0.8427603 0.7962665 0.7482662 0.7263301 0.6985177 0.6640562\r\n[13] 0.6544223 0.5997338 0.5573358 0.5424657 0.5117708 0.4961194\r\n[19] 0.4602673 0.4487993 0.4259742 0.4020084 0.3868776 0.3675815\r\n[25] 0.2770178\r\n\r\n\r\n\r\nShow code\r\n\r\n# Then use the correlation matrix to create the scree plot\r\n\r\nscree(bfi_EFA_cor, factors = FALSE)\r\n\r\n\r\n\r\n\r\nFrom the above plot, The point where the slope of the curve is clearly leveling off (the “elbow) indicates that the number of factors that should be retained. For this case, the plot indicated that six factors should be retained.\r\nHowever, the dataset we use is for Big 5 personality traits with 5 factors, so we can investigate both 5 and 6 factors model to compare them both.\r\nFit EFA models\r\nWe can initially explore the factor structure of 5 groups first as intended by the big 5 personality theory.\r\n\r\n\r\nShow code\r\n\r\n# Fit an EFA\r\nefa <- psych::fa(data, nfactors = 5) %>% \r\n  model_parameters(sort = TRUE, threshold = \"max\")\r\n\r\nefa\r\n\r\n\r\n# Rotated loadings from Factor Analysis (oblimin-rotation)\r\n\r\nVariable | MR2  |  MR1  |  MR3  |  MR5  |  MR4  | Complexity | Uniqueness\r\n-------------------------------------------------------------------------\r\nN1       | 0.81 |       |       |       |       |    1.08    |    0.35   \r\nN2       | 0.78 |       |       |       |       |    1.04    |    0.40   \r\nN3       | 0.71 |       |       |       |       |    1.07    |    0.45   \r\nN5       | 0.49 |       |       |       |       |    1.96    |    0.65   \r\nN4       | 0.47 |       |       |       |       |    2.27    |    0.51   \r\nE2       |      | -0.68 |       |       |       |    1.07    |    0.46   \r\nE4       |      | 0.59  |       |       |       |    1.49    |    0.47   \r\nE1       |      | -0.56 |       |       |       |    1.21    |    0.65   \r\nE5       |      | 0.42  |       |       |       |    2.60    |    0.60   \r\nE3       |      | 0.42  |       |       |       |    2.55    |    0.56   \r\nC2       |      |       | 0.67  |       |       |    1.17    |    0.55   \r\nC4       |      |       | -0.61 |       |       |    1.18    |    0.55   \r\nC3       |      |       | 0.57  |       |       |    1.11    |    0.68   \r\nC5       |      |       | -0.55 |       |       |    1.44    |    0.57   \r\nC1       |      |       | 0.55  |       |       |    1.19    |    0.67   \r\nA3       |      |       |       | 0.66  |       |    1.07    |    0.48   \r\nA2       |      |       |       | 0.64  |       |    1.04    |    0.55   \r\nA5       |      |       |       | 0.53  |       |    1.49    |    0.54   \r\nA4       |      |       |       | 0.43  |       |    1.74    |    0.72   \r\nA1       |      |       |       | -0.41 |       |    1.97    |    0.81   \r\nO3       |      |       |       |       | 0.61  |    1.17    |    0.54   \r\nO5       |      |       |       |       | -0.54 |    1.21    |    0.70   \r\nO1       |      |       |       |       | 0.51  |    1.13    |    0.69   \r\nO2       |      |       |       |       | -0.46 |    1.75    |    0.74   \r\nO4       |      |       |       |       | 0.37  |    2.69    |    0.75   \r\n\r\nThe 5 latent factors (oblimin rotation) accounted for 41.48% of the total variance of the original data (MR2 = 10.28%, MR1 = 8.80%, MR3 = 8.12%, MR5 = 7.94%, MR4 = 6.34%).\r\n\r\nShow code\r\n\r\nsummary(efa)\r\n\r\n\r\n# (Explained) Variance of Components\r\n\r\nParameter                       |   MR2 |   MR1 |   MR3 |   MR5 |   MR4\r\n-----------------------------------------------------------------------\r\nEigenvalues                     | 4.493 | 2.249 | 1.505 | 1.188 | 0.934\r\nVariance Explained              | 0.103 | 0.088 | 0.081 | 0.079 | 0.063\r\nVariance Explained (Cumulative) | 0.103 | 0.191 | 0.272 | 0.351 | 0.415\r\nVariance Explained (Proportion) | 0.248 | 0.212 | 0.196 | 0.191 | 0.153\r\n\r\nAs we can see, the 25 items nicely spread on the 5 latent factors as the theory suggests. Based on this model, we can now predict back the scores for each individual for these new variables:\r\n\r\n\r\nShow code\r\n\r\npredict_result <- predict(efa, names = c(\"Neuroticism\", \"Conscientiousness\", \"Extraversion\", \"Agreeableness\", \"Opennness\"))\r\n\r\nhead(predict_result)\r\n\r\n\r\n  Neuroticism Conscientiousness Extraversion Agreeableness  Opennness\r\n1 -0.21410935        0.06924675  -1.33208860   -0.85364725 -1.5809244\r\n2  0.15008464        0.48139729  -0.59950262   -0.08478873 -0.1876070\r\n3  0.62827949        0.10964162  -0.04800816   -0.55616873  0.2502735\r\n4 -0.09425827        0.03836489  -1.05089539   -0.10394941 -1.1000032\r\n5 -0.16368420        0.44253657  -0.10519669   -0.71857460 -0.6612203\r\n6  0.18984314        1.08439177   1.40730835    0.39278790  0.6222356\r\n\r\nHow many factors should we retain?\r\nWhen running a factor analysis (FA), one often needs to specify how many components (or latent variables) to retain or to extract. This decision is often supported by some statistical indices and procedures aiming at finding the optimal number of factors, e.g., scree plot from (scree()).\r\nInterestingly, a huge amount of methods exist to statistically address this issue. These methods can sometimes contradict with each other in terms of retained factor. As a result, seeking the number that is supported by most methods is a reasonable compromise.\r\nThe Method Agreement procedure\r\nThe Method Agreement procedure, first implemented in the psycho package, proposes to rely on the consensus of methods, rather than on one method in particular. This procedure can be used through the n_factors() by providing a dataframe, and the function will run a large number of routines and return the optimal number of factors based on the higher consensus.\r\n\r\n\r\nShow code\r\n\r\nn_factor <- parameters::n_factors(data_complete)\r\n\r\nn_factor\r\n\r\n\r\n# Method Agreement Procedure:\r\n\r\nThe choice of 6 dimensions is supported by 4 (21.05%) methods out of 19 (Optimal coordinates, Parallel analysis, Kaiser criterion, Scree (SE)).\r\n\r\n\r\n\r\nShow code\r\n\r\nas.data.frame(n_factor)\r\n\r\n\r\n   n_Factors              Method              Family\r\n1          1 Acceleration factor               Scree\r\n2          3                 CNG                 CNG\r\n3          4                beta Multiple_regression\r\n4          4          Scree (R2)            Scree_SE\r\n5          4    VSS complexity 1                 VSS\r\n6          5    VSS complexity 2                 VSS\r\n7          5       Velicer's MAP        Velicers_MAP\r\n8          6 Optimal coordinates               Scree\r\n9          6   Parallel analysis               Scree\r\n10         6    Kaiser criterion               Scree\r\n11         6          Scree (SE)            Scree_SE\r\n12         7                   t Multiple_regression\r\n13         7                   p Multiple_regression\r\n14         8                 BIC                 BIC\r\n15        12      BIC (adjusted)                 BIC\r\n16        22             Bentler             Bentler\r\n17        24            Bartlett             Barlett\r\n18        24            Anderson             Barlett\r\n19        24              Lawley             Barlett\r\n\r\nFor more details, a summary table can be obtained\r\n\r\n\r\nShow code\r\n\r\nsummary(n_factor)\r\n\r\n\r\n   n_Factors n_Methods\r\n1          1         1\r\n2          3         1\r\n3          4         3\r\n4          5         2\r\n5          6         4\r\n6          7         2\r\n7          8         1\r\n8         12         1\r\n9         22         1\r\n10        24         3\r\n\r\n\r\n\r\nShow code\r\n\r\nplot(n_factor) + theme_bw()\r\n\r\n\r\n\r\n\r\nInterestingly, most methods also suggest six factors from the model, which is consistent with the HEXACO model of personalities that is similar with the Big 5 personality theory.\r\nConfirmatory Factor Analysis\r\nWe’ve seen above that while an EFA with 5 latent variables works great on our dataset, a structure with 6 latent factors may be statistically viable as well. This topic can be statistically tested with a CFA to bridge factor analysis with Structural Equation Modelling (SEM).\r\nHowever, in order to do that cleanly, EFA should be independent from CFA in the sense that the factor structure should be explored on a training set, and then tested (or “confirmed”) on a test set.\r\nTrain test split\r\nThe data can be easily split into two sets with the initial_split() of the tidymodel package, through which we will use 70% of the sample as the training and the rest as the test dataset.\r\n\r\n\r\nShow code\r\n\r\nset.seed(999)\r\n\r\n# Put 3/4 of the data into the training set \r\ndata_split <- initial_split(bfi_CFA, prop = 0.7)\r\n\r\n# Create data frames for the two sets:\r\ntraining <- training(data_split) #3/4\r\ntest  <- testing(data_split)  #1/4\r\n\r\n\r\n\r\nCreate CFA structures out of EFA models\r\nIn the next step, we will run two EFA models on the training set and specifying 5 and 6 latent factors respectively. We will also request for path diagram of the models as well as their density plot of factor score.\r\nFirst, we will examine structure of the 5 factors model.\r\n\r\n\r\nShow code\r\n\r\nstructure_big5 <- psych::fa(training, nfactors = 5)\r\nfa.diagram(structure_big5)\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nplot(density(structure_big5$scores, na.rm = TRUE), \r\n     main = \"Factor Scores\")\r\n\r\n\r\n\r\n\r\nThen, for the structure of the 6 factors model.\r\n\r\n\r\nShow code\r\n\r\nstructure_big6 <- psych::fa(training, nfactors = 6) \r\nfa.diagram(structure_big6)\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nplot(density(structure_big6$scores, na.rm = TRUE), \r\n     main = \"Factor Scores\")\r\n\r\n\r\n\r\n\r\nWe will then transform both EFA models into lavaan syntax to perform CFA.\r\n\r\n\r\nShow code\r\n\r\n#Converting EFA into a lavaan-ready syntax\r\ncfa_big5 <- efa_to_cfa(structure_big5)\r\n\r\n#Investigate how the model looks\r\ncfa_big5\r\n\r\n\r\n# Latent variables\r\nMR2 =~ N1 + N2 + N3 + N4 + N5\r\nMR5 =~ A1 + A2 + A3 + A4 + A5\r\nMR1 =~ E1 + E2 + E3 + E4 + E5\r\nMR3 =~ C1 + C2 + C3 + C4 + C5\r\nMR4 =~ O1 + O2 + O3 + O4 + O5\r\n\r\n\r\n\r\nShow code\r\n\r\ncfa_big6 <- efa_to_cfa(structure_big6)\r\ncfa_big6\r\n\r\n\r\n# Latent variables\r\nMR2 =~ N1 + N2 + N3 + N4 + N5\r\nMR5 =~ A1 + A2 + A3 + A4 + A5\r\nMR1 =~ E1 + E2 + E4 + E5\r\nMR3 =~ C1 + C2 + C3 + C4 + C5\r\nMR4 =~ E3 + O1 + O3 + O4\r\nMR6 =~ O2 + O5\r\n\r\nFit and Compare models\r\nNext, we will fit both models with lavaan package before requesting for model fit measure with fitmeasures. We can also compare both models head-to-head with compare_performance.\r\n\r\n\r\nShow code\r\n\r\nmodel_big5 <- lavaan::cfa(cfa_big5, data = test)\r\nmodel_big6 <- lavaan::cfa(cfa_big6, data = test)\r\n\r\nfitmeasures(model_big5, fit.measures = \"all\", output = \"text\")\r\n\r\n\r\n\r\nModel Test User Model:\r\n\r\n  Test statistic                               948.270\r\n  Degrees of freedom                               265\r\n  P-value                                        0.000\r\n\r\nModel Test Baseline Model:\r\n\r\n  Test statistic                              3122.085\r\n  Degrees of freedom                               300\r\n  P-value                                        0.000\r\n\r\nUser Model versus Baseline Model:\r\n\r\n  Comparative Fit Index (CFI)                    0.758\r\n  Tucker-Lewis Index (TLI)                       0.726\r\n  Bentler-Bonett Non-normed Fit Index (NNFI)     0.726\r\n  Bentler-Bonett Normed Fit Index (NFI)          0.696\r\n  Parsimony Normed Fit Index (PNFI)              0.615\r\n  Bollen's Relative Fit Index (RFI)              0.656\r\n  Bollen's Incremental Fit Index (IFI)           0.761\r\n  Relative Noncentrality Index (RNI)             0.758\r\n\r\nLoglikelihood and Information Criteria:\r\n\r\n  Loglikelihood user model (H0)             -17371.604\r\n  Loglikelihood unrestricted model (H1)     -16897.469\r\n                                                      \r\n  Akaike (AIC)                               34863.208\r\n  Bayesian (BIC)                             35105.766\r\n  Sample-size adjusted Bayesian (BIC)        34915.367\r\n\r\nRoot Mean Square Error of Approximation:\r\n\r\n  RMSEA                                          0.078\r\n  90 Percent confidence interval - lower         0.073\r\n  90 Percent confidence interval - upper         0.084\r\n  P-value RMSEA <= 0.05                          0.000\r\n\r\nStandardized Root Mean Square Residual:\r\n\r\n  RMR                                            0.174\r\n  RMR (No Mean)                                  0.174\r\n  SRMR                                           0.083\r\n\r\nOther Fit Indices:\r\n\r\n  Hoelter Critical N (CN) alpha = 0.05         135.952\r\n  Hoelter Critical N (CN) alpha = 0.01         143.725\r\n                                                      \r\n  Goodness of Fit Index (GFI)                    0.833\r\n  Adjusted Goodness of Fit Index (AGFI)          0.795\r\n  Parsimony Goodness of Fit Index (PGFI)         0.679\r\n                                                      \r\n  McDonald Fit Index (MFI)                       0.444\r\n                                                      \r\n  Expected Cross-Validation Index (ECVI)         2.537\r\n\r\n\r\n\r\nShow code\r\n\r\nfitmeasures(model_big6, fit.measures = \"all\", output = \"text\")\r\n\r\n\r\n\r\nModel Test User Model:\r\n\r\n  Test statistic                               931.725\r\n  Degrees of freedom                               260\r\n  P-value                                        0.000\r\n\r\nModel Test Baseline Model:\r\n\r\n  Test statistic                              3122.085\r\n  Degrees of freedom                               300\r\n  P-value                                        0.000\r\n\r\nUser Model versus Baseline Model:\r\n\r\n  Comparative Fit Index (CFI)                    0.762\r\n  Tucker-Lewis Index (TLI)                       0.725\r\n  Bentler-Bonett Non-normed Fit Index (NNFI)     0.725\r\n  Bentler-Bonett Normed Fit Index (NFI)          0.702\r\n  Parsimony Normed Fit Index (PNFI)              0.608\r\n  Bollen's Relative Fit Index (RFI)              0.656\r\n  Bollen's Incremental Fit Index (IFI)           0.765\r\n  Relative Noncentrality Index (RNI)             0.762\r\n\r\nLoglikelihood and Information Criteria:\r\n\r\n  Loglikelihood user model (H0)             -17363.331\r\n  Loglikelihood unrestricted model (H1)     -16897.469\r\n                                                      \r\n  Akaike (AIC)                               34856.662\r\n  Bayesian (BIC)                             35119.433\r\n  Sample-size adjusted Bayesian (BIC)        34913.168\r\n\r\nRoot Mean Square Error of Approximation:\r\n\r\n  RMSEA                                          0.078\r\n  90 Percent confidence interval - lower         0.073\r\n  90 Percent confidence interval - upper         0.084\r\n  P-value RMSEA <= 0.05                          0.000\r\n\r\nStandardized Root Mean Square Residual:\r\n\r\n  RMR                                            0.168\r\n  RMR (No Mean)                                  0.168\r\n  SRMR                                           0.081\r\n\r\nOther Fit Indices:\r\n\r\n  Hoelter Critical N (CN) alpha = 0.05         135.927\r\n  Hoelter Critical N (CN) alpha = 0.01         143.771\r\n                                                      \r\n  Goodness of Fit Index (GFI)                    0.837\r\n  Adjusted Goodness of Fit Index (AGFI)          0.797\r\n  Parsimony Goodness of Fit Index (PGFI)         0.670\r\n                                                      \r\n  McDonald Fit Index (MFI)                       0.450\r\n                                                      \r\n  Expected Cross-Validation Index (ECVI)         2.522\r\n\r\n\r\n\r\nShow code\r\n\r\nmodel_comparison <-performance::compare_performance(model_big5, model_big6)\r\n\r\nrmarkdown::paged_table(model_comparison)\r\n\r\n\r\n\r\n\r\n\r\nConclusion\r\nThe model comparison indicated that both model are empirically viable, but we would need a different dataset to re-assess the 6 factor model with appropriate item distribution (i.e., each factor has its own dedicated variables).\r\nAnother thing that should be noted is our conclusion should be theoretically permissible; that is, whether we use the big 5 or big 6 models, we should have theories and evidence to support our decision. Otherwise, if we only rely on statistical results, we can use 24 personality factors models because the result said it has the second-most approval rate by method consensus, but that would not make any theoretical sense.\r\nFor social science research where we care about the “how”, our decisions should be theoretically-driven, so that we can explain the way our model works in both statistical and theoretical sense.\r\nAnyway, this is all for this post. Thank you so much for reading this as always. The next semester is on around the corner, but I still have some time to write my blog before going back to the regularly-scheduled paper writing. Also, Happy New Year, everyone! Let us do our best in both personal and professional endeavor.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-01-efacfa/corrmatrix.jpg",
    "last_modified": "2022-03-27T18:09:28-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-29-movie-similarity/",
    "title": "Measuring Text Similarity with Movie plot data",
    "description": "For this post, I will be analyzing textual data of movie plots to determine their similarity with TF-IDF and Clustering.\n\n(7 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2021-12-29",
    "categories": [
      "Python",
      "Natural Language Processing",
      "Unsupervised Machine Learning"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nCombine Wikipedia and IMDb plot summaries\r\nTokenization\r\nStemming\r\nTokenization and Stemming together\r\nCreate TF-IDF Vectorizer\r\nFit transform TF-IDF Vectorizer\r\nImport K-Means and create clusters\r\nCalculate similarity distance\r\nImport Matplotlib, Linkage, and Dendrograms\r\nConcluding remark\r\n\r\nIntroduction\r\nOne characteristic of textual data in the real world setting is that most of them possess meaning that to convey to their intended audience. The meaning of one message could be similar to another when they are crafted for similar purposes. With the right tool, we can identify such similarities and visualize them to extract insights from textual data.\r\nThe data set I will use in this post are movie plot summaries available on IMDb and Wikipedia. Here, I will quantify the similarity of movies based on their plot and separate them into groups before plotting them on a dendrogram to represent how closely the movies are related to each other.\r\nAs always, we will begin by importing necessary modules and dataset.\r\n\r\n\r\nShow code\r\n# Import modules\r\nimport numpy as np\r\nimport pandas as pd\r\nimport nltk\r\n\r\n# Set seed for reproducibility\r\nnp.random.seed(5)\r\n\r\n# Read in IMDb and Wikipedia movie data (both in the same file)\r\nmovies_df = pd.read_csv(\"movies.csv\")\r\n\r\nprint(\"Number of movies loaded: %s \" % (len(movies_df)))\r\n\r\n# Display the data\r\nNumber of movies loaded: 100 \r\n\r\nShow code\r\nmovies_df\r\n    rank  ...                                          imdb_plot\r\n0      0  ...  In late summer 1945, guests are gathered for t...\r\n1      1  ...  In 1947, Andy Dufresne (Tim Robbins), a banker...\r\n2      2  ...  The relocation of Polish Jews from surrounding...\r\n3      3  ...  The film opens in 1964, where an older and fat...\r\n4      4  ...  In the early years of World War II, December 1...\r\n..   ...  ...                                                ...\r\n95    95  ...  Shortly after moving to Los Angeles with his p...\r\n96    96  ...  L.B. \"Jeff\" Jeffries (James Stewart) recuperat...\r\n97    97  ...  Sights of Vienna, Austria, flash across the sc...\r\n98    98  ...  At the end of an ordinary work day, advertisin...\r\n99    99  ...                                                NaN\r\n\r\n[100 rows x 5 columns]\r\n\r\nCombine Wikipedia and IMDb plot summaries\r\nThe dataset we imported currently contains two columns titled wiki_plot and imdb_plot. They are the plot found for the movies on Wikipedia and IMDb, respectively. The text in the two columns is similar, however, they are often written in different tones and thus provide context on a movie in a different manner of linguistic expression. Further, sometimes the text in one column may mention a feature of the plot that is not present in the other column. For example, consider the following plot extracts from The Godfather:\r\nWikipedia: “On the day of his only daughter’s wedding, Vito Corleone…”\r\nIMDb: “In late summer 1945, guests are gathered for the wedding reception of Don Vito Corleone’s daughter Connie”\r\n\r\nWhile the Wikipedia plot only mentions it is the day of the daughter’s wedding, the IMDb plot also mentions the year of the scene and the name of the daughter. We can combine them to avoid the overheads in computation associated with extra columns to process.\r\n\r\n\r\nShow code\r\n# Combine wiki_plot and imdb_plot into a single column\r\nmovies_df[\"plot\"] = movies_df[\"wiki_plot\"].astype(str) + \"\\n\" + \\\r\n                    movies_df[\"imdb_plot\"].astype(str)\r\n                    \r\nmovies_df.head()\r\n   rank  ...                                               plot\r\n0     0  ...  On the day of his only daughter's wedding, Vit...\r\n1     1  ...  In 1947, banker Andy Dufresne is convicted of ...\r\n2     2  ...  In 1939, the Germans move Polish Jews into the...\r\n3     3  ...  In a brief scene in 1964, an aging, overweight...\r\n4     4  ...  It is early December 1941. American expatriate...\r\n\r\n[5 rows x 6 columns]\r\n\r\nTokenization\r\nTokenization is the process by which we break down articles into individual sentences or words, as needed. We can also use the regular expression (Regex) method to remove tokens that are entirely numeric values or punctuation to retain only words with meaning.\r\nAs an example, we will perform tokenization on a part of Godfather’s plot. Notice that quotation marks and numbers were removed in the output.\r\n\r\n\r\nShow code\r\n# Tokenize a paragraph into sentences and store in sent_tokenized\r\nsent_tokenized = [sent for sent in nltk.sent_tokenize(\"\"\"\r\n                        Today (May 19, 2016) is his only daughter's wedding. \r\n                        Vito Corleone is the Godfather.\r\n                        \"\"\")]\r\n                        \r\n# Word Tokenize first sentence from sent_tokenized, save as words_tokenized\r\n\r\nwords_tokenized = [word for word in nltk.word_tokenize(sent_tokenized[0])]\r\n\r\n# Remove tokens that do not contain any letters from words_tokenized\r\nimport re\r\n\r\nfiltered = [word for word in words_tokenized if re.search('[a-zA-Z]', word)]\r\n\r\n# Display filtered words to observe words after tokenization\r\nfiltered\r\n['Today', 'May', 'is', 'his', 'only', 'daughter', \"'s\", 'wedding']\r\n\r\nStemming\r\nStemming is the process by which we bring down a word from its different forms to the root word (or to stem). This helps us establish meaning to different forms of the same words without having to deal with each form separately. For example, the words ‘fishing’, ‘fished’, and ‘fisher’ all get stemmed to the word ‘fish’.\r\nConsider the following sentences:\r\n“Young William Wallace witnesses the treachery of Longshanks” - Gladiator\r\n“escapes to the city walls only to witness Cicero’s death” - Braveheart\r\nInstead of building separate dictionary entries for both witnesses and witness, which mean the same thing outside of quantity, stemming them reduces them to ‘wit’.\r\nThere are different algorithms available for stemming such as the Porter Stemmer and Snowball Stemmer. Here, we will use Snowball Stemmer.\r\n\r\n\r\nShow code\r\n# Import the SnowballStemmer to perform stemming\r\nfrom nltk.stem.snowball import SnowballStemmer\r\n\r\n# Create an English language SnowballStemmer object\r\nstemmer = SnowballStemmer(\"english\")\r\n\r\n# Print filtered to observe words without stemming\r\nprint(\"Without stemming: \", filtered)\r\n\r\n# Stem the words from filtered and store in stemmed_words\r\nWithout stemming:  ['Today', 'May', 'is', 'his', 'only', 'daughter', \"'s\", 'wedding']\r\n\r\nShow code\r\nstemmed_words = [stemmer.stem(t) for t in filtered]\r\n\r\n# Print the stemmed_words to observe words after stemming\r\nprint(\"After stemming:   \", stemmed_words)\r\nAfter stemming:    ['today', 'may', 'is', 'his', 'onli', 'daughter', \"'s\", 'wed']\r\n\r\nTokenization and Stemming together\r\nWe are now able to tokenize and stem sentences. But we may have to use the two functions repeatedly one after the other to handle a large amount of data, hence we can think of wrapping them in a function and passing the text to be tokenized and stemmed as the function argument. Then we can pass the new wrapping function, which shall perform both tokenizing and stemming instead of just tokenizing, as the tokenizer argument while creating the TF-IDF vector of the text (we will get there to what TF-IDF means).\r\nAll the words are in their root form, which will lead to a better establishment of meaning as some of the non-root forms may not be present in the NLTK training corpus.\r\n\r\n\r\nShow code\r\n# Define a function to perform both stemming and tokenization\r\ndef tokenize_and_stem(text):\r\n    \r\n    # Tokenize by sentence, then by word\r\n    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\r\n    \r\n    # Filter out raw tokens to remove noise\r\n    filtered_tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]\r\n    \r\n    # Stem the filtered_tokens\r\n    stems = [stemmer.stem(t) for t in filtered_tokens]\r\n    \r\n    return stems\r\n\r\nwords_stemmed = tokenize_and_stem(\"Today (May 19, 2016) is his only daughter's wedding.\")\r\nprint(words_stemmed)\r\n['today', 'may', 'is', 'his', 'onli', 'daughter', \"'s\", 'wed']\r\n\r\nCreate TF-IDF Vectorizer\r\nComputers do not understand text. These are machines only capable of understanding numbers and performing numerical computation. Hence, we must convert our textual plot summaries to numbers for the computer to be able to extract meaning from them. One simple method of doing this would be to count all the occurrences of each word in the entire vocabulary and return the counts in a vector. This method is called CountVectorizer.\r\nConsider the word ‘the’. It appears quite frequently in almost all movie plots and will have a high count in each case. However, “the” could hardly be counted as the movie plot itself. For that, Term Frequency-Inverse Document Frequency (TF-IDF) is one method that overcomes the shortcomings of CountVectorizer.\r\nIn TF-IDF, frequency of a word is the measure of how often it appears in a document, while the Inverse Document Frequency is the parameter which reduces the importance of a word if it frequently appears in several documents. In simplest terms, TF-IDF recognizes words which are unique and important to any given document.\r\n\r\n\r\nShow code\r\n# Import TfidfVectorizer to create TF-IDF vectors\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\n\r\n# Instantiate TfidfVectorizer object with stopwords and tokenizer\r\n# parameters for efficient processing of text\r\ntfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\r\n                                 min_df=0.2, stop_words='english',\r\n                                 use_idf=True, tokenizer=tokenize_and_stem,\r\n                                 ngram_range=(1,3))\r\n\r\nFit transform TF-IDF Vectorizer\r\nOnce we create a TF-IDF Vectorizer, we must fit the text to it and then transform the text to produce the corresponding numeric form of the data which the computer will be able to understand and derive meaning from. To do this, we use the fit_transform() method of the TfidfVectorizer object.\r\nIn the TF-IDF object, there is a parameter called stopwords. Stopwords are those words in a given text which do not contribute considerably towards the meaning of the sentence and are generally grammatical filler words. For example, in the sentence ‘Dorothy Gale lives with her dog Toto on the farm of her Aunt Em and Uncle Henry’, we could drop the words ‘her’ and ‘the’, and still have a similar overall meaning to the sentence. Thus, ‘her’ and ‘the’ are stopwords and can be conveniently dropped from the sentence.\r\nOn setting the stopwords to ‘english’, we direct the vectorizer to drop all stopwords from a pre-defined list of English language stopwords present in the nltk module. Another parameter, ngram_range, defines the length of the ngrams to be formed while vectorizing the text.\r\n\r\n\r\nShow code\r\n# Fit and transform the tfidf_vectorizer with the \"plot\" of each movie\r\n# to create a vector representation of the plot summaries\r\ntfidf_matrix = tfidf_vectorizer.fit_transform([x for x in movies_df[\"plot\"]])\r\nC:\\Users\\tarid\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\r\n  warnings.warn('Your stop_words may be inconsistent with '\r\n\r\nShow code\r\nprint(tfidf_matrix.shape)\r\n(100, 564)\r\n\r\nImport K-Means and create clusters\r\nTo determine how closely one movie is related to the other by the help of unsupervised machine learning, we can use clustering techniques. Clustering is the method of grouping together a number of items such that they exhibit similar properties. According to the measure of similarity desired, a given sample of items can have one or more clusters.\r\nA good basis of clustering in our data set could be the genre of the movies. Say we could have a cluster ‘0’ which holds movies of the ‘Drama’ genre, and ‘1’ for the ‘Adventure’ genre.\r\nK-means is an algorithm which helps us to implement clustering in Python. The name derives from its method of implementation: the given sample is divided into K clusters where each cluster is denoted by the mean of all the items lying in that cluster.\r\nHere, we will examine how many movies we have in each of the five clusters we specified; then, we will visualize them with a category plot.\r\n\r\n\r\nShow code\r\n# Import k-means to perform clustering\r\nfrom sklearn.cluster import KMeans\r\n\r\n# Create a KMeans object with 5 clusters and save as km\r\nkm = KMeans(n_clusters=5)\r\n\r\n# Fit the k-means object with tfidf_matrix\r\nkm.fit(tfidf_matrix)\r\nKMeans(n_clusters=5)\r\n\r\nShow code\r\nclusters = km.labels_.tolist()\r\n\r\n# Create a column cluster to denote the generated cluster for each movie\r\nmovies_df[\"cluster\"] = clusters\r\n\r\n# Display number of films per cluster (clusters from 0 to 4)\r\nmovies_df['cluster'].value_counts() \r\n3    31\r\n1    27\r\n0    22\r\n4    13\r\n2     7\r\nName: cluster, dtype: int64\r\n\r\n\r\n\r\nShow code\r\nimport seaborn as sns\r\nimport matplotlib.pyplot as plt\r\n\r\n#convert the cluster list into a dataframe\r\nclusters_df = pd.DataFrame(clusters, columns = ['cluster_group'])\r\n\r\nsns.set_theme(style=\"whitegrid\")\r\nsns.catplot(x=\"cluster_group\", kind=\"count\", data=clusters_df)\r\n\r\n\r\nCalculate similarity distance\r\nBy using countvectorizer, we can turn a sentence into numbers for the computer to calculate similarity distance with the cosine similarity measurement (it is basically a number that indicates how closely related the two sets of words are).\r\n\r\n\r\nShow code\r\n# Import cosine_similarity to calculate similarity of movie plots\r\nfrom sklearn.metrics.pairwise import cosine_similarity\r\n\r\n# Calculate the similarity distance\r\nsimilarity_distance = 1 - cosine_similarity(tfidf_matrix)\r\n\r\nImport Matplotlib, Linkage, and Dendrograms\r\nWe will then create a dendrogram of the movie title based on its plot similarity to visualize the level of similarity between our data points.\r\nDendrograms help visualize the results of hierarchical clustering, which is an alternative to k-means clustering. Two pairs of movies at the same level of hierarchical clustering are expected to have similar strength of similarity between the corresponding pairs of movies.\r\nThe more similar the two movies are, the closer they will be together as they travel down the dendrogram path. The plot is a little large to accommodate the number of data points, so you might need to zoom in to see which movie is similar to which.\r\n\r\n\r\nShow code\r\n# Import modules necessary to plot dendrogram\r\nfrom scipy.cluster.hierarchy import linkage, dendrogram\r\n\r\n# Create mergings matrix \r\nmergings = linkage(similarity_distance, method='complete')\r\n\r\n# Plot the dendrogram, using title as label column\r\ndendrogram_ = dendrogram(mergings,\r\n               labels=[x for x in movies_df[\"title\"]],\r\n               leaf_rotation=90,\r\n               leaf_font_size=16,\r\n)\r\n\r\n# Adjust the plot\r\nfig = plt.gcf()\r\n_ = [lbl.set_color('r') for lbl in plt.gca().get_xmajorticklabels()]\r\nfig.set_size_inches(120, 50)\r\n\r\n# Show the plotted dendrogram\r\nplt.grid(False)\r\nplt.show()\r\n\r\n\r\nConcluding remark\r\nWhile I am not an expert in movie critique, the movie plot data is a good venue to practice text cleaning with tokenization and stemming. The TF-IDF method is also widely implemented to extract meaningful information from textual data in general. Lastly, clustering is also a useful exploratory machine learning method to gain insights from unlabeled data to inform our decisions.\r\nThis post combines both Narutal Language Processing and Machine Learning techniques to calculate similarity score between sets of words. This method can be used to establish a groundwork for a recommendation system that we often seen in popular sites such as Netflix or Spotify by grouping movies or musics together to recommend them to users. As always, thank you very much for reading!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-29-movie-similarity/movie-similarity_files/figure-html5/unnamed-chunk-11-1.png",
    "last_modified": "2021-12-29T16:48:32-07:00",
    "input_file": {},
    "preview_width": 484,
    "preview_height": 483
  },
  {
    "path": "posts/2021-12-27-missingdata/",
    "title": "Missing Data Analysis",
    "description": "For this post, I will examine missing data in a large-scale dataset and discuss about numerous ways we can clean them as a part of data preparation.\n\n(10 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2021-12-27",
    "categories": [
      "R",
      "Data Visualization",
      "Statistics"
    ],
    "contents": "\r\n\r\nContents\r\nLooking for what that is not there\r\nImport and read the data set\r\nCheck for missing data\r\nVisualize Missing data\r\nWhat should we do with the missing data\r\nConcluding Remark\r\n\r\nLooking for what that is not there\r\n“If I had eight hours to chop down a tree, I’d spend six sharpening my axe.” - Abraham Lincoln via Kline (2016). This adage is appropriate to set the tone for this post, as well as applicable to most things in general, including working with data.\r\nMy professors taught me that real data never works, and my experience attested to their statement countless times as I iterated over the data work procedure of importing, cleaning, model building, model tuning, and communicating results. One thing about it that I used to find frustrating is the data I got if oftentimes incomplete (or partly missing).\r\nMissing data are values that should have been recorded but were not. The best way to treat missing data is not to have them, but unfortunately, real data is oftentimes ugly unorganized. Missing data could potentially caused by nonresponse in surveys, or technical issues with data-collecting equipment.\r\nMy previous posts were about visualizing data that we have, but this time, we will be visualizing things that we ‘do not’ have (aka missing data), as well as discussing about ways we can deal with them via complete case analysis or imputation.\r\nPhoto by Adam LingelbachImport and read the data set\r\nAs usual, we will begin by importing essential libraries and load in the data set to preprocess it.\r\n\r\n\r\nShow code\r\n\r\nlibrary(foreign) #To read SPSS data\r\nlibrary(tidyverse) #datawork toolbox\r\nlibrary(dlookr) #for missing data diagnosis\r\nlibrary(visdat) #for overall missingness visualization\r\nlibrary(naniar) #for missingness visualization\r\nlibrary(VIM) #for donor-based imputation\r\nlibrary(simputation) #for model-based imputation\r\nlibrary(mice) #for multiple imputation\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n#Import the data set\r\nPISA_CAN <-read.spss(\"PISA2018CAN.sav\",to.data.frame = TRUE, use.value.labels = FALSE)\r\n\r\n#Subset and rename the variables\r\nPISA_Subsetted <-  PISA_CAN %>% \r\n  select(REPEAT, FEMALE = ST004D01T, ESCS, DAYSKIP = ST062Q01TA,\r\n         CLASSSKIP = ST062Q02TA, LATE = ST062Q03TA,\r\n         BEINGBULLIED, DISCLIMA, ADAPTIVITY)\r\n\r\n#Recode variables into factor\r\nPISA_Subsetted$DAYSKIP <-as.factor(PISA_Subsetted$DAYSKIP)\r\nPISA_Subsetted$CLASSSKIP <-as.factor(PISA_Subsetted$CLASSSKIP)\r\nPISA_Subsetted$LATE <-as.factor(PISA_Subsetted$LATE)\r\nPISA_Subsetted$FEMALE <-as.factor(PISA_Subsetted$FEMALE)\r\nPISA_Subsetted$REPEAT <-as.factor(PISA_Subsetted$REPEAT)\r\n\r\n# Renaming factor levels with dplyr\r\nPISA_Subsetted$FEMALE <- recode_factor(PISA_Subsetted$FEMALE, \r\n                                       \"1\" = \"1\", \"2\" = \"0\")\r\n\r\nglimpse(PISA_Subsetted)\r\n\r\n\r\nRows: 22,653\r\nColumns: 9\r\n$ REPEAT       <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\r\n$ FEMALE       <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~\r\n$ ESCS         <dbl> -0.7302, 0.3078, 0.5059, 1.1147, 1.3626, -0.857~\r\n$ DAYSKIP      <fct> 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 2, 1, 2,~\r\n$ CLASSSKIP    <fct> 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 3, 1, 1, 2, 2, 2,~\r\n$ LATE         <fct> 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 3,~\r\n$ BEINGBULLIED <dbl> 1.2618, 1.7669, 0.1462, -0.7823, 0.2907, 0.7703~\r\n$ DISCLIMA     <dbl> -0.4186, -1.4179, 0.6019, -0.4995, -0.1045, 1.0~\r\n$ ADAPTIVITY   <dbl> -0.4708, 0.6350, -0.5786, -0.5786, -0.0763, 0.5~\r\n\r\nThe data set in this post was a Canadian student data subsetted from the Programme for Internal Student Assessment (PISA), which is an international assessment that measures 15-year-old students’ reading, mathematics, and science literacy every three years.\r\nFrom the glimpse call above, our dataset has 9 variables and 22,653 data points.\r\nCheck for missing data\r\nFirst, we will use the dlookr package to diagnose missingness of the data set, as well as plot missing data map with vis_miss.\r\nThe plot provides a specific visualization of the amount of missing data, showing in black the location of missing values, and also providing information on the overall percentage of missing values overall (in the legend), and in each variable.\r\n\r\n\r\nShow code\r\n\r\ndlookr::diagnose(PISA_Subsetted)\r\n\r\n\r\n# A tibble: 9 x 6\r\n  variables    types   missing_count missing_percent unique_count\r\n  <chr>        <chr>           <int>           <dbl>        <int>\r\n1 REPEAT       factor           1926         8.50               3\r\n2 FEMALE       factor              2         0.00883            3\r\n3 ESCS         numeric          1163         5.13           15366\r\n4 DAYSKIP      factor           3341        14.7                5\r\n5 CLASSSKIP    factor           3330        14.7                5\r\n6 LATE         factor           3316        14.6                5\r\n7 BEINGBULLIED numeric          3833        16.9               63\r\n8 DISCLIMA     numeric          1300         5.74             934\r\n9 ADAPTIVITY   numeric          2197         9.70              65\r\n# ... with 1 more variable: unique_rate <dbl>\r\n\r\nShow code\r\n\r\nvisdat::vis_miss(PISA_Subsetted, sort_miss = TRUE)\r\n\r\n\r\n\r\n\r\nIf we are curious about the proportion of missing data by groups, we can also group the dataset by our categorical variable of interest, say, gender, before examining the missingness ratio.\r\n\r\n\r\nShow code\r\n\r\nPISA_Subsetted %>% group_by (FEMALE) %>%\r\n  miss_var_summary()\r\n\r\n\r\n# A tibble: 24 x 4\r\n# Groups:   FEMALE [3]\r\n   FEMALE variable     n_miss pct_miss\r\n   <fct>  <chr>         <int>    <dbl>\r\n 1 1      BEINGBULLIED   1651    14.6 \r\n 2 1      DAYSKIP        1451    12.8 \r\n 3 1      CLASSSKIP      1445    12.8 \r\n 4 1      LATE           1430    12.6 \r\n 5 1      ADAPTIVITY      907     8.02\r\n 6 1      REPEAT          812     7.18\r\n 7 1      DISCLIMA        563     4.98\r\n 8 1      ESCS            527     4.66\r\n 9 0      BEINGBULLIED   2180    19.2 \r\n10 0      DAYSKIP        1888    16.6 \r\n# ... with 14 more rows\r\n\r\nTypes of Missing Data\r\nYes, we know now that our data is missing, but not all missing data are created (or not created, pun wholeheartedly intended) equal. There are three types of missing data, MCAR, MAR, and MNAR.\r\nMissing Completely at Random (MCAR)\r\nLocations of missing values in the dataset are purely random. they do not depend on any other data.\r\nFor example, if a doctor forgets to record the age of every tenth patient entering an ICU, the presence of missing value would not depend on the characteristic of the patients.\r\n\r\nMissing at Random (MAR)\r\nLocations of missing values in the dataset depend on some other, observed data.\r\nData are considered as MAR if the probability of missingness is unrelated to the actual value on that variable after controlling for the other variables in the dataset\r\nIn survey data, high-income respondents are less likely to inform the researcher about the number of properties owned.\r\nBelow is an example of MAR missingness. See that sea_temp and air_temp are missing at a certain part of the year. Maybe the measuring tools broke down or something before they got them fixed.\r\n\r\n\r\n\r\nShow code\r\n\r\noceanbuoys %>% arrange(year) %>% vis_miss()\r\n\r\n\r\n\r\n\r\nMissing Not at Random (MNAR)\r\nIf it is not MCAR or MAR, it is probably MNAR. This is the most tricky type of missingness to handle.\r\nThe missing values depend on both characteristics of the data and also on missing values. In this case, determining the mechanism of the generation of missing value is difficult.\r\nMissing values for a variable like blood pressure may partially depend on the values of blood pressure as patients who have low blood pressure are less likely to get their blood pressure checked at frequently.\r\n\r\nVisualize Missing data\r\nOkay, now we know what missing data is, and what are types of missing data, here are some ways we can visualize them so that we know their patterns and what they are up to.\r\nMissing pattern w/Upset plot\r\nAn upset plot from the UpSetR package can be used to visualize the patterns of missingness, or rather the combinations of missingness across cases and variables.\r\n\r\n\r\nShow code\r\n\r\ngg_miss_upset(PISA_Subsetted, nsets = 9, nintersects = 15)\r\n\r\n\r\n\r\n\r\nThe small bar plot to the left indicated the amount of missingness in variables. Consistent with the missingness diagnosis, the variable BEINGBULLIED has the most missing data, following by DAYSKIP and CLASSKIP.\r\nThe dot plot to the right showed combinations of variable that are missing in the data set. For example, there are 1,234 cases that have missing data in the variable LATE, CLASSKIP, DAYSKIP, and BEINGBULLIED.\r\nThe parameter nsets looks at 9 sets of variables, while the parameter nintersects looks at 15 variable combinations.\r\nGeneral visual summaries of missing data\r\nThis section demonstrates numerous ways to visualize missing data to determine their patterns.\r\nMissingness in variables with gg_miss_var\r\nThis plot shows the number of missing values in each variable in a dataset.\r\n\r\n\r\nShow code\r\n\r\nPISA_Subsetted %>% miss_var_table()\r\n\r\n\r\n# A tibble: 9 x 3\r\n  n_miss_in_var n_vars pct_vars\r\n          <int>  <int>    <dbl>\r\n1             2      1     11.1\r\n2          1163      1     11.1\r\n3          1300      1     11.1\r\n4          1926      1     11.1\r\n5          2197      1     11.1\r\n6          3316      1     11.1\r\n7          3330      1     11.1\r\n8          3341      1     11.1\r\n9          3833      1     11.1\r\n\r\nShow code\r\n\r\ngg_miss_var(PISA_Subsetted, show_pct = TRUE) \r\n\r\n\r\n\r\n\r\nMissingness in cases with gg_miss_case\r\nThis plot shows the number of missing values in each case. For example, the table showed that there are 2 cases with 9 missing variables (i.e., no data in all variables), and there are 1050 cases with 8 missing variables.\r\n\r\n\r\nShow code\r\n\r\nPISA_Subsetted %>% miss_case_table()\r\n\r\n\r\n# A tibble: 10 x 3\r\n   n_miss_in_case n_cases pct_cases\r\n            <int>   <int>     <dbl>\r\n 1              0   18327  80.9    \r\n 2              1     870   3.84   \r\n 3              2     140   0.618  \r\n 4              3      81   0.358  \r\n 5              4    1254   5.54   \r\n 6              5      83   0.366  \r\n 7              6     756   3.34   \r\n 8              7      90   0.397  \r\n 9              8    1050   4.64   \r\n10              9       2   0.00883\r\n\r\nShow code\r\n\r\ngg_miss_case(PISA_Subsetted)\r\n\r\n\r\n\r\n\r\nMissingness across factors with gg_miss_fct\r\nThis plot shows the number of missingness in each column, broken down by a categorical variable from the dataset.\r\n\r\n\r\nShow code\r\n\r\ngg_miss_fct(x = PISA_Subsetted, fct = REPEAT) + \r\n  labs(title = \"Missing data by the History of Class Repetition\")\r\n\r\n\r\n\r\nShow code\r\n\r\ngg_miss_fct(x = PISA_Subsetted, fct = LATE) + \r\n  labs(title = \"Missing data by Lateness History\")\r\n\r\n\r\n\r\n\r\nThe heatmap above showed the proportion of missing data we have in each response of the selected categorical variable; for example, the history of class repetition (REPEAT), with 0 as no, 1 as yes, and NA as missing.\r\nMissingness along a repeating span with gg_miss_span\r\nThis plot showed the number of missings in a given span, or breaksize, for a single selected variable.\r\n\r\n\r\nShow code\r\n\r\ngg_miss_span(PISA_Subsetted, REPEAT, span_every = 2000) +\r\n  theme_dark()\r\n\r\n\r\n\r\n\r\nThe plot went over the data and showed us how many missing data we have every 2000 data points that it went through.\r\nCumulative missing\r\nThis plot showed the cumulative amount of missing value over the data set. A sharp increase in cumulative missing value could indicate missing patterns to be discovered.\r\n\r\n\r\nShow code\r\n\r\nPISA_Subsetted %>% gg_miss_case_cumsum(breaks = 2000) + theme_bw()\r\n\r\n\r\n\r\n\r\nThis plot showed the cumulative amount of missing value over the variable. We could examine the relative proportion of missing values across variables via this plot.\r\n\r\n\r\nShow code\r\n\r\nPISA_Subsetted %>% gg_miss_var_cumsum() + theme_bw()\r\n\r\n\r\n\r\n\r\nWhat should we do with the missing data\r\nNow that we know we have missing data, there are numerous ways we can deal with it such as disregarding them with complete case analysis, or making educated guesses with imputation.\r\nAnyway, dealing with missing data helps minimizing bias in the data, maximizing the use of available information (We don’t want to throw away any of our hard-earned data), and increasing the chance of getting a good reliability estimates such as standard errors, confidence intervals, and p-values.\r\nComplete Case Analysis\r\nListwise deletion is the method of deleting all cases with missing value, so that we get a clean and complete data set as a result, at the expense of losing a chunk of data in the process.\r\nListwise deletion is often a default way to handle missing data (e.g., SPSS).\r\nThis often results in losing 20% to 50% of the data.\r\n\r\n\r\n\r\nShow code\r\n\r\nPISA_Listwise <- PISA_Subsetted[complete.cases(PISA_Subsetted), ]\r\nglimpse(PISA_Listwise)\r\n\r\n\r\nRows: 18,327\r\nColumns: 9\r\n$ REPEAT       <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~\r\n$ FEMALE       <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~\r\n$ ESCS         <dbl> -0.7302, 0.3078, 0.5059, 1.1147, 1.3626, -0.857~\r\n$ DAYSKIP      <fct> 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 2, 1, 2,~\r\n$ CLASSSKIP    <fct> 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 3, 1, 1, 2, 2, 2,~\r\n$ LATE         <fct> 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 3,~\r\n$ BEINGBULLIED <dbl> 1.2618, 1.7669, 0.1462, -0.7823, 0.2907, 0.7703~\r\n$ DISCLIMA     <dbl> -0.4186, -1.4179, 0.6019, -0.4995, -0.1045, 1.0~\r\n$ ADAPTIVITY   <dbl> -0.4708, 0.6350, -0.5786, -0.5786, -0.0763, 0.5~\r\n\r\nNotice that the size of our dataset got reduced to 18,327 cases. This happaned from deleting all cases with missing value.\r\nPairwise deletion is the method that deletes cases only if they have missing data on variables involved in a particular computation, so we can still retain the data for other analyses that do not involve variables that are missing. However, the effective sample size can vary from one analysis to another.\r\nAs a demonstration, we will calculate a covariance matrix using pairwise complete observation method.\r\n\r\n\r\nShow code\r\n\r\npairwise_var <- c(\"BEINGBULLIED\", \"DISCLIMA\")\r\ncov(PISA_Subsetted[pairwise_var], use=\"pairwise.complete.obs\")\r\n\r\n\r\n             BEINGBULLIED   DISCLIMA\r\nBEINGBULLIED    1.1315670 -0.1982891\r\nDISCLIMA       -0.1982891  1.1373620\r\n\r\nHOWEVER, the bias caused by using listwise/pairwise deletion has been shown in simulations to grossly exaggerate or underestimate some effects.\r\nDespite giving valid estimates when data are MCAR, the statistical power will be severely reduced when there is a lot of missingness. If the missingness is MAR or MNAR, removing them introduces bias to models built on these data.\r\nMissing Data Imputation\r\nOther than disregarding them, we can replace the missing value with our best guess with imputation. There are three approaches we can use, donor-based imputation, model-based imputation, and multiple imputation.\r\nDonor-based imputation\r\nDonor-based imputation replaces missing values based on other complete observations.\r\nMean Imputation\r\nMean imputation replaces all missing values with the mean of that variable.\r\nFirst, we will create a binary indicator for whether each value was originally missing.\r\n\r\n\r\nShow code\r\n\r\nPISA_meanimp <- PISA_Subsetted %>%\r\n  mutate(DISCLIMA_imp = ifelse(is.na(DISCLIMA), TRUE, FALSE)) %>%\r\n  mutate(ADAPTIVITY_imp = ifelse(is.na(ADAPTIVITY), TRUE, FALSE))\r\n\r\nPISA_meanimp[c(\"DISCLIMA_imp\",\"ADAPTIVITY_imp\")] %>% head()\r\n\r\n\r\n  DISCLIMA_imp ADAPTIVITY_imp\r\n1        FALSE          FALSE\r\n2        FALSE          FALSE\r\n3        FALSE          FALSE\r\n4        FALSE          FALSE\r\n5        FALSE          FALSE\r\n6        FALSE          FALSE\r\n\r\nReplace missing values in DISCLIMA and ADAPTIVITY variables with their respective means.\r\n\r\n\r\nShow code\r\n\r\nPISA_meanimp <- PISA_meanimp %>%\r\nmutate(DISCLIMA = ifelse(is.na(DISCLIMA), mean(DISCLIMA, na.rm = TRUE), DISCLIMA)) %>%\r\nmutate(ADAPTIVITY = ifelse(is.na(ADAPTIVITY), mean(ADAPTIVITY, na.rm = TRUE), ADAPTIVITY))\r\n\r\nPISA_meanimp %>%select(DISCLIMA, ADAPTIVITY, DISCLIMA_imp, ADAPTIVITY_imp) %>%\r\nhead()\r\n\r\n\r\n  DISCLIMA ADAPTIVITY DISCLIMA_imp ADAPTIVITY_imp\r\n1  -0.4186    -0.4708        FALSE          FALSE\r\n2  -1.4179     0.6350        FALSE          FALSE\r\n3   0.6019    -0.5786        FALSE          FALSE\r\n4  -0.4995    -0.5786        FALSE          FALSE\r\n5  -0.1045    -0.0763        FALSE          FALSE\r\n6   1.0832     0.5464        FALSE          FALSE\r\n\r\nWe can try plotting the data on a margin plot to see the result of our mean imputation.\r\n\r\n\r\nShow code\r\n\r\nPISA_meanimp %>% select(DISCLIMA, ADAPTIVITY, DISCLIMA_imp, ADAPTIVITY_imp) %>% marginplot(delimiter=\"imp\")\r\n\r\n\r\n\r\n\r\nYou can see that all missing values were replaced by the mean of that variable. Yes, we got the data back, but what did it cost?\r\nMean imputation destroys relationship between variables.\r\nModels predicting one using the other will be fooled by the outlying imputed values and will produce biased results.\r\nMean imputation crushes takes away variance in the data, which could potentially underestimate all standard errors. This prevents reliable hypothesis testing and calculating confidence interval.\r\nThis method is not generally recommended, but to each their own. Use it at your own discretion. With the right justification from the literature, mean imputation can be a viable method in your analysis as well.\r\nK-Nearest Neighbor(kNN) Imputation\r\nFor kNN imputation, we identify ‘k’ samples in the dataset that are similar or close in the space. Then we use these ‘k’ samples to estimate the value of the missing data points.\r\nBasically, it is like you have a data point with missing values asks its neighbors what value do they have on the variable that it is missing. That data point then replace its missing values with the value of its nearest neighbor.\r\n\r\n\r\nShow code\r\n\r\nPISA_kNNimp <- VIM::kNN(PISA_Subsetted, k = 6, variable = c(\"DISCLIMA\", \"ADAPTIVITY\"))\r\n\r\nPISA_kNNimp[c(\"DISCLIMA\", \"ADAPTIVITY\",\"DISCLIMA_imp\",\"ADAPTIVITY_imp\")] %>% head()\r\n\r\n\r\n  DISCLIMA ADAPTIVITY DISCLIMA_imp ADAPTIVITY_imp\r\n1  -0.4186    -0.4708        FALSE          FALSE\r\n2  -1.4179     0.6350        FALSE          FALSE\r\n3   0.6019    -0.5786        FALSE          FALSE\r\n4  -0.4995    -0.5786        FALSE          FALSE\r\n5  -0.1045    -0.0763        FALSE          FALSE\r\n6   1.0832     0.5464        FALSE          FALSE\r\n\r\nNote that there are two more columns added, DISCLIMA_imp and ADAPTIVITY_imp. The two added columns tell us if our variables of interest were imputed with the kNN method or not, with TRUE indicates that the value was imputed and FALSE indicates otherwise.\r\nModel-based imputation\r\nFor model-based imputation, missing values are predicted with a statistical or machine learning model.\r\nThe model that we used depends on the type of the missing variable:\r\nContinuous variables - linear regression\r\nBinary variables - logistic regression\r\nCategorical variables - multinomial logistic regression\r\nCount variables - Poisson regression\r\n\r\nLinear Regression Imputation\r\n\r\n\r\nShow code\r\n\r\nPISA_lmreg <- impute_lm(PISA_Subsetted, DISCLIMA + ADAPTIVITY ~.)\r\n\r\nPISA_lmreg %>% \r\n  is.na() %>%\r\n  colSums()\r\n\r\n\r\n      REPEAT       FEMALE         ESCS      DAYSKIP    CLASSSKIP \r\n        1926            2         1163         3341         3330 \r\n        LATE BEINGBULLIED     DISCLIMA   ADAPTIVITY \r\n        3316         3833         1278         2056 \r\n\r\nNotice that we have managed to impute some cases of DISCLIMA and ADAPTIVITY based on the availability of other variables in the same case. However, if there is no other variable on that case (i.e., complete missing), the model won’t be able to predict the target value as there is no predictor available.\r\nLogistic Regression Imputation\r\nLogistic regression imputation is similar to linear regression imputation, with a difference in the nature of missing value (Continuous vs Binary).\r\n\r\n\r\nShow code\r\n\r\nmissing_REPEAT <- is.na(PISA_Subsetted$REPEAT)\r\nhead(missing_REPEAT, 20)\r\n\r\n\r\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n[12] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\r\n\r\nShow code\r\n\r\nPISA_logregimp <- PISA_Subsetted\r\n\r\nlogreg_model <- glm(REPEAT ~ DAYSKIP + BEINGBULLIED + ESCS,\r\n                data = PISA_Subsetted, family = binomial)\r\n\r\n\r\npreds <- predict(logreg_model, type = \"response\")\r\n\r\npreds <- ifelse(preds >= 0.5, 1, 0)\r\n\r\nPISA_logregimp[missing_REPEAT, \"REPEAT\"] <- preds[missing_REPEAT]\r\n\r\n\r\ntable(preds[missing_REPEAT])\r\n\r\n\r\n\r\n   0    1 \r\n1669    1 \r\n\r\nShow code\r\n\r\ntable(PISA_Subsetted$REPEAT)\r\n\r\n\r\n\r\n    0     1 \r\n19575  1152 \r\n\r\nShow code\r\n\r\nPISA_logregimp %>% \r\n  is.na() %>%\r\n  colSums()\r\n\r\n\r\n      REPEAT       FEMALE         ESCS      DAYSKIP    CLASSSKIP \r\n         256            2         1163         3341         3330 \r\n        LATE BEINGBULLIED     DISCLIMA   ADAPTIVITY \r\n        3316         3833         1300         2197 \r\n\r\nMultiple Imputation by Chained Equation\r\nMultiple Imputation by Chained Equation (MICE) - also known as sequential regression multiple imputation - is an emerging method in dealing with missing values by implementing the imputation multiple times as opposed to the single imputation methods mentioned above (Azur et al., 2011).\r\nWith the right model, MICE was found to be effective in reducing bias, especially in a large data set with MCAR and MAR. The method basically imputed the missing value with a statistical model (say, linear regression) multiple times for different imputed values before pooling the results together for the final most likely value that the algorithm can come up.\r\nThe package mice has several statistics and machine learning models we can use such as Predictive mean matching (pmm), Classification and Regression Tree (cart), and Random Forest Imputation (rf). Keep in mind that it is a best practice to justify our selected model in missing data imputation to make the analysis as less ‘black box’ as possible for explainability.\r\nFor this post, I will use the predictive mean matchmaking method that calculates the predicted value from a randomly drawn set of candidate donors that have the value closest to the missing entry. The assumption is the distribution of the missing cell is the same as the observed data of the candidate donors.\r\nThe rationale is that PMM produces little biased estimates when missing data is below 50% and not systematically missing in a large data set (van Buuren, 2018).\r\n\r\n\r\nShow code\r\n\r\nmice_model <- mice(PISA_Subsetted, method='pmm', seed = 123)\r\n\r\n\r\n\r\n iter imp variable\r\n  1   1  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  1   2  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  1   3  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  1   4  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  1   5  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  2   1  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  2   2  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  2   3  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  2   4  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  2   5  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  3   1  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  3   2  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  3   3  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  3   4  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  3   5  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  4   1  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  4   2  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  4   3  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  4   4  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  4   5  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  5   1  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  5   2  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  5   3  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  5   4  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n  5   5  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY\r\n\r\nShow code\r\n\r\nPISA_miceimp <- complete(mice_model)\r\n\r\npsych::describe(PISA_Subsetted)\r\n\r\n\r\n             vars     n  mean   sd median trimmed  mad   min  max\r\nREPEAT*         1 20727  1.06 0.23   1.00    1.00 0.00  1.00 2.00\r\nFEMALE*         2 22651  1.50 0.50   2.00    1.50 0.00  1.00 2.00\r\nESCS            3 21490  0.38 0.83   0.48    0.41 0.87 -6.69 4.04\r\nDAYSKIP*        4 19312  1.31 0.66   1.00    1.16 0.00  1.00 4.00\r\nCLASSSKIP*      5 19323  1.44 0.76   1.00    1.27 0.00  1.00 4.00\r\nLATE*           6 19337  1.82 0.98   2.00    1.65 1.48  1.00 4.00\r\nBEINGBULLIED    7 18820  0.17 1.06   0.15    0.03 1.38 -0.78 3.86\r\nDISCLIMA        8 21353 -0.12 1.07  -0.04   -0.11 1.00 -2.71 2.03\r\nADAPTIVITY      9 20456  0.18 1.08   0.19    0.20 0.98 -2.27 2.01\r\n             range  skew kurtosis   se\r\nREPEAT*       1.00  3.88    13.05 0.00\r\nFEMALE*       1.00  0.00    -2.00 0.00\r\nESCS         10.72 -0.47     0.85 0.01\r\nDAYSKIP*      3.00  2.41     5.80 0.00\r\nCLASSSKIP*    3.00  1.84     2.87 0.01\r\nLATE*         3.00  1.01    -0.06 0.01\r\nBEINGBULLIED  4.64  0.88     0.31 0.01\r\nDISCLIMA      4.75 -0.13     0.16 0.01\r\nADAPTIVITY    4.27 -0.18    -0.19 0.01\r\n\r\nShow code\r\n\r\npsych::describe(PISA_miceimp)\r\n\r\n\r\n             vars     n  mean   sd median trimmed  mad   min  max\r\nREPEAT*         1 22653  1.06 0.23   1.00    1.00 0.00  1.00 2.00\r\nFEMALE*         2 22653  1.50 0.50   2.00    1.50 0.00  1.00 2.00\r\nESCS            3 22653  0.38 0.83   0.48    0.41 0.86 -6.69 4.04\r\nDAYSKIP*        4 22653  1.32 0.67   1.00    1.17 0.00  1.00 4.00\r\nCLASSSKIP*      5 22653  1.45 0.76   1.00    1.27 0.00  1.00 4.00\r\nLATE*           6 22653  1.83 0.98   2.00    1.66 1.48  1.00 4.00\r\nBEINGBULLIED    7 22653  0.18 1.07   0.15    0.04 1.38 -0.78 3.86\r\nDISCLIMA        8 22653 -0.12 1.07  -0.04   -0.11 1.00 -2.71 2.03\r\nADAPTIVITY      9 22653  0.17 1.09   0.19    0.20 0.98 -2.27 2.01\r\n             range  skew kurtosis   se\r\nREPEAT*       1.00  3.84    12.74 0.00\r\nFEMALE*       1.00  0.00    -2.00 0.00\r\nESCS         10.72 -0.47     0.84 0.01\r\nDAYSKIP*      3.00  2.38     5.62 0.00\r\nCLASSSKIP*    3.00  1.83     2.81 0.01\r\nLATE*         3.00  1.00    -0.09 0.01\r\nBEINGBULLIED  4.64  0.88     0.35 0.01\r\nDISCLIMA      4.75 -0.13     0.15 0.01\r\nADAPTIVITY    4.27 -0.17    -0.20 0.01\r\n\r\nThe message above shows that the algorithm went over the data set 5 times per iteration, with the total of 25 times in 5 iterations (5 x 5). In other words, the machine imputed the missing over and over again until the change becomes minimal to give us the most stable replacement value as possible.\r\nThere is no substantial difference in descriptive statistics of the pre-imputed and post imputed data set. Given that we gained 10% of our data back, it is a win for us.\r\nConcluding Remark\r\nData cleaning is a challenging, but necessary, process in data work. That is why it is important for us to know how to identify and deal with missing data appropriately before proceeding further into developing a statistical model and drawing conclusions from it. With a solid data preparation, combining with a thorough literature review, it is likely that we can draw meaningful conclusions from the data to inform our future decisions. The opposite is also true as well for poorly processed data sets. We wouldn’t want to waste our time and resources to know that the conclusion we draw is not well-supported.\r\nA bit of controversial topic here. Non-methodologists might have some concerns that we cannot just make up the obtained scores. Like, what if the participants did not answer that question for a reason? How can we be sure that the number we generated will represent characteristics of the targeted population? The million-dollar question is, would you still do this, knowing that the number you generated might have some degree of error? Are you willing to trade authenticity of the data for the data point that might improve your statistical models? It is your task as a researcher and an informed individual to justify your choice in this matter, as well as other choices that you made in your endeavor.\r\nAnyway, thank you so much for your read as always! Happy Holiday, everyone! I hope you have an awesome break! :)\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-27-missingdata/missingpic.jpg",
    "last_modified": "2021-12-27T15:12:54-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-11-applying-machine-learning-to-audio-data/",
    "title": "Applying Machine Learning to Audio Data: Visualization, Classification, and Recommendation",
    "description": "For this entry, I am trying my hands on audio data to extract its features for exploratory data analysis (EDA), using machine learning algorithms to perform music classification, and finally build up on that result to develop a recommendation system for music of similar characteristics.\n\n(13 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2021-12-11",
    "categories": [
      "Python",
      "Data Visualization",
      "Supervised Machine Learning"
    ],
    "contents": "\r\n\r\nContents\r\nMachine Learning with Audio data\r\nExplore Audio Data\r\nAudio Features\r\nExploratory Data Analysis\r\nMachine Learning Classification\r\nMusic recommendation algorithm\r\nConcluding note\r\n\r\nMachine Learning with Audio data\r\nWhen we think of data, people may think of numbers and texts in tables. Some may even think of using images as data, but just so you know, we can also convert and extract features from audio data (i.e., music) to understand and make use of it as well! Here, we will visualize music sound wave from .wav files to understand about what differentiates one tone from another (we can actually see soundwaves!).\r\nI primarily relied on Olteanu et al. (2019)’s work, Music genre classification article, and Analytics Vidhya guide to the same topic to guide this reproduction and experimentation with the data.\r\nTo introduce the data set a bit. I will be using the GTZAN dataset, which is a public data set for evaluation in machine listening research for music genre recognition (MGR). The files were collected in 2000-2001 from a variety of sources including personal CDs, radio, microphone recordings to represent a variety of recording conditions.\r\nWe will start from importing audio data into our Python environment for data visualization; then, we will explore its feature such as sound wave, spectogram, mel-spectogram, harmonics and perceptrual, tempo, spectral centroid, and chroma frequencies. We will then conduct an exploratory data analysis with correlation heatmap with the extracted features, generating a box plot for genres distribution, and perform a principal component analysis to divide genres into groups.\r\nLastly, we will perform machine learning classification to train the algorithm to recognize and predict new audio files into genres (e.g., rock, pop, jazz), as well as develop a music recommendation system using the cosine similarity statistics. This function is a part of music delivery platforms such as Spotify, Youtube music, or Apple Music.\r\nWe will begin by importing necessary libraries for graphing (seaborn and matplotlib), data manipulation (pandas), machine learning (sklearn), and audio work (librosa).\r\n\r\n\r\nShow code\r\n# Usual Libraries\r\nimport pandas as pd\r\nimport numpy as np\r\nimport seaborn as sns\r\nimport matplotlib.pyplot as plt\r\nimport sklearn\r\n\r\nimport librosa\r\nimport librosa.display\r\n\r\nExplore Audio Data\r\nWe will use librosa, which is the main library for audio work in Python. Let us first Explore our Audio Data to see how it looks (we’ll work with pop.00002.wav file). We will check for sound - the sequence of vibrations in varying pressure strengths (y) and sample rate (sr) the number of samples of audio carried per second, measured in Hz or kHz.\r\n\r\n\r\nShow code\r\n# Importing 1 file\r\ny, sr = librosa.load('D:/Program/Private_project/DistillSite/_posts/2021-12-11-applying-machine-learning-to-audio-data/genres_original/pop/pop.00002.wav')\r\n\r\nprint('y:', y, '\\n')\r\ny: [-0.09274292 -0.11630249 -0.11886597 ...  0.14419556  0.16311646\r\n  0.09634399] \r\n\r\nShow code\r\nprint('y shape:', np.shape(y), '\\n')\r\ny shape: (661504,) \r\n\r\nShow code\r\nprint('Sample Rate (KHz):', sr, '\\n')\r\n\r\n# Verify length of the audio\r\nSample Rate (KHz): 22050 \r\n\r\nShow code\r\nprint('Check Length of the audio in second:', 661794/22050)\r\nCheck Length of the audio in second: 30.013333333333332\r\n\r\nWe will then clean the data by trimming all leading and trailing silence from the audio signal.\r\n\r\n\r\nShow code\r\n# Trim leading and trailing silence from an audio signal (silence before and after the actual audio)\r\naudio_file, _ = librosa.effects.trim(y)\r\n\r\n# the result is an numpy ndarray\r\nprint('Audio File:', audio_file, '\\n')\r\nAudio File: [-0.09274292 -0.11630249 -0.11886597 ...  0.14419556  0.16311646\r\n  0.09634399] \r\n\r\nShow code\r\nprint('Audio File shape:', np.shape(audio_file))\r\nAudio File shape: (661504,)\r\n\r\n2D Representation: Sound Waves\r\nWe can view a 2D representation of a sound with sound waves\r\n\r\n\r\nShow code\r\nplt.figure(figsize = (16, 6))\r\n<Figure size 1600x600 with 0 Axes>\r\n\r\nShow code\r\nlibrosa.display.waveplot(y = audio_file, sr = sr, color = \"#A300F9\");\r\nplt.title(\"Sound Waves in Pop 02\", fontsize = 23);\r\nplt.show()\r\n\r\n\r\nFourier Transform\r\nWe will then perform a fourier transform to convert the y-axis (frequency) to log scale, and the “color” axis (amplitude) to Decibels.\r\n\r\n\r\nShow code\r\n# Default FFT window size\r\nn_fft = 2048 # FFT window size\r\nhop_length = 512 # number audio of frames between STFT columns (looks like a good default)\r\n\r\n# Short-time Fourier transform (STFT)\r\nD = np.abs(librosa.stft(audio_file, n_fft = n_fft, hop_length = hop_length))\r\n\r\nprint('Shape of D object:', np.shape(D))\r\nShape of D object: (1025, 1293)\r\n\r\n\r\n\r\nShow code\r\nplt.figure(figsize = (16, 6))\r\n<Figure size 1600x600 with 0 Axes>\r\n\r\nShow code\r\nplt.plot(D);\r\nplt.show()\r\n\r\n\r\nThe Spectrogram\r\nAnother characteristics that can represent a sound is its spectogram - a visual representation of signal frequencies across time (aka sonographs, voiceprints, or voicegrams).\r\n\r\n\r\nShow code\r\n# Convert an amplitude spectrogram to Decibels-scaled spectrogram.\r\nDB = librosa.amplitude_to_db(D, ref = np.max)\r\n\r\n# Creating the Spectogram\r\nplt.figure(figsize = (16, 6))\r\n<Figure size 1600x600 with 0 Axes>\r\n\r\nShow code\r\nlibrosa.display.specshow(DB, sr = sr, hop_length = hop_length, x_axis = 'time', y_axis = 'log', cmap = 'cool')\r\n<matplotlib.collections.QuadMesh object at 0x000000006FAB1310>\r\n\r\nShow code\r\nplt.colorbar();\r\nplt.show()\r\n\r\n\r\nMel Spectrogram\r\nThe Mel Spectogram is a non-linear version of spectogram with a Mel scale on the y-axis. Mel scale converts the normal specrogram to frequencies that are perceptible by human ears, so basically, the difference between spectogram and mel spectogram is in its mathematical structure and its ability to be perceived by human. Each music genre has different spectogram (and mel spectogram) structure.\r\n\r\n\r\nShow code\r\ny, sr = librosa.load('D:/Program/Private_project/DistillSite/_posts/2021-12-11-applying-machine-learning-to-audio-data/genres_original/metal/metal.00036.wav')\r\ny, _ = librosa.effects.trim(y)\r\n\r\n\r\nS = librosa.feature.melspectrogram(y, sr=sr)\r\nS_DB = librosa.amplitude_to_db(S, ref=np.max)\r\nplt.figure(figsize = (16, 6))\r\n<Figure size 1600x600 with 0 Axes>\r\n\r\nShow code\r\nlibrosa.display.specshow(S_DB, sr=sr, hop_length=hop_length, x_axis = 'time', y_axis = 'log',\r\n                        cmap = 'cool');\r\nplt.colorbar();\r\nplt.title(\"Metal Mel Spectrogram\", fontsize = 23);\r\nplt.show()\r\n\r\n\r\n\r\n\r\nShow code\r\ny, sr = librosa.load('D:/Program/Private_project/DistillSite/_posts/2021-12-11-applying-machine-learning-to-audio-data/genres_original/classical/classical.00036.wav')\r\ny, _ = librosa.effects.trim(y)\r\n\r\n\r\nS = librosa.feature.melspectrogram(y, sr=sr)\r\nS_DB = librosa.amplitude_to_db(S, ref=np.max)\r\nplt.figure(figsize = (16, 6))\r\n<Figure size 1600x600 with 0 Axes>\r\n\r\nShow code\r\nlibrosa.display.specshow(S_DB, sr=sr, hop_length=hop_length, x_axis = 'time', y_axis = 'log',\r\n                        cmap = 'cool');\r\nplt.colorbar();\r\nplt.title(\"Classical Mel Spectrogram\", fontsize = 23);\r\nplt.show()\r\n\r\n\r\nAudio Features\r\nNow that we have explored an audio file with several visualizations of Spectogram, fourier transform, and sound waves, let us try extracting audio features that we may use with data manipulation and machine learning.\r\nZero Crossing Rate\r\nthe rate at which the sound signal changes from positive to negative and vice versa. This feature is usually used for speech recognition and music information retrieval. Music genre with high percussive sound like rock or metal usually have high Zero Crossing Rate than other genres.\r\n\r\n\r\nShow code\r\n# Total zero_crossings in our 1 song\r\nzero_crossings = librosa.zero_crossings(audio_file, pad=False)\r\nprint(sum(zero_crossings))\r\n78769\r\n\r\nHarmonics and Perceptual\r\nHarmonics (the orange wave) are audio characteristics that human ears can’t distinguish (represents the sound color)\r\nPerceptual (the purple wave) are sound waves that represent rhythm and emotion of the music.\r\n\r\n\r\nShow code\r\ny_harm, y_perc = librosa.effects.hpss(audio_file)\r\n\r\nplt.figure(figsize = (16, 6))\r\n<Figure size 1600x600 with 0 Axes>\r\n\r\nShow code\r\nplt.plot(y_harm, color = '#A300F9');\r\nplt.plot(y_perc, color = '#FFB100');\r\nplt.show()\r\n\r\n\r\nTempo BMP (beats per minute)\r\nTempo is the number of beat per one minute.\r\n\r\n\r\nShow code\r\ntempo, _ = librosa.beat.beat_track(y, sr = sr)\r\ntempo\r\n107.666015625\r\n\r\nSpectral Centroid\r\nThis variable represents brightness of a sound by calculating the center of sound spectrum (where the sound signal is at its peak). We can also plot it into a wave form.\r\n\r\n\r\nShow code\r\n# Calculate the Spectral Centroids\r\nspectral_centroids = librosa.feature.spectral_centroid(audio_file, sr=sr)[0]\r\n\r\n# Shape is a vector\r\nprint('Centroids:', spectral_centroids, '\\n')\r\nCentroids: [3042.39242043 3057.96296504 3043.45666379 ... 3476.4010229  3908.31319501\r\n 3834.930348  ] \r\n\r\nShow code\r\nprint('Shape of Spectral Centroids:', spectral_centroids.shape, '\\n')\r\n\r\n# Computing the time variable for visualization\r\nShape of Spectral Centroids: (1293,) \r\n\r\nShow code\r\nframes = range(len(spectral_centroids))\r\n\r\n# Converts frame counts to time (seconds)\r\nt = librosa.frames_to_time(frames)\r\n\r\nprint('frames:', frames, '\\n')\r\nframes: range(0, 1293) \r\n\r\nShow code\r\nprint('t:', t)\r\n\r\n# Function that normalizes the Sound Data\r\nt: [0.00000000e+00 2.32199546e-02 4.64399093e-02 ... 2.99537415e+01\r\n 2.99769615e+01 3.00001814e+01]\r\n\r\nShow code\r\ndef normalize(x, axis=0):\r\n    return sklearn.preprocessing.minmax_scale(x, axis=axis)\r\n\r\n\r\n\r\nShow code\r\n#Plotting the Spectral Centroid along the waveform\r\nplt.figure(figsize = (16, 6))\r\n<Figure size 1600x600 with 0 Axes>\r\n\r\nShow code\r\nlibrosa.display.waveplot(audio_file, sr=sr, alpha=0.4, color = '#A300F9');\r\nplt.plot(t, normalize(spectral_centroids), color='#FFB100');\r\nplt.show()\r\n\r\n\r\nSpectral Rolloff\r\nSpectral Rolloff is a frequency below a specified percentage of the total spectral energy. It is like we have a cut-point, and we visualize the sound wave that is below that cut-point. Let’s just call it as another characteristic of a sound.\r\n\r\n\r\nShow code\r\n# Spectral RollOff Vector\r\nspectral_rolloff = librosa.feature.spectral_rolloff(audio_file, sr=sr)[0]\r\n\r\n# The plot\r\nplt.figure(figsize = (16, 6))\r\n<Figure size 1600x600 with 0 Axes>\r\n\r\nShow code\r\nlibrosa.display.waveplot(audio_file, sr=sr, alpha=0.4, color = '#A300F9');\r\nplt.plot(t, normalize(spectral_rolloff), color='#FFB100');\r\nplt.show()\r\n\r\n\r\nMel-Frequency Cepstral Coefficients\r\nThe Mel frequency Cepstral coefficients (MFCCs) of a signal are a small set of features that describes the overall shape of a spectral envelope. It imitates characteristics of human voice.\r\n\r\n\r\nShow code\r\nmfccs = librosa.feature.mfcc(audio_file, sr=sr)\r\nprint('mfccs shape:', mfccs.shape)\r\n\r\n#Displaying  the MFCCs:\r\nmfccs shape: (20, 1293)\r\n\r\nShow code\r\nplt.figure(figsize = (16, 6))\r\n<Figure size 1600x600 with 0 Axes>\r\n\r\nShow code\r\nlibrosa.display.specshow(mfccs, sr=sr, x_axis='time', cmap = 'cool');\r\nplt.show()\r\n\r\n\r\nWe can scale the data a bit to make the feature (blue part) more apparent.\r\n\r\n\r\nShow code\r\n# Perform Feature Scaling\r\nmfccs = sklearn.preprocessing.scale(mfccs, axis=1)\r\nC:\\Users\\tarid\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:174: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\r\n  warnings.warn(\"Numerical issues were encountered \"\r\nC:\\Users\\tarid\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py:191: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \r\n  warnings.warn(\"Numerical issues were encountered \"\r\n\r\nShow code\r\nprint('Mean:', mfccs.mean(), '\\n')\r\nMean: 3.097782e-09 \r\n\r\nShow code\r\nprint('Var:', mfccs.var())\r\nVar: 1.0\r\n\r\nShow code\r\nplt.figure(figsize = (16, 6))\r\n<Figure size 1600x600 with 0 Axes>\r\n\r\nShow code\r\nlibrosa.display.specshow(mfccs, sr=sr, x_axis='time', cmap = 'cool');\r\nplt.show()\r\n\r\n\r\nChroma Frequencies\r\nChroma feature represents the tone of music or sound by projecting its sound spectrum into a space that represents musical octave. This feature is usually used in chord recognition task.\r\n\r\n\r\nShow code\r\n# Increase or decrease hop_length to change how granular you want your data to be\r\nhop_length = 5000\r\n\r\n# Chromogram\r\nchromagram = librosa.feature.chroma_stft(audio_file, sr=sr, hop_length=hop_length)\r\nprint('Chromogram shape:', chromagram.shape)\r\nChromogram shape: (12, 133)\r\n\r\nShow code\r\nplt.figure(figsize=(16, 6))\r\n<Figure size 1600x600 with 0 Axes>\r\n\r\nShow code\r\nlibrosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm');\r\nplt.show()\r\n\r\n\r\nExploratory Data Analysis\r\nWe will perform an exploratory data analysis with the features_30_sec.csv data that contains the mean and variance of the features discussed above for all audio file in the data bank. We have 10 genres of music, each genre has 100 audio files. That makes the total of 1000 songs that we have. There are 60 features in total for each song.\r\n\r\n\r\nShow code\r\ndata = pd.read_csv('features_30_sec.csv')\r\ndata.head()\r\n          filename  length  chroma_stft_mean  ...  mfcc20_mean  mfcc20_var  label\r\n0  blues.00000.wav  661794          0.350088  ...     1.221291   46.936035  blues\r\n1  blues.00001.wav  661794          0.340914  ...     0.531217   45.786282  blues\r\n2  blues.00002.wav  661794          0.363637  ...    -2.231258   30.573025  blues\r\n3  blues.00003.wav  661794          0.404785  ...    -3.407448   31.949339  blues\r\n4  blues.00004.wav  661794          0.308526  ...   -11.703234   55.195160  blues\r\n\r\n[5 rows x 60 columns]\r\n\r\nCorrelation Heatmap for feature means\r\nHere, we are making a correlation heatmap among feature means to see which feature correlates with which. The redder a square is, the more negative the correlation between that pair of variable becomes.\r\n\r\n\r\nShow code\r\n\r\n# Computing the Correlation Matrix\r\nspike_cols = [col for col in data.columns if 'mean' in col]\r\ncorr = data[spike_cols].corr()\r\n\r\n# Generate a mask for the upper triangle\r\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\r\n\r\n# Set up the matplotlib figure\r\nf, ax = plt.subplots(figsize=(16, 11));\r\n\r\n# Generate a custom diverging colormap\r\ncmap = sns.diverging_palette(0, 25, as_cmap=True, s = 90, l = 45, n = 5)\r\n\r\n# Draw the heatmap with the mask and correct aspect ratio\r\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\r\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\r\n<AxesSubplot:>\r\n\r\nShow code\r\nplt.title('Correlation Heatmap (for the MEAN variables)', fontsize = 25)\r\nText(0.5, 1.0, 'Correlation Heatmap (for the MEAN variables)')\r\n\r\nShow code\r\nplt.xticks(fontsize = 10)\r\n(array([ 0.5,  1.5,  2.5,  3.5,  4.5,  5.5,  6.5,  7.5,  8.5,  9.5, 10.5,\r\n       11.5, 12.5, 13.5, 14.5, 15.5, 16.5, 17.5, 18.5, 19.5, 20.5, 21.5,\r\n       22.5, 23.5, 24.5, 25.5, 26.5, 27.5]), [Text(0.5, 0, 'chroma_stft_mean'), Text(1.5, 0, 'rms_mean'), Text(2.5, 0, 'spectral_centroid_mean'), Text(3.5, 0, 'spectral_bandwidth_mean'), Text(4.5, 0, 'rolloff_mean'), Text(5.5, 0, 'zero_crossing_rate_mean'), Text(6.5, 0, 'harmony_mean'), Text(7.5, 0, 'perceptr_mean'), Text(8.5, 0, 'mfcc1_mean'), Text(9.5, 0, 'mfcc2_mean'), Text(10.5, 0, 'mfcc3_mean'), Text(11.5, 0, 'mfcc4_mean'), Text(12.5, 0, 'mfcc5_mean'), Text(13.5, 0, 'mfcc6_mean'), Text(14.5, 0, 'mfcc7_mean'), Text(15.5, 0, 'mfcc8_mean'), Text(16.5, 0, 'mfcc9_mean'), Text(17.5, 0, 'mfcc10_mean'), Text(18.5, 0, 'mfcc11_mean'), Text(19.5, 0, 'mfcc12_mean'), Text(20.5, 0, 'mfcc13_mean'), Text(21.5, 0, 'mfcc14_mean'), Text(22.5, 0, 'mfcc15_mean'), Text(23.5, 0, 'mfcc16_mean'), Text(24.5, 0, 'mfcc17_mean'), Text(25.5, 0, 'mfcc18_mean'), Text(26.5, 0, 'mfcc19_mean'), Text(27.5, 0, 'mfcc20_mean')])\r\n\r\nShow code\r\nplt.yticks(fontsize = 10);\r\nplt.show()\r\n\r\n\r\nBox Plot for Genres Distributions\r\nWe will also make a boxplot for tempo of all music genres.\r\n\r\n\r\nShow code\r\nx = data[[\"label\", \"tempo\"]]\r\n\r\nf, ax = plt.subplots(figsize=(16, 9));\r\nsns.boxplot(x = \"label\", y = \"tempo\", data = x, palette = 'husl');\r\n\r\nplt.title('BPM Boxplot for Genres', fontsize = 25)\r\nText(0.5, 1.0, 'BPM Boxplot for Genres')\r\n\r\nShow code\r\nplt.xticks(fontsize = 14)\r\n(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), [Text(0, 0, 'blues'), Text(1, 0, 'classical'), Text(2, 0, 'country'), Text(3, 0, 'disco'), Text(4, 0, 'hiphop'), Text(5, 0, 'jazz'), Text(6, 0, 'metal'), Text(7, 0, 'pop'), Text(8, 0, 'reggae'), Text(9, 0, 'rock')])\r\n\r\nShow code\r\nplt.yticks(fontsize = 10);\r\nplt.xlabel(\"Genre\", fontsize = 15)\r\nText(0.5, 0, 'Genre')\r\n\r\nShow code\r\nplt.ylabel(\"BPM\", fontsize = 15)\r\nText(0, 0.5, 'BPM')\r\n\r\nShow code\r\nplt.show()\r\n\r\n\r\nPrincipal Component Analysis\r\nFor this part, we will conduct a principal component analysis (PCA) to visualize possible groups of genres and display its results with a scatter plot. We can see that a lot of songs have ambiguous genres; that is, it could be classified into more than one similar genres such as disco or hiphop based on the sound characteristics that we extract from them. There is also a song that is exclusively classified into a genre (reggae, for example).\r\n\r\n\r\nShow code\r\nfrom sklearn import preprocessing\r\n\r\ndata = data.iloc[0:, 1:]\r\ny = data['label']\r\nX = data.loc[:, data.columns != 'label']\r\n\r\n#### NORMALIZE X ####\r\ncols = X.columns\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\nnp_scaled = min_max_scaler.fit_transform(X)\r\nX = pd.DataFrame(np_scaled, columns = cols)\r\n\r\n\r\n#### PCA 2 COMPONENTS ####\r\nfrom sklearn.decomposition import PCA\r\n\r\npca = PCA(n_components=2)\r\nprincipalComponents = pca.fit_transform(X)\r\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\r\n\r\n# concatenate with target label\r\nfinalDf = pd.concat([principalDf, y], axis = 1)\r\n\r\npca.explained_variance_ratio_\r\n\r\n# 44.93 variance explained\r\narray([0.2439355 , 0.21781804])\r\n\r\n\r\n\r\nShow code\r\nplt.figure(figsize = (16, 9))\r\n<Figure size 1600x900 with 0 Axes>\r\n\r\nShow code\r\nsns.scatterplot(x = \"principal component 1\", y = \"principal component 2\", data = finalDf, hue = \"label\", alpha = 0.7,\r\n               s = 100);\r\n\r\nplt.title('PCA on Genres', fontsize = 25)\r\nText(0.5, 1.0, 'PCA on Genres')\r\n\r\nShow code\r\nplt.xticks(fontsize = 14)\r\n(array([-1.5, -1. , -0.5,  0. ,  0.5,  1. ,  1.5]), [Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, ''), Text(0, 0, '')])\r\n\r\nShow code\r\nplt.yticks(fontsize = 10);\r\nplt.xlabel(\"Principal Component 1\", fontsize = 15)\r\nText(0.5, 0, 'Principal Component 1')\r\n\r\nShow code\r\nplt.ylabel(\"Principal Component 2\", fontsize = 15)\r\nText(0, 0.5, 'Principal Component 2')\r\n\r\nShow code\r\nplt.show()\r\n\r\n\r\nMachine Learning Classification\r\nUsing features from features_3_sec.csv file, we can build a machine learning classification model that predicts genre of a new audio file. We will be loading a lot of machine learning models to see which model performs best.\r\n\r\n\r\nShow code\r\nfrom sklearn.naive_bayes import GaussianNB\r\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.svm import SVC\r\nfrom sklearn.neural_network import MLPClassifier\r\nfrom xgboost import XGBClassifier, XGBRFClassifier\r\nfrom xgboost import plot_tree, plot_importance\r\n\r\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\r\nfrom sklearn import preprocessing\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.feature_selection import RFE\r\n\r\nReading in the Data\r\nWe will read the data, split it into training and testing data sets, and create a function to assess accuracy of the models.\r\n\r\n\r\nShow code\r\ndata = pd.read_csv('features_3_sec.csv')\r\ndata = data.iloc[0:, 1:] \r\ndata.head()\r\n   length  chroma_stft_mean  chroma_stft_var  ...  mfcc20_mean  mfcc20_var  label\r\n0   66149          0.335406         0.091048  ...    -0.243027   43.771767  blues\r\n1   66149          0.343065         0.086147  ...     5.784063   59.943081  blues\r\n2   66149          0.346815         0.092243  ...     2.517375   33.105122  blues\r\n3   66149          0.363639         0.086856  ...     3.630866   32.023678  blues\r\n4   66149          0.335579         0.088129  ...     0.536961   29.146694  blues\r\n\r\n[5 rows x 59 columns]\r\n\r\nFeatures and Target variable\r\nCreate features and target variable, as well as normalizing the data.\r\n\r\n\r\nShow code\r\ny = data['label'] # genre variable.\r\nX = data.loc[:, data.columns != 'label'] #select all columns but not the labels\r\n\r\n#### NORMALIZE X ####\r\n\r\n# Normalize so everything is on the same scale. \r\n\r\ncols = X.columns\r\nmin_max_scaler = preprocessing.MinMaxScaler()\r\nnp_scaled = min_max_scaler.fit_transform(X)\r\n\r\n# new data frame with the new scaled data. \r\nX = pd.DataFrame(np_scaled, columns = cols)\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\r\n\r\n\r\n\r\nShow code\r\n#Creating a Predefined function to assess the accuracy of a model\r\n\r\ndef model_assess(model, title = \"Default\"):\r\n    model.fit(X_train, y_train)\r\n    preds = model.predict(X_test)\r\n    #print(confusion_matrix(y_test, preds))\r\n    print('Accuracy', title, ':', round(accuracy_score(y_test, preds), 5), '\\n')\r\n\r\nHere, we will test 10 different machine learning models to see which model is most suitable to music classification task.\r\n\r\n\r\nShow code\r\n# Naive Bayes\r\nnb = GaussianNB()\r\nmodel_assess(nb, \"Naive Bayes\")\r\n\r\n# Stochastic Gradient Descent\r\nAccuracy Naive Bayes : 0.51952 \r\n\r\nShow code\r\nsgd = SGDClassifier(max_iter=5000, random_state=0)\r\nmodel_assess(sgd, \"Stochastic Gradient Descent\")\r\n\r\n# KNN\r\nAccuracy Stochastic Gradient Descent : 0.65532 \r\n\r\nShow code\r\nknn = KNeighborsClassifier(n_neighbors=19)\r\nmodel_assess(knn, \"KNN\")\r\n\r\n# Decission trees\r\nAccuracy KNN : 0.80581 \r\n\r\nShow code\r\ntree = DecisionTreeClassifier()\r\nmodel_assess(tree, \"Decission trees\")\r\n\r\n# Random Forest\r\nAccuracy Decission trees : 0.6383 \r\n\r\nShow code\r\nrforest = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=0)\r\nmodel_assess(rforest, \"Random Forest\")\r\n\r\n# Support Vector Machine\r\nAccuracy Random Forest : 0.81415 \r\n\r\nShow code\r\nsvm = SVC(decision_function_shape=\"ovo\")\r\nmodel_assess(svm, \"Support Vector Machine\")\r\n\r\n# Logistic Regression\r\nAccuracy Support Vector Machine : 0.75409 \r\n\r\nShow code\r\nlg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\r\nmodel_assess(lg, \"Logistic Regression\")\r\n\r\n# Neural Nets\r\nAccuracy Logistic Regression : 0.6977 \r\n\r\n\r\nC:\\Users\\tarid\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\r\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\r\n\r\nIncrease the number of iterations (max_iter) or scale the data as shown in:\r\n    https://scikit-learn.org/stable/modules/preprocessing.html\r\nPlease also refer to the documentation for alternative solver options:\r\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\r\n  n_iter_i = _check_optimize_result(\r\n\r\nShow code\r\nnn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5000, 10), random_state=1)\r\nmodel_assess(nn, \"Neural Nets\")\r\n\r\n# Cross Gradient Booster\r\nAccuracy Neural Nets : 0.67401 \r\n\r\n\r\nC:\\Users\\tarid\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\r\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\r\n\r\nIncrease the number of iterations (max_iter) or scale the data as shown in:\r\n    https://scikit-learn.org/stable/modules/preprocessing.html\r\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\r\n\r\nShow code\r\nxgb = XGBClassifier(n_estimators=1000, learning_rate=0.05, eval_metric='mlogloss')\r\nmodel_assess(xgb, \"Cross Gradient Booster\")\r\n\r\n# Cross Gradient Booster (Random Forest)\r\nAccuracy Cross Gradient Booster : 0.90224 \r\n\r\n\r\nC:\\Users\\tarid\\ANACON~1\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\r\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\r\n\r\nShow code\r\nxgbrf = XGBRFClassifier(objective= 'multi:softmax', eval_metric='mlogloss')\r\nmodel_assess(xgbrf, \"Cross Gradient Booster (Random Forest)\")\r\nAccuracy Cross Gradient Booster (Random Forest) : 0.74575 \r\n\r\nThe function Extreme Gradient Boosting (XGBoost) achieves the highest performance with 90% accuracy. We will be using this model to create the final prediction model and compute feature importance output along with its confusion matrix.\r\nNote that I have also included Multilayer Perception - a variant of Neural Networks model - into the list of candidate models as well. While neural networks may be known for its complexity, it does not mean that the model is a silver bullet for every machine learning task. This idea is derived from the No Free Lunch Theorem that implies that there is no single best algorithm.\r\n\r\n\r\nShow code\r\n#Final model\r\nxgb = XGBClassifier(n_estimators=1000, learning_rate=0.05, eval_metric='mlogloss')\r\nxgb.fit(X_train, y_train)\r\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\r\n              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\r\n              eval_metric='mlogloss', gamma=0, gpu_id=-1, importance_type=None,\r\n              interaction_constraints='', learning_rate=0.05, max_delta_step=0,\r\n              max_depth=6, min_child_weight=1, missing=nan,\r\n              monotone_constraints='()', n_estimators=1000, n_jobs=8,\r\n              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\r\n              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\r\n              subsample=1, tree_method='exact', validate_parameters=1,\r\n              verbosity=None)\r\n\r\nShow code\r\npreds = xgb.predict(X_test)\r\n\r\nprint('Accuracy', ':', round(accuracy_score(y_test, preds), 5), '\\n')\r\n\r\n# Confusion Matrix\r\nAccuracy : 0.90224 \r\n\r\nShow code\r\nconfusion_matr = confusion_matrix(y_test, preds) #normalize = 'true'\r\nplt.figure(figsize = (16, 9))\r\n<Figure size 1600x900 with 0 Axes>\r\n\r\nShow code\r\nsns.heatmap(confusion_matr, cmap=\"Blues\", annot=True,\r\n            xticklabels = [\"blues\", \"classical\", \"country\", \"disco\", \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"],\r\n           yticklabels=[\"blues\", \"classical\", \"country\", \"disco\", \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"]);\r\nplt.show()\r\n\r\n\r\nFeature Importance\r\nFrom the feature importance output, we can see that varianve and mean of the perceptual variable perceptr_var are the two most important variable in genre classification.\r\n\r\n\r\nShow code\r\nimport eli5\r\nfrom eli5.sklearn import PermutationImportance\r\n\r\nperm = PermutationImportance(estimator=xgb, random_state=1)\r\nperm.fit(X_test, y_test)\r\nPermutationImportance(estimator=XGBClassifier(base_score=0.5, booster='gbtree',\r\n                                              colsample_bylevel=1,\r\n                                              colsample_bynode=1,\r\n                                              colsample_bytree=1,\r\n                                              enable_categorical=False,\r\n                                              eval_metric='mlogloss', gamma=0,\r\n                                              gpu_id=-1, importance_type=None,\r\n                                              interaction_constraints='',\r\n                                              learning_rate=0.05,\r\n                                              max_delta_step=0, max_depth=6,\r\n                                              min_child_weight=1, missing=nan,\r\n                                              monotone_constraints='()',\r\n                                              n_estimators=1000, n_jobs=8,\r\n                                              num_parallel_tree=1,\r\n                                              objective='multi:softprob',\r\n                                              predictor='auto', random_state=0,\r\n                                              reg_alpha=0, reg_lambda=1,\r\n                                              scale_pos_weight=None,\r\n                                              subsample=1, tree_method='exact',\r\n                                              validate_parameters=1,\r\n                                              verbosity=None),\r\n                      random_state=1)\r\n\r\nShow code\r\neli5.show_weights(estimator=perm, feature_names = X_test.columns.tolist())\r\n<IPython.core.display.HTML object>\r\n\r\nMusic recommendation algorithm\r\nThe music recommendation system assumes that the audience likes to listen to music of similar genres or similar characteristics. The system allows us to find the best similarity, ranked in descending order, from the bast match to the least best match with the cosine_similarity statistics.\r\n\r\n\r\nShow code\r\n# Libraries\r\nimport IPython.display as ipd\r\nfrom sklearn.metrics.pairwise import cosine_similarity\r\nfrom sklearn import preprocessing\r\n\r\n# Read data\r\ndata = pd.read_csv('features_30_sec.csv', index_col='filename')\r\n\r\n# Extract labels\r\nlabels = data[['label']]\r\n\r\n# Drop labels from original dataframe\r\ndata = data.drop(columns=['length','label'])\r\ndata.head()\r\n\r\n# Scale the data\r\n                 chroma_stft_mean  chroma_stft_var  ...  mfcc20_mean  mfcc20_var\r\nfilename                                            ...                         \r\nblues.00000.wav          0.350088         0.088757  ...     1.221291   46.936035\r\nblues.00001.wav          0.340914         0.094980  ...     0.531217   45.786282\r\nblues.00002.wav          0.363637         0.085275  ...    -2.231258   30.573025\r\nblues.00003.wav          0.404785         0.093999  ...    -3.407448   31.949339\r\nblues.00004.wav          0.308526         0.087841  ...   -11.703234   55.195160\r\n\r\n[5 rows x 57 columns]\r\n\r\nShow code\r\ndata_scaled=preprocessing.scale(data)\r\nprint('Scaled data type:', type(data_scaled))\r\nScaled data type: <class 'numpy.ndarray'>\r\n\r\nCosine Similarity\r\nWe will calculate the pairwise cosine similarity for each combination of songs in the data. The closer the value is to “1”, the more similar the two songs can be.\r\n\r\n\r\nShow code\r\n# Cosine similarity\r\nsimilarity = cosine_similarity(data_scaled)\r\nprint(\"Similarity shape:\", similarity.shape)\r\n\r\n# Convert into a dataframe and then set the row index and column names as labels\r\nSimilarity shape: (1000, 1000)\r\n\r\nShow code\r\nsim_df_labels = pd.DataFrame(similarity)\r\nsim_df_names = sim_df_labels.set_index(labels.index)\r\nsim_df_names.columns = labels.index\r\n\r\nsim_df_names.head()\r\nfilename         blues.00000.wav  ...  rock.00099.wav\r\nfilename                          ...                \r\nblues.00000.wav         1.000000  ...        0.304098\r\nblues.00001.wav         0.049231  ...        0.311723\r\nblues.00002.wav         0.589618  ...        0.321069\r\nblues.00003.wav         0.284862  ...        0.183210\r\nblues.00004.wav         0.025561  ...        0.061785\r\n\r\n[5 rows x 1000 columns]\r\n\r\nSong similarity scoring\r\nWe will define a function find_similar_songs() to take the name of the song and return top 5 best matches for that song.\r\n\r\n\r\nShow code\r\ndef find_similar_songs(name):\r\n    # Find songs most similar to another song\r\n    series = sim_df_names[name].sort_values(ascending = False)\r\n    \r\n    # Remove cosine similarity == 1 (songs will always have the best match with themselves)\r\n    series = series.drop(name)\r\n    \r\n    # Display the 5 top matches \r\n    print(\"\\n*******\\nSimilar songs to \", name)\r\n    print(series.head(5))\r\n\r\nNow let us try putting it to the test:\r\n\r\n\r\nShow code\r\nfind_similar_songs('pop.00023.wav') \r\n\r\n*******\r\nSimilar songs to  pop.00023.wav\r\nfilename\r\npop.00075.wav    0.875235\r\npop.00089.wav    0.874246\r\npop.00088.wav    0.872443\r\npop.00091.wav    0.871975\r\npop.00024.wav    0.869849\r\nName: pop.00023.wav, dtype: float64\r\n\r\nShow code\r\nfind_similar_songs('pop.00078.wav') \r\n\r\n*******\r\nSimilar songs to  pop.00078.wav\r\nfilename\r\npop.00088.wav       0.914322\r\nhiphop.00077.wav    0.876289\r\npop.00089.wav       0.871822\r\npop.00074.wav       0.855630\r\npop.00023.wav       0.854349\r\nName: pop.00078.wav, dtype: float64\r\n\r\nShow code\r\nfind_similar_songs('rock.00018.wav') \r\n\r\n*******\r\nSimilar songs to  rock.00018.wav\r\nfilename\r\nrock.00017.wav     0.921997\r\nmetal.00028.wav    0.913790\r\nmetal.00058.wav    0.912421\r\nrock.00016.wav     0.912421\r\nrock.00026.wav     0.910113\r\nName: rock.00018.wav, dtype: float64\r\n\r\nShow code\r\nfind_similar_songs('metal.00002.wav') \r\n\r\n*******\r\nSimilar songs to  metal.00002.wav\r\nfilename\r\nmetal.00028.wav    0.904367\r\nmetal.00059.wav    0.896096\r\nrock.00018.wav     0.891910\r\nrock.00017.wav     0.886526\r\nrock.00016.wav     0.867508\r\nName: metal.00002.wav, dtype: float64\r\n\r\nThe output above shows similarity score for the sampled song. For example, the top three similar songs to pop.00023 - Britney Spears - “I’m so curious (2009 remaster)” are pop.00075, pop.00089, and pop.00088 respectively.\r\nThe algorithm can also recommend similar songs from other genres as well, for example, metal.00002 - Iron Maiden “Flight of Icarus”has similar songs in both metal and rock genre. The same thing also applies to rock.00018 - Queens - “Another One Bites The Dust” that has similar songs in both metal and rock genre as well.\r\nConcluding note\r\nIt is interesting in how we are able to process audio data into numbers or images. The application of music recognition algorithm could be highly beneficial to entertainment industry in meeting the needs of consumer market. Researchers can also apply algorithm of this nature to extract characteristics that may be useful to their variable of interest such as attention or mental concentration.\r\nOne thing worth noting is, I am not a music expert, though I would love to practice piano at some point. The algorithm that I used is just one way of classifying musics into genres with the available information (e.g., tempo, harmonic wave). Domain expertise is important in data work regardless of your skill in data science. That is why it is crucial to consult with experts of the subject matter (i.e., musician) to make the most out of the insight we gained from this data. This also applies to other area such as testing as well. I can do the math and the programming, but I don’t know much about students or English testing. This is where domain experts come into play. I just want to emphasize the importance of collaboration between fields to ensure the best results for the collective good.\r\nDue to the nature of my field (education), it is unlikely that I will have much chance to work with audio data, but this practice is still valuable regardless. The model_assess function that I used can be applied to any machine learning work that requires the use of several models to find the most suitable algorithm for the task. The cosine_similarity statistics is also useful to recommendation system of any products such as textbooks or novels. Anyway, it was a good practice, and I had fun nonetheless. As always, thank you very much for your read! I hope you have a good day wherever you are!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-11-applying-machine-learning-to-audio-data/applying-machine-learning-to-audio-data_files/figure-html5/unnamed-chunk-13-11.png",
    "last_modified": "2021-12-11T17:08:48-07:00",
    "input_file": {},
    "preview_width": 3072,
    "preview_height": 1152
  },
  {
    "path": "posts/2021-12-08-interactive-dashboard-for-suicide-data/",
    "title": "Interactive plots for Suicide Data",
    "description": "For this entry, will be visualizing suicide data from 1958 to 2015 with interactive plots to communicate insights to non-technical audience.  \n\n(14 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2021-12-09",
    "categories": [
      "R",
      "Data Visualization",
      "Quantitative research"
    ],
    "contents": "\r\n\r\nContents\r\nIntroducing the data set\r\nGlobal Level\r\nContinent Level\r\nCountry Level\r\nSearch for specific countries\r\nConcluding note\r\n\r\nIntroducing the data set\r\nThe data set used in this post is the Suicide Rates Overview 1985 to 2016 data from Kaggle - an online community of data scientists and machine learning practitioners funded by Google. The data set was compiled from numerous sources of the Human development index (HDI) of United Nations Development Program (2018), World development indicators from World Banks (2018), the Suicide in the Twenty-First Century data set, and data from the suicide prevention program by WHO.\r\nIn this time and age, a lot of things we do is driven by data and conclusions we drawn from it. The question is, how? It is not like we are looking at tables of data every single time we want to decide on something. Data is oftentimes unstructured, unlabeled, and most of all, overwhelming when we don’t know what to do with it.\r\nWith the right tool and the right framework, we will know what to look for and how to look for it. After that, it is a matter of whether we can effectively communicate what we learned from the data to our audience. Data visualization is an effective way to tell stories about data to non-technical stakeholders (e.g., educators, mental health practitioners, or policy makers).\r\nThe plot can be even more engaging with the audience when we make it interactive, so that it can emphasize the part that really matters (while fading out the irrelevant part). That is what I aim to do with this entry, building interactive plots for the suicide data!\r\n\r\n\r\nShow code\r\n\r\n# Load necessary packages. \r\nlibrary(flexdashboard) # Dashboard package\r\nlibrary(highcharter) # Interactive data visualizations\r\nlibrary(plotly) # Interactive data visualizations\r\nlibrary(viridis) # Color gradients\r\nlibrary(tidyverse) # Metapackge for data management\r\nlibrary(countrycode) # Converts country names/codes\r\nlibrary(rjson) # JSON reader\r\nlibrary(crosstalk) # Provides interactivity for HTML widgets\r\nlibrary(DT) # Displaying data tables\r\n\r\n\r\n\r\nFirst of all, we will load the necessary package and import the data set as usual. We will rely on plotly, crosstalk, flexdashboard, and highcharter packages to make the plot. We will also use glimpse to check the data set of its content.\r\n\r\n\r\nShow code\r\n\r\n# Read data. \r\n\r\ndata <- read.csv('master.csv') %>%\r\n  filter(year != 2016, # filter out 2016 and countries with 0 data. \r\n         country != 'Dominica',\r\n         country != 'Saint Kitts and Nevis')\r\n         \r\n# Fix the names of some of the countries in our data to match the country names \r\n# used by our map later on so that they'll be interpreted and displayed. \r\ndata <- data %>%\r\n  mutate(country = fct_recode(country, \"The Bahamas\" = \"Bahamas\"),\r\n         country = fct_recode(country, \"Cape Verde\" = \"Cabo Verde\"),\r\n         country = fct_recode(country, \"South Korea\" = \"Republic of Korea\"),\r\n         country = fct_recode(country, \"Russia\" = \"Russian Federation\"),\r\n         country = fct_recode(country, \"Republic of Serbia\" = \"Serbia\"),\r\n         country = fct_recode(country, \"United States of America\" = \"United States\"))\r\n\r\n# Reorder levels of age to be in chronological order. \r\ndata$age <- factor(data$age, levels = c(\"5-14 years\", \"15-24 years\", \"25-34 years\", \"35-54 years\", \"55-74 years\", \"75+ years\"))\r\n\r\nglimpse(data)\r\n\r\n\r\nRows: 27,612\r\nColumns: 12\r\n$ country            <fct> Albania, Albania, Albania, Albania, Alban~\r\n$ year               <int> 1987, 1987, 1987, 1987, 1987, 1987, 1987,~\r\n$ sex                <chr> \"male\", \"male\", \"female\", \"male\", \"male\",~\r\n$ age                <fct> 15-24 years, 35-54 years, 15-24 years, 75~\r\n$ suicides_no        <int> 21, 16, 14, 1, 9, 1, 6, 4, 1, 0, 0, 0, 2,~\r\n$ population         <int> 312900, 308000, 289700, 21800, 274300, 35~\r\n$ suicides.100k.pop  <dbl> 6.71, 5.19, 4.83, 4.59, 3.28, 2.81, 2.15,~\r\n$ country.year       <chr> \"Albania1987\", \"Albania1987\", \"Albania198~\r\n$ HDI.for.year       <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~\r\n$ gdp_for_year....   <chr> \"2,156,624,900\", \"2,156,624,900\", \"2,156,~\r\n$ gdp_per_capita.... <int> 796, 796, 796, 796, 796, 796, 796, 796, 7~\r\n$ generation         <chr> \"Generation X\", \"Silent\", \"Generation X\",~\r\n\r\nWe will create our custom theme here with color code, so that we don’t have to to it repeatedly with every plot.\r\n\r\n\r\nShow code\r\n\r\n# Create a custom theme for the plots. \r\ncustom_theme <- hc_theme(\r\n  colors = c('#5CACEE', 'green', 'red'),\r\n  chart = list(\r\n         backgroundColor = '#FAFAFA', \r\n         plotBorderColor = \"black\"),\r\n  xAxis = list(\r\n         gridLineColor = \"E5E5E5\", \r\n         labels = list(style = list(color = \"#333333\")), \r\n         lineColor = \"#E5E5E5\", \r\n         minorGridLineColor = \"#E5E5E5\", \r\n         tickColor = \"#E5E5E5\", \r\n         title = list(style = list(color = \"#333333\"))), \r\n  yAxis = list(\r\n         gridLineColor = \"#E5E5E5\", \r\n         labels = list(style = list(color = \"#333333\")), \r\n         lineColor = \"#E5E5E5\", \r\n         minorGridLineColor = \"#E5E5E5\", \r\n         tickColor = \"#E5E5E5\", \r\n         tickWidth = 1, \r\n         title = list(style = list(color = \"#333333\"))),   \r\n  title = list(style = list(color = '#333333')),\r\n  subtitle = list(style = list(color = '#666666')),\r\n  legend = list(\r\n         itemStyle = list(color = \"#333333\"), \r\n         itemHoverStyle = list(color = \"#FFF\"), \r\n         itemHiddenStyle = list(color = \"#606063\")), \r\n  credits = list(style = list(color = \"#666\")),\r\n  itemHoverStyle = list(color = 'gray'))\r\n\r\n\r\n\r\nGlobal Level\r\nFirst, we will examine the data at global level. The unit is in suicide per 100k people; for example, the average of 13.2 means for every 100k people, 13.2 people had committed suicide. The global trend peaked around 1995, and we can see that the suicide rate in 2015 is the lowest across years. Try hovering your mouse over each point to see specific year and its corresponding suicide rate.\r\n\r\n\r\nShow code\r\n\r\n# Create tibble for our line plot.  \r\noverall_tibble <- data %>%\r\n  select(year, suicides_no, population) %>%\r\n  group_by(year) %>%\r\n  summarise(suicide_capita = round((sum(suicides_no)/sum(population))*100000, 2)) \r\n\r\n# Create a line plot.\r\nhighchart() %>% \r\n    hc_add_series(overall_tibble, hcaes(x = year, y = suicide_capita, color = suicide_capita), type = \"line\") %>%\r\n    hc_tooltip(crosshairs = TRUE, borderWidth = 1.5, headerFormat = \"\", pointFormat = paste(\"Year: <b>{point.x}<\/b> <br> Suicides: <b>{point.y}<\/b>\")) %>%\r\n    hc_title(text = \"Worldwide suicides by year\") %>% \r\n    hc_subtitle(text = \"1985-2015\") %>%\r\n    hc_xAxis(title = list(text = \"Year\")) %>%\r\n    hc_yAxis(title = list(text = \"Suicides per 100K people\"),\r\n             allowDecimals = FALSE,\r\n             plotLines = list(list(\r\n                    color = \"black\", width = 1, dashStyle = \"Dash\", \r\n                    value = mean(overall_tibble$suicide_capita),\r\n                    label = list(text = \"Mean = 13.12\", \r\n                                 style = list(color = \"black\", fontSize = 11))))) %>%\r\n    hc_legend(enabled = FALSE) %>% \r\n    hc_add_theme(custom_theme)\r\n\r\n\r\n\r\n\r\nWorldwide suicide rate by gender\r\nWe can also compare the worldwide suicide rate by gender (male vs female). It seems like male has an overall higher rate of suicide comparing to female population. Hovering your mouse above a trend (say, male) makes the graph highlight that trend while showing specific year, gender, and its corresponding suicide rate.\r\n\r\n\r\nShow code\r\n\r\n# Create tibble for sex so we can use it when creating our line plot.  \r\nsex_tibble <- data %>%\r\n  select(year, sex, suicides_no, population) %>%\r\n  group_by(year, sex) %>%\r\n  summarise(suicide_capita = round((sum(suicides_no)/sum(population))*100000, 2))\r\n\r\n# Pick color for gender.\r\nsex_color <- c(\"#EE6AA7\", \"#87CEEB\") # baby blue & pink\r\n\r\n# Create line plot.\r\nhighchart() %>% \r\n    hc_add_series(sex_tibble, hcaes(x = year, y = suicide_capita, group = sex), type = \"line\", color = sex_color) %>%\r\n    hc_tooltip(crosshairs = TRUE, borderWidth = 1.5, headerFormat = \"\", pointFormat = paste(\"Year: <b>{point.x}<\/b> <br>\",\"Gender: <b>{point.sex}<\/b><br>\", \"Suicides: <b>{point.y}<\/b>\")) %>%\r\n    hc_title(text = \"Worldwide suicides by Gender\") %>% \r\n    hc_subtitle(text = \"1985-2015\") %>%\r\n    hc_xAxis(title = list(text = \"Year\")) %>%\r\n    hc_yAxis(title = list(text = \"Suicides per 100K people\"),\r\n             allowDecimals = FALSE,\r\n             plotLines = list(list(\r\n                    color = \"black\", width = 1, dashStyle = \"Dash\",\r\n                    value = mean(overall_tibble$suicide_capita),\r\n                    label = list(text = \"Mean = 13.12\", \r\n                                 style = list(color = 'black', fontSize = 11))))) %>% \r\n    hc_add_theme(custom_theme)\r\n\r\n\r\n\r\n\r\nWe can also make a pie chart to compare the proportion of male vs female in the data set. The proportion of male who committed suicide is much higher than female here, but note that this is just an insight we got from the data. Each data has its limitations and can only be used to draw conclusions with those constraints in mind (e.g., data coverage, context).\r\n\r\n\r\nShow code\r\n\r\n# First, make a tibble of suicide by sex. We will use this for our pie chart.\r\npie_sex <- data %>%\r\n  select(sex, suicides_no, population) %>%\r\n  group_by(sex) %>%\r\n  summarise(suicide_capita = round((sum(suicides_no)/sum(population))*100000, 2))\r\n  \r\n# Create pie chart for sex. \r\nhighchart() %>% \r\n  hc_add_series(pie_sex, hcaes(x = sex, y = suicide_capita, \r\n                               color = sex_color), type = \"pie\") %>%\r\n  hc_tooltip(borderWidth = 1.5, headerFormat = \"\", pointFormat = paste(\"Gender: <b>{point.sex} ({point.percentage:.1f}%)<\/b> <br> Suicides per 100K: <b>{point.y}<\/b>\")) %>%\r\n  hc_title(text = \"<b>Worldwide suicides by Gender<\/b>\", style = (list(fontSize = '14px'))) %>% \r\n  hc_subtitle(text = \"1985-2015\", style = (list(fontSize = '10px'))) %>%\r\n  hc_plotOptions(pie = list(dataLabels = list(distance = 5, \r\n                            style = list(fontSize = 10)), \r\n                            size = 130)) %>% \r\n  hc_add_theme(custom_theme)\r\n\r\n\r\n\r\n\r\nWorldwide suicides by Age\r\nHere, we use an interactive line plot to compare worldwide suicide rate across age groups. Try clicking on age group labels (e.g., 25-34 years) below to toggle it on or off, so that we can compare the trend between each age group easily without having other irrelevant lines to distract.\r\n\r\n\r\nShow code\r\n\r\n# Create tibble for age so we can use it when creating our line plot.  \r\nage_tibble <- data %>%\r\n  select(year, age, suicides_no, population) %>%\r\n  group_by(year, age) %>%\r\n  summarise(suicide_capita = round((sum(suicides_no)/sum(population))*100000, 2))\r\n\r\n# Pick color for graph. \r\nage_color <- rev(plasma(6))\r\n\r\n# Create a line plot.\r\nhighchart() %>% \r\n    hc_add_series(age_tibble, hcaes(x = year, y = suicide_capita, group = age), type = \"line\", color = age_color) %>%\r\n    hc_tooltip(crosshairs = TRUE, borderWidth = 1.5, headerFormat = \"\", pointFormat = paste(\"Year: <b>{point.x}<\/b> <br>\",\"Age: <b>{point.age}<\/b><br>\", \"Suicides: <b>{point.y}<\/b>\")) %>%\r\n    hc_title(text = \"Worldwide suicides by Age\") %>% \r\n    hc_subtitle(text = \"1985-2015\") %>%\r\n    hc_xAxis(title = list(text = \"Year\")) %>%\r\n    hc_yAxis(title = list(text = \"Suicides per 100K people\"),\r\n             allowDecimals = FALSE,\r\n             plotLines = list(list(\r\n                    color = \"black\", width = 1, dashStyle = \"Dash\",\r\n                    value = mean(overall_tibble$suicide_capita),\r\n                    label = list(text = \"Mean = 13.12\", \r\n                                 style = list(color = 'black', fontSize = 11))))) %>% \r\n    hc_add_theme(custom_theme)\r\n\r\n\r\n\r\n\r\nWe can also see the proportion of age groups with a pie chart here as well. This pie chart works like the gender pie chart above.\r\n\r\n\r\nShow code\r\n\r\n# First, create a tibble of suicide by Age. We will use this for our pie chart.\r\npie_age <- data %>%\r\n  select(age, suicides_no, population) %>%\r\n  group_by(age) %>%\r\n  summarise(suicide_capita = round((sum(suicides_no)/sum(population))*100000, 2)) %>%\r\n  arrange(suicide_capita)\r\n\r\n# Create pie chart for Age. \r\nhighchart() %>% \r\n  hc_add_series(pie_age, hcaes(x = age, y = suicide_capita, \r\n                               color = age_color), type = \"pie\") %>%\r\n  hc_tooltip(borderWidth = 1.5, headerFormat = \"\", pointFormat = paste(\"Age: <b>{point.age} ({point.percentage:.1f}%)<\/b> <br> Suicides per 100K: <b>{point.y}<\/b>\")) %>%  \r\n  hc_title(text = \"<b>Worldwide suicides by Age<\/b>\", style = (list(fontSize = '14px'))) %>% \r\n  hc_subtitle(text = \"1985-2015\", style = (list(fontSize = '10px'))) %>%\r\n  hc_plotOptions(pie = list(dataLabels = list(distance = 5, \r\n                            style = list(fontSize = 10)), \r\n                            size = 130)) %>% \r\n  hc_add_theme(custom_theme)\r\n\r\n\r\n\r\n\r\nContinent Level\r\nDiving a little bit deeper, we will compare suicide rate at the continent level (i.e., Africa, Asia, Europe, North America, Oceania, and South America).\r\n\r\n\r\nShow code\r\n\r\n# Create new column in our data for continent. Use countrycode() to extract continents from country names. \r\ndata$continent <- countrycode(sourcevar = data$country,\r\n                              origin = \"country.name\",\r\n                              destination = \"continent\")\r\n\r\n# Reclassify countries that have been coded as 'Americas', by countrycode(), into 'North America' and 'South America'. \r\nsouth_america <- c('Argentina', 'Brazil', 'Chile', 'Colombia', 'Ecuador', 'Guyana', 'Paraguay', 'Suriname', 'Uruguay')\r\n\r\ndata$continent[data$country %in% south_america] <- 'South America'\r\ndata$continent[data$continent=='Americas'] <- 'North America'\r\n\r\n\r\n\r\nSuicides by continent and Gender\r\nWe can use a histogram to compare suicide rate of each continent by gender (male vs female).\r\n\r\n\r\nShow code\r\n\r\n# Create a tibble for continent and sex.\r\ncontinent_sex_tibble <- data %>%\r\n  select(continent, sex, suicides_no, population) %>%\r\n  group_by(continent, sex) %>%\r\n  summarize(suicide_capita = round((sum(suicides_no)/sum(population))*100000, 2))\r\n\r\n# Create histogram of suicides by continent.\r\nhighchart() %>%\r\nhc_add_series(continent_sex_tibble, hcaes(x = continent, y = suicide_capita, group = sex), type = \"column\")  %>% \r\n    hc_colors(colors = sex_color) %>%\r\n    hc_title(text = \"Suicides by continent and <b>Gender<\/b>\", style = (list(fontSize = '14px'))) %>% \r\n    hc_subtitle(text = \"1985-2015\") %>%\r\n    hc_tooltip(borderWidth = 1.5, pointFormat = paste(\"Gender: <b> {point.sex} <\/b> <br> Suicides: <b>{point.y}<\/b>\")) %>%\r\n    hc_xAxis(categories = c(\"Africa\", \"Asia\", \"Europe\", \"North <br> America\", \"Oceania\", \"South <br> America\"), labels = list(style = list(fontSize = 8))) %>%\r\n    hc_yAxis(labels = list(style = list(fontSize = 10)),\r\n             title = list(text = \"Suicides per 100K people\",\r\n             style = list(fontSize = 10)),\r\n        plotLines = list(\r\n          list(color = \"black\", width = 1, dashStyle = \"Dash\", \r\n               value = mean(overall_tibble$suicide_capita),\r\n               label = list(text = \"Mean = 13.12\", style = list(color = \"black\", fontSize = 6))))) %>%     \r\n    hc_legend(verticalAlign = 'top', enabled = FALSE) %>% \r\n    hc_add_theme(custom_theme)\r\n\r\n\r\n\r\n\r\nSuicides by continent and Age\r\nWe can also compare suicide rate of each continent by age group here.\r\n\r\n\r\nShow code\r\n\r\n# Create a tibble for continent and sex.\r\ncontinent_age_tibble <- data %>%\r\n  select(continent, age, suicides_no, population) %>%\r\n  group_by(continent, age) %>%\r\n  summarize(suicide_capita = round((sum(suicides_no)/sum(population))*100000, 2)) \r\n\r\n# Create histogram of suicides by continent.\r\nhighchart() %>%\r\nhc_add_series(continent_age_tibble, hcaes(x = continent, y = suicide_capita, group = age), type = \"column\")  %>% \r\n    hc_colors(colors = age_color) %>%\r\n    hc_title(text = \"Suicides by continent and <b>Age<\/b>\", style = (list(fontSize = '14px'))) %>% \r\n    hc_subtitle(text = \"1985-2015\") %>%\r\n    hc_tooltip(borderWidth = 1.5, pointFormat = paste(\"Age: <b> {point.age} <\/b> <br> Suicides: <b>{point.y}<\/b>\")) %>%\r\n    hc_xAxis(categories = c(\"Africa\", \"Asia\", \"Europe\", \"North <br> America\", \"Oceania\", \"South <br> America\"), labels = list(style = list(fontSize = 8))) %>%\r\n    hc_yAxis(labels = list(style = list(fontSize = 10)),\r\n             title = list(text = \"Suicides per 100K people\",\r\n                          style = list(fontSize = 10)),\r\n        plotLines = list(\r\n          list(color = \"black\", width = 1, dashStyle = \"Dash\", \r\n               value = mean(overall_tibble$suicide_capita),\r\n               label = list(text = \"Mean = 13.12\", style = list(color = \"black\", fontSize = 6))))) %>%    \r\n    hc_legend(verticalAlign = 'top', enabled = FALSE) %>% \r\n    hc_add_theme(custom_theme)\r\n\r\n\r\n\r\n\r\nSuicides by continent as represented by global mapping\r\nFor this part, I have imported the pre-downloaded world continent data from Highcharts map collection, but you can also download the map directly from the site with map_data <- download_map_data(\"custom/world-continents\").\r\nHere, we can compare suicide rate of each continent using color gradient to see which continent has the highest (or the lowest) suicide rate.\r\n\r\n\r\nShow code\r\n\r\n# Import continent map data. \r\nmap_data <- fromJSON(file = \"world-continents.geo.json\")\r\n# Create a tibble for continent.\r\ncontinent_tibble <- data %>%\r\n  select(continent, suicides_no, population) %>%\r\n  group_by(continent) %>%\r\n  summarize(suicide_capita = round((sum(suicides_no)/sum(population))*100000, 2)) %>%\r\n  arrange(suicide_capita)\r\n\r\n# Create continent map with suicide data. \r\nhighchart() %>%\r\nhc_add_series_map(map_data, continent_tibble, value = \"suicide_capita\", joinBy = c('name','continent'), name = \"Suicides (per 100K people)\")  %>% \r\n    hc_add_series(continent_tibble, hcaes(x = continent, y = suicide_capita, color = suicide_capita), type = \"pie\", name = 'Suicides (per 100K people)')  %>% \r\n    hc_colorAxis(stops = color_stops()) %>% \r\n    hc_title(text = \"Suicides by Continent\") %>% \r\n    hc_subtitle(text = \"1985-2015\") %>%\r\n    hc_tooltip(borderWidth = 1.5, valueSuffix = '') %>%\r\n    hc_plotOptions(\r\n    pie = list(center = c('10%', '80%'), size = 110, dataLabels = list(enabled = FALSE))) %>% \r\n    hc_add_theme(custom_theme)\r\n\r\n\r\n\r\n\r\nCountry Level\r\nWe can also compare suicide rate at country level by plotting them all on a bar chart as separated by gender, age group, and geographical location as well. For the age group bar chart, you can hover your mouse over each age group to compare that specific group across countries, and you can also toggle each age group on and off as well to filter out irrelevant information.\r\n\r\n\r\nShow code\r\n\r\n# Create tibble for overall suicides by country\r\ncountry_bar <- data %>%\r\n  select(country, suicides_no, population) %>%\r\n  group_by(country) %>%\r\n  summarise(suicide_capita = round((sum(suicides_no)/sum(population))*100000, 2)) %>%\r\n  arrange(desc(suicide_capita))\r\n\r\n# Create interactive bar plot\r\nhighchart() %>%\r\n    hc_add_series(country_bar, hcaes(x = country, y = suicide_capita, color = suicide_capita), type = \"bar\")  %>% \r\n    hc_tooltip(borderWidth = 1.5, \r\n               pointFormat = paste(\"Suicides: <b>{point.y}<\/b>\")) %>%\r\n    hc_legend(enabled = FALSE) %>%\r\n    hc_title(text = \"Suicides by country\") %>% \r\n    hc_subtitle(text = \"1985-2015\") %>%\r\n    hc_xAxis(categories = country_bar$country, \r\n             labels = list(step = 1),\r\n             min = 0, max = 25,\r\n             scrollbar = list(enabled = TRUE)) %>%\r\n    hc_yAxis(title = list(text = \"Suicides per 100K people\")) %>%\r\n    hc_plotOptions(bar = list(stacking = \"normal\", \r\n                              pointPadding = 0, groupPadding = 0, borderWidth = 0.5)) %>% \r\n    hc_add_theme(custom_theme)\r\n\r\n\r\n\r\n\r\nBy gender\r\n\r\n\r\nShow code\r\n\r\n# Create tibble for suicide by countries and sex. \r\ncountry_bar_sex <- data %>%\r\n  select(country, sex, suicides_no, population) %>%\r\n  group_by(country, sex) %>%\r\n  summarise(suicide_capita = round((sum(suicides_no)/sum(population))*100000, 2))\r\n\r\ncountry_tibble <- data %>%\r\n  select(country, suicides_no, population) %>%\r\n  group_by(country) %>%\r\n  summarise(suicide_capita = round((sum(suicides_no)/sum(population))*100000, 2)) \r\n\r\n# Create bar chart of suicide by sex.\r\nhighchart() %>%\r\n    hc_add_series(country_bar_sex, hcaes(x = country, y = suicide_capita, group = sex), type = \"bar\", color = sex_color)  %>% \r\n    hc_tooltip(borderWidth = 1.5, pointFormat = paste(\"Gender: <b>{point.sex} ({point.percentage:.1f}%)<\/b> <br> Suicides per 100K: <b>{point.y}<\/b>\")) %>%\r\n    hc_legend(enabled = TRUE, colorByPoint = TRUE) %>%\r\n    hc_title(text = \"Suicides by country and gender\") %>% \r\n    hc_subtitle(text = \"1985-2015\") %>%\r\n    hc_xAxis(categories = country_tibble$country,\r\n             labels = list(step = 1),\r\n             min = 0, max = 25,\r\n             scrollbar = list(enabled = TRUE)) %>%\r\n    hc_yAxis(title = list(text = \"Percentage of total suicides\")) %>%\r\n    hc_plotOptions(bar = list(stacking = \"percent\", \r\n                              pointPadding = 0, groupPadding = 0, borderWidth = 0.4)) %>% \r\n    hc_add_theme(custom_theme)\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# Create tibble for suicide by countries and age \r\ncountry_bar_age <- data %>%\r\n  select(country, age, suicides_no, population) %>%\r\n  group_by(country, age) %>%\r\n  summarise(suicide_capita = round((sum(suicides_no)/sum(population))*100000, 2))\r\n\r\n# Create interactive bar plot.\r\nhighchart() %>%\r\n    hc_add_series(country_bar_age, hcaes(x = country, y = suicide_capita, group = age), type = \"bar\", color = age_color)  %>% \r\n    hc_tooltip(borderWidth = 1.5, pointFormat = paste(\"Age: <b>{point.age} ({point.percentage:.1f}%)<\/b> <br> Suicides per 100K: <b>{point.y}<\/b>\")) %>%\r\n    hc_title(text = \"Suicides by country and age\") %>% \r\n    hc_subtitle(text = \"1985-2015\") %>%\r\n    hc_xAxis(categories = country_tibble$country,\r\n             labels = list(step = 1),\r\n             min = 0, max = 25,\r\n             scrollbar = list(enabled = TRUE)) %>%\r\n    hc_yAxis(title = list(text = \"Percent of total suicides\")) %>%\r\n    hc_plotOptions(bar = list(stacking = \"percent\", \r\n                              pointPadding = 0, groupPadding = 0, borderWidth = 0.5)) %>% \r\n    hc_add_theme(custom_theme)\r\n\r\n\r\n\r\n\r\nThe interactive world map below can be used to check which country that we have data on. The white space shows that we do not have much data on African countries, as well as mainland China, and South-East Asian countries.\r\n\r\n\r\nShow code\r\n\r\n# Create a tibble with suicide per capita by country for 1985-2015. \r\ncountry_tibble <- data %>%\r\n  select(country, suicides_no, population) %>%\r\n  group_by(country) %>%\r\n  summarize(suicide_capita = round((sum(suicides_no)/sum(population))*100000, 2))\r\n\r\n# Create interactive world map.\r\nhighchart() %>%\r\nhc_add_series_map(worldgeojson, country_tibble, value = \"suicide_capita\", joinBy = c('name','country'))  %>% \r\n    hc_colorAxis(stops = color_stops()) %>% \r\n    hc_title(text = \"Suicides by Country\") %>% \r\n    hc_subtitle(text = \"1985-2015\") %>%\r\n    hc_tooltip(borderWidth = 1.5, headerFormat = \"\", valueSuffix = \" suicides (per 100K people)\") %>% \r\n    hc_add_theme(custom_theme)\r\n\r\n\r\n\r\n\r\nSearch for specific countries\r\nNow this is the fanciest (and the hardest part to compile). We can use crosstalk package to link our data set across country filter, country data table, and line plot of suicide rate trend across years. Try typing country names that you are interested in in the search box, and the table will filter out suicide data for those countries for you to compare, as well as displaying a line plot for them as well.\r\nYou can use plotly option to download the plot as png, compare data of the two countries on hover, and darg a box to zoom in at a specific period of time.\r\n\r\n\r\nShow code\r\n\r\n# Create tibble for our line plot.  \r\ncountry_year_tibble <- data %>%\r\n  select(country, year, suicides_no, population) %>%\r\n  group_by(country, year) %>%\r\n  summarise(suicide_capita = round((sum(suicides_no)/sum(population))*100000, 2)) \r\n\r\n# Create shared data that will be used to link filters, data table, and line plot. \r\nshared_data <- SharedData$new(country_year_tibble)\r\n\r\n# Create filter for year and country. These filters will adjust the DT datatable and PLOTLY plot. \r\nfilter_slider(\"year\", \"Year\", shared_data, ~year, step = 1)\r\n\r\n\r\n\r\nYear\r\n\r\n\r\nShow code\r\n\r\nfilter_select(\"country\", \"Country\", shared_data, ~country, allLevels = TRUE, multiple = TRUE)\r\n\r\n\r\n\r\nCountry\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# Create datatable. \r\ndatatable(shared_data,\r\n          rownames = FALSE,\r\n          colnames = c('Country', 'Year', 'Suicides /100K'),\r\n          class = 'cell-border stripe',\r\n          width = '50%',\r\n          extensions = \"Scroller\",\r\n          options=list(deferRender = FALSE, \r\n                       scrollY = 200, \r\n                       scrollCollapse = TRUE,\r\n                       scroller = TRUE,\r\n                       dom = 't'))\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# Set a random seed. We will do this so that we can reproduce the random sample of colors we will use for our next graph. \r\nset.seed(80085)\r\n\r\n# Create line graph. \r\nplot_ly(shared_data, x = ~year, y = ~suicide_capita, \r\n       color = ~country, colors = sample(colours(), 120),\r\n       type = 'scatter', mode = 'lines',\r\n       hoverinfo = 'text', text = ~paste(\"Country: \", country, '<br>Year: ', year, \"<br>Suicides: \", suicide_capita)) %>%\r\n       layout(showlegend = FALSE,\r\n              title = \"Suicide by country\",\r\n              xaxis = list(title = \"Year\"),\r\n              yaxis = list(title = \"Suicides per 100K people\")) %>%\r\n       layout(plot_bgcolor = 'transparent') %>% \r\n       layout(paper_bgcolor = 'transparent') %>% \r\n       add_markers()\r\n\r\n\r\n\r\n\r\nConcluding note\r\nThis entry is just me playing around with codes to generate interactive plots. While it may not be as flashy as machine learning or natural language processing, data visualization is an important part of data work to communicate results to our audience without relying to heavily on texts and numbers.\r\nA little bit on graphing. When we make a plot, simplicity and efficiency are vital. We cannot just throw every last bit of information up there and expect the audience to understand everything as us, who were invested in that data for weeks, months, or even years. Use only the most relevant information, so that the main information doesn’t get overshadowed by bits and pieces such as sound effect, animation, or unnecessary numbers. Again, thank you very much for reading!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-08-interactive-dashboard-for-suicide-data/preview.jpg",
    "last_modified": "2021-12-09T20:09:20-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-07-image-recognition-with-artificial-neural-networks/",
    "title": "Image Recognition with Artificial Neural Networks",
    "description": "In this entry, we will be developing a deep learning algorithm - a sub-field of machine learning inspired by the structure of human brain (neural networks) - to classify images of single digit number (0-9).  \n\n(9 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2021-12-07",
    "categories": [
      "Python",
      "Supervised Machine Learning",
      "Deep Learning"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is deep learning? Why does it have to be that name?\r\nPreparation\r\nConvolutional Neural Network (CNN)\r\nData augmentation\r\nModel evaluation\r\nFinal remarks and Conclusions\r\n\r\nWhat is deep learning? Why does it have to be that name?\r\nAs we may have experienced, there has been a lot of progress made on developing computer vision systems that can recognize images such as the Facebook tagging system, speech recognition (ever use Google home or Alexa?), and even medical image analysis of cancer cell or injury from X-ray images. A lot of the mentioned technologies are based on deep learning, which is a machine learning technique that teach machines to go through similar learning process as humans (or as close as it can be). For this entry, it is basically like I am teaching toddlers to recognize numbers, but they learn at a much faster rate because they are machines (well, I don’t have to feed them, and they don’t cry when I ordered them to go through like 1000 math questions).\r\nThe adjective “deep” comes from the fact that the structure of this algorithm comprises of multiple layers of network between the input and the output, and that we could hardly explain what is going on in the middle of the process. We only know if the computer learns or not; for that, this machine learning approach is also known as the black box approach.\r\nimage from IBM neural network learning modulePreparation\r\nBefore we dive in, let us load the essential libraries to read the data, plot the graphs, and construct a neural network algorithm.\r\n\r\n\r\nShow code\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib.image as mpimg\r\nimport seaborn as sns\r\n\r\nnp.random.seed(2)\r\n\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import confusion_matrix\r\nimport itertools\r\n\r\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\r\nfrom tensorflow.keras.optimizers import RMSprop\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.callbacks import ReduceLROnPlateau\r\n\r\n\r\nsns.set(style='white', context='notebook', palette='deep')\r\n\r\nWe will load the training and testing data set to train the machine and test whether the machine has learned as intended. Normally, data for machine learning are in forms of texts or numbers, but if our algorithm is complex enough, it can even process images or sounds.\r\nI have also plotted the distribution of all training digit images below (0 to 9). The training data set has 42,000 images and the testing data set has 28,000 images.\r\n\r\n\r\nShow code\r\n# Load the data\r\ntrain = pd.read_csv(\"train.csv\")\r\ntest = pd.read_csv(\"test.csv\")\r\n\r\nY_train = train[\"label\"]\r\n\r\n# Drop 'label' column\r\nX_train = train.drop(labels = [\"label\"],axis = 1) \r\n\r\ng = sns.countplot(Y_train)\r\nC:\\Users\\tarid\\ANACON~1\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\r\n  warnings.warn(\r\n\r\nShow code\r\nplt.show()\r\n\r\nShow code\r\nY_train.value_counts()\r\n\r\n#Check for missing value\r\n# Check the data\r\n1    4684\r\n7    4401\r\n3    4351\r\n9    4188\r\n2    4177\r\n6    4137\r\n0    4132\r\n4    4072\r\n8    4063\r\n5    3795\r\nName: label, dtype: int64\r\n\r\nShow code\r\nX_train.isnull().any().describe()\r\ncount       784\r\nunique        1\r\ntop       False\r\nfreq        784\r\ndtype: object\r\n\r\nShow code\r\ntest.isnull().any().describe()\r\n\r\n#Normalization\r\n# Normalize the data\r\ncount       784\r\nunique        1\r\ntop       False\r\nfreq        784\r\ndtype: object\r\n\r\nShow code\r\nX_train = X_train / 255.0\r\ntest = test / 255.0\r\n\r\n# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\r\nX_train = X_train.values.reshape(-1,28,28,1)\r\ntest = test.values.reshape(-1,28,28,1)\r\n\r\n# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\r\nY_train = to_categorical(Y_train, num_classes = 10)\r\n\r\nWe will also take a portion of training data set out for validation purpose as well. We will call it, the validation set. The validation data set is usually used to estimate how well the model has been trained. Basically, it is like a mock exam for the machine before the real test with the testing set, so that teachers will know which kid (or in our case, algorithm) is the brighest among the cohort.\r\nWe will also print a sample of images below. It is a simple number image.\r\n\r\n\r\nShow code\r\n# Set the random seed\r\nrandom_seed = 2\r\n\r\n# Split the train and the validation set for the fitting\r\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)\r\n\r\n# Some example\r\ng = plt.imshow(X_train[0][:,:,0])\r\nplt.show()\r\n\r\n\r\nConvolutional Neural Network (CNN)\r\nNow comes the main part. We will build a machine architecture of five layers convolutional neural network - a class of artificial neural network that takes after a human brain - to perform digit recognition.\r\nHere, we will add one-layer at a time starting from the input layer (remember the picture above). It is like peeling an onion, but instead of peeling, you are sticking each layer together to form one (I know it is not a good analogy).\r\nThen, we will add a convolutional (Conv2D) layer as the first layer with 32 filters to filter the number image. The second layer also has 32 filters, and the last two layers have 64 filters. Each filter transforms a part of the image for the machine to memorize.\r\n## WARNING: Things could get a bit nerdy technical here. Skip if you don’t understand\r\nATTN NERDS: The second important layer in CNN is the pooling (MaxPool2D) layer for the machine to look at the two neighboring pixels in a picture and picks the maximal value, so that it has more clue to classify which image contains which number. This technique can be used to reduce overfitting (where the machine learns too much of the training data and unable to use what it learns with the actual test) and computational cost.\r\nATTN NERDS: We will then use the relu activation function to add non-linearity to the network and use the Flatten layer is use to convert the final feature maps into a one single 1D vector. At the end, the two fully-connected (Dense) layers were added as artificial an neural networks (ANN) classifiers with the softmax activation function to compute the probability distribution of each class.\r\n\r\n\r\nShow code\r\n# my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\r\n\r\nmodel = Sequential()\r\n\r\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \r\n                 activation ='relu', input_shape = (28,28,1)))\r\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \r\n                 activation ='relu'))\r\nmodel.add(MaxPool2D(pool_size=(2,2)))\r\nmodel.add(Dropout(0.25))\r\n\r\n\r\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \r\n                 activation ='relu'))\r\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \r\n                 activation ='relu'))\r\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\r\nmodel.add(Dropout(0.25))\r\n\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(256, activation = \"relu\"))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(10, activation = \"softmax\"))\r\n\r\n# Summarize the model\r\nmodel.summary()\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\n Layer (type)                Output Shape              Param #   \r\n=================================================================\r\n conv2d (Conv2D)             (None, 28, 28, 32)        832       \r\n                                                                 \r\n conv2d_1 (Conv2D)           (None, 28, 28, 32)        25632     \r\n                                                                 \r\n max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \r\n )                                                               \r\n                                                                 \r\n dropout (Dropout)           (None, 14, 14, 32)        0         \r\n                                                                 \r\n conv2d_2 (Conv2D)           (None, 14, 14, 64)        18496     \r\n                                                                 \r\n conv2d_3 (Conv2D)           (None, 14, 14, 64)        36928     \r\n                                                                 \r\n max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \r\n 2D)                                                             \r\n                                                                 \r\n dropout_1 (Dropout)         (None, 7, 7, 64)          0         \r\n                                                                 \r\n flatten (Flatten)           (None, 3136)              0         \r\n                                                                 \r\n dense (Dense)               (None, 256)               803072    \r\n                                                                 \r\n dropout_2 (Dropout)         (None, 256)               0         \r\n                                                                 \r\n dense_1 (Dense)             (None, 10)                2570      \r\n                                                                 \r\n=================================================================\r\nTotal params: 887,530\r\nTrainable params: 887,530\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n\r\nAfter the model is constructed, we need to set up how we will optimize the algorithm as well as how to evaluate its result. We define the loss function to measure how poorly our model performs on images with known labels with the categorical_crossentropy method.\r\nHere, we are setting epochs to 2 to have the machine go through the entire data set exactly TWO times. It’s like you order your kids to do the homework, then erase them all and have them do it again entirely after they are done (I know, it’s not a good parenting). We also set batch_size to 86, which means 86 images will be present at a time to the machine. We cannot have the machine read 40,000 images at once, or you could if you have a god-level CPU (maybe NASA or MIT can provide you one).\r\nWe can have the machine go through the data more than two times (say, 30) for 99% accuracy, but that would take an hour or two to train it. For demonstration purposes, I will request for only two training rounds.\r\n\r\n\r\nShow code\r\n#%% Set the optimizer and annealer\r\n\r\n# Define the optimizer\r\noptimizer = RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\r\n\r\n# Compile the model\r\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\r\n\r\n# Set a learning rate annealer\r\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \r\n                                            patience=3, \r\n                                            verbose=1, \r\n                                            factor=0.5, \r\n                                            min_lr=0.00001)\r\n\r\nepochs = 2 # Turn epochs to 30 to get 0.9967 accuracy\r\nbatch_size = 86\r\n\r\nData augmentation\r\nTo avoid overfitting, which makes the machine learn more than it needs to, we will augment the image data by alter the training image a bit to reproduce the variations occurring when someone is writing a digit. You know, when you want your kids to learn well, you make the homework a little bit challenging.\r\nBy applying just a couple of these transformations to our training data, we create a robust model and got a higher accuracy in the result.\r\n\r\n\r\nShow code\r\n# With data augmentation to prevent overfitting (accuracy 0.99286)\r\n\r\ndatagen = ImageDataGenerator(\r\n        featurewise_center=False,  # set input mean to 0 over the dataset\r\n        samplewise_center=False,  # set each sample mean to 0\r\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\r\n        samplewise_std_normalization=False,  # divide each input by its std\r\n        zca_whitening=False,  # apply ZCA whitening\r\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\r\n        zoom_range = 0.1, # Randomly zoom image \r\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\r\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\r\n        horizontal_flip=False,  # randomly flip images\r\n        vertical_flip=False)  # randomly flip images\r\n\r\n\r\ndatagen.fit(X_train)\r\n\r\n# Fit the model\r\nhistory = model.fit(datagen.flow(X_train,Y_train, batch_size=batch_size),\r\n                              epochs = epochs, validation_data = (X_val,Y_val),\r\n                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size\r\n                              , callbacks=[learning_rate_reduction])\r\nEpoch 1/2\r\n439/439 - 127s - loss: 0.4209 - accuracy: 0.8659 - val_loss: 0.0737 - val_accuracy: 0.9774 - lr: 0.0010 - 127s/epoch - 290ms/step\r\nEpoch 2/2\r\n439/439 - 130s - loss: 0.1301 - accuracy: 0.9622 - val_loss: 0.0418 - val_accuracy: 0.9862 - lr: 0.0010 - 130s/epoch - 297ms/step\r\n\r\nModel evaluation\r\nBelow are plots that evaluate accuracy of the model. With two epochs, the model achieved 95% accuracy when reading the image data with 0.04 error rate. See that the loss (or error) is going down while the accuracy is going up. With more training round (epochs), the accuracy would go even higher.\r\n\r\n\r\nShow code\r\n# Plot the loss and accuracy curves for training and validation \r\nfig, ax = plt.subplots(2,1)\r\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\r\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\r\nlegend = ax[0].legend(loc='best', shadow=True)\r\n\r\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\r\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\r\nlegend = ax[1].legend(loc='best', shadow=True)\r\nplt.show()\r\n\r\n\r\nThe confusion plot below shows that the model did pretty well, with some errors when the machine classified “4” as “9”.\r\n\r\n\r\nShow code\r\n#%% Confusion matrix\r\n\r\ndef plot_confusion_matrix(cm, classes,\r\n                          normalize=False,\r\n                          title='Confusion matrix',\r\n                          cmap=plt.cm.Blues):\r\n    \"\"\"\r\n    This function prints and plots the confusion matrix.\r\n    Normalization can be applied by setting `normalize=True`.\r\n    \"\"\"\r\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\r\n    plt.title(title)\r\n    plt.colorbar()\r\n    tick_marks = np.arange(len(classes))\r\n    plt.xticks(tick_marks, classes, rotation=45)\r\n    plt.yticks(tick_marks, classes)\r\n\r\n    if normalize:\r\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\r\n\r\n    thresh = cm.max() / 2.\r\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\r\n        plt.text(j, i, cm[i, j],\r\n                 horizontalalignment=\"center\",\r\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\r\n\r\n    plt.tight_layout()\r\n    plt.ylabel('True label')\r\n    plt.xlabel('Predicted label')\r\n\r\n# Predict the values from the validation dataset\r\nY_pred = model.predict(X_val)\r\n# Convert predictions classes to one hot vectors \r\nY_pred_classes = np.argmax(Y_pred,axis = 1) \r\n# Convert validation observations to one hot vectors\r\nY_true = np.argmax(Y_val,axis = 1) \r\n# compute the confusion matrix\r\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \r\n# plot the confusion matrix\r\nplot_confusion_matrix(confusion_mtx, classes = range(10))\r\nplt.show()\r\n  \r\n\r\n\r\nWe will also print out six most challenging errors for the machine as well.\r\n\r\n\r\nShow code\r\n#%% Check error result\r\n\r\n# Display some error results \r\n\r\n# Errors are difference between predicted labels and true labels\r\nerrors = (Y_pred_classes - Y_true != 0)\r\n\r\nY_pred_classes_errors = Y_pred_classes[errors]\r\nY_pred_errors = Y_pred[errors]\r\nY_true_errors = Y_true[errors]\r\nX_val_errors = X_val[errors]\r\n\r\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\r\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\r\n    n = 0\r\n    nrows = 2\r\n    ncols = 3\r\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True, figsize=(10,10))\r\n    for row in range(nrows):\r\n        for col in range(ncols):\r\n            error = errors_index[n]\r\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\r\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\r\n            n += 1\r\n\r\n# Probabilities of the wrong predicted numbers\r\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\r\n\r\n# Predicted probabilities of the true values in the error set\r\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\r\n\r\n# Difference between the probability of the predicted label and the true label\r\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\r\n\r\n# Sorted list of the delta prob errors\r\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\r\n\r\n# Top 6 errors \r\nmost_important_errors = sorted_dela_errors[-6:]\r\n\r\n# Show the top 6 errors\r\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)\r\nplt.show()\r\n\r\n\r\nAs you can see, the numbers are actually confusing and the error is understandable for some cases. Sometimes, people actually made this kind of error when they write their numbers hastily or with bad handwriting. Even human could get this wrong at some point.\r\nFinal remarks and Conclusions\r\nDeep learning is a very powerful approach to develop advanced tools that go beyond the capability of traditional machine learning tasks in analyzing unstructured data such as images, videos, and sounds - including musics. With robust tools developed based on this algorithm, we could significantly reduce workload for human labor.\r\nThe thing is, there are also numerous down sides of this algorithm as well. We would need a lot of data to develop an accurate model, and I mean a lot as in ten thousand or more. Even with that much data, the accuracy could drop as well as we, humans, change our way of living (like how we changed from watching cable TV to Netflix); then, we would need more data to train or even a new machine architecture to predict new features that we are interested in.\r\nThe explainability (the ability to be explained) of the model is clear as mud (TL: It is NOT clear). I mean, yes, we know that the machine can learn and classify number images, but we don’t actually know step-by-step procedures of how it works on the inside. For normal machine learning tasks such as logistic regression, there is a mathematics formula or two for those who are curious on how what makes the machine ticks, but that doesn’t seem to be the case for deep learning. See explanable AI for how experts remedy this problem.\r\nThe explainability drawback is especially critical for us academics. While we seek results, we also care to explain how we obtained that results in the process. That is why we tend to receive a lot of pushbacks when we use complex models that we cannot explain; for this reason, simpler model is preferred if it can accomplish the task with similar, if not the same, performance. Explanability is usually (a little bit) preferred over accuracy in our case.\r\nNevertheless, it is also important to know how the algorithm works (roughly), so that we are aware of what makes Facebook able to automatically tag our friends or show you advertisements about the thing you talked about, what makes Shazam able to identify musics and movies, and so we can potentially find the right tool should we need to accomplish similar tasks. That is why I wrote this entry to share what I know about this topic. Thank you very much for reading this. Have a good one!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-07-image-recognition-with-artificial-neural-networks/deepnn.jpg",
    "last_modified": "2021-12-07T22:43:28-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data/",
    "title": "Anomaly Detection with New York City taxi data",
    "description": "In this entry, I will be conducting anomaly detection to identify points of anomaly in the taxi passengers data in New York City from July 2014 to January 2015 at half-hourly intervals.\n \n (4 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2021-12-04",
    "categories": [
      "Unsupervised Machine Learning",
      "R"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction to Anomaly Detection\r\nDataset\r\nInvestigating the Moving Average\r\nTime Series Decomposition with Anomalies\r\nAnomaly Detection\r\nApplication of Anomaly Detection\r\n\r\nIntroduction to Anomaly Detection\r\nAnomaly detection is an unsupervised machine learning technique that identifies outliers - a data point that differs from other majority data points - and their patterns in the data set. Such outliers could be a super hot day (as in 50 degree celcius) in the middle of winter with the average temperature of -10 degree Celcius. This technique can be used to detect outliers to remove in data preprocessing or even to identify potential frauds or failures in health system monitoring.\r\nDataset\r\nFirst off, we will load the dataset and the essential libraries as usual and read the first five rows to see what is going on. The output below shows that we have timestamp data and the number of passengers as indicated by the value column.\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse) #for data manipulation\r\nlibrary(lubridate) #for date/time data management\r\nlibrary(zoo) # moving averages  \r\nlibrary(anomalize) #for anomaly detection\r\n\r\ndata <- read_csv(\"nyc_taxi.csv\")\r\nhead(data)\r\n\r\n\r\n# A tibble: 6 x 2\r\n  timestamp           value\r\n  <dttm>              <dbl>\r\n1 2014-07-01 00:00:00 10844\r\n2 2014-07-01 00:30:00  8127\r\n3 2014-07-01 01:00:00  6210\r\n4 2014-07-01 01:30:00  4656\r\n5 2014-07-01 02:00:00  3820\r\n6 2014-07-01 02:30:00  2873\r\n\r\nLet us visualize the data with ggplot2 to see what the data set is like before we proceed further.\r\n\r\n\r\nShow code\r\n\r\nggplot(data, aes(x = timestamp, y = value)) + \r\n  geom_point(shape = 1, alpha = 0.5) +\r\n  labs(x = \"Time\", y = \"Count\") +\r\n  labs(alpha = \"\", colour=\"Legend\")\r\n\r\n\r\n\r\n\r\nTime series data is usually represented in patterns across time, but fluctuations could occur by the influence of events that affect the variable of interest such as holidays or natural phenomenon.\r\nInvestigating the Moving Average\r\nIn simple word, a moving average is an indicator that shows the average value of the variable of interest over a period (i.e. 10 days, 50 days, 200 days, etc) and is usually plotted across a large time interval as in months or years.\r\nWe will create two moving averages, one with 48 value in each group, and another one with 336 value in each group.\r\n\r\n\r\nShow code\r\n\r\ndata <- data %>% \r\n  dplyr::mutate (MA48 = zoo::rollmean(value, k = 48, fill = NA),\r\n                MA336 = zoo::rollmean(value, k = 336, fill = NA)) %>%\r\n  dplyr::ungroup()\r\n\r\n#Plot the moving average line chart\r\n\r\ndata %>%\r\n  gather(metric, value, MA48:MA336) %>%\r\n  ggplot(aes(timestamp, value, color = metric)) +\r\n  geom_line() +\r\n  ggtitle(\"Rolling average of NYC taxi passenger count\")+\r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nWe can see that the anomaly is more apparent when we divide the data into groups with 48 data points.\r\nTime Series Decomposition with Anomalies\r\nBefore we dive into anomaly detection, we should conduct time series decomposition where time series data is decomposed into Seasonal, Trend and remainder components with time_decompose().\r\nOnce the components are decomposed, anomalize() can detect and flag anomalies in the decomposed data of the reminder component which then could be visualized with plot_anomaly_decomposition()\r\n\r\n\r\nShow code\r\n\r\ndata %>% \r\n  time_decompose(value, method = \"stl\", frequency = \"auto\", trend = \"auto\") %>%\r\n  anomalize(remainder, method = \"gesd\", alpha = 0.05, max_anoms = 0.2) %>%\r\n  plot_anomaly_decomposition()\r\n\r\n\r\n\r\n\r\nFor each element of the graph, observed represents the actual value, season represents the seasonal or cyclic trend, trend is a long term trend throughout the whole time period, and remainder is the observed data minus by results from season and trend.\r\nAnomaly Detection\r\nAnomaly Detection and Plotting the detected anomalies are similar to the time series decomposition above, but we do not have to adjust parameters such as frequency, trend, or method like we did above. Basically, it is easier than time series decomposition as we can use it out of the box.\r\n\r\n\r\nShow code\r\n\r\ndata %>% \r\n  time_decompose(value) %>%\r\n  anomalize(remainder) %>%\r\n  time_recompose() %>%\r\n  plot_anomalies(time_recomposed = TRUE, ncol = 3, alpha_dots = 0.5)\r\n\r\n\r\n\r\n\r\nNotice that the model has picked several anomalies around January, which contains the New Year festival and the occurrence of a blizzard storm at the New York City. Points of anomaly usually occurs with events of some sort, so we might be able to identify sources of anomaly with further information search using the date of anomaly as the lead.\r\nWe can also extract the actual anomalous data point via the following codes:\r\n\r\n\r\nShow code\r\n\r\ndata_anomalous <- data %>% \r\n  time_decompose(value) %>%\r\n  anomalize(remainder) %>%\r\n  time_recompose() %>%\r\n  filter(anomaly == 'Yes') \r\n\r\nhead(data_anomalous)\r\n\r\n\r\n# A time tibble: 6 x 10\r\n# Index: timestamp\r\n  timestamp           observed season  trend remainder remainder_l1\r\n  <dttm>                 <dbl>  <dbl>  <dbl>     <dbl>        <dbl>\r\n1 2014-07-04 07:30:00     4926  2817. 14875.   -12767.       -9534.\r\n2 2014-07-04 08:00:00     5165  3903. 14875.   -13612.       -9534.\r\n3 2014-07-04 08:30:00     5776  4346. 14874.   -13445.       -9534.\r\n4 2014-07-04 09:00:00     7338  3304. 14874.   -10840.       -9534.\r\n5 2014-07-05 07:00:00     3658  -735. 14848.   -10454.       -9534.\r\n6 2014-07-05 07:30:00     4345  2817. 14847.   -13319.       -9534.\r\n# ... with 4 more variables: remainder_l2 <dbl>, anomaly <chr>,\r\n#   recomposed_l1 <dbl>, recomposed_l2 <dbl>\r\n\r\nI know that the amount of detected anomaly is a lot. We can adjust parameter of the detector to make the algorithm less sensitive and detect stronger outliers by decreasing alpha and max_anoms to control for sensitivity and the maximum percentage of data that can be an anomaly respectively. max_anoms = 0.20 means the algorithm will flag anomalies up to 20% of the whole data set.\r\nThe example above used default parameter, which is alpha = 0.05 and max_anoms = 0.20. Let us try tuning down the sensitivity a little bit, say, alpha = 0.025 and max_anoms = 0.05 to identify only extreme outliers.\r\n\r\n\r\nShow code\r\n\r\ndata %>% \r\n  time_decompose(value) %>%\r\n  anomalize(remainder, alpha = 0.025, max_anoms = 0.05) %>%\r\n  time_recompose() %>%\r\n  plot_anomalies(time_recomposed = TRUE, ncol = 3, alpha_dots = 0.5)\r\n\r\n\r\n\r\n\r\nEven with parameter tuning, the algorithm still detects anomalies at the end of January. Something must be going on there. Let us extract the actual anomalous data point of the tuned algorithm.\r\n\r\n\r\nShow code\r\n\r\ndata_anomalous_tuned <- data %>% \r\n  time_decompose(value) %>%\r\n  anomalize(remainder, alpha = 0.025, max_anoms = 0.05) %>%\r\n  time_recompose() %>%\r\n  filter(anomaly == 'Yes') \r\n\r\nhead(data_anomalous_tuned)\r\n\r\n\r\n# A time tibble: 6 x 10\r\n# Index: timestamp\r\n  timestamp           observed  season  trend remainder remainder_l1\r\n  <dttm>                 <dbl>   <dbl>  <dbl>     <dbl>        <dbl>\r\n1 2014-09-21 01:00:00    25371  -8034. 14994.    18411.      -17691.\r\n2 2014-10-19 01:00:00    25610  -8034. 15905.    17739.      -17691.\r\n3 2014-11-01 01:30:00    23736  -9536. 15566.    17707.      -17691.\r\n4 2014-11-01 02:00:00    23245 -10442. 15565.    18121.      -17691.\r\n5 2014-11-02 01:00:00    39197  -8034. 15597.    31634.      -17691.\r\n6 2014-11-02 01:30:00    35212  -9536. 15598.    29151.      -17691.\r\n# ... with 4 more variables: remainder_l2 <dbl>, anomaly <chr>,\r\n#   recomposed_l1 <dbl>, recomposed_l2 <dbl>\r\n\r\nApplication of Anomaly Detection\r\nAnomaly detection has wide applications across industries. Data scientists can identify anomalies in the amount of deposits that go outside the usual pattern, thus flagging it for potential fraud or system error for a deeper investigation. The technique can also be used to identify anomalous data in student pattern from their e-learning platform, which could lead us to topics that can be further explored such as a fluctuation in the duration of content access. A certain content could be exceedingly difficult that students have to spend more time than usual studying it.\r\nAside from using R, anomaly can also be done in Python as well. If you are interested, you can check out my Jupyter notebook here for anomaly detection with Pycaret, a low-code machine learning library in Python that covers end-to-end machine learning pipeline from preprocessing to model deploying. Thank you very much for reading!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data/anomaly-detection-with-new-york-city-taxi-data_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2021-12-04T21:35:40-07:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data/",
    "title": "Crime mapping in San Francisco with police data",
    "description": "In this entry, we will explore San Francisco crime data from 2016 to 2018 to understand the relationship between civilian-reported incidents of crime and police-reported incidents of crime. Along the way we will use table intersection methods to subset our data, aggregation methods to calculate important statistics, and simple visualizations to understand crime trends.  \n\n(7 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2021-11-30",
    "categories": [
      "Data Visualization",
      "R",
      "Quantitative research"
    ],
    "contents": "\r\n\r\nContents\r\nIntroducing the data\r\nData exploration\r\nInspect frequency trends\r\nCorrelation between trends\r\nFiltering joins\r\nCrime categories\r\nGrand theft auto\r\nDensity map\r\nConclusion and key takeaways\r\n\r\nIntroducing the data\r\nThe data was provided by SF open data, a governmental agency of the City and County of San Francisco that publishes data to improve decision-making and service delivery. Basically, the governing body of San Francisco made their data publicly available for data scientists to play around for fun and practice their data work skill.\r\nFor this project, we will be using R to read and manipulate our data. Findings will be communicated via plots and graphs. First of all, let us read the data set and load essential libraries.\r\n\r\n\r\nShow code\r\n\r\n# Load required packages\r\nlibrary(tidyverse) #for data manipulation\r\nlibrary(lubridate) #for date/time data management\r\nlibrary(maptools) #to load map data\r\nlibrary(ggmap) #to work with map data\r\n\r\n# Read datasets and assign to variables\r\nincidents <- read_csv(\"police-department-incidents.csv\")\r\ncalls <- read_csv(\"police-department-calls-for-service.csv\")\r\n\r\n\r\n\r\nData exploration\r\nFirst things first: we need to wrap our heads around the data in order to understand what we have. We will use glimpse() to explore the variable of both civilian-reported an police-reported crime\r\nFor police-reported crime, we have incident number, incident category, date/time information, location of the crime, crime description, and the means in which the incident was resolved.\r\nFor civilian-reported crime, we have crime ID, date/time information, location, and crime description.\r\n\r\n\r\nShow code\r\n\r\n# Glimpse the structure of both datasets\r\nglimpse(incidents)\r\n\r\n\r\nRows: 84,000\r\nColumns: 13\r\n$ IncidntNum <dbl> 176122807, 160569314, 160362475, 160435298, 90543~\r\n$ Category   <chr> \"LARCENY/THEFT\", \"ASSAULT\", \"ROBBERY\", \"KIDNAPPIN~\r\n$ Descript   <chr> \"GRAND THEFT FROM UNLOCKED AUTO\", \"BATTERY\", \"ROB~\r\n$ DayOfWeek  <chr> \"Saturday\", \"Thursday\", \"Tuesday\", \"Friday\", \"Tue~\r\n$ Date       <dttm> 2017-05-13, 2016-07-14, 2016-05-03, 2016-05-27, ~\r\n$ Time       <time> 10:20:00, 16:00:00, 14:19:00, 23:57:00, 07:40:00~\r\n$ PdDistrict <chr> \"SOUTHERN\", \"MISSION\", \"NORTHERN\", \"SOUTHERN\", \"T~\r\n$ Resolution <chr> \"NONE\", \"NONE\", \"ARREST, BOOKED\", \"ARREST, BOOKED~\r\n$ Address    <chr> \"800 Block of BRYANT ST\", \"MISSION ST / CESAR CHA~\r\n$ X          <dbl> -122.4034, -122.4182, -122.4299, -122.4050, -122.~\r\n$ Y          <dbl> 37.77542, 37.74817, 37.77744, 37.78512, 37.71912,~\r\n$ Location   <chr> \"{'latitude': '37.775420706711', 'human_address':~\r\n$ PdId       <dbl> 1.761228e+13, 1.605693e+13, 1.603625e+13, 1.60435~\r\n\r\nShow code\r\n\r\nglimpse(calls)\r\n\r\n\r\nRows: 100,000\r\nColumns: 14\r\n$ `Crime Id`        <dbl> 163003307, 180870423, 173510362, 163272811~\r\n$ Descript          <chr> \"Bicyclist\", \"586\", \"Suspicious Person\", \"~\r\n$ `Report Date`     <dttm> 2016-10-26, 2018-03-28, 2017-12-17, 2016-~\r\n$ Date              <dttm> 2016-10-26, 2018-03-28, 2017-12-17, 2016-~\r\n$ `Offense Date`    <dttm> 2016-10-26, 2018-03-28, 2017-12-17, 2016-~\r\n$ `Call Time`       <time> 17:47:00, 05:49:00, 03:00:00, 17:39:00, 0~\r\n$ `Call Date Time`  <dttm> 2016-10-26 17:47:00, 2018-03-28 05:49:00,~\r\n$ Disposition       <chr> \"GOA\", \"HAN\", \"ADV\", \"NOM\", \"GOA\", \"ADV\", ~\r\n$ Address           <chr> \"The Embarcadero Nor/kearny St\", \"Ingalls ~\r\n$ City              <chr> \"San Francisco\", \"San Francisco\", \"San Fra~\r\n$ State             <chr> \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", ~\r\n$ `Agency Id`       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~\r\n$ `Address Type`    <chr> \"Intersection\", \"Intersection\", \"Intersect~\r\n$ `Common Location` <chr> NA, NA, NA, NA, NA, NA, \"Midori Hotel Sro ~\r\n\r\nWe have 84,000 cases of police-reported crime and 100,000 cases of civilian reported crime.We will then aggregate the number of incidents by date for both civilian-reported an police-reported crime for comparison.\r\n\r\n\r\nShow code\r\n\r\n# Aggregate the number of reported incidents by date\r\ndaily_incidents <- incidents %>% \r\n    count(Date, sort = TRUE) %>% \r\n    rename(n_incidents = n)\r\n\r\n# Aggregate the number of calls for police service by date\r\ndaily_calls <- calls %>% \r\n    count(Date, sort = TRUE) %>% \r\n    rename(n_calls = n)\r\n\r\n# Join data frames to create a new \"mutated\" set of information\r\nshared_dates <- inner_join(daily_incidents, daily_calls, by = c(\"Date\" = \"Date\"))\r\n\r\n# Take a glimpse of this new data frame\r\nhead(shared_dates, 10)\r\n\r\n\r\n# A tibble: 10 x 3\r\n   Date                n_incidents n_calls\r\n   <dttm>                    <int>   <int>\r\n 1 2017-03-01 00:00:00         124     133\r\n 2 2017-09-01 00:00:00         124     138\r\n 3 2016-04-01 00:00:00         120     124\r\n 4 2017-03-17 00:00:00         118     129\r\n 5 2016-11-25 00:00:00         117     115\r\n 6 2017-06-25 00:00:00         114     120\r\n 7 2017-05-25 00:00:00         111     127\r\n 8 2017-08-28 00:00:00         110     106\r\n 9 2017-09-28 00:00:00         110     124\r\n10 2017-11-27 00:00:00         110     108\r\n\r\nWe can see that the number of civilian-reported incident is generally lower than the number of police-reported incidence. Sometime, people dialed 911 for police to check on suspicious activities, which might be or might not be an actual crime.\r\nInspect frequency trends\r\nLet us visualize the number of a plot to compare both incidents at a big picture.\r\n\r\n\r\nShow code\r\n\r\n# Gather into long format using the \"Date\" column to define observations\r\nplot_shared_dates <- shared_dates %>%\r\n  gather(key = report, value = count, -Date)\r\n\r\n# Plot points and regression trend lines\r\nggplot(plot_shared_dates, aes(x = Date, y = count, color = report)) +\r\n  geom_point() +\r\n  geom_smooth(method = \"lm\", formula = y ~ x)\r\n\r\n\r\n\r\n\r\nThe plot above visualizes he frequency of calls and incidents across time for us to see if there is any relationships between both civilian-reported and police-reported incident. We also have regression lines that cut across the plot to determine the trend in which the data is going.\r\nCorrelation between trends\r\nTo make the relationship more understandable, we could calculate a correlation value between the number of both incident types to see how they get along with each other.\r\nA quick refresh on correlation, this value ranges from -1 to 1 as a representation of relationships between two sets of data. The closest the value is to 1 or -1, the stronger the relationship can be. Positive value indicates that the two variables go the same way while negative value indicates otherwise (direction of the two variables are opposite; that is, X increases while Y decreases).\r\n\r\n\r\nShow code\r\n\r\n# Calculate correlation coefficient between daily frequencies\r\ndaily_cor <- cor(shared_dates$n_incidents, shared_dates$n_calls)\r\ndaily_cor\r\n\r\n\r\n[1] 0.1469688\r\n\r\nThe correlation is not that much. Just 0.146, but what if we look at a broader perspective by summarising the data into monthly counts and calculating a correlation coefficient.\r\n\r\n\r\nShow code\r\n\r\n# Summarise frequencies by month\r\ncorrelation_df <- shared_dates %>% \r\n  mutate(month = month(Date)) %>%\r\n  group_by(month) %>% \r\n  summarize(n_incidents = sum(n_incidents),\r\n            n_calls = sum(n_calls))\r\n\r\n# Calculate correlation coefficient between monthly frequencies\r\nmonthly_cor <- cor(correlation_df$n_incidents, correlation_df$n_calls)\r\nmonthly_cor\r\n\r\n\r\n[1] 0.970683\r\n\r\nThe correlation is .97! I wonder why this is the case. Maybe breaking the data into days format would make it too detailed as correlations might be more apparent if the data is separated into clusters (month, in this case). But I won’t look too much into it as this is not the focus on this entry.\r\nFiltering joins\r\nATTN Nerds: When working with relational datasets, there are situations in which it is helpful to subset information based on another set of values. Filtering joins are a complementary type of join which allows us to keep all specific cases within a data frame while preserving the structure of the data frame itself.\r\nIt will be helpful to have all the information from each police reported incident and each civilian call on their shared dates so we can calculate similar statistics from each dataset and compare results. This step will prepare us to make the pattern more apparent with data visualization.\r\n\r\n\r\nShow code\r\n\r\n# Filter calls to police by shared_dates\r\ncalls_shared_dates <- calls %>%\r\n  semi_join(shared_dates, by = c(\"Date\" = \"Date\"))\r\n\r\n# Filter recorded incidents by shared_dates\r\nincidents_shared_dates <- incidents %>% \r\n  semi_join(shared_dates, by = c(\"Date\" = \"Date\"))\r\n\r\nhead(calls_shared_dates)\r\n\r\n\r\n# A tibble: 6 x 14\r\n  `Crime Id` Descript          `Report Date`       Date               \r\n       <dbl> <chr>             <dttm>              <dttm>             \r\n1  163003307 Bicyclist         2016-10-26 00:00:00 2016-10-26 00:00:00\r\n2  180870423 586               2018-03-28 00:00:00 2018-03-28 00:00:00\r\n3  173510362 Suspicious Person 2017-12-17 00:00:00 2017-12-17 00:00:00\r\n4  163272811 911 Drop          2016-11-22 00:00:00 2016-11-22 00:00:00\r\n5  172811002 Drugs             2017-10-08 00:00:00 2017-10-08 00:00:00\r\n6  170902193 At Risk           2017-03-31 00:00:00 2017-03-31 00:00:00\r\n# ... with 10 more variables: Offense Date <dttm>, Call Time <time>,\r\n#   Call Date Time <dttm>, Disposition <chr>, Address <chr>,\r\n#   City <chr>, State <chr>, Agency Id <dbl>, Address Type <chr>,\r\n#   Common Location <chr>\r\n\r\nShow code\r\n\r\nhead(incidents_shared_dates)\r\n\r\n\r\n# A tibble: 6 x 13\r\n  IncidntNum Category  Descript    DayOfWeek Date                Time \r\n       <dbl> <chr>     <chr>       <chr>     <dttm>              <tim>\r\n1  176122807 LARCENY/~ GRAND THEF~ Saturday  2017-05-13 00:00:00 10:20\r\n2  160569314 ASSAULT   BATTERY     Thursday  2016-07-14 00:00:00 16:00\r\n3  160362475 ROBBERY   ROBBERY, B~ Tuesday   2016-05-03 00:00:00 14:19\r\n4  160435298 KIDNAPPI~ KIDNAPPING~ Friday    2016-05-27 00:00:00 23:57\r\n5  180018692 VEHICLE ~ STOLEN MOT~ Sunday    2018-01-07 00:00:00 18:00\r\n6  176045481 LARCENY/~ GRAND THEF~ Wednesday 2017-02-15 00:00:00 20:00\r\n# ... with 7 more variables: PdDistrict <chr>, Resolution <chr>,\r\n#   Address <chr>, X <dbl>, Y <dbl>, Location <chr>, PdId <dbl>\r\n\r\nThe above tables are some examples of civilian-reported and police-reported crimes when grouped together by dates. It will all make sense when we plot them all into graphs below.\r\nCrime categories\r\nNow we need to see what the data look like after joining the datasets. Previously, we have a scatter plot to see if there was a trend in the frequency of calls and the frequency of reported incidents over time. Scatterplots are a great tool to look at overall trends of continuous data. However, to see trends in categorical data, we need to visualize the ranked order of the variables to understand their levels of importance.\r\n\r\n\r\nShow code\r\n\r\n# Create a bar chart of the number of calls for each crime\r\nplot_calls_freq <- calls_shared_dates %>% \r\n  count(Descript) %>% \r\n  top_n(15, n) %>% \r\n  ggplot(aes(x = reorder(Descript, n), y = n, fill = Descript)) +\r\n  geom_bar(stat = 'identity') +\r\n  ylab(\"Count\") +\r\n  xlab(\"Crime Description\") +\r\n  ggtitle(\"Civilian-Reported Crimes\") +\r\n  coord_flip()+\r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n# Output the plots\r\nplot_calls_freq\r\n\r\n\r\n\r\n\r\nFrom the plot above, passing call was ranked as the most frequent cause for civilian-reported crime, following by traffic stop and homeless complaint.\r\n\r\n\r\nShow code\r\n\r\n# Create a bar chart of the number of reported incidents for each crime\r\nplot_incidents_freq <- incidents_shared_dates %>% \r\n  count(Descript) %>% \r\n  top_n(15, n)  %>% \r\n  ggplot(aes(x = reorder(Descript, n), y = n, fill = Descript)) +\r\n  geom_bar(stat = 'identity') +\r\n  ylab(\"Count\") +\r\n  xlab(\"Crime Description\") +\r\n  ggtitle(\"Police-Reported Crimes\") +\r\n  coord_flip()+\r\n  theme(plot.title = element_text(hjust = 0.5))+\r\n  theme(\r\n    legend.position = c(.95, .60),\r\n    legend.justification = c(\"right\", \"top\"),\r\n    legend.box.just = \"right\",\r\n    legend.margin = margin(6, 6, 6, 6)\r\n    )\r\n\r\nplot_incidents_freq\r\n\r\n\r\n\r\n\r\nThe number of grand theft from locked auto is off-the-chart! This means there were several police-reported incident where unsupervised vehicles are broken into. What happened here?\r\nGrand theft auto\r\nMy friends in the States told me that never leave your belongings in your parked car, especially if the number of grand theft auto crime is high in the area. However, there may be good citizens iut there trying to prevent crime. The 12th most civilian reported crime is “Auto Boost / Strip”, which could be the case where they are trying to prevent the grand theft auto crime. Yet, this is probably only the case where the location of a called-in-crime is similar to the location of crime incidence. Let’s check to see if the locations of the most frequent civilian reported crime and police reported crime are similar.\r\n\r\n\r\nShow code\r\n\r\n# Arrange the top 10 locations of called in crimes in a new variable\r\nlocation_calls <- calls_shared_dates %>%\r\n  filter(Descript == \"Auto Boost / Strip\") %>% \r\n  count(Address) %>% \r\n  arrange(desc(n))%>% \r\n  top_n(10, n)\r\n\r\n# Arrange the top 10 locations of reported incidents in a new variable\r\nlocation_incidents <- incidents_shared_dates %>%\r\n  filter(Descript == \"GRAND THEFT FROM LOCKED AUTO\") %>% \r\n  count(Address) %>% \r\n  arrange(desc(n))%>% \r\n  top_n(10, n)\r\n\r\n# Output the top locations of each dataset for comparison\r\nlocation_calls\r\n\r\n\r\n# A tibble: 11 x 2\r\n   Address                                  n\r\n   <chr>                                <int>\r\n 1 1100 Block Of Point Lobos Av            21\r\n 2 3600 Block Of Lyon St                   20\r\n 3 100 Block Of Christmas Tree Point Rd    18\r\n 4 1300 Block Of Webster St                12\r\n 5 500 Block Of 6th Av                     12\r\n 6 800 Block Of Vallejo St                 10\r\n 7 1000 Block Of Great Hy                   9\r\n 8 100 Block Of Hagiwara Tea Garden Dr      7\r\n 9 1100 Block Of Fillmore St                7\r\n10 3300 Block Of 20th Av                    7\r\n11 800 Block Of Mission St                  7\r\n\r\nShow code\r\n\r\nlocation_incidents\r\n\r\n\r\n# A tibble: 10 x 2\r\n   Address                          n\r\n   <chr>                        <int>\r\n 1 800 Block of BRYANT ST         441\r\n 2 500 Block of JOHNFKENNEDY DR    89\r\n 3 1000 Block of POINTLOBOS AV     84\r\n 4 800 Block of MISSION ST         61\r\n 5 2600 Block of GEARY BL          38\r\n 6 3600 Block of LYON ST           36\r\n 7 1300 Block of WEBSTER ST        35\r\n 8 1100 Block of FILLMORE ST       34\r\n 9 22ND ST / ILLINOIS ST           33\r\n10 400 Block of 6TH AV             30\r\n\r\nDensity map\r\nThe police-reported dataset shares locations where auto crimes occur and are reported most frequently - such as on Point Lobos Avenue, Lyon Street, and Mission Street. We will plot the frequency of auto crime occurrence on the map of San Francisco.\r\n\r\n\r\nShow code\r\n\r\n# Read it into R as a spatial polygons data frame & plot\r\nneighb <- readShapePoly(\"SF_neighborhoods\")\r\n\r\n# Define the bounding box\r\nbbox <- neighb@bbox\r\n \r\n# Manipulate these values slightly so that we get some padding on our basemap between the edge of the data and the edge of the map\r\nsf_bbox <- c(left = bbox[1, 1] - .01, bottom = bbox[2, 1] - .005, \r\n             right = bbox[1, 2] + .01, top = bbox[2, 2] + .005)\r\n\r\n# Download the basemap\r\nbasemap <- get_stamenmap(\r\n  bbox = sf_bbox,\r\n  zoom = 13,\r\n  maptype = \"toner-lite\")\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# Filter grand theft auto incidents\r\nauto_incidents <- incidents_shared_dates %>%\r\n  filter(Descript == \"GRAND THEFT FROM LOCKED AUTO\")\r\n\r\n# Overlay a density plot of auto incidents on the map\r\nggmap(basemap) +\r\n  stat_density_2d(\r\n    aes(x = X, y = Y, fill = ..level..), alpha = 0.15,\r\n    size = 0.01, bins = 30, data = auto_incidents,\r\n    geom = \"polygon\") +\r\n  labs(title=\"The occurence of car theft in San Francisco\") +\r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nFrom the map above, the blue spot means there was a higher rate of grand theft auto crime occurring in that area. Maybe the police could consider patrolling that area more frequently.\r\nConclusion and key takeaways\r\nFrom the data, we can see that the grand theft from locked vehicles crime usually occurs at some specific areas in the city. We could further investigate that area specifically to address the cause or potentially lessen the problem. It is interesting in how open data can benefit both the learning of independent data scientists (those who work with data for fun) and the government at the same time.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data/crime-mapping-in-san-francisco-with-police-data_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-11-30T18:19:18-07:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1920
  },
  {
    "path": "posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds/",
    "title": "Exploring COVID-19 data from twitter with topic modeling",
    "description": "This entry focuses on the exploration of twitter data from Alberta's Chief Medical Officer of Health via word cloud and topic modeling to gain insights in characteristics of public health messaging during the COVID-19 pandemic.  \n\n(7 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2021-11-23",
    "categories": [
      "Python",
      "Natural Language Processing",
      "Unsupervised Machine Learning",
      "COVID-19"
    ],
    "contents": "\r\n\r\nContents\r\nCOVID-19 situation\r\nin Alberta, Canada\r\nText mining and\r\nword cloud fundamentals\r\nLet’s clean the text\r\nfirst\r\nSo this is what’s\r\nhappening over time\r\nLet’s see the\r\nbig picture with word cloud\r\nCommon\r\nword bar plot and text preprocessing for topic modeling\r\nFinally,\r\nlet’s see potential topics from Dr. Hinshaw’s tweet\r\nWrapping up here.\r\nWhat can we conclude?\r\n\r\nCOVID-19 situation in\r\nAlberta, Canada\r\nThe province of Alberta, Canada, has suffered from the COVID-19\r\npandemic like all other places. Alberta has gone through cycles of\r\nreopening and returning to provincial lock down since April 2020. The\r\nprovince, however, has lifted almost all restrictions and enacted its\r\nreopening plan on the recent Canada\r\nday when 70% of Alberta population has received at least one dose of\r\napproved\r\nCOVID-19 vaccination.\r\nThe navigation of the province through this pandemic was led by\r\nthe Alberta’s Chief Medical Officer of Health, Dr. Deena\r\nHinshaw. Dr.Hinshaw usually held public health briefings almost\r\nevery day during wave 1 to wave 3 of the pandemic, but her communication\r\nchannel has changed in wave 4 as less public health briefing was held\r\nand more tweets were posted on the her account.\r\nFor that, I believe we could use Natural\r\nLanguage Processing (NLP) techniques to extract themes and\r\ncharacteristics from Dr.Hinshaw’s tweet to examine the essence of public\r\nhealth messages since the provincial reopening date, specifically from\r\nJuly 1st to October 31st, 2021.\r\nText mining and word\r\ncloud fundamentals\r\nFor this post, we will use text mining and word clouds to\r\ninitially explore characteristics of the data set. Text mining is an\r\nexploratory method for textual data under Natural Language Processing\r\n(NLP), a branch of Artificial Intelligence concerning the understanding\r\nof words and spoken texts. NLP is also a type of unsupervised machine\r\nlearning approach to discover hidden structures in the data to inform\r\ndecisions made by experts of the subject matter.\r\nWord cloud is also a popular way to to communicate findings from\r\ntextual data in a visually engaging way. The more frequent a word appear\r\nin the data set (or corpus) the bigger that word will be in the\r\ncloud.\r\nWe will use Python to perform this analysis on R platform with\r\nreticulate::repl_python(). First of all, we will be\r\nimporting necessary modules and twitter data set that we mined from\r\nDr. Hinshaw’s account with pd.read_csv. There are 538\r\ntweets in total, and we can print out examples of the tweets via\r\ntweets_df.Text.head(5).\r\n\r\n\r\nShow code\r\n#Import necessary modules\r\n\r\nimport numpy as np #for numpy array\r\nimport pandas as pd #for data reading and processing\r\nimport matplotlib.pyplot as plt #for plotting\r\nimport re #for Regex text cleaning\r\nfrom wordcloud import WordCloud, STOPWORDS #for word clouds\r\nfrom nltk.stem import WordNetLemmatizer #to reduce text to base form\r\nfrom sklearn.feature_extraction import text\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA #for topic modeling\r\nimport warnings\r\n\r\nwarnings.filterwarnings(\"ignore\") #suppress the warning that Python kindly gave me\r\n\r\ntweets_df = pd.read_csv(\"text-query-tweets.csv\")\r\n\r\ntweets_df.shape\r\n\r\n# Print out the first rows of papers\r\n(538, 4)\r\n\r\nShow code\r\nprint(tweets_df.Text.head(5))\r\n0    We all have the ability to take small actions ...\r\n1    As we head into Halloween weekend, I encourage...\r\n2    Sadly, 9 new deaths related to COVID-19 were a...\r\n3    Over the past 24 hours, we ID’d 603 new cases ...\r\n4    Here is a summary of the latest #COVID19AB num...\r\nName: Text, dtype: object\r\n\r\nLet’s clean the text first\r\nAfter we imported our data into the system, we have to clean our\r\ndata to get rid of textual elements that we do not need such as\r\npunctuation, numbers, as well as convert all words to lower case.\r\nPainful as it may be, this has to be done. It took me days (not that\r\nmuch, but I felt it that way) to clean all of this and make sure that no\r\njunk is left behind (well, there could be. Do let me know if you find\r\nany).\r\nThe phrase “Garbage in, garbage out” is really applicable here in\r\ndata work context. If you let any junk (corrupted data) in, the most you\r\nwill get is processed junk. After we cleaned the text, let us print them\r\nout again to see what they look like. All numbers are gone. All texts\r\nare in lowercase. All URLs and punctuation is gone. Good\r\nriddance!\r\nATTN nerds: Note that in the code below, we will pass the\r\noriginal Text column in tweets_df to the\r\nre.sub function only once. For the second cleaning function\r\nonward, we will pass tweets_df['Text_processed'] instead to\r\nstack our text cleaning results on the same column. Yes, I wrote this to\r\nremind myself because I struggled on it for hours (half an hour,\r\nactually).\r\n\r\n\r\nShow code\r\n\r\n#remove all numbers from the text with list comprehension\r\ntweets_df['Text_processed'] = tweets_df['Text'].map(lambda x: re.sub(r'[0-9]+', '', x))\r\n\r\n# Remove punctuation\r\ntweets_df['Text_processed'] = tweets_df['Text_processed'].map(lambda x: re.sub(r'[^\\w\\s\\,\\.!?]', '', x))\r\n\r\n# Convert the tweets to lowercase\r\ntweets_df['Text_processed'] = tweets_df['Text_processed'].map(lambda x: x.lower())\r\n\r\n#Clean out URLs\r\ntweets_df['Text_processed'] = tweets_df['Text_processed'].map(lambda x: re.sub(r\"http\\S+\", \"\", x))\r\n\r\n# Print the processed titles of the first rows \r\nprint(tweets_df['Text_processed'].head())\r\n0    we all have the ability to take small actions ...\r\n1    as we head into halloween weekend, i encourage...\r\n2    sadly,  new deaths related to covid were also ...\r\n3    over the past  hours, we idd  new cases amp co...\r\n4    here is a summary of the latest covidab number...\r\nName: Text_processed, dtype: object\r\n\r\nSo this is what’s\r\nhappening over time\r\n\r\n\r\nShow code\r\n\r\n#Change datetime format to datetime\r\ntweets_df['Datetime'] = pd.to_datetime(tweets_df['Datetime'])\r\n\r\n#Extract month from datetime\r\ntweets_df['Month'] = tweets_df['Datetime'].dt.month\r\n\r\n# Group the papers by year\r\ngroups = tweets_df.groupby('Month')\r\n\r\n# Determine the size of each group\r\ncounts = groups.size()\r\n\r\n# Visualize the counts as a bar plot\r\n\r\n# Vertical lines\r\nplt.axvline(x = 7.0, color = 'forestgreen', label = 'The reopening date', linestyle='--')\r\nplt.axvline(x = 8.0, color = 'firebrick', label = 'Wave 4 started', linestyle='--')\r\nplt.legend(bbox_to_anchor = (1.0, 1), loc = 'upper right')\r\n\r\nplt.title(\"Tweet count across months\")\r\nplt.ylabel(\"Tweet count\")\r\nplt.xlabel(\"Month\")\r\ncounts.plot()\r\nplt.show()\r\n\r\n\r\nThe line plot above represents tweet counts across months after the\r\nprovincial reopening date. The x-axis indicates months and the y-axis\r\nindicates the number of twitter post of Dr. Hinshaw. The number of tweet\r\ndropped slightly from July to August as cases decreased, but wave 4 of\r\nthe pandemic started in August as cases were on the rise again. We can\r\nsee that the number of cases aligns with the number of tweets posted on\r\nDr. Hinshaw’s account.\r\nLet’s see the big\r\npicture with word cloud\r\nNow that we know the frequency of tweets over months, we can plot a\r\nword cloud from our processed text to see the big picture of twitter\r\ndata. There are 114,362 words in total after combining all 538 tweets\r\ntogether. The word cloud below suggests that “covid” was mentioned the\r\nmost during the past four months, following by “vaccine”, “new cases”,\r\nand “unvaccinated”.\r\n\r\n\r\nShow code\r\n\r\ntext_all = \" \".join(tweet for tweet in tweets_df.Text_processed)\r\nprint (\"There are {} words in the combination of all tweets\".format(len(text_all)))\r\n\r\n#lemmatize all words\r\nThere are 114362 words in the combination of all tweets\r\n\r\nShow code\r\nlemmatizer = WordNetLemmatizer()\r\ntext_all = \"\".join([lemmatizer.lemmatize(i) for i in text_all])\r\n\r\n# Create Stopword list:\r\nstopwords_cloud = set(STOPWORDS)\r\nstopwords_cloud.update([\"https://\", \"(/)\", \"Online:\", \r\n                        \"Twitter:\", \"Join\", \"us\", \"virtually\",\r\n                        \"pm\", \":\", \"https\", \"t\", \"d\", \"co\", \"amp\", \"will\"])\r\n                      \r\n#Generate a word cloud image\r\nwordcloud_tweet = WordCloud(stopwords=stopwords_cloud, background_color=\"white\",random_state=7).generate(text_all)\r\n\r\n#Display the generated image:\r\n#the matplotlib way:\r\nplt.figure(figsize=[10,10])\r\nplt.imshow(wordcloud_tweet, interpolation='bilinear')\r\nplt.axis(\"off\")\r\n(-0.5, 399.5, 199.5, -0.5)\r\n\r\nShow code\r\nplt.show()\r\n\r\n\r\nThe thing is, word cloud can only provide a rough visual\r\npresentation for the characteristics of our textual data. We would need\r\nto dive a little bit deeper to graphs and numbers to examine what is\r\ntruly going on. Let us visualize them all on a bar plot.\r\nCommon\r\nword bar plot and text preprocessing for topic modeling\r\n\r\n\r\nShow code\r\n\r\n# Helper function to count common words\r\n\r\ndef plot_10_most_common_words(count_data, tfidf_vectorizer):\r\n    import matplotlib.pyplot as plt\r\n    words = tfidf_vectorizer.get_feature_names()\r\n    total_counts = np.zeros(len(words))\r\n    for t in count_data:\r\n        total_counts+=t.toarray()[0]\r\n    \r\n    count_dict = (zip(words, total_counts))\r\n    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\r\n    words = [w[0] for w in count_dict]\r\n    counts = [w[1] for w in count_dict]\r\n    x_pos = np.arange(len(words)) \r\n\r\n    plt.bar(x_pos, counts,align='center')\r\n    plt.xticks(x_pos, words, rotation=90) \r\n    plt.xlabel('words')\r\n    plt.ylabel('counts')\r\n    plt.title('10 most common words')\r\n    plt.show()\r\n\r\n#Make your own list of stop words\r\nmy_additional_stop_words = (\"https://\", \"(/)\", \"➡Online:\", \r\n                        \"➡Twitter:\", \"Join\", \"us\", \"virtually\",\r\n                        \"pm\", \":\", \"https\", \"t\", \"d\", \"co\", \"amp\", \"today\", \"new\", \"covid\",\r\n                        \"covidab\", \"hours\", \"completed\")\r\n                        \r\nstop_words_lda = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)    \r\n\r\n# Initialize the count vectorizer with the English stop words\r\ntfidf_vectorizer = TfidfVectorizer(stop_words=stop_words_lda)\r\n\r\n# Fit and transform the processed titles\r\ncount_data = tfidf_vectorizer.fit_transform(tweets_df['Text_processed'])\r\n\r\n# Visualise the 10 most common words\r\nplot_10_most_common_words(count_data, tfidf_vectorizer)\r\n\r\n\r\nThe bar plot above gave us a more detailed information of which\r\nword occurs more frequently than the others based on the term\r\nfrequency–inverse document frequency (TFIDF) statistics. TFIDF gives\r\neach word a weight that reflects its importance to a document. For\r\nTFIDF, words that occur too frequent like “the” provides little meaning\r\nwhile words rarely occur doesn’t tell us much as well. We are taking\r\nabout the COVID-19 pandemic here, so it is obvious that “vaccinated” is\r\ngoing to be mentioned the most in Dr. Hinshaw’s tweet. “Cases” and\r\n“unvaccinated” seem to be reasonable to be mentioned as the second- and\r\nthird most important words as the government of Alberta has been putting\r\nmore effort in identifying more cases in the province and encourage\r\nunvaccinated individuals to get their vaccine.\r\nWe will also create a tfidf_vectorizer model with\r\nour own list of stopwords (or words that have little meaning such as\r\n“is, am, are”) to prepare our data for Latent Dirichlet Allocation (LDA)\r\ntopic modeling.\r\nFinally,\r\nlet’s see potential topics from Dr. Hinshaw’s tweet\r\nLatent Dirichlet Allocation is a powerful natural language\r\nprocessing technique that discovers hidden patterns in topic from\r\nunstructured textual data with statistical models (Jelodar\r\net al., 2019).\r\nHere, we can use LDA to discover potential topics among the sea\r\nof tweets posted by Dr. Hinshaw to find out what she talked about since\r\nthe provincial reopening and wave 4 of the pandemic. I have specified\r\nthe model to extract 8 topics from the data, with 5 words per topics.\r\nNote that these numbers are arbitrary chosen.\r\nIf we extracted too few topics, we might not be able to capture\r\nthe whole picture of the data. On the other hand, extracting too much\r\ntopics could just give us more of the same overlapping themes. We need\r\nto find the middle ground.\r\n\r\n\r\nShow code\r\n\r\n# Helper function to print out the topics\r\ndef print_topics(model, tfidf_vectorizer, n_top_words):\r\n    words = tfidf_vectorizer.get_feature_names()\r\n    for topic_idx, topic in enumerate(model.components_):\r\n        print(\"\\nTopic #%d:\" % topic_idx)\r\n        print(\" \".join([words[i]\r\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\r\n                        \r\n#How many topic and words per topic we want to see\r\nnumber_topics = 8\r\nnumber_words = 5 \r\n                      \r\n# Create and fit the LDA model\r\nlda = LDA(n_components=number_topics, random_state = 1)\r\nlda.fit(count_data)\r\n\r\n# Print the topics found by the LDA model\r\nLatentDirichletAllocation(n_components=8, random_state=1)\r\n\r\nShow code\r\nprint_topics(lda, tfidf_vectorizer, number_words)\r\n\r\nTopic #0:\r\ntwitter online join video transcript\r\n\r\nTopic #1:\r\npossible information protection protect soon\r\n\r\nTopic #2:\r\nvaccines protect vaccine dose book\r\n\r\nTopic #3:\r\ncases tests partially unvaccinated idd\r\n\r\nTopic #4:\r\nreported deaths sadly condolences alberta\r\n\r\nTopic #5:\r\noct age steps important dr\r\n\r\nTopic #6:\r\nmatter pandemic report continue health\r\n\r\nTopic #7:\r\nahs participating prevent book available\r\n\r\nWrapping up here. What\r\ncan we conclude?\r\nThe topics we discovered above can be inferred as follows:\r\nTopic 0: An invitation for the general population to join a live\r\nupdate video on Twitter.\r\nTopic 1: The availability of possible information on COVID-19\r\nprotection\r\nTopic 2: Encouragement to book for a vaccination for more\r\nprotection.\r\nTopic 3: The proportion of unvaxxed vs vaxxed vs partiallyvaxxed\r\npatients.\r\nTopic 4: Covid-related death.\r\n\r\nThe insights that we gained could also be further supported by\r\nopinion from public health experts as they could provide information at\r\na greater depth into their field.\r\nFrom what we have discussed so far, we can see that with the\r\nright tool, LDA for our case, we could take advantage of the vast\r\navailability of textual data that revolves around us in our everyday\r\nlives and use that information to deepen our understanding of social\r\nphenomena. We could explore how students opinion changed from pre- to\r\npost-COVID era, or we could use this technique to media transcription of\r\nsocial events such as political protests, election speech, or even\r\nproduct review in the marketing field. Thank you for your\r\nreading!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds/word-cloud-pv.png",
    "last_modified": "2022-05-15T19:16:58-06:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1920
  },
  {
    "path": "posts/2021-11-07-introducing-dialectical-pluralism/",
    "title": "Finding a home among the paradigm push-back with Dialectical Pluralism",
    "description": "This entry discusses the reconciliation of quantitative and qualitative worldviews amidst the paradigm wars with pluralistic stance.\n\n(2 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2021-11-09",
    "categories": [
      "Mixed methods research"
    ],
    "contents": "\r\n\r\nContents\r\nThe current status-quo of paradigm wars\r\nThen, what should we do about it?\r\nWhat about the how?\r\nFinal remarks and food for thought\r\n\r\nPhoto by Relevant Insight LLCThe current status-quo of paradigm wars\r\nIn learning of mixed methods research, the issue of paradigm wars is usually brought up in how it affects us, scholars, and the academia as a whole. I occasionally came across the over-endorsement of one method (oftentimes quantitative) to another in my conversation with colleagues like, “Psychology is the field that is largely explained by quantitative research”, “We do not buy the notion that p-value can conclude anything beyond numbers”; these statements are the byproduct of paradigm wars, a methodological preference that divided academic community to these days (Williams, 2020). Implicit as it may be, the remnant of this conflict is still there.\r\nThen, what should we do about it?\r\nAt methodological level, Mixed Methods Research (MMR) seems to be a realistic answer to this problem by synergizing both qualitative and quantitative data into a greater whole than the sum of the two (Fetters & Freshwater, 2015). One philosophical mechanism behind MMR is Dialectical Pluralism (DP), a process philosophy that serves as a middle ground for both qualitative and quantitative paradigms for equal acknowledgement while offers equal voice to paradigm with lesser recognition.\r\nAn interesting suggestion from Shannon-Baker (2016) is that qualitative or quantitative view should be used as our regard to data rather than the whole research; for that, we should consider our research framework as a fluid “stance” to research instead of an archaic notion of static concept.\r\nWhat about the how?\r\nAs I realize the existence of multiple realities in social science, the next step could be to move beyond our current paradigm as Johnson (2017) suggests, “the next theoretical step in the paradigms dialog is to articulate a metaparadigm” (p.159). Researchers are encouraged to apply the dialectic worldview in their practice, both interpersonally and intrapersonally.\r\nInterpersonally, researchers could seek a collaborative space with heterogeneous team members, and conclusions should be made based on the evidence of shared values. Ultimately, practical truths should be emphasized instead of absolute truths.\r\nIntrapersonally, researchers could have internal dialogues that reconcile differences of their perspective through personal reflection. The more ideas a researcher has in their toolbox, the more versatile they can be, as Fitzgerald (1936) said, “the test of a first-rate intelligence is the ability to hold two opposed ideas in the mind at the same time and still retain the ability to function”.\r\nFinal remarks and food for thought\r\nTo me, learning about DP is quite impactful as it offers a middle ground for seemingly conflicting ideas to collaborate equally to further the body of knowledge than wresting internally over the some-old quarrel. That, my friend, could be a promising way for us to grow up as a versatile individuals (and researchers). Have a good one!\r\n\r\n\r\n\r\n",
    "preview": "https://www.relevantinsights.com/wp-content/uploads/2020/05/Qualitative-Quantitative-Research-For-New-Product-Development.png",
    "last_modified": "2022-03-27T18:08:04-06:00",
    "input_file": {}
  }
]
