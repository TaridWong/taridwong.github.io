[
  {
    "path": "posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data/",
    "title": "Anomaly Detection with New York City taxi data",
    "description": "In this entry, I will be conducting anomaly detection to identify points of anomaly in the taxi passengers data in New York City from July 2014 to January 2015 at half-hourly intervals.\n \n (4 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2021-12-04",
    "categories": [
      "Unsupervised Machine Learning",
      "R"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction to Anomaly Detection\r\nDataset\r\nInvestigating the Moving Average\r\nTime Series Decomposition with Anomalies\r\nAnomaly Detection\r\nApplication of Anomaly Detection\r\n\r\nIntroduction to Anomaly Detection\r\nAnomaly detection is an unsupervised machine learning technique that identifies outliers - a data point that differs from other majority data points - and their patterns in the data set. Such outliers could be a super hot day (as in 50 degree celcius) in the middle of winter with the average temperature of -10 degree Celcius. This technique can be used to detect outliers to remove in data preprocessing or even to identify potential frauds or failures in health system monitoring.\r\nDataset\r\nFirst off, we will load the dataset and the essential libraries as usual and read the first five rows to see what is going on. The output below shows that we have timestamp data and the number of passengers as indicated by the value column.\r\n\r\n\r\nShow code\r\n\r\nlibrary(tidyverse) #for data manipulation\r\nlibrary(lubridate) #for date/time data management\r\nlibrary(zoo) # moving averages  \r\nlibrary(anomalize) #for anomaly detection\r\n\r\ndata <- read_csv(\"nyc_taxi.csv\")\r\nhead(data)\r\n\r\n\r\n# A tibble: 6 x 2\r\n  timestamp           value\r\n  <dttm>              <dbl>\r\n1 2014-07-01 00:00:00 10844\r\n2 2014-07-01 00:30:00  8127\r\n3 2014-07-01 01:00:00  6210\r\n4 2014-07-01 01:30:00  4656\r\n5 2014-07-01 02:00:00  3820\r\n6 2014-07-01 02:30:00  2873\r\n\r\nLet us visualize the data with ggplot2 to see what the data set is like before we proceed further.\r\n\r\n\r\nShow code\r\n\r\nggplot(data, aes(x = timestamp, y = value)) + \r\n  geom_point(shape = 1, alpha = 0.5) +\r\n  labs(x = \"Time\", y = \"Count\") +\r\n  labs(alpha = \"\", colour=\"Legend\")\r\n\r\n\r\n\r\n\r\nTime series data is usually represented in patterns across time, but fluctuations could occur by the influence of events that affect the variable of interest such as holidays or natural phenomenon.\r\nInvestigating the Moving Average\r\nIn simple word, a moving average is an indicator that shows the average value of the variable of interest over a period (i.e. 10 days, 50 days, 200 days, etc) and is usually plotted across a large time interval as in months or years.\r\nWe will create two moving averages, one with 48 value in each group, and another one with 336 value in each group.\r\n\r\n\r\nShow code\r\n\r\ndata <- data %>% \r\n  dplyr::mutate (MA48 = zoo::rollmean(value, k = 48, fill = NA),\r\n                MA336 = zoo::rollmean(value, k = 336, fill = NA)) %>%\r\n  dplyr::ungroup()\r\n\r\n#Plot the moving average line chart\r\n\r\ndata %>%\r\n  gather(metric, value, MA48:MA336) %>%\r\n  ggplot(aes(timestamp, value, color = metric)) +\r\n  geom_line() +\r\n  ggtitle(\"Rolling average of NYC taxi passenger count\")+\r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nWe can see that the anomaly is more apparent when we divide the data into groups with 48 data points.\r\nTime Series Decomposition with Anomalies\r\nBefore we dive into anomaly detection, we should conduct time series decomposition where time series data is decomposed into Seasonal, Trend and remainder components with time_decompose().\r\nOnce the components are decomposed, anomalize() can detect and flag anomalies in the decomposed data of the reminder component which then could be visualized with plot_anomaly_decomposition()\r\n\r\n\r\nShow code\r\n\r\ndata %>% \r\n  time_decompose(value, method = \"stl\", frequency = \"auto\", trend = \"auto\") %>%\r\n  anomalize(remainder, method = \"gesd\", alpha = 0.05, max_anoms = 0.2) %>%\r\n  plot_anomaly_decomposition()\r\n\r\n\r\n\r\n\r\nFor each element of the graph, observed represents the actual value, season represents the seasonal or cyclic trend, trend is a long term trend throughout the whole time period, and remainder is the observed data minus by results from season and trend.\r\nAnomaly Detection\r\nAnomaly Detection and Plotting the detected anomalies are similar to the time series decomposition above, but we do not have to adjust parameters such as frequency, trend, or method like we did above. Basically, it is easier than time series decomposition as we can use it out of the box.\r\n\r\n\r\nShow code\r\n\r\ndata %>% \r\n  time_decompose(value) %>%\r\n  anomalize(remainder) %>%\r\n  time_recompose() %>%\r\n  plot_anomalies(time_recomposed = TRUE, ncol = 3, alpha_dots = 0.5)\r\n\r\n\r\n\r\n\r\nNotice that the model has picked several anomalies around January, which contains the New Year festival and the occurrence of a blizzard storm at the New York City. Points of anomaly usually occurs with events of some sort, so we might be able to identify sources of anomaly with further information search using the date of anomaly as the lead.\r\nWe can also extract the actual anomalous data point via the following codes:\r\n\r\n\r\nShow code\r\n\r\ndata_anomalous <- data %>% \r\n  time_decompose(value) %>%\r\n  anomalize(remainder) %>%\r\n  time_recompose() %>%\r\n  filter(anomaly == 'Yes') \r\n\r\nhead(data_anomalous)\r\n\r\n\r\n# A time tibble: 6 x 10\r\n# Index: timestamp\r\n  timestamp           observed season  trend remainder remainder_l1\r\n  <dttm>                 <dbl>  <dbl>  <dbl>     <dbl>        <dbl>\r\n1 2014-07-04 07:30:00     4926  2817. 14875.   -12767.       -9534.\r\n2 2014-07-04 08:00:00     5165  3903. 14875.   -13612.       -9534.\r\n3 2014-07-04 08:30:00     5776  4346. 14874.   -13445.       -9534.\r\n4 2014-07-04 09:00:00     7338  3304. 14874.   -10840.       -9534.\r\n5 2014-07-05 07:00:00     3658  -735. 14848.   -10454.       -9534.\r\n6 2014-07-05 07:30:00     4345  2817. 14847.   -13319.       -9534.\r\n# ... with 4 more variables: remainder_l2 <dbl>, anomaly <chr>,\r\n#   recomposed_l1 <dbl>, recomposed_l2 <dbl>\r\n\r\nI know that the amount of detected anomaly is a lot. We can adjust parameter of the detector to make the algorithm less sensitive and detect stronger outliers by decreasing alpha and max_anoms to control for sensitivity and the maximum percentage of data that can be an anomaly respectively. max_anoms = 0.20 means the algorithm will flag anomalies up to 20% of the whole data set.\r\nThe example above used default parameter, which is alpha = 0.05 and max_anoms = 0.20. Let us try tuning down the sensitivity a little bit, say, alpha = 0.025 and max_anoms = 0.05 to identify only extreme outliers.\r\n\r\n\r\nShow code\r\n\r\ndata %>% \r\n  time_decompose(value) %>%\r\n  anomalize(remainder, alpha = 0.025, max_anoms = 0.05) %>%\r\n  time_recompose() %>%\r\n  plot_anomalies(time_recomposed = TRUE, ncol = 3, alpha_dots = 0.5)\r\n\r\n\r\n\r\n\r\nEven with parameter tuning, the algorithm still detects anomalies at the end of January. Something must be going on there. Let us extract the actual anomalous data point of the tuned algorithm.\r\n\r\n\r\nShow code\r\n\r\ndata_anomalous_tuned <- data %>% \r\n  time_decompose(value) %>%\r\n  anomalize(remainder, alpha = 0.025, max_anoms = 0.05) %>%\r\n  time_recompose() %>%\r\n  filter(anomaly == 'Yes') \r\n\r\nhead(data_anomalous_tuned)\r\n\r\n\r\n# A time tibble: 6 x 10\r\n# Index: timestamp\r\n  timestamp           observed  season  trend remainder remainder_l1\r\n  <dttm>                 <dbl>   <dbl>  <dbl>     <dbl>        <dbl>\r\n1 2014-09-21 01:00:00    25371  -8034. 14994.    18411.      -17691.\r\n2 2014-10-19 01:00:00    25610  -8034. 15905.    17739.      -17691.\r\n3 2014-11-01 01:30:00    23736  -9536. 15566.    17707.      -17691.\r\n4 2014-11-01 02:00:00    23245 -10442. 15565.    18121.      -17691.\r\n5 2014-11-02 01:00:00    39197  -8034. 15597.    31634.      -17691.\r\n6 2014-11-02 01:30:00    35212  -9536. 15598.    29151.      -17691.\r\n# ... with 4 more variables: remainder_l2 <dbl>, anomaly <chr>,\r\n#   recomposed_l1 <dbl>, recomposed_l2 <dbl>\r\n\r\nApplication of Anomaly Detection\r\nAnomaly detection has wide applications across industries. Data scientists can identify anomalies in the amount of deposits that go outside the usual pattern, thus flagging it for potential fraud or system error for a deeper investigation. The technique can also be used to identify anomalous data in student pattern from their e-learning platform, which could lead us to topics that can be further explored such as a fluctuation in the duration of content access. A certain content could be exceedingly difficult that students have to spend more time than usual studying it.\r\nAside from using R, anomaly can also be done in Python as well. If you are interested, you can check out my Jupyter notebook here for anomaly detection with Pycaret, a low-code machine learning library in Python that covers end-to-end machine learning pipeline from preprocessing to model deploying. Thank you very much for reading!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data/anomaly-detection-with-new-york-city-taxi-data_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2021-12-04T21:35:40-07:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data/",
    "title": "Crime mapping in San Francisco with police data",
    "description": "In this entry, we will explore San Francisco crime data from 2016 to 2018 to understand the relationship between civilian-reported incidents of crime and police-reported incidents of crime. Along the way we will use table intersection methods to subset our data, aggregation methods to calculate important statistics, and simple visualizations to understand crime trends.  \n\n(7 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2021-11-30",
    "categories": [
      "Data Visualization",
      "R",
      "Quantitative research"
    ],
    "contents": "\r\n\r\nContents\r\nIntroducing the data\r\nData exploration\r\nInspect frequency trends\r\nCorrelation between trends\r\nFiltering joins\r\nCrime categories\r\nGrand theft auto\r\nDensity map\r\nConclusion and key takeaways\r\n\r\nIntroducing the data\r\nThe data was provided by SF open data, a governmental agency of the City and County of San Francisco that publishes data to improve decision-making and service delivery. Basically, the governing body of San Francisco made their data publicly available for data scientists to play around for fun and practice their data work skill.\r\nFor this project, we will be using R to read and manipulate our data. Findings will be communicated via plots and graphs. First of all, let us read the data set and load essential libraries.\r\n\r\n\r\nShow code\r\n\r\n# Load required packages\r\nlibrary(tidyverse) #for data manipulation\r\nlibrary(lubridate) #for date/time data management\r\nlibrary(maptools) #to load map data\r\nlibrary(ggmap) #to work with map data\r\n\r\n# Read datasets and assign to variables\r\nincidents <- read_csv(\"police-department-incidents.csv\")\r\ncalls <- read_csv(\"police-department-calls-for-service.csv\")\r\n\r\n\r\n\r\nData exploration\r\nFirst things first: we need to wrap our heads around the data in order to understand what we have. We will use glimpse() to explore the variable of both civilian-reported an police-reported crime\r\nFor police-reported crime, we have incident number, incident category, date/time information, location of the crime, crime description, and the means in which the incident was resolved.\r\nFor civilian-reported crime, we have crime ID, date/time information, location, and crime description.\r\n\r\n\r\nShow code\r\n\r\n# Glimpse the structure of both datasets\r\nglimpse(incidents)\r\n\r\n\r\nRows: 84,000\r\nColumns: 13\r\n$ IncidntNum <dbl> 176122807, 160569314, 160362475, 160435298, 90543~\r\n$ Category   <chr> \"LARCENY/THEFT\", \"ASSAULT\", \"ROBBERY\", \"KIDNAPPIN~\r\n$ Descript   <chr> \"GRAND THEFT FROM UNLOCKED AUTO\", \"BATTERY\", \"ROB~\r\n$ DayOfWeek  <chr> \"Saturday\", \"Thursday\", \"Tuesday\", \"Friday\", \"Tue~\r\n$ Date       <dttm> 2017-05-13, 2016-07-14, 2016-05-03, 2016-05-27, ~\r\n$ Time       <time> 10:20:00, 16:00:00, 14:19:00, 23:57:00, 07:40:00~\r\n$ PdDistrict <chr> \"SOUTHERN\", \"MISSION\", \"NORTHERN\", \"SOUTHERN\", \"T~\r\n$ Resolution <chr> \"NONE\", \"NONE\", \"ARREST, BOOKED\", \"ARREST, BOOKED~\r\n$ Address    <chr> \"800 Block of BRYANT ST\", \"MISSION ST / CESAR CHA~\r\n$ X          <dbl> -122.4034, -122.4182, -122.4299, -122.4050, -122.~\r\n$ Y          <dbl> 37.77542, 37.74817, 37.77744, 37.78512, 37.71912,~\r\n$ Location   <chr> \"{'latitude': '37.775420706711', 'human_address':~\r\n$ PdId       <dbl> 1.761228e+13, 1.605693e+13, 1.603625e+13, 1.60435~\r\n\r\nShow code\r\n\r\nglimpse(calls)\r\n\r\n\r\nRows: 100,000\r\nColumns: 14\r\n$ `Crime Id`        <dbl> 163003307, 180870423, 173510362, 163272811~\r\n$ Descript          <chr> \"Bicyclist\", \"586\", \"Suspicious Person\", \"~\r\n$ `Report Date`     <dttm> 2016-10-26, 2018-03-28, 2017-12-17, 2016-~\r\n$ Date              <dttm> 2016-10-26, 2018-03-28, 2017-12-17, 2016-~\r\n$ `Offense Date`    <dttm> 2016-10-26, 2018-03-28, 2017-12-17, 2016-~\r\n$ `Call Time`       <time> 17:47:00, 05:49:00, 03:00:00, 17:39:00, 0~\r\n$ `Call Date Time`  <dttm> 2016-10-26 17:47:00, 2018-03-28 05:49:00,~\r\n$ Disposition       <chr> \"GOA\", \"HAN\", \"ADV\", \"NOM\", \"GOA\", \"ADV\", ~\r\n$ Address           <chr> \"The Embarcadero Nor/kearny St\", \"Ingalls ~\r\n$ City              <chr> \"San Francisco\", \"San Francisco\", \"San Fra~\r\n$ State             <chr> \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", ~\r\n$ `Agency Id`       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~\r\n$ `Address Type`    <chr> \"Intersection\", \"Intersection\", \"Intersect~\r\n$ `Common Location` <chr> NA, NA, NA, NA, NA, NA, \"Midori Hotel Sro ~\r\n\r\nWe have 84,000 cases of police-reported crime and 100,000 cases of civilian reported crime.We will then aggregate the number of incidents by date for both civilian-reported an police-reported crime for comparison.\r\n\r\n\r\nShow code\r\n\r\n# Aggregate the number of reported incidents by date\r\ndaily_incidents <- incidents %>% \r\n    count(Date, sort = TRUE) %>% \r\n    rename(n_incidents = n)\r\n\r\n# Aggregate the number of calls for police service by date\r\ndaily_calls <- calls %>% \r\n    count(Date, sort = TRUE) %>% \r\n    rename(n_calls = n)\r\n\r\n# Join data frames to create a new \"mutated\" set of information\r\nshared_dates <- inner_join(daily_incidents, daily_calls, by = c(\"Date\" = \"Date\"))\r\n\r\n# Take a glimpse of this new data frame\r\nhead(shared_dates, 10)\r\n\r\n\r\n# A tibble: 10 x 3\r\n   Date                n_incidents n_calls\r\n   <dttm>                    <int>   <int>\r\n 1 2017-03-01 00:00:00         124     133\r\n 2 2017-09-01 00:00:00         124     138\r\n 3 2016-04-01 00:00:00         120     124\r\n 4 2017-03-17 00:00:00         118     129\r\n 5 2016-11-25 00:00:00         117     115\r\n 6 2017-06-25 00:00:00         114     120\r\n 7 2017-05-25 00:00:00         111     127\r\n 8 2017-08-28 00:00:00         110     106\r\n 9 2017-09-28 00:00:00         110     124\r\n10 2017-11-27 00:00:00         110     108\r\n\r\nWe can see that the number of civilian-reported incident is generally lower than the number of police-reported incidence. Sometime, people dialed 911 for police to check on suspicious activities, which might be or might not be an actual crime.\r\nInspect frequency trends\r\nLet us visualize the number of a plot to compare both incidents at a big picture.\r\n\r\n\r\nShow code\r\n\r\n# Gather into long format using the \"Date\" column to define observations\r\nplot_shared_dates <- shared_dates %>%\r\n  gather(key = report, value = count, -Date)\r\n\r\n# Plot points and regression trend lines\r\nggplot(plot_shared_dates, aes(x = Date, y = count, color = report)) +\r\n  geom_point() +\r\n  geom_smooth(method = \"lm\", formula = y ~ x)\r\n\r\n\r\n\r\n\r\nThe plot above visualizes he frequency of calls and incidents across time for us to see if there is any relationships between both civilian-reported and police-reported incident. We also have regression lines that cut across the plot to determine the trend in which the data is going.\r\nCorrelation between trends\r\nTo make the relationship more understandable, we could calculate a correlation value between the number of both incident types to see how they get along with each other.\r\nA quick refresh on correlation, this value ranges from -1 to 1 as a representation of relationships between two sets of data. The closest the value is to 1 or -1, the stronger the relationship can be. Positive value indicates that the two variables go the same way while negative value indicates otherwise (direction of the two variables are opposite; that is, X increases while Y decreases).\r\n\r\n\r\nShow code\r\n\r\n# Calculate correlation coefficient between daily frequencies\r\ndaily_cor <- cor(shared_dates$n_incidents, shared_dates$n_calls)\r\ndaily_cor\r\n\r\n\r\n[1] 0.1469688\r\n\r\nThe correlation is not that much. Just 0.146, but what if we look at a broader perspective by summarising the data into monthly counts and calculating a correlation coefficient.\r\n\r\n\r\nShow code\r\n\r\n# Summarise frequencies by month\r\ncorrelation_df <- shared_dates %>% \r\n  mutate(month = month(Date)) %>%\r\n  group_by(month) %>% \r\n  summarize(n_incidents = sum(n_incidents),\r\n            n_calls = sum(n_calls))\r\n\r\n# Calculate correlation coefficient between monthly frequencies\r\nmonthly_cor <- cor(correlation_df$n_incidents, correlation_df$n_calls)\r\nmonthly_cor\r\n\r\n\r\n[1] 0.970683\r\n\r\nThe correlation is .97! I wonder why this is the case. Maybe breaking the data into days format would make it too detailed as correlations might be more apparent if the data is separated into clusters (month, in this case). But I won’t look too much into it as this is not the focus on this entry.\r\nFiltering joins\r\nATTN Nerds: When working with relational datasets, there are situations in which it is helpful to subset information based on another set of values. Filtering joins are a complementary type of join which allows us to keep all specific cases within a data frame while preserving the structure of the data frame itself.\r\nIt will be helpful to have all the information from each police reported incident and each civilian call on their shared dates so we can calculate similar statistics from each dataset and compare results. This step will prepare us to make the pattern more apparent with data visualization.\r\n\r\n\r\nShow code\r\n\r\n# Filter calls to police by shared_dates\r\ncalls_shared_dates <- calls %>%\r\n  semi_join(shared_dates, by = c(\"Date\" = \"Date\"))\r\n\r\n# Filter recorded incidents by shared_dates\r\nincidents_shared_dates <- incidents %>% \r\n  semi_join(shared_dates, by = c(\"Date\" = \"Date\"))\r\n\r\nhead(calls_shared_dates)\r\n\r\n\r\n# A tibble: 6 x 14\r\n  `Crime Id` Descript          `Report Date`       Date               \r\n       <dbl> <chr>             <dttm>              <dttm>             \r\n1  163003307 Bicyclist         2016-10-26 00:00:00 2016-10-26 00:00:00\r\n2  180870423 586               2018-03-28 00:00:00 2018-03-28 00:00:00\r\n3  173510362 Suspicious Person 2017-12-17 00:00:00 2017-12-17 00:00:00\r\n4  163272811 911 Drop          2016-11-22 00:00:00 2016-11-22 00:00:00\r\n5  172811002 Drugs             2017-10-08 00:00:00 2017-10-08 00:00:00\r\n6  170902193 At Risk           2017-03-31 00:00:00 2017-03-31 00:00:00\r\n# ... with 10 more variables: Offense Date <dttm>, Call Time <time>,\r\n#   Call Date Time <dttm>, Disposition <chr>, Address <chr>,\r\n#   City <chr>, State <chr>, Agency Id <dbl>, Address Type <chr>,\r\n#   Common Location <chr>\r\n\r\nShow code\r\n\r\nhead(incidents_shared_dates)\r\n\r\n\r\n# A tibble: 6 x 13\r\n  IncidntNum Category  Descript    DayOfWeek Date                Time \r\n       <dbl> <chr>     <chr>       <chr>     <dttm>              <tim>\r\n1  176122807 LARCENY/~ GRAND THEF~ Saturday  2017-05-13 00:00:00 10:20\r\n2  160569314 ASSAULT   BATTERY     Thursday  2016-07-14 00:00:00 16:00\r\n3  160362475 ROBBERY   ROBBERY, B~ Tuesday   2016-05-03 00:00:00 14:19\r\n4  160435298 KIDNAPPI~ KIDNAPPING~ Friday    2016-05-27 00:00:00 23:57\r\n5  180018692 VEHICLE ~ STOLEN MOT~ Sunday    2018-01-07 00:00:00 18:00\r\n6  176045481 LARCENY/~ GRAND THEF~ Wednesday 2017-02-15 00:00:00 20:00\r\n# ... with 7 more variables: PdDistrict <chr>, Resolution <chr>,\r\n#   Address <chr>, X <dbl>, Y <dbl>, Location <chr>, PdId <dbl>\r\n\r\nThe above tables are some examples of civilian-reported and police-reported crimes when grouped together by dates. It will all make sense when we plot them all into graphs below.\r\nCrime categories\r\nNow we need to see what the data look like after joining the datasets. Previously, we have a scatter plot to see if there was a trend in the frequency of calls and the frequency of reported incidents over time. Scatterplots are a great tool to look at overall trends of continuous data. However, to see trends in categorical data, we need to visualize the ranked order of the variables to understand their levels of importance.\r\n\r\n\r\nShow code\r\n\r\n# Create a bar chart of the number of calls for each crime\r\nplot_calls_freq <- calls_shared_dates %>% \r\n  count(Descript) %>% \r\n  top_n(15, n) %>% \r\n  ggplot(aes(x = reorder(Descript, n), y = n, fill = Descript)) +\r\n  geom_bar(stat = 'identity') +\r\n  ylab(\"Count\") +\r\n  xlab(\"Crime Description\") +\r\n  ggtitle(\"Civilian-Reported Crimes\") +\r\n  coord_flip()+\r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n# Output the plots\r\nplot_calls_freq\r\n\r\n\r\n\r\n\r\nFrom the plot above, passing call was ranked as the most frequent cause for civilian-reported crime, following by traffic stop and homeless complaint.\r\n\r\n\r\nShow code\r\n\r\n# Create a bar chart of the number of reported incidents for each crime\r\nplot_incidents_freq <- incidents_shared_dates %>% \r\n  count(Descript) %>% \r\n  top_n(15, n)  %>% \r\n  ggplot(aes(x = reorder(Descript, n), y = n, fill = Descript)) +\r\n  geom_bar(stat = 'identity') +\r\n  ylab(\"Count\") +\r\n  xlab(\"Crime Description\") +\r\n  ggtitle(\"Police-Reported Crimes\") +\r\n  coord_flip()+\r\n  theme(plot.title = element_text(hjust = 0.5))+\r\n  theme(\r\n    legend.position = c(.95, .60),\r\n    legend.justification = c(\"right\", \"top\"),\r\n    legend.box.just = \"right\",\r\n    legend.margin = margin(6, 6, 6, 6)\r\n    )\r\n\r\nplot_incidents_freq\r\n\r\n\r\n\r\n\r\nThe number of grand theft from locked auto is off-the-chart! This means there were several police-reported incident where unsupervised vehicles are broken into. What happened here?\r\nGrand theft auto\r\nMy friends in the States told me that never leave your belongings in your parked car, especially if the number of grand theft auto crime is high in the area. However, there may be good citizens iut there trying to prevent crime. The 12th most civilian reported crime is “Auto Boost / Strip”, which could be the case where they are trying to prevent the grand theft auto crime. Yet, this is probably only the case where the location of a called-in-crime is similar to the location of crime incidence. Let’s check to see if the locations of the most frequent civilian reported crime and police reported crime are similar.\r\n\r\n\r\nShow code\r\n\r\n# Arrange the top 10 locations of called in crimes in a new variable\r\nlocation_calls <- calls_shared_dates %>%\r\n  filter(Descript == \"Auto Boost / Strip\") %>% \r\n  count(Address) %>% \r\n  arrange(desc(n))%>% \r\n  top_n(10, n)\r\n\r\n# Arrange the top 10 locations of reported incidents in a new variable\r\nlocation_incidents <- incidents_shared_dates %>%\r\n  filter(Descript == \"GRAND THEFT FROM LOCKED AUTO\") %>% \r\n  count(Address) %>% \r\n  arrange(desc(n))%>% \r\n  top_n(10, n)\r\n\r\n# Output the top locations of each dataset for comparison\r\nlocation_calls\r\n\r\n\r\n# A tibble: 11 x 2\r\n   Address                                  n\r\n   <chr>                                <int>\r\n 1 1100 Block Of Point Lobos Av            21\r\n 2 3600 Block Of Lyon St                   20\r\n 3 100 Block Of Christmas Tree Point Rd    18\r\n 4 1300 Block Of Webster St                12\r\n 5 500 Block Of 6th Av                     12\r\n 6 800 Block Of Vallejo St                 10\r\n 7 1000 Block Of Great Hy                   9\r\n 8 100 Block Of Hagiwara Tea Garden Dr      7\r\n 9 1100 Block Of Fillmore St                7\r\n10 3300 Block Of 20th Av                    7\r\n11 800 Block Of Mission St                  7\r\n\r\nShow code\r\n\r\nlocation_incidents\r\n\r\n\r\n# A tibble: 10 x 2\r\n   Address                          n\r\n   <chr>                        <int>\r\n 1 800 Block of BRYANT ST         441\r\n 2 500 Block of JOHNFKENNEDY DR    89\r\n 3 1000 Block of POINTLOBOS AV     84\r\n 4 800 Block of MISSION ST         61\r\n 5 2600 Block of GEARY BL          38\r\n 6 3600 Block of LYON ST           36\r\n 7 1300 Block of WEBSTER ST        35\r\n 8 1100 Block of FILLMORE ST       34\r\n 9 22ND ST / ILLINOIS ST           33\r\n10 400 Block of 6TH AV             30\r\n\r\nDensity map\r\nThe police-reported dataset shares locations where auto crimes occur and are reported most frequently - such as on Point Lobos Avenue, Lyon Street, and Mission Street. We will plot the frequency of auto crime occurrence on the map of San Francisco.\r\n\r\n\r\nShow code\r\n\r\n# Read it into R as a spatial polygons data frame & plot\r\nneighb <- readShapePoly(\"SF_neighborhoods\")\r\n\r\n# Define the bounding box\r\nbbox <- neighb@bbox\r\n \r\n# Manipulate these values slightly so that we get some padding on our basemap between the edge of the data and the edge of the map\r\nsf_bbox <- c(left = bbox[1, 1] - .01, bottom = bbox[2, 1] - .005, \r\n             right = bbox[1, 2] + .01, top = bbox[2, 2] + .005)\r\n\r\n# Download the basemap\r\nbasemap <- get_stamenmap(\r\n  bbox = sf_bbox,\r\n  zoom = 13,\r\n  maptype = \"toner-lite\")\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# Filter grand theft auto incidents\r\nauto_incidents <- incidents_shared_dates %>%\r\n  filter(Descript == \"GRAND THEFT FROM LOCKED AUTO\")\r\n\r\n# Overlay a density plot of auto incidents on the map\r\nggmap(basemap) +\r\n  stat_density_2d(\r\n    aes(x = X, y = Y, fill = ..level..), alpha = 0.15,\r\n    size = 0.01, bins = 30, data = auto_incidents,\r\n    geom = \"polygon\") +\r\n  labs(title=\"The occurence of car theft in San Francisco\") +\r\n  theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\n\r\nFrom the map above, the blue spot means there was a higher rate of grand theft auto crime occurring in that area. Maybe the police could consider patrolling that area more frequently.\r\nConclusion and key takeaways\r\nFrom the data, we can see that the grand theft from locked vehicles crime usually occurs at some specific areas in the city. We could further investigate that area specifically to address the cause or potentially lessen the problem. It is interesting in how open data can benefit both the learning of independent data scientists (those who work with data for fun) and the government at the same time.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data/crime-mapping-in-san-francisco-with-police-data_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-11-30T18:19:18-07:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1920
  },
  {
    "path": "posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds/",
    "title": "Exploring COVID-19 data from twitter with topic modeling",
    "description": "This entry focuses on the exploration of twitter data from Alberta's Chief Medical Officer of Health via word cloud and topic modeling to gain insights in characteristics of public health messaging during the COVID-19 pandemic.  \n\n(7 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2021-11-23",
    "categories": [
      "Natural Language Processing",
      "Python",
      "COVID-19"
    ],
    "contents": "\r\n\r\nContents\r\nCOVID-19 situation in Alberta, Canada\r\nText mining and word cloud fundamentals\r\nLet’s clean the text first\r\nSo this is what’s happening over time\r\nLet’s see the big picture with word cloud\r\nCommon word bar plot and text preprocessing for topic modeling\r\nFinally, let’s see potential topics from Dr. Hinshaw’s tweet\r\nWrapping up here. What can we conclude?\r\n\r\nCOVID-19 situation in Alberta, Canada\r\nThe province of Alberta, Canada, has suffered from the COVID-19 pandemic like all other places. Alberta has gone through cycles of reopening and returning to provincial lock down since April 2020. The province, however, has lifted almost all restrictions and enacted its reopening plan on the recent Canada day when 70% of Alberta population has received at least one dose of approved COVID-19 vaccination.\r\nThe navigation of the province through this pandemic was led by the Alberta’s Chief Medical Officer of Health, Dr. Deena Hinshaw. Dr.Hinshaw usually held public health briefings almost every day during wave 1 to wave 3 of the pandemic, but her communication channel has changed in wave 4 as less public health briefing was held and more tweets were posted on the her account.\r\nFor that, I believe we could use Natural Language Processing (NLP) techniques to extract themes and characteristics from Dr.Hinshaw’s tweet to examine the essence of public health messages since the provincial reopening date, specifically from July 1st to October 31st, 2021.\r\nText mining and word cloud fundamentals\r\nFor this post, we will use text mining and word clouds to initially explore characteristics of the data set. Text mining is an exploratory method for textual data under Natural Language Processing (NLP), a branch of Artificial Intelligence concerning the understanding of words and spoken texts. NLP is also a type of unsupervised machine learning approach to discover hidden structures in the data to inform decisions made by experts of the subject matter.\r\nWord cloud is also a popular way to to communicate findings from textual data in a visually engaging way. The more frequent a word appear in the data set (or corpus) the bigger that word will be in the cloud.\r\nWe will use Python to perform this analysis on R platform with reticulate::repl_python(). First of all, we will be importing necessary modules and twitter data set that we mined from Dr. Hinshaw’s account with pd.read_csv. There are 538 tweets in total, and we can print out examples of the tweets via tweets_df.Text.head(5).\r\n\r\n\r\nShow code\r\n#Import necessary modules\r\n\r\nimport numpy as np #for numpy array\r\nimport pandas as pd #for data reading and processing\r\nimport matplotlib.pyplot as plt #for plotting\r\nimport re #for Regex text cleaning\r\nfrom wordcloud import WordCloud, STOPWORDS #for word clouds\r\nfrom nltk.stem import WordNetLemmatizer #to reduce text to base form\r\nfrom sklearn.feature_extraction import text\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA #for topic modeling\r\nimport warnings\r\n\r\nwarnings.filterwarnings(\"ignore\") #suppress the warning that Python kindly gave me\r\n\r\ntweets_df = pd.read_csv(\"text-query-tweets.csv\")\r\n\r\ntweets_df.shape\r\n\r\n# Print out the first rows of papers\r\n(538, 4)\r\n\r\nShow code\r\nprint(tweets_df.Text.head(5))\r\n0    We all have the ability to take small actions ...\r\n1    As we head into Halloween weekend, I encourage...\r\n2    Sadly, 9 new deaths related to COVID-19 were a...\r\n3    Over the past 24 hours, we ID’d 603 new cases ...\r\n4    Here is a summary of the latest #COVID19AB num...\r\nName: Text, dtype: object\r\n\r\nLet’s clean the text first\r\nAfter we imported our data into the system, we have to clean our data to get rid of textual elements that we do not need such as punctuation, numbers, as well as convert all words to lower case. Painful as it may be, this has to be done. It took me days (not that much, but I felt it that way) to clean all of this and make sure that no junk is left behind (well, there could be. Do let me know if you find any).\r\nThe phrase “Garbage in, garbage out” is really applicable here in data work context. If you let any junk (corrupted data) in, the most you will get is processed junk. After we cleaned the text, let us print them out again to see what they look like. All numbers are gone. All texts are in lowercase. All URLs and punctuation is gone. Good riddance!\r\nATTN nerds: Note that in the code below, we will pass the original Text column in tweets_df to the re.sub function only once. For the second cleaning function onward, we will pass tweets_df['Text_processed'] instead to stack our text cleaning results on the same column. Yes, I wrote this to remind myself because I struggled on it for hours (half an hour, actually).\r\n\r\n\r\nShow code\r\n\r\n#remove all numbers from the text with list comprehension\r\ntweets_df['Text_processed'] = tweets_df['Text'].map(lambda x: re.sub(r'[0-9]+', '', x))\r\n\r\n# Remove punctuation\r\ntweets_df['Text_processed'] = tweets_df['Text_processed'].map(lambda x: re.sub(r'[^\\w\\s\\,\\.!?]', '', x))\r\n\r\n# Convert the tweets to lowercase\r\ntweets_df['Text_processed'] = tweets_df['Text_processed'].map(lambda x: x.lower())\r\n\r\n#Clean out URLs\r\ntweets_df['Text_processed'] = tweets_df['Text_processed'].map(lambda x: re.sub(r\"http\\S+\", \"\", x))\r\n\r\n# Print the processed titles of the first rows \r\nprint(tweets_df['Text_processed'].head())\r\n0    we all have the ability to take small actions ...\r\n1    as we head into halloween weekend, i encourage...\r\n2    sadly,  new deaths related to covid were also ...\r\n3    over the past  hours, we idd  new cases amp co...\r\n4    here is a summary of the latest covidab number...\r\nName: Text_processed, dtype: object\r\n\r\nSo this is what’s happening over time\r\n\r\n\r\nShow code\r\n\r\n#Change datetime format to datetime\r\ntweets_df['Datetime'] = pd.to_datetime(tweets_df['Datetime'])\r\n\r\n#Extract month from datetime\r\ntweets_df['Month'] = tweets_df['Datetime'].dt.month\r\n\r\n# Group the papers by year\r\ngroups = tweets_df.groupby('Month')\r\n\r\n# Determine the size of each group\r\ncounts = groups.size()\r\n\r\n# Visualize the counts as a bar plot\r\n\r\n# Vertical lines\r\nplt.axvline(x = 7.0, color = 'forestgreen', label = 'The reopening date', linestyle='--')\r\nplt.axvline(x = 8.0, color = 'firebrick', label = 'Wave 4 started', linestyle='--')\r\nplt.legend(bbox_to_anchor = (1.0, 1), loc = 'upper right')\r\n\r\nplt.title(\"Tweet count across months\")\r\nplt.ylabel(\"Tweet count\")\r\nplt.xlabel(\"Month\")\r\ncounts.plot()\r\nplt.show()\r\n\r\n\r\nThe line plot above represents tweet counts across months after the provincial reopening date. The x-axis indicates months and the y-axis indicates the number of twitter post of Dr. Hinshaw. The number of tweet dropped slightly from July to August as cases decreased, but wave 4 of the pandemic started in August as cases were on the rise again. We can see that the number of cases aligns with the number of tweets posted on Dr. Hinshaw’s account.\r\nLet’s see the big picture with word cloud\r\nNow that we know the frequency of tweets over months, we can plot a word cloud from our processed text to see the big picture of twitter data. There are 114,362 words in total after combining all 538 tweets together. The word cloud below suggests that “covid” was mentioned the most during the past four months, following by “vaccine”, “new cases”, and “unvaccinated”.\r\n\r\n\r\nShow code\r\n\r\ntext_all = \" \".join(tweet for tweet in tweets_df.Text_processed)\r\nprint (\"There are {} words in the combination of all tweets\".format(len(text_all)))\r\n\r\n#lemmatize all words\r\nThere are 114362 words in the combination of all tweets\r\n\r\nShow code\r\nlemmatizer = WordNetLemmatizer()\r\ntext_all = \"\".join([lemmatizer.lemmatize(i) for i in text_all])\r\n\r\n# Create Stopword list:\r\nstopwords_cloud = set(STOPWORDS)\r\nstopwords_cloud.update([\"https://\", \"(/)\", \"Online:\", \r\n                        \"Twitter:\", \"Join\", \"us\", \"virtually\",\r\n                        \"pm\", \":\", \"https\", \"t\", \"d\", \"co\", \"amp\", \"will\"])\r\n                      \r\n#Generate a word cloud image\r\nwordcloud_tweet = WordCloud(stopwords=stopwords_cloud, background_color=\"white\",random_state=7).generate(text_all)\r\n\r\n#Display the generated image:\r\n#the matplotlib way:\r\nplt.figure(figsize=[10,10])\r\nplt.imshow(wordcloud_tweet, interpolation='bilinear')\r\nplt.axis(\"off\")\r\n(-0.5, 399.5, 199.5, -0.5)\r\n\r\nShow code\r\nplt.show()\r\n\r\n\r\nThe thing is, word cloud can only provide a rough visual presentation for the characteristics of our textual data. We would need to dive a little bit deeper to graphs and numbers to examine what is truly going on. Let us visualize them all on a bar plot.\r\nCommon word bar plot and text preprocessing for topic modeling\r\n\r\n\r\nShow code\r\n\r\n# Helper function to count common words\r\n\r\ndef plot_10_most_common_words(count_data, count_vectorizer):\r\n    import matplotlib.pyplot as plt\r\n    words = count_vectorizer.get_feature_names()\r\n    total_counts = np.zeros(len(words))\r\n    for t in count_data:\r\n        total_counts+=t.toarray()[0]\r\n    \r\n    count_dict = (zip(words, total_counts))\r\n    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\r\n    words = [w[0] for w in count_dict]\r\n    counts = [w[1] for w in count_dict]\r\n    x_pos = np.arange(len(words)) \r\n\r\n    plt.bar(x_pos, counts,align='center')\r\n    plt.xticks(x_pos, words, rotation=90) \r\n    plt.xlabel('words')\r\n    plt.ylabel('counts')\r\n    plt.title('10 most common words')\r\n    plt.show()\r\n\r\n#Make your own list of stop words\r\nmy_additional_stop_words = (\"https://\", \"(/)\", \"➡Online:\", \r\n                        \"➡Twitter:\", \"Join\", \"us\", \"virtually\",\r\n                        \"pm\", \":\", \"https\", \"t\", \"d\", \"co\", \"amp\", \"today\", \"new\")\r\n                        \r\nstop_words_lda = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)    \r\n\r\n# Initialize the count vectorizer with the English stop words\r\ncount_vectorizer = CountVectorizer(stop_words=stop_words_lda)\r\n\r\n# Fit and transform the processed titles\r\ncount_data = count_vectorizer.fit_transform(tweets_df['Text_processed'])\r\n\r\n# Visualise the 10 most common words\r\nplot_10_most_common_words(count_data, count_vectorizer)\r\n\r\n\r\nThe bar plot above gave us a more detailed information of which word occurs more frequently than the others. We are taking about the COVID-19 pandemic here, so it is obvious that “covid” is going to be mentioned the most in Dr. Hinshaw’s tweet. “Vaccinated” and “cases” seem to be reasonable to be mentioned as the second- and third-most frequent words as the government of Alberta has been putting more effort in increasing the vaccination rate and identifying more cases in the province.\r\nWe will also create a count_vectorizer model with our own list of stopwords (or words that have little meaning such as “is, am, are”) to prepare our data for Latent Dirichlet Allocation (LDA) topic modeling.\r\nFinally, let’s see potential topics from Dr. Hinshaw’s tweet\r\nLatent Dirichlet Allocation is a powerful natural language processing technique that discovers hidden patterns in topic from unstructured textual data with statistical models (Jelodar et al., 2019).\r\nHere, we can use LDA to discover potential topics among the sea of tweets posted by Dr. Hinshaw to find out what she talked about since the provincial reopening and wave 4 of the pandemic. I have specified the model to extract 8 topics from the data, with 5 words per topics. Note that these numbers are arbitrary chosen.\r\nIf we extracted too few topics, we might not be able to capture the whole picture of the data. On the other hand, extracting too much topics could just give us more of the same overlapping themes. We need to find the middle ground.\r\n\r\n\r\nShow code\r\n\r\n# Helper function to print out the topics\r\ndef print_topics(model, count_vectorizer, n_top_words):\r\n    words = count_vectorizer.get_feature_names()\r\n    for topic_idx, topic in enumerate(model.components_):\r\n        print(\"\\nTopic #%d:\" % topic_idx)\r\n        print(\" \".join([words[i]\r\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\r\n                        \r\n#How many topic and words per topic we want to see\r\nnumber_topics = 8\r\nnumber_words = 5 \r\n                      \r\n# Create and fit the LDA model\r\nlda = LDA(n_components=number_topics, random_state = 1)\r\nlda.fit(count_data)\r\n\r\n# Print the topics found by the LDA model\r\nLatentDirichletAllocation(n_components=8, random_state=1)\r\n\r\nShow code\r\nprint_topics(lda, count_vectorizer, number_words)\r\n\r\nTopic #0:\r\ncases tests idd data uptodate\r\n\r\nTopic #1:\r\ncases tests completed idd covidab\r\n\r\nTopic #2:\r\nvaccinated covid health fully care\r\n\r\nTopic #3:\r\nvaccinated unvaccinated covid partially icu\r\n\r\nTopic #4:\r\nvaccines health offer care need\r\n\r\nTopic #5:\r\nonline covidab follow update twitter\r\n\r\nTopic #6:\r\ndeaths reported sadly covid health\r\n\r\nTopic #7:\r\ncovid measures health continue work\r\n\r\nWrapping up here. What can we conclude?\r\nThe topics we discovered above can be inferred as follows:\r\nTopic 0: Data of identified cases must be up to date.\r\nTopic 1: Case identification (ID’d) via testing\r\nTopic 2: How fully vaccinated people could take care of themselves.\r\nTopic 3: The proportion of unvaxxed vs vaxxed patients in ICU.\r\nTopic 4: The availability of vaccine as offerred by the province.\r\nTopic 5: Dr.Hinshaw urged people to follow covid news online via twitter.\r\nTopic 6: Covid-related death.\r\nTopic 7: The province continues to work on health measure against covid.\r\n\r\nThe insights that we gained could also be further supported by opinion from public health experts as they could provide information at a greater depth into their field.\r\nFrom what we have discussed so far, we can see that with the right tool, LDA for our case, we could take advantage of the vast availability of textual data that revolves around us in our everyday lives and use that information to deepen our understanding of social phenomena. We could explore how students opinion changed from pre- to post-covid era, or we could use this technique to media transcription of social events such as political protests, election speech, or even product review in the marketing field. Thank you for your reading!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds/word-cloud-pv.png",
    "last_modified": "2021-12-04T21:43:52-07:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1920
  },
  {
    "path": "posts/2021-11-07-introducing-dialectical-pluralism/",
    "title": "Finding a home among the paradigm push-back with Dialectical Pluralism",
    "description": "This entry discusses the reconciliation of quantitative and qualitative worldviews amidst the paradigm wars with pluralistic stance.\n\n(2 min read)",
    "author": [
      {
        "name": "Tarid Wongvorachan",
        "url": {}
      }
    ],
    "date": "2021-11-09",
    "categories": [
      "Mixed methods research"
    ],
    "contents": "\r\n\r\nContents\r\nThe current status-quo of paradigm wars\r\nThen, what should we do about it?\r\nWhat about the how?\r\nFinal remarks and food for thought\r\n\r\nPhoto by Relevant Insight LLCThe current status-quo of paradigm wars\r\nIn learning of mixed methods research, the issue of paradigm wars is usually brought up in how it affects us, scholars, and the academia as a whole. I occasionally came across the over-endorsement of one method (oftentimes quantitative) to another in my conversation with colleagues like, “Psychology is the field that is largely explained by quantitative research”, “We do not buy the notion that p-value can conclude anything beyond numbers”; these statements are the byproduct of paradigm wars, a methodological preference that divided academic community to these days (Williams, 2020). Implicit as it may be, the remnant of this conflict is still there.\r\nThen, what should we do about it?\r\nAt methodological level, Mixed Methods Research (MMR) seems to be a realistic answer to this problem by synergizing both qualitative and quantitative data into a greater whole than the sum of the two (Fetters & Freshwater, 2015). One philosophical mechanism behind MMR is Dialectical Pluralism (DP), a process philosophy that serves as a middle ground for both qualitative and quantitative paradigms for equal acknowledgement while offers equal voice to paradigm with lesser recognition.\r\nAn interesting suggestion from Shannon-Baker (2016) is that qualitative or quantitative view should be used as our regard to data rather than the whole research; for that, we should consider our research framework as a fluid “stance” to research instead of an archaic notion of static concept.\r\nWhat about the how?\r\nAs I realize the existence of multiple realities in social science, the next step could be to move beyond our current paradigm as Johnson (2017) suggests, “the next theoretical step in the paradigms dialog is to articulate a metaparadigm” (p.159). Researchers are encouraged to apply the dialectic worldview in their practice, both interpersonally and intrapersonally.\r\nInterpersonally, researchers could seek a collaborative space with heterogeneous team members, and conclusions should be made based on the evidence of shared values. Ultimately, practical truths should be emphasized instead of absolute truths.\r\nIntrapersonally, researchers could have internal dialogues that reconcile differences of their perspective through personal reflection. The more ideas a researcher has in their toolbox, the more versatile they can be, as Fitzgerald (1936) said, “the test of a first-rate intelligence is the ability to hold two opposed ideas in the mind at the same time and still retain the ability to function”.\r\nFinal remarks and food for thought\r\nTo me, learning about DP is quite impactful as it offers a middle ground for seemingly conflicting ideas to collaborate equally to further the body of knowledge than wresting internally over the some-old quarrel. That, my friend, could be a promising way for us to grow up as a versatile individuals (and researchers). Have a good one!\r\n\r\n\r\n\r\n",
    "preview": "https://www.relevantinsights.com/wp-content/uploads/2020/05/Qualitative-Quantitative-Research-For-New-Product-Development.png",
    "last_modified": "2021-11-21T21:58:02-07:00",
    "input_file": {}
  }
]
