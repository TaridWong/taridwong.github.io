<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>Tarid Wongvorachan</title>
    <link>https://taridwong.github.io/</link>
    <atom:link href="https://taridwong.github.io/index.xml" rel="self" type="application/rss+xml"/>
    <description>Blogs about what I learned from my academic lives
</description>
    <generator>Distill</generator>
    <lastBuildDate>Thu, 28 Apr 2022 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Addressing Data Imbalance with Semi-Supervised Learning</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-28-semisupervised</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://blogs.sas.com/content/subconsciousmusings/2020/12/09/machine-learning-algorithm-use/"&gt;Machine Learning (ML)&lt;/a&gt; is a process of learning the best possible and most relevant patterns, relationships, or associations from a data set to predict the outcomes on unseen data. Broadly, there are three types of ML process:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;1.  Supervised Learning, which is a process that trains a ML model on a labelled data set. The model aims to find the relationships among the independent and dependent variable to predict unseen data we may receive in the future.  
2.  Unsupervised Learning, which is a process of training a ML model on a dataset in which the target variable is not known. The model aims to find the most relevant patterns in the data or the segments of data.  
3.  Semi-Supervised Learning, which is a combination of supervised and unsupervised learning processes in which the unlabeled data is used to train a model as well. In this approach, the properties of unsupervised learning are used to learn the best possible representation of data, and the properties of supervised learning are used to learn the relationships to make predictions.&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Each ML algorithm has its own use under the consideration of a) the size, quality, and nature of data, b) The available computational time, c) The urgency of the task, and d) the expected result. Among the vast availability of algorithms, each ML type has its own role in tackling different types of data science problem depending on the mentioned considerations. The ML algorithm cheat sheet below is a good starting point to choose algorithms that are appropriate for your specific problems.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-semisupervisedmldiagram.png" style="width:60.0%" alt="" /&gt;
&lt;p class="caption"&gt;Image from &lt;a href="https://blogs.sas.com/content/subconsciousmusings/2020/12/09/machine-learning-algorithm-use/" class="uri"&gt;https://blogs.sas.com/content/subconsciousmusings/2020/12/09/machine-learning-algorithm-use/&lt;/a&gt;. No copyright infringement is intended&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;We have previously discussed about the use supervised and unsupervised ML separately in other posts. However, the problem we will tackle in this post requires the ability of both unsupervised and supervised learning, which is why I will use semi-supervised learning in this post to solve data imbalance problem by generating a pseudo-labeled data and use it to train an ML model to perform a classification.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-semisupervisedsemi-ml.png" style="width:70.0%" alt="" /&gt;
&lt;p class="caption"&gt;Image from &lt;a href="https://teksands.ai/blog/semi-supervised-learning" class="uri"&gt;https://teksands.ai/blog/semi-supervised-learning&lt;/a&gt;. No copyright infringement is intended&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="data-set-preparation"&gt;Data Set Preparation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Semi-supervised learning is a restatement of the missing data imputation problem, which is specific to small data set with missing-label cases. This problem is commonly encountered in data set generation contexts as retrieving clean data can be costly and time consuming. Applying supervised machine learning techniques to small data set might yield poor results that cannot be further used; thus, it would be more useful to address this problem with a combination of both machine learning approaches (i.e., unsupervised and supervised) for optimal results.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For this post, I will be using the &lt;a href="https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud"&gt;Credit Card Fraud Detection&lt;/a&gt; data set from the &lt;a href="https://mlg.ulb.ac.be/"&gt;Université Libre de Bruxelles (Brussels, Belgium) machine learning group&lt;/a&gt; The dataset contains transactions made by credit cards in September 2013 by European cardholders. This dataset presents transactions that occurred in two days, where we have 492 fraud transactions out of 284,807 transactions. We will first importing in Python modules and the data set as usual.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;from keras.layers import Input, Dense
from keras.models import Model, Sequential
from keras import regularizers
from sklearn.model_selection import train_test_split 
from sklearn import metrics
from sklearn.metrics import classification_report, accuracy_score
from sklearn.manifold import TSNE
from sklearn import preprocessing 
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix

import matplotlib.pyplot as plt
import pandas as pd 
import numpy as np
import seaborn as sns&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;sns.set(style=&amp;quot;whitegrid&amp;quot;)
np.random.seed(203)

data = pd.read_csv(&amp;quot;creditcard.csv&amp;quot;)
data[&amp;quot;Time&amp;quot;] = data[&amp;quot;Time&amp;quot;].apply(lambda x : x / 3600 % 24)
data.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       Time        V1        V2        V3  ...       V27       V28  Amount  Class
0  0.000000 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0
1  0.000000  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69      0
2  0.000278 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66      0
3  0.000278 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50      0
4  0.000556 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99      0

[5 rows x 31 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;To check if the data is imbalance, we will count the target variable by its value (i.e., 0, 1). The result below indicates that the data is highly imbalance as only 0.17 % cases are fraud transactions. This problem could lower performance of a supervised ML model as the machine learns only one class (0) and discard the other class as noises.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;vc = data[&amp;#39;Class&amp;#39;].value_counts().to_frame().reset_index()
vc[&amp;#39;percent&amp;#39;] = vc[&amp;quot;Class&amp;quot;].apply(lambda x : round(100*float(x) / len(data), 2))
vc = vc.rename(columns = {&amp;quot;index&amp;quot; : &amp;quot;Target&amp;quot;, &amp;quot;Class&amp;quot; : &amp;quot;Count&amp;quot;})
vc&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Target   Count  percent
0       0  284315    99.83
1       1     492     0.17&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We will also sample 1000 non-fraud data and merge it with fraud data to form a data set for this task.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;non_fraud = data[data[&amp;#39;Class&amp;#39;] == 0].sample(1000)
fraud = data[data[&amp;#39;Class&amp;#39;] == 1]

df = non_fraud.append(fraud).sample(frac=1).reset_index(drop=True)
X = df.drop([&amp;#39;Class&amp;#39;], axis = 1).values
Y = df[&amp;quot;Class&amp;quot;].values  &lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="visualize-fraud-and-nonfraud-transactions"&gt;Visualize Fraud and NonFraud Transactions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Let’s visualize the nature of fraud and non-fraud transactions using T-SNE. T-SNE (t-Distributed Stochastic Neighbor Embedding) is an unsupervised technique primarily used for data exploration and visualizing high-dimensional data. The technique reduces dimensionality of the data set and produces only components with maximum information. Every dot in the following represents a transaction. Non Fraud transactions are coded as “0” - the red dot, while Fraud transactions are coded as “1” - the blue dot. The two axis are the components extracted by T-SNE.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;tsne = TSNE(n_components=2, random_state=0)

TSNE_result = tsne.fit_transform(X)

plt.figure(figsize=(16,10))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1600x1000 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;sns.scatterplot(TSNE_result[:,0], TSNE_result[:,1], hue=Y, legend=&amp;#39;full&amp;#39;, palette=&amp;quot;hls&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;AxesSubplot:&amp;gt;

C:\Users\tarid\ANACON~1\lib\site-packages\seaborn\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
  warnings.warn(&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The graph above shoes that there are many non_fraud transactions which are very close to fraud transactions. This pattern makes the data challenging to classify as they are ambiguous. This problem can be mitigated by using AutoEncoders, which is another unsupervised learning algorithm.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="autoencoders"&gt;AutoEncoders&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Autoencoders are a special type of neural network architectures in which the output is same as the input. In other words, an autoencoder, once trained on appropriate training data, can be used to generate compressed copies of input data point while preserve most of the information (features) from the input. For our case, the model will try to learn the best representation of non-fraud cases and generate the representations of fraud cases.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As for how it works, Autoencoders are designed to have a bottle-neck architecture with a few neurons to comprehensively compress the knowledge about representation of the original input for the machine to reproduce it by using decoders. The picture below shows architecture of the machine in how it learns from the input data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2022-04-28-semisupervisedautoencoder.png" style="width:60.0%" alt="Image from https://www.jeremyjordan.me/autoencoders. No copyright infringement is intended" /&gt;&lt;br /&gt;
- First, we will create a network with one input layer and one output layer. Both of them will have identical dimensions.&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;## input layer 
input_layer = Input(shape=(X.shape[1],))

## encoding part
encoded = Dense(100, activation=&amp;#39;tanh&amp;#39;, activity_regularizer=regularizers.l1(10e-5))(input_layer)
encoded = Dense(50, activation=&amp;#39;relu&amp;#39;)(encoded)

## decoding part
decoded = Dense(50, activation=&amp;#39;tanh&amp;#39;)(encoded)
decoded = Dense(100, activation=&amp;#39;tanh&amp;#39;)(decoded)

## output layer
output_layer = Dense(X.shape[1], activation=&amp;#39;relu&amp;#39;)(decoded)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Next, we will create the model architecture by compiling the above input layer and output layers. We will also add the optimizer and loss function. I am using “adadelta” as the optimizer and “mse” as the loss function. I will also transform the data with min-max scaling to make it easier for the model to process.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;autoencoder = Model(input_layer, output_layer)
autoencoder.compile(optimizer=&amp;quot;adadelta&amp;quot;, loss=&amp;quot;mse&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;x = data.drop([&amp;quot;Class&amp;quot;], axis=1)
y = data[&amp;quot;Class&amp;quot;].values

x_scale = preprocessing.MinMaxScaler().fit_transform(x.values)
x_norm, x_fraud = x_scale[y == 0], x_scale[y == 1]&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;One advantage of using autoencoders is that we do not need a large amount of data to teach the machine. We will use only 2000 rows of non fraud cases to train the autoencoder. The choice of small samples from the original data set is based on the intuition that one class characteristics (non fraud) will differ from that of the other (fraud). To distinguish these characteristics, we need to show the autoencoders only one class of data. This is because the autoencoder will try to learn only one class and automatically distinguish the other class.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;autoencoder.fit(x_norm[0:2000], x_norm[0:2000], 
                batch_size = 256, epochs = 10, 
                shuffle = True, validation_split = 0.20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch 1/10

1/7 [===&amp;gt;..........................] - ETA: 3s - loss: 0.2599
7/7 [==============================] - 1s 32ms/step - loss: 0.2596 - val_loss: 0.2582
Epoch 2/10

1/7 [===&amp;gt;..........................] - ETA: 0s - loss: 0.2595
7/7 [==============================] - 0s 8ms/step - loss: 0.2593 - val_loss: 0.2578
Epoch 3/10

1/7 [===&amp;gt;..........................] - ETA: 0s - loss: 0.2595
7/7 [==============================] - 0s 7ms/step - loss: 0.2590 - val_loss: 0.2575
Epoch 4/10

1/7 [===&amp;gt;..........................] - ETA: 0s - loss: 0.2587
7/7 [==============================] - 0s 8ms/step - loss: 0.2586 - val_loss: 0.2571
Epoch 5/10

1/7 [===&amp;gt;..........................] - ETA: 0s - loss: 0.2586
7/7 [==============================] - 0s 8ms/step - loss: 0.2583 - val_loss: 0.2568
Epoch 6/10

1/7 [===&amp;gt;..........................] - ETA: 0s - loss: 0.2581
7/7 [==============================] - 0s 8ms/step - loss: 0.2579 - val_loss: 0.2564
Epoch 7/10

1/7 [===&amp;gt;..........................] - ETA: 0s - loss: 0.2572
7/7 [==============================] - 0s 8ms/step - loss: 0.2575 - val_loss: 0.2560
Epoch 8/10

1/7 [===&amp;gt;..........................] - ETA: 0s - loss: 0.2569
7/7 [==============================] - 0s 8ms/step - loss: 0.2571 - val_loss: 0.2556
Epoch 9/10

1/7 [===&amp;gt;..........................] - ETA: 0s - loss: 0.2566
7/7 [==============================] - 0s 8ms/step - loss: 0.2567 - val_loss: 0.2551
Epoch 10/10

1/7 [===&amp;gt;..........................] - ETA: 0s - loss: 0.2571
7/7 [==============================] - 0s 8ms/step - loss: 0.2562 - val_loss: 0.2547
&amp;lt;keras.callbacks.History object at 0x000000000D36F8B0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="obtain-the-generated-data"&gt;Obtain the Generated Data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;After training the model, we can obtain representation of the data (aka its latent pattern) to learn how the data is distributed by accessing the model. A latent variable is a random variable that cannot be observed directly, but it lays the foundation of how the data is distributed. Latent variables also give us a low-level representation of high-dimensional data. They give us an abstract representation of how the data is distributed. We will also extract the generated “fraud” and “non-fraud” data to create a new data set to be used. We will also visualize the new data for comparison.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;hidden_representation = Sequential()
hidden_representation.add(autoencoder.layers[0])
hidden_representation.add(autoencoder.layers[1])
hidden_representation.add(autoencoder.layers[2])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;norm_hid_rep = hidden_representation.predict(x_norm[:3000])
fraud_hid_rep = hidden_representation.predict(x_fraud)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;new_x = np.append(norm_hid_rep, fraud_hid_rep, axis = 0)
new_y_not_fraud = np.zeros(norm_hid_rep.shape[0])
new_y_fraud = np.ones(fraud_hid_rep.shape[0])
new_y = np.append(new_y_not_fraud, new_y_fraud)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;TSNE_result_new = tsne.fit_transform(new_x)
plt.figure(figsize=(16,10))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1600x1000 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;sns.scatterplot(TSNE_result_new[:,0], TSNE_result_new[:,1], hue=new_y, legend=&amp;#39;full&amp;#39;, palette=&amp;quot;hls&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;AxesSubplot:&amp;gt;

C:\Users\tarid\ANACON~1\lib\site-packages\seaborn\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
  warnings.warn(&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The new TSNE plot shows that both fraud and non-fraud transactions are pretty visible and linearly separable. We can use simple linear models such as logistic regression to classify this data without the needs for complex models like Random Forest that could be harder to interpret.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="linear-classifier"&gt;Linear Classifier&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;As usual, we will divide the new data set into a training and a testing asset before passing them to a logistic regression model. We will request for a Receiver Operator Characteristic (ROC) curve to evaluate accuracy of our classifier. We will also request for a confusion matrix to map out a summary of prediction results.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;X_train, X_test, y_train, y_test = train_test_split(new_x, new_y, test_size=0.3)

clf_lr = LogisticRegression(max_iter = 450, random_state = 123)
clf_lr.fit(X_train,y_train)  &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;LogisticRegression(max_iter=450, random_state=123)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;pred_lr = clf_lr.predict(X_test)
print(&amp;quot;Accuracy: {:0.4f}&amp;quot;.format(accuracy_score(y_test, pred_lr)))  &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Accuracy: 0.9828&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;report_lr = classification_report(y_test, pred_lr)
print(report_lr)  &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;              precision    recall  f1-score   support

         0.0       0.98      1.00      0.99       894
         1.0       1.00      0.88      0.94       154

    accuracy                           0.98      1048
   macro avg       0.99      0.94      0.96      1048
weighted avg       0.98      0.98      0.98      1048&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;y_pred_proba_lr = clf_lr.predict_proba(X_test)[::,1]
fpr_lr, tpr_lr, _ = metrics.roc_curve(y_test,  y_pred_proba_lr)

auc_lr = metrics.roc_auc_score(y_test, y_pred_proba_lr)
plt.plot(fpr_lr,tpr_lr, label=&amp;quot;AUC for Logistic Regression = &amp;quot;+str(auc_lr.round(3)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;lt;matplotlib.lines.Line2D object at 0x000000000C25F130&amp;gt;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.legend(loc=&amp;quot;lower right&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.legend.Legend object at 0x000000000C24DE50&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;True Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;True Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.xlabel(&amp;#39;False Positive Rate&amp;#39;)
           &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;False Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Receiver-Operator Curve (ROC)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Receiver-Operator Curve (ROC)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f82da2519_files/figure-html/unnamed-chunk-17-1.png" width="1536" /&gt;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;conf_matrix = confusion_matrix(y_test, pred_lr)
sns.heatmap(conf_matrix.T, square=True, annot=True, fmt=&amp;#39;d&amp;#39;, cbar=False)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;AxesSubplot:&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.xlabel(&amp;#39;true label&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 77.24999999999999, &amp;#39;true label&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;predicted label&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(402.25000000000006, 0.5, &amp;#39;predicted label&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Confusion matrix of Logistic Regression&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Confusion matrix of Logistic Regression&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f82da2519_files/figure-html/unnamed-chunk-18-3.png" width="1536" /&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Our logistic regression model shows a satisfaction result with the accuracy of 99.52%. The application of autoencoders as the unsupervised part in this task shows that semi-supervised learning can be effective at handling the case where there is a lack of labeled data to train our machine. Manual data labeling by human is an expensive and time-consuming process, which is why this approach to machine learning could be very helpful in lessening the task on our end. Semi-supervised learning can be used for the classification of web pages, audio, images, or even textual and numerical data where human labeling is not possible. As always, thank you for reading this!&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>5b75cb2ac18e56d7a34e19c0032dbc97</distill:md5>
      <category>Python</category>
      <category>Unsupervised Machine Learning</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-28-semisupervised</guid>
      <pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-28-semisupervised/semi-ml.png" medium="image" type="image/png" width="900" height="450"/>
    </item>
    <item>
      <title>Examining Customer Cluster with Unsupervised Machine Learning</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-18-unsupervisedml</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A lot of my recent posts are about supervised machine learning problems, which is defined by its use of labeled datasets to train algorithms to classify or predict outcomes of the unseen data set. In other words, the model was trained (or supervised) by humans to achieve the best possible predictive capability. However, unsupervised machine learning is defined in opposition to supervised learning. Unsupervised learning, in contrast, is learning without labels. It is pure pattern discovery, unguided by a prediction task. The model learns from raw data without any prior knowledge or human training.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For example, you have a group of customers with a variety of characteristics such as age, location, and financial history. You wish to discover patterns and sort them into natural “clusters” without tampering with them in any ways. Or perhaps you have a set of texts, such as Wikipedia pages, and you wish to segment them into categories based on their content. These are examples of unsupervised learning techniques called “clustering” and “dimension reduction”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Unsupervised learning is called as such because you are not guiding the pattern discovery by some prediction task, but instead uncovering hidden structure from unlabeled data. In this post, I will be using two unsupervised learning techniques with a data set, namely K-means clustering and Hierarchical clustering, to determine groups of customers from their age, income, and spending behavior data. As usual, we will import the necessary modules first to set up the environment.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns

from sklearn.cluster import KMeans
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering 

import warnings
warnings.filterwarnings(&amp;#39;ignore&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="about-the-data-set"&gt;About the Data Set&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This data set is created by Kaggle for the learning purpose of the data segmentation concepts. The scenario is as follows: you are owning a supermarket mall. Through membership cards , you have some basic data about your customers like Customer ID, age, gender, annual income and spending score. Spending Score is something you assign to the customer based on your defined parameters like customer behavior and purchasing data. Specifically put, if you buy a lot, you get a lot of scores to be posted on a top spender board. Below is the information about the data set we will be using in this task.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;df = pd.read_csv(&amp;quot;customers.csv&amp;quot;)

# data set shape
print(&amp;quot;The data set has&amp;quot;, df.shape[0], &amp;quot;cases and&amp;quot;, df.shape[1], &amp;quot;variables&amp;quot;)

# print head of data set&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The data set has 200 cases and 5 variables&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(df.head(10))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)
0           1       1   19                  15                      39
1           2       1   21                  15                      81
2           3       0   20                  16                       6
3           4       0   23                  16                      77
4           5       0   31                  17                      40
5           6       0   22                  17                      76
6           7       0   35                  18                       6
7           8       0   23                  18                      94
8           9       1   64                  19                       3
9          10       0   30                  19                      72&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;df.info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
RangeIndex: 200 entries, 0 to 199
Data columns (total 5 columns):
 #   Column                  Non-Null Count  Dtype
---  ------                  --------------  -----
 0   CustomerID              200 non-null    int64
 1   Gender                  200 non-null    int64
 2   Age                     200 non-null    int64
 3   Annual Income (k$)      200 non-null    int64
 4   Spending Score (1-100)  200 non-null    int64
dtypes: int64(5)
memory usage: 7.9 KB&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;df.describe()

#Check for missing data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       CustomerID      Gender  ...  Annual Income (k$)  Spending Score (1-100)
count  200.000000  200.000000  ...          200.000000              200.000000
mean   100.500000    0.440000  ...           60.560000               50.200000
std     57.879185    0.497633  ...           26.264721               25.823522
min      1.000000    0.000000  ...           15.000000                1.000000
25%     50.750000    0.000000  ...           41.500000               34.750000
50%    100.500000    0.000000  ...           61.500000               50.000000
75%    150.250000    1.000000  ...           78.000000               73.000000
max    200.000000    1.000000  ...          137.000000               99.000000

[8 rows x 5 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;df.isnull().sum()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;CustomerID                0
Gender                    0
Age                       0
Annual Income (k$)        0
Spending Score (1-100)    0
dtype: int64&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We will first begin by exploring distribution of the data set to examine if their distribution is normal. Then, we can examine the distribution of each variable pair as indicated by density plots and scatter plots. We will also examine correlation coefficients between all variables with a heatmap and potential clusters with a cluster map as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.figure(1 , figsize = (15 , 6))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1600x1000 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;n = 0 
for x in [&amp;#39;Age&amp;#39; , &amp;#39;Annual Income (k$)&amp;#39; , &amp;#39;Spending Score (1-100)&amp;#39;]:
    n += 1
    plt.subplot(1 , 3 , n)
    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)
    sns.distplot(df[x] , bins = 15)
    plt.title(&amp;#39;Distplot of {}&amp;#39;.format(x))
    &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;AxesSubplot:&amp;gt;
&amp;lt;AxesSubplot:xlabel=&amp;#39;Age&amp;#39;, ylabel=&amp;#39;Density&amp;#39;&amp;gt;
Text(0.5, 1.0, &amp;#39;Distplot of Age&amp;#39;)
&amp;lt;AxesSubplot:&amp;gt;
&amp;lt;AxesSubplot:xlabel=&amp;#39;Annual Income (k$)&amp;#39;, ylabel=&amp;#39;Density&amp;#39;&amp;gt;
Text(0.5, 1.0, &amp;#39;Distplot of Annual Income (k$)&amp;#39;)
&amp;lt;AxesSubplot:&amp;gt;
&amp;lt;AxesSubplot:xlabel=&amp;#39;Spending Score (1-100)&amp;#39;, ylabel=&amp;#39;Density&amp;#39;&amp;gt;
Text(0.5, 1.0, &amp;#39;Distplot of Spending Score (1-100)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f84fbc143f_files/figure-html/unnamed-chunk-5-1.png" width="1536" /&gt;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;sns.heatmap(df.corr(), annot=True, cmap=&amp;quot;YlGnBu&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;AxesSubplot:&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f84fbc143f_files/figure-html/unnamed-chunk-6-3.png" width="1536" /&gt;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.figure(1, figsize = (16 ,8))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1600x1000 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;sns.clustermap(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;seaborn.matrix.ClusterGrid object at 0x000000000D22A1C0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;sns.pairplot(df, vars = [&amp;#39;Spending Score (1-100)&amp;#39;, &amp;#39;Annual Income (k$)&amp;#39;, &amp;#39;Age&amp;#39;], hue = &amp;quot;Gender&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;seaborn.axisgrid.PairGrid object at 0x000000000D22A1C0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="k-means-clustering"&gt;K-Means Clustering&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;K-means clustering is one of the simplest and popular unsupervised machine learning algorithms. For this method, we define a target number &lt;em&gt;&lt;strong&gt;k&lt;/strong&gt;&lt;/em&gt;, which refers to the number of centroids you need in the dataset. A centroid is the imaginary or real location representing the center of the cluster; then, the algorithm allocates every data point to the nearest cluster, while keeping the centroids as small as possible. The &lt;strong&gt;means&lt;/strong&gt; in the K-means refers to averaging of the data in finding their corresponding centroids.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Instead of using equations, this short animation by &lt;a href="https://github.com/allisonhorst/stats-illustrations"&gt;Allison Horst&lt;/a&gt; explains k-means clustering in a very cute and comprehensive way.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://media.giphy.com/media/iw6Gq7ZTM2m5rDS9Su/giphy.gif" alt="" /&gt;
&lt;p class="caption"&gt;The Process of K-means Clustering by Allison Horst. No copyright infringement intended&lt;/p&gt;
&lt;/div&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.figure(1 , figsize = (15 , 7))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1600x1000 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;#39;Scatter plot of Age v/s Spending Score&amp;#39;, fontsize = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Scatter plot of Age v/s Spending Score&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.xlabel(&amp;#39;Age&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;Age&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;Spending Score&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;Spending Score&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.scatter( x = &amp;#39;Age&amp;#39;, y = &amp;#39;Spending Score (1-100)&amp;#39;, data = df, s = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.collections.PathCollection object at 0x000000000C2684F0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f84fbc143f_files/figure-html/unnamed-chunk-9-5.png" width="1536" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clustering is basically the process of mapping data points into classes of similar objects. The scatter plot above maps the relationship between customers’ age and their spending score. It could be challenging at this point to determine how many clusters can be seen in this 2D pane. This is where clustering comes into play. We will first examine how many clusters should we group the data points into with the elbow method. To find the optimal K for a dataset, we need to find the point where the decrease in inertia begins to slow. K=3 is the “elbow” of this graph, so the points after that are K=4 or K=5.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;X1 = df[[&amp;#39;Age&amp;#39; , &amp;#39;Spending Score (1-100)&amp;#39;]].iloc[: , :].values
inertia = []

for n in range(1 , 15):
    algorithm = (KMeans(n_clusters = n ,init=&amp;#39;k-means++&amp;#39;, n_init = 10 ,max_iter=300, 
                        tol=0.0001,  random_state= 111  , algorithm=&amp;#39;full&amp;#39;) )
    algorithm.fit(X1)
    inertia.append(algorithm.inertia_)
    &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=1, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=2, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=3, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=4, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=5, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=6, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=7, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=9, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=10, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=11, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=12, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=13, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=14, random_state=111)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.figure(1 , figsize = (15 ,6))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1600x1000 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.plot(np.arange(1 , 15) , inertia , &amp;#39;o&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;lt;matplotlib.lines.Line2D object at 0x000000007FE43700&amp;gt;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.plot(np.arange(1 , 15) , inertia , &amp;#39;-&amp;#39; , alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;lt;matplotlib.lines.Line2D object at 0x000000007FEA8CD0&amp;gt;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.xlabel(&amp;#39;Number of Clusters&amp;#39;) , plt.ylabel(&amp;#39;Inertia&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(Text(0.5, 0, &amp;#39;Number of Clusters&amp;#39;), Text(0, 0.5, &amp;#39;Inertia&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f84fbc143f_files/figure-html/unnamed-chunk-10-7.png" width="1536" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The elbow plot shows that grouping the data into 4 or 5 clusters seems to be the best as the inertia, which measures how well a dataset was clustered by K-Means, seems to reach the optimal point at 4 and 5 clusters. A good model is the one with low inertia AND a low number of clusters (K). However, this is a tradeoff because as K increases, inertia decreases. We want the inertia to be as low as possible, but we don’t want to have 12+ clusters as well (I mean, why would you group your data anyway if you want it to have a lot of clusters). We can try clustering the data into 4 clusters first, then we can go with 5 later.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;algorithm = (KMeans(n_clusters = 4 ,init=&amp;#39;k-means++&amp;#39;, n_init = 10 ,max_iter=300, 
                        tol=0.0001,  random_state= 111  , algorithm=&amp;#39;full&amp;#39;) )
algorithm.fit(X1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=4, random_state=111)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;labels1 = algorithm.labels_
centroids1 = algorithm.cluster_centers_

h = 0.02
x_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1
y_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) 

plt.figure(1 , figsize = (15 , 7) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1600x1000 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.clf()
Z = Z.reshape(xx.shape)
plt.imshow(Z , interpolation=&amp;#39;nearest&amp;#39;, 
           extent=(xx.min(), xx.max(), yy.min(), yy.max()),
           cmap = plt.cm.Pastel2, aspect = &amp;#39;auto&amp;#39;, origin=&amp;#39;lower&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage object at 0x000000001061FA30&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.scatter( x = &amp;#39;Age&amp;#39;, y = &amp;#39;Spending Score (1-100)&amp;#39;, data = df, c = labels1, s = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.collections.PathCollection object at 0x00000000102AA700&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = &amp;#39;red&amp;#39; , alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.collections.PathCollection object at 0x000000001029E400&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;Spending Score (1-100)&amp;#39;) , plt.xlabel(&amp;#39;Age&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(Text(0, 0.5, &amp;#39;Spending Score (1-100)&amp;#39;), Text(0.5, 0, &amp;#39;Age&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Four Clusters&amp;quot;, loc=&amp;#39;center&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Four Clusters&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f84fbc143f_files/figure-html/unnamed-chunk-11-9.png" width="1536" /&gt;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;#%%Applying KMeans for k=5

algorithm = (KMeans(n_clusters = 5, init=&amp;#39;k-means++&amp;#39;, n_init = 10, max_iter=300, 
                        tol=0.0001, random_state= 111 , algorithm=&amp;#39;elkan&amp;#39;))
algorithm.fit(X1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;KMeans(algorithm=&amp;#39;elkan&amp;#39;, n_clusters=5, random_state=111)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;labels1 = algorithm.labels_
centroids1 = algorithm.cluster_centers_

h = 0.02
x_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1
y_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) 

plt.figure(1 , figsize = (15 , 7) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1600x1000 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.clf()
Z = Z.reshape(xx.shape)
plt.imshow(Z , interpolation=&amp;#39;nearest&amp;#39;, 
           extent=(xx.min(), xx.max(), yy.min(), yy.max()),
           cmap = plt.cm.Pastel2, aspect = &amp;#39;auto&amp;#39;, origin=&amp;#39;lower&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage object at 0x00000000100A55E0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.scatter( x = &amp;#39;Age&amp;#39;, y = &amp;#39;Spending Score (1-100)&amp;#39;, data = df, c = labels1, s = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.collections.PathCollection object at 0x00000000100A5C70&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = &amp;#39;red&amp;#39; , alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.collections.PathCollection object at 0x000000007FE57130&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;Spending Score (1-100)&amp;#39;) , plt.xlabel(&amp;#39;Age&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(Text(0, 0.5, &amp;#39;Spending Score (1-100)&amp;#39;), Text(0.5, 0, &amp;#39;Age&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Five Clusters&amp;quot;, loc=&amp;#39;center&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Five Clusters&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f84fbc143f_files/figure-html/unnamed-chunk-12-11.png" width="1536" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The two diagrams above show what it is like when we group the data into 4 and 5 clusters. It is important that the clusters are stable. Even though the algorithm begins by randomly initializing the cluster centers, if the k-means algorithm is the right choice for the data, then different runs of the algorithm will result in similar clusters in terms of size and variable distribution. If there is a lot of change in clusters between the different iterations of the algorithm, then k-means clustering may not be the right choice for the data. However, it is not possible to validate that the clusters obtained from the algorithm are accurate because there is no patient labeling; thus, it is necessary to examine how the clusters change between different iterations of the algorithm and check if the number of clusters makes sense in both theoretical and practical sense. We can also have domain experts give their opinions about if the clusters of customer make practical sense.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For one more practice, we can try making a cluster based on annual income and spending score.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;X2 = df[[&amp;#39;Annual Income (k$)&amp;#39; , &amp;#39;Spending Score (1-100)&amp;#39;]].iloc[: , :].values
inertia = []
for n in range(1 , 11):
    algorithm = (KMeans(n_clusters = n ,init=&amp;#39;k-means++&amp;#39;, n_init = 10 ,max_iter=300, 
                        tol=0.0001,  random_state= 111  , algorithm=&amp;#39;full&amp;#39;) )
    algorithm.fit(X2)
    inertia.append(algorithm.inertia_)
    &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=1, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=2, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=3, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=4, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=5, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=6, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=7, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=9, random_state=111)
KMeans(algorithm=&amp;#39;full&amp;#39;, n_clusters=10, random_state=111)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.figure(1 , figsize = (15 ,6))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1600x1000 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.plot(np.arange(1 , 11) , inertia , &amp;#39;o&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;lt;matplotlib.lines.Line2D object at 0x00000000086EC0A0&amp;gt;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.plot(np.arange(1 , 11) , inertia , &amp;#39;-&amp;#39; , alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;lt;matplotlib.lines.Line2D object at 0x00000000086EC520&amp;gt;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.xlabel(&amp;#39;Number of Clusters&amp;#39;) , plt.ylabel(&amp;#39;Inertia&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(Text(0.5, 0, &amp;#39;Number of Clusters&amp;#39;), Text(0, 0.5, &amp;#39;Inertia&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f84fbc143f_files/figure-html/unnamed-chunk-13-13.png" width="1536" /&gt;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;algorithm = (KMeans(n_clusters = 5 ,init=&amp;#39;k-means++&amp;#39;, n_init = 10 ,max_iter=300, 
                        tol=0.0001,  random_state= 111  , algorithm=&amp;#39;elkan&amp;#39;) )
algorithm.fit(X2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;KMeans(algorithm=&amp;#39;elkan&amp;#39;, n_clusters=5, random_state=111)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;labels2 = algorithm.labels_
centroids2 = algorithm.cluster_centers_

#%%
h = 0.02
x_min, x_max = X2[:, 0].min() - 1, X2[:, 0].max() + 1
y_min, y_max = X2[:, 1].min() - 1, X2[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z2 = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) 


#%%

plt.figure(1 , figsize = (15 , 7) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1600x1000 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.clf()
Z2 = Z2.reshape(xx.shape)
plt.imshow(Z2 , interpolation=&amp;#39;nearest&amp;#39;, 
           extent=(xx.min(), xx.max(), yy.min(), yy.max()),
           cmap = plt.cm.Pastel2, aspect = &amp;#39;auto&amp;#39;, origin=&amp;#39;lower&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage object at 0x000000007FA51C10&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.scatter( x = &amp;#39;Annual Income (k$)&amp;#39; ,y = &amp;#39;Spending Score (1-100)&amp;#39; , data = df , c = labels2 , 
            s = 100 )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.collections.PathCollection object at 0x000000007FA51700&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.scatter(x = centroids2[: , 0] , y =  centroids2[: , 1] , s = 300 , c = &amp;#39;red&amp;#39; , alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.collections.PathCollection object at 0x000000007FA51A90&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;Spending Score (1-100)&amp;#39;) , plt.xlabel(&amp;#39;Annual Income (k$)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(Text(0, 0.5, &amp;#39;Spending Score (1-100)&amp;#39;), Text(0.5, 0, &amp;#39;Annual Income (k$)&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Five Clusters&amp;quot;, loc=&amp;#39;center&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Five Clusters&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f84fbc143f_files/figure-html/unnamed-chunk-14-15.png" width="1536" data-distill-preview=1 /&gt;&lt;/p&gt;
&lt;h2 id="hierarchical-clustering"&gt;Hierarchical Clustering&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;An alternative to k-means clustering is hierarchical clustering (also known as hierarchical cluster analysis), which groups similar objects into hierarchies (or levels) of clusters. The end product is a set of clusters, where each cluster is distinct on its own, and the objects within each cluster are broadly similar to each other. This method works well when data have a nested structure - meaning that one characteristic is related to another (e.g., spending habit of a certain age group).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Again, &lt;a href="https://github.com/allisonhorst/stats-illustrations"&gt;Allison Horst&lt;/a&gt; did a really good job explaining how hierarchical clustering works with her visuals below. Note that the visual is for the “single” method, but we will be using the “ward” method. However, they are similar in terms of how they build a hierarchical dendrogram. A dendrogram is a diagram representing a tree that, in this context, illustrates the arrangement of the clusters produced by the corresponding analyses.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will try performing divisive hierarchical clustering first. This method is known as a top-down approach that splits a cluster that contains the whole data into smaller clusters recursively until each single data point have been splitted into singleton clusters or the termination condition holds. This method is rigid. Once a merging or splitting is done, it can never be undone.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://media.giphy.com/media/lAIiI3FGKBw8jhCHYG/giphy.gif" alt="" /&gt;
&lt;p class="caption"&gt;The Process of Hierarchical Clustering by Allison Horst. No copyright infringement intended&lt;/p&gt;
&lt;/div&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.figure(1, figsize = (16 ,8))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1600x1000 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;dendrogram = sch.dendrogram(sch.linkage(df, method  = &amp;quot;ward&amp;quot;))

plt.title(&amp;#39;Dendrogram&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Dendrogram&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.xlabel(&amp;#39;Customers&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;Customers&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;Euclidean distances&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;Euclidean distances&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f84fbc143f_files/figure-html/unnamed-chunk-15-17.png" width="1536" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Next, we will try agglomerative clustering. This method is known as a “bottom-up” approach; that is, each observation starts in its own cluster, and pairs of clusters are merged as one amd moved up the hierarchy. We start with each object forming a separate group. It keeps on merging the objects or groups that are close to one another. The iteration keeps on doing so until all of the groups are merged into one or until the termination condition holds.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;hc = AgglomerativeClustering(n_clusters = 5, affinity = &amp;#39;euclidean&amp;#39;, linkage =&amp;#39;average&amp;#39;)

y_hc = hc.fit_predict(df)
y_hc&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4,
       3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 2,
       3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 1, 2, 1, 0, 1, 0, 1,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
       0, 1], dtype=int64)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;X = df.iloc[:, [3,4]].values
plt.scatter(X[y_hc==0, 0], X[y_hc==0, 1], s=100, c=&amp;#39;red&amp;#39;, label =&amp;#39;Cluster 1&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.collections.PathCollection object at 0x00000000106BA250&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.scatter(X[y_hc==1, 0], X[y_hc==1, 1], s=100, c=&amp;#39;blue&amp;#39;, label =&amp;#39;Cluster 2&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.collections.PathCollection object at 0x00000000106BAFD0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.scatter(X[y_hc==2, 0], X[y_hc==2, 1], s=100, c=&amp;#39;green&amp;#39;, label =&amp;#39;Cluster 3&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.collections.PathCollection object at 0x00000000106BAA00&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.scatter(X[y_hc==3, 0], X[y_hc==3, 1], s=100, c=&amp;#39;purple&amp;#39;, label =&amp;#39;Cluster 4&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.collections.PathCollection object at 0x000000000C1B6D90&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.scatter(X[y_hc==4, 0], X[y_hc==4, 1], s=100, c=&amp;#39;orange&amp;#39;, label =&amp;#39;Cluster 5&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.collections.PathCollection object at 0x000000000C1B6820&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;#39;Clusters of Customers (Hierarchical Clustering Model)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Clusters of Customers (Hierarchical Clustering Model)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.xlabel(&amp;#39;Annual Income(k$)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;Annual Income(k$)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;Spending Score(1-100)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;Spending Score(1-100)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f84fbc143f_files/figure-html/unnamed-chunk-16-19.png" width="1536" /&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The main advantage of clustering over classification is that it is adaptable to changes and helps single out useful features that distinguish different groups. This is a useful tool for data scientists to understand raw data with unsupervised learning. The methods is best used for exploratory purposes for applications such as the investigation of a group that stands out the most and identify useful features that affect them. Specifically, The method can be used to discover distinct groups in their customer base, categorize genes with similar functionality, gain insight into structures inherent to populations in Biology, or even classify documents on the web for information discovery by combining clustering with natural language processing techniques. As always, thank you very much for reading this!&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>ef11dcb783b5d3275bae174044c9d031</distill:md5>
      <category>Python</category>
      <category>Unsupervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-18-unsupervisedml</guid>
      <pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-18-unsupervisedml/unsupervisedml_files/figure-html5/unnamed-chunk-14-19.png" medium="image" type="image/png" width="2880" height="1152"/>
    </item>
    <item>
      <title>Combining Multiple Machine Learning Models with the Ensemble Methods</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-09-ensemble</link>
      <description>


&lt;h2 id="what-is-ensemble-methods"&gt;What is ensemble methods?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;When you’re building a machine learning model, people generally choose the one that performs the best according to some evaluation metric such as accuracy score or &lt;a href="https://en.wikipedia.org/wiki/F-score"&gt;F1 score&lt;/a&gt;. However, choosing one model over another, say decision tree over k-nearest Neighbor (Knn) or logistic Regression, may result in us discarding strengths of the remaining models which were able to learn different patterns that might have additional useful properties.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When you conduct a survey, you don’t accept only one “best” answer. You consider a combined response of all the participants, and use statistics like the mode or the mean to represent the responses. The combined responses will likely lead to a better decision than relying on a single response. The same principle applies to ensemble methods, where we could form a new model by combining the existing ones. The combined model will have better performance than any of the individual models, or at least, be as good as the best individual model. In other words, ensemble learning methods is the usage of multiple machine learning models to maximize their predictive capability.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In this post, I will be exploring the usage of ensemble machine learning models to predict which mushrooms are edible based on their properties (e.g., cap size, color, odor). The data set is from the UC-Irvine Machine Learning repository and is currently distributed for practice on &lt;a href="https://www.kaggle.com/datasets/uciml/mushroom-classification"&gt;Kaggle&lt;/a&gt;. I will explore the usage of one model, model voting, bootstrap aggregating (aka bagging), model boosting, and model stacking via Python.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-09-ensemblerobot.png" style="width:40.0%" alt="" /&gt;
&lt;p class="caption"&gt;Image from &lt;a href="https://www.irasutoya.com" class="uri"&gt;https://www.irasutoya.com&lt;/a&gt;. No copyright infringement is intended&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="setting-up-the-environment-and-data-set"&gt;Setting up the Environment and Data set&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Importing the necessary modules.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB 
from sklearn.tree import DecisionTreeClassifier
from mlxtend.classifier import StackingClassifier
import xgboost as xgb

from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import classification_report&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Importing the data set and examine the distribution of our targeted variable - the mushroom class. E stands for edible and P stands for poisonous.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;df = pd.read_csv(&amp;quot;mushrooms.csv&amp;quot;)

# data set shape
print(&amp;quot;There are {} rows and {} columns in this dataset&amp;quot;.format(df.shape[0], df.shape[1]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;There are 8124 rows and 23 columns in this dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;df.info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
RangeIndex: 8124 entries, 0 to 8123
Data columns (total 23 columns):
 #   Column                    Non-Null Count  Dtype 
---  ------                    --------------  ----- 
 0   class                     8124 non-null   object
 1   cap-shape                 8124 non-null   object
 2   cap-surface               8124 non-null   object
 3   cap-color                 8124 non-null   object
 4   bruises                   8124 non-null   object
 5   odor                      8124 non-null   object
 6   gill-attachment           8124 non-null   object
 7   gill-spacing              8124 non-null   object
 8   gill-size                 8124 non-null   object
 9   gill-color                8124 non-null   object
 10  stalk-shape               8124 non-null   object
 11  stalk-root                8124 non-null   object
 12  stalk-surface-above-ring  8124 non-null   object
 13  stalk-surface-below-ring  8124 non-null   object
 14  stalk-color-above-ring    8124 non-null   object
 15  stalk-color-below-ring    8124 non-null   object
 16  veil-type                 8124 non-null   object
 17  veil-color                8124 non-null   object
 18  ring-number               8124 non-null   object
 19  ring-type                 8124 non-null   object
 20  spore-print-color         8124 non-null   object
 21  population                8124 non-null   object
 22  habitat                   8124 non-null   object
dtypes: object(23)
memory usage: 1.4+ MB&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;class_dict= {&amp;#39;e&amp;#39;: &amp;#39;edible&amp;#39; , &amp;#39;p&amp;#39;:&amp;#39;poisonous&amp;#39;}
df[&amp;#39;class&amp;#39;] = df[&amp;#39;class&amp;#39;].map(class_dict)

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme(style=&amp;quot;darkgrid&amp;quot;)
ax = sns.countplot(x=&amp;quot;class&amp;quot;, data = df)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f852b16296_files/figure-html/unnamed-chunk-4-1.png" width="1536" /&gt;&lt;/p&gt;
&lt;h2 id="data-preprocessing"&gt;Data Preprocessing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The thing is, the data point is still coded in strings (i.e., alphabets, words or other characters). We need to re-code them into the numerical format for the machine to recognize patterns of the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;
from sklearn.preprocessing import LabelEncoder
labelencoder=LabelEncoder()

for col in df.columns:
    df[col] = labelencoder.fit_transform(df[col])

#The machine does not know about mushrooms. It only knows pattern of the data as reflected by the type of occurrence.

#Checking the encoded values
df[&amp;#39;stalk-color-above-ring&amp;#39;].unique()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([7, 3, 6, 4, 0, 2, 5, 1, 8])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(df.groupby(&amp;#39;class&amp;#39;).size())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;class
0    4208
1    3916
dtype: int64&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;I have re-coded labels of the data into numbers, with 0 means edible, 1 means poisonous.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;sns.set_theme(style=&amp;quot;darkgrid&amp;quot;)
ax = sns.countplot(x=&amp;quot;class&amp;quot;, data = df)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f852b16296_files/figure-html/unnamed-chunk-6-3.png" width="1536" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As always, I will create a training and a testing set for the models to learn from.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;
RANDOM_STATE = 123

X = df.drop(&amp;#39;class&amp;#39;, axis=1) #features
y = df[&amp;#39;class&amp;#39;] #label

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, 
                                                    random_state = RANDOM_STATE)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="performance-of-a-single-model"&gt;Performance of a Single Model&lt;/h2&gt;
&lt;h3 id="naive-bayes"&gt;Naive Bayes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;First, I will test out the performance of Naive Bayes Classifier, which is one of the simplest machine learning algorithms in the field. The algorithm assumes that all predictor variables are independent to each other;this is not usually possible in the practical context; hence the name naive. However, Naive Bayes is fast, does not require much data, and can achieve great results if the assumption holds.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;#Instantiate a Naive Bayes classifier
clf_nb = GaussianNB()

# Fit the model to the training set
clf_nb.fit(X_train,y_train)

# Calculate the predictions on the test set&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;GaussianNB()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;pred_nb = clf_nb.predict(X_test)

# Evaluate the performance using the accuracy score
print(&amp;quot;Accuracy: {:0.4f}&amp;quot;.format(accuracy_score(y_test, pred_nb)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Accuracy: 0.9130&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;F1: {:0.4f}&amp;quot;.format(f1_score(y_test, pred_nb)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;F1: 0.9091&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="logistic-regression"&gt;Logistic Regression&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Next, I will try performing Logistic Regression to with the same task to see if the result if going to be different. Logistic Regression is a regression model that predicts the probability of which category an input data point belongs to. The algorithm is one of the simplest and most commonly used models for for various classification problems such as spam detection or Diabetes prediction.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;clf_lr = LogisticRegression(max_iter = 450, random_state = RANDOM_STATE)

clf_lr.fit(X_train,y_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;LogisticRegression(max_iter=450, random_state=123)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;pred_lr = clf_lr.predict(X_test)

print(&amp;quot;Accuracy: {:0.4f}&amp;quot;.format(accuracy_score(y_test, pred_lr)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Accuracy: 0.9471&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;F1: {:0.4f}&amp;quot;.format(f1_score(y_test, pred_lr)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;F1: 0.9444&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The results show that logistic regression performs better than Naive Bayes as seen from higher accuracy and F-1 score (94 vs 91). This is just the performance of a single model. We can actually improve the prediction result if we pool multiple models together to tackle the same task. Let us try diving into model voting and averaging.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="voting-and-averaging"&gt;Voting and Averaging&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;One type of ensemble methods is majority voting, which combines the output of many classifiers by using the mode of the individual predictions. It is recommended to use an odd number of classifiers. For example, if we use four classifiers, the predictions for positive and negative classes could be tied. Therefore, we need at least three classifiers, and when problem constraints allow it, use five or more.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There are some characteristics you need in your “crowd” for a voting ensemble to be effective. First, the ensemble needs to be diverse: you can do this by using different algorithms or different data sets. Second, each prediction needs to be independent and uncorrelated from the rest. Third, each model should be able to make its own prediction without relying on the other predictions. Finally, the ensemble model should aggregate individual predictions into a collective one. Keep in mind that Majority Voting is a technique which can only be applied to classification problems.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In addition to the Naive Bayes model we called for earlier, I will call for two additional models to participate in this majority voting method, namely k-Nearest Neighbors (Knn) and Decision Tree (DT). Knn is a classification algorithm in which a new data point is classified based on similarity in the specific group of neighboring data points. DR is another algorithm that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility to predict outcomes of a data point.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;clf_knn = KNeighborsClassifier(n_neighbors = 5)

clf_dt = DecisionTreeClassifier(min_samples_leaf = 3, min_samples_split = 9, random_state = RANDOM_STATE)

from mlxtend.classifier import EnsembleVoteClassifier

clf_vote = EnsembleVoteClassifier(clfs=[clf_nb, clf_knn, clf_dt], voting = &amp;quot;hard&amp;quot;)

clf_vote.fit(X_train, y_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;EnsembleVoteClassifier(clfs=[GaussianNB(), KNeighborsClassifier(),
                             DecisionTreeClassifier(min_samples_leaf=3,
                                                    min_samples_split=9,
                                                    random_state=123)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;pred_vote = clf_vote.predict(X_test)

score_vote = f1_score(pred_vote, y_test)

print(&amp;quot;Accuracy: {:0.4f}&amp;quot;.format(accuracy_score(y_test, pred_vote)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Accuracy: 0.9996&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;#39;F1-Score: {:.3f}&amp;#39;.format(score_vote))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;F1-Score: 1.000&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The accuracy and F1 scores increase to almost 1.00, which means that the combination of the three models works well in improving their performance. The voting we did above is called “hard voting”, where every individual classifier votes for a class, and the majority wins. In statistical terms, the predicted target label of the ensemble is the mode of the distribution of individually predicted labels. There is also “soft voting”, where every individual classifier provides a probability value that a specific data point belongs to a particular target class.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;In this technique, the combined prediction is the mean of the individual predictions. For Regression, we use the predicted values. And for Classification, we use the predicted probabilities. As the mean doesn’t have ambiguous cases like the mode, we can use any number of estimators, as long as we have at least two of them.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;clf_ave = EnsembleVoteClassifier(clfs=[clf_nb, clf_lr, clf_knn, clf_dt], voting = &amp;quot;soft&amp;quot;)

clf_ave.fit(X_train, y_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;EnsembleVoteClassifier(clfs=[GaussianNB(),
                             LogisticRegression(max_iter=450, random_state=123),
                             KNeighborsClassifier(),
                             DecisionTreeClassifier(min_samples_leaf=3,
                                                    min_samples_split=9,
                                                    random_state=123)],
                       voting=&amp;#39;soft&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;pred_ave = clf_ave.predict(X_test)

score_ave = f1_score(pred_ave, y_test)

print(&amp;quot;Accuracy: {:0.4f}&amp;quot;.format(accuracy_score(y_test, pred_ave)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Accuracy: 0.9979&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;#39;F1-Score: {:.3f}&amp;#39;.format(score_ave))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;F1-Score: 0.998&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="bagging"&gt;Bagging&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Model Voting and averaging work by combining the predictions of already trained models. These estimators that we included in the voting process are so well trained that, in some cases, they produce decent results on their own. Voting is appropriate when you already have optimized models and want to improve performance further by combining them. But what happens when you don’t have these estimators trained beforehand? Well, that’s when “weak” estimators come into play.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The idea of “weak” doesn’t mean that it is a bad model, just that it is not as strong as a highly optimized, fine-tuned model. Bootstrap Aggregating (aka Bagging) is the ensemble method behind powerful machine learning algorithms such as random forests that works by combining several weak models together to work on the same task. To clarify, a weak model (e.g., a single DT) is the model which works just slightly better than random guessing (approximately 50%). Therefore, the error rate is less than 50% but close to it. However, they are light in terms of space and computational requirements, and fast during training and evaluation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You might be wondering how it is possible for a large group of “weak” models to be able to achieve good performance? This is the work of the “wisdom of the crowd”. Do ensemble methods with the same model have that potential? We have to refer to “&lt;a href="https://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem"&gt;Condorcet’s Jury Theorem&lt;/a&gt;”. If a jury (or a model, in our case) has more than 50% probability of getting the right answer, adding more voters increases the probability that the majority decision is correct up to 100% (not exactly 100%, but close enough).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The requirements for this theorem are the following: First, all the models must be independent. Secondly, each model performs better than random guessing. And finally, all individual models have similar performance. If these three conditions are met, then adding more models increases the probability of the ensemble to be correct, and makes this probability tend to 1, equivalent to 100%! The second and third requirements can be fulfilled by using the same “weak” model for all the estimators, as then all will have a similar performance and be better than random guessing. Several jury theorems carry the optimistic message that, in suitable circumstances, “crowds are wise”: many individuals together (using, for instance, majority voting) tend to make good decisions, outperforming fewer or just one individual.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Let’s try using the Random Forest classifier, which is an ensemble of a large number of individual decision trees that are designed to be uncorrelated through randomization. Each tree is unique and has its own errors. A group of unique trees are able to produce results that are moving toward the right direction should the requirements mentioned above are fulfilled.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# random forest model creation
clf_rf = RandomForestClassifier(random_state = RANDOM_STATE)
clf_rf.fit(X_train, y_train)

# predictions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;RandomForestClassifier(random_state=123)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;pred_rfc = clf_rf.predict(X_test)

print(&amp;quot;Accuracy: {:0.4f}&amp;quot;.format(accuracy_score(y_test, pred_rfc)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Accuracy: 1.0000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;F1: {:0.4f}&amp;quot;.format(f1_score(y_test, pred_rfc)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;F1: 1.0000&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The codes below visualize the first five trees from the forest and export it as a png file.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;from sklearn import tree

fn = X.columns #for features
cn = df.columns[0] #for class

fig, axes = plt.subplots(nrows = 1,ncols = 5,figsize = (10,2), dpi=900)
for index in range(0, 5):
    tree.plot_tree(clf_rf.estimators_[index],
                   feature_names = fn, 
                   class_names = cn,
                   filled = True,
                   ax = axes[index]);

    axes[index].set_title(&amp;#39;Estimator: &amp;#39; + str(index), fontsize = 11)
fig.savefig(&amp;#39;rf_5trees.png&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-09-ensemblerf_5trees.png" style="width:150.0%" alt="" /&gt;
&lt;p class="caption"&gt;Five Samples of Trees from the Random Forest Model&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="boosting"&gt;Boosting&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The ensemble methods you’ve seen so far are based on an idea known as collective learning - that is, the wisdom of the crowd. For collective learning to be efficient, the estimators need to be independent and uncorrelated. In addition, all the estimators are learning the same task, for the same goal: to predict the target variable given the features. Gradual learning methods, on the other hand, are based on the principle of iterative learning. In this approach, each subsequent model tries to fix the errors of the previous model. Gradual learning creates dependent estimators, as each model takes advantage of the knowledge from the previous estimator.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In gradual learning, instead of the same model being corrected in every iteration, a new model is built that tries to fix the errors of the previous model. While this learning approach sounds promising, you should remain vigilant. It’s possible that some incorrect predictions may be made due to noise in the data, not because those data points are hard to predict. Gradient Boosting is another popular and powerful gradual learning ensemble model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To explain how it works, let’s say that you want to predict values of a variable (e.g., whether a mushroom is edible, in our case). On the first iteration, our initial model is a weak estimator that is fit to the dataset. Then, on each subsequent iteration, a new model is built based pn error from the previous iteration. We repeat this process until the error is small enough such that the difference in performance is negligible. This is a peculiarity of Gradient Boosting, as the individual estimators are not combined through voting or averaging, but by addition. We had a model learn from its previous mistake.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;#Call the model
clf_gbm = GradientBoostingClassifier(n_estimators = 100, learning_rate = 0.1, random_state=RANDOM_STATE)

#Fit the model
clf_gbm.fit(X_train, y_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;GradientBoostingClassifier(random_state=123)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;pred_gbm = clf_gbm.predict(X_test)

print(&amp;quot;Accuracy: {:0.4f}&amp;quot;.format(accuracy_score(y_test, pred_gbm)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Accuracy: 1.0000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;F1: {:0.4f}&amp;quot;.format(f1_score(y_test, pred_gbm)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;F1: 1.0000&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Another example of boosting model is Extreme Gradient Boosting (aka XGBoost). XGBoost regularly wins online data science competitions and is widely used across different industries. The model is known in its speed and performance. Because the core XGBoost algorithm is parallelizable (as opposed to sequential ensemble of normal Gradient boosting), it can harness all of the processing power of modern multi-core computers. The model also consistently outperforms almost all other single-algorithm methods in machine learning competitions and has been shown to achieve state-of-the-art performance on a variety of benchmark machine learning data sets.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;clf_xgb = xgb.XGBClassifier(objective=&amp;#39;binary:logistic&amp;#39;, n_estimators = 10, seed = RANDOM_STATE, use_label_encoder = False)
clf_xgb.fit(X_train, y_train)

#n_estimator is the number of boosting round

# predictions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[18:49:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &amp;#39;binary:logistic&amp;#39; was changed from &amp;#39;error&amp;#39; to &amp;#39;logloss&amp;#39;. Explicitly set eval_metric if you&amp;#39;d like to restore the old behavior.
XGBClassifier(base_score=0.5, booster=&amp;#39;gbtree&amp;#39;, colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,
              gamma=0, gpu_id=-1, importance_type=None,
              interaction_constraints=&amp;#39;&amp;#39;, learning_rate=0.300000012,
              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,
              monotone_constraints=&amp;#39;()&amp;#39;, n_estimators=10, n_jobs=8,
              num_parallel_tree=1, predictor=&amp;#39;auto&amp;#39;, random_state=123,
              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=123,
              subsample=1, tree_method=&amp;#39;exact&amp;#39;, use_label_encoder=False,
              validate_parameters=1, verbosity=None)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;pred_xgb = clf_xgb.predict(X_test)

print(&amp;quot;Accuracy: {:0.4f}&amp;quot;.format(accuracy_score(y_test, pred_xgb)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Accuracy: 1.0000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;F1: {:0.4f}&amp;quot;.format(f1_score(y_test, pred_xgb)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;F1: 1.0000&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;It is worth mentioning that you should consider using XGBoost for any supervised machine learning task that fits the following criteria: first, having at least 1000 examples; second, you have a mixture of categorical and numeric features, or when you have just numeric features. XGBoost is not ideally suited for image recognition, computer vision, or natural language processing as such problems can be much better tackled using deep learning approaches.XGBoost is also not suitable when you have very small training sets (less than 100 training examples) or when the number of training examples is significantly smaller than the number of features being used for training as it could lead to model &lt;a href="https://www.ibm.com/cloud/learn/overfitting#:~:text=When%20the%20model%20memorizes%20the,that%20it%20was%20intended%20for."&gt;overfitting&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="stacking"&gt;Stacking&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The final type of ensemble model we will be discussing here is model stacking. Consider a relay race, in which sprinters run until they pass the baton over to the next on track. This is a good example of teamwork. While all team members must be strong competitors, each individual has a special role to play based on their abilities. In this case, we will have the lead runner (aka anchor model) that knows individual strengths and weaknesses of each team member. Second, they should have clearly define tasks to do. Each team member must know their responsibilities and focus on them. Finally, the anchor must participate in the race to carry out the final run.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Here’s a diagram depicting the architecture of stacking ensembles. Each individual model uses the same data set and input features. These are the first-layer estimators. Then, estimators pass their predictions as additional input features to the second-layer estimator.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-09-ensemblestack.png" style="width:150.0%" alt="" /&gt;
&lt;p class="caption"&gt;Model stacking Flow chart. Image from &lt;a href="https://towardsai.net/p/l/machine-learning-model-stacking-in-python" class="uri"&gt;https://towardsai.net/p/l/machine-learning-model-stacking-in-python&lt;/a&gt;. No copyright infringement is intended&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;So far, we have seen ensemble methods that use simple arithmetic operations like the mean or the mode as combiners. However, in Stacking, the combiner is itself a trainable model. In addition, this combiner model has not only the predictions as input features, but also the original dataset. This allows it to determine which estimator is more accurate depending on the input features. In other words, it brings out the best ability in its team members to complete the task. The combiner model (aka meta learner) plays similar roles to the anchor in the relay race. It is also the last team member and the one which provides the final predictions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In this post, I will stack Knn, DT, Naive Bayes, and Random Forest classifiers together with logistic regression as the meta model (team leader).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;clf_stack = StackingClassifier(classifiers=[clf_knn, clf_dt, clf_nb, clf_rf], meta_classifier=clf_lr)
clf_stack.fit(X_train, y_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;StackingClassifier(classifiers=[KNeighborsClassifier(),
                                DecisionTreeClassifier(min_samples_leaf=3,
                                                       min_samples_split=9,
                                                       random_state=123),
                                GaussianNB(),
                                RandomForestClassifier(random_state=123)],
                   meta_classifier=LogisticRegression(max_iter=450,
                                                      random_state=123))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;pred_stack = clf_stack.predict(X_test)

print(&amp;quot;Accuracy: {:0.4f}&amp;quot;.format(accuracy_score(y_test, pred_stack)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Accuracy: 1.0000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;F1: {:0.4f}&amp;quot;.format(f1_score(y_test, pred_stack)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;F1: 1.0000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;report_stack = classification_report(y_test, pred_stack)
print(report_stack)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;              precision    recall  f1-score   support

           0       1.00      1.00      1.00      1271
           1       1.00      1.00      1.00      1167

    accuracy                           1.00      2438
   macro avg       1.00      1.00      1.00      2438
weighted avg       1.00      1.00      1.00      2438&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, pred_stack)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[1271,    0],
       [   0, 1167]], dtype=int64)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;To lessen our burden in model optimization, we can also tune hyperparameters of our models (both base model and meta model) all at the same time as well. This way, we will know which combination of hyperparameter values yield the best result. We can use it in our final model development.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;params = {&amp;#39;kneighborsclassifier__n_neighbors&amp;#39;: [1, 5],
          &amp;#39;randomforestclassifier__n_estimators&amp;#39;: [10, 50],
          &amp;#39;meta_classifier__C&amp;#39;: [0.1, 10.0]}

grid = GridSearchCV(estimator=clf_stack, 
                    param_grid=params, 
                    cv=5,
                    refit=True)

grid.fit(X_train, y_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;GridSearchCV(cv=5,
             estimator=StackingClassifier(classifiers=[KNeighborsClassifier(),
                                                       DecisionTreeClassifier(min_samples_leaf=3,
                                                                              min_samples_split=9,
                                                                              random_state=123),
                                                       GaussianNB(),
                                                       RandomForestClassifier(random_state=123)],
                                          meta_classifier=LogisticRegression(max_iter=450,
                                                                             random_state=123)),
             param_grid={&amp;#39;kneighborsclassifier__n_neighbors&amp;#39;: [1, 5],
                         &amp;#39;meta_classifier__C&amp;#39;: [0.1, 10.0],
                         &amp;#39;randomforestclassifier__n_estimators&amp;#39;: [10, 50]})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;cv_keys = (&amp;#39;mean_test_score&amp;#39;, &amp;#39;std_test_score&amp;#39;, &amp;#39;params&amp;#39;)

for r, _ in enumerate(grid.cv_results_[&amp;#39;mean_test_score&amp;#39;]):
    print(&amp;quot;%0.3f +/- %0.2f %r&amp;quot;
          % (grid.cv_results_[cv_keys[0]][r],
             grid.cv_results_[cv_keys[1]][r] / 2.0,
             grid.cv_results_[cv_keys[2]][r]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1.000 +/- 0.00 {&amp;#39;kneighborsclassifier__n_neighbors&amp;#39;: 1, &amp;#39;meta_classifier__C&amp;#39;: 0.1, &amp;#39;randomforestclassifier__n_estimators&amp;#39;: 10}
1.000 +/- 0.00 {&amp;#39;kneighborsclassifier__n_neighbors&amp;#39;: 1, &amp;#39;meta_classifier__C&amp;#39;: 0.1, &amp;#39;randomforestclassifier__n_estimators&amp;#39;: 50}
1.000 +/- 0.00 {&amp;#39;kneighborsclassifier__n_neighbors&amp;#39;: 1, &amp;#39;meta_classifier__C&amp;#39;: 10.0, &amp;#39;randomforestclassifier__n_estimators&amp;#39;: 10}
1.000 +/- 0.00 {&amp;#39;kneighborsclassifier__n_neighbors&amp;#39;: 1, &amp;#39;meta_classifier__C&amp;#39;: 10.0, &amp;#39;randomforestclassifier__n_estimators&amp;#39;: 50}
1.000 +/- 0.00 {&amp;#39;kneighborsclassifier__n_neighbors&amp;#39;: 5, &amp;#39;meta_classifier__C&amp;#39;: 0.1, &amp;#39;randomforestclassifier__n_estimators&amp;#39;: 10}
1.000 +/- 0.00 {&amp;#39;kneighborsclassifier__n_neighbors&amp;#39;: 5, &amp;#39;meta_classifier__C&amp;#39;: 0.1, &amp;#39;randomforestclassifier__n_estimators&amp;#39;: 50}
1.000 +/- 0.00 {&amp;#39;kneighborsclassifier__n_neighbors&amp;#39;: 5, &amp;#39;meta_classifier__C&amp;#39;: 10.0, &amp;#39;randomforestclassifier__n_estimators&amp;#39;: 10}
1.000 +/- 0.00 {&amp;#39;kneighborsclassifier__n_neighbors&amp;#39;: 5, &amp;#39;meta_classifier__C&amp;#39;: 10.0, &amp;#39;randomforestclassifier__n_estimators&amp;#39;: 50}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;#39;Best parameters: %s&amp;#39; % grid.best_params_)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Best parameters: {&amp;#39;kneighborsclassifier__n_neighbors&amp;#39;: 1, &amp;#39;meta_classifier__C&amp;#39;: 0.1, &amp;#39;randomforestclassifier__n_estimators&amp;#39;: 10}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;#39;Accuracy: %.2f&amp;#39; % grid.best_score_)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Accuracy: 1.00&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;I will be plotting the decision region below to visualize how each model perform compared to their collective performance from model stacking. The decision region of each model changes in accordance with its performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;import matplotlib.pyplot as plt
from mlxtend.plotting import plot_decision_regions
import matplotlib.gridspec as gridspec
import itertools
from sklearn.decomposition import PCA

pca = PCA(n_components = 2)

X_np = X_train.to_numpy()
y_np = y_train.to_numpy()

X_np_reduced = pca.fit_transform(X_np)

gs = gridspec.GridSpec(2, 2)

fig = plt.figure(figsize=(10,8))

for clf, lab, grd in zip([clf_knn, clf_dt, clf_nb, clf_stack], 
                         [&amp;#39;KNN&amp;#39;, 
                          &amp;#39;Decision Tree&amp;#39;, 
                          &amp;#39;Naive Bayes&amp;#39;,
                          &amp;#39;StackingClassifier&amp;#39;],
                          itertools.product([0, 1], repeat=2)):

    clf.fit(X_np_reduced, y_np)
    ax = plt.subplot(gs[grd[0], grd[1]])
    fig = plot_decision_regions(X=X_np_reduced, y=y_np, clf=clf)
    plt.title(lab)
    &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;KNeighborsClassifier()
Text(0.5, 1.0, &amp;#39;KNN&amp;#39;)
DecisionTreeClassifier(min_samples_leaf=3, min_samples_split=9,
                       random_state=123)
Text(0.5, 1.0, &amp;#39;Decision Tree&amp;#39;)
GaussianNB()
Text(0.5, 1.0, &amp;#39;Naive Bayes&amp;#39;)
StackingClassifier(classifiers=[KNeighborsClassifier(),
                                DecisionTreeClassifier(min_samples_leaf=3,
                                                       min_samples_split=9,
                                                       random_state=123),
                                GaussianNB(),
                                RandomForestClassifier(random_state=123)],
                   meta_classifier=LogisticRegression(max_iter=450,
                                                      random_state=123))
Text(0.5, 1.0, &amp;#39;StackingClassifier&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f852b16296_files/figure-html/unnamed-chunk-18-5.png" width="960" /&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To summarize, Voting combines the predictions of individual models using the mode. As the mode is a categorical measure, Voting can only be applied to Classification. The Averaging method combines the individual predictions using the mean. In contrast to Voting, Averaging can be applied on both classification and regression. Bagging uses a large amount of “weak” estimators. Their predictions are then aggregated by Voting or Averaging. Boosting is based on the iterative learning principle, in which each model attempts to fix the errors from the previous one. Therefore, this approach uses a sequential model building. Finally, model stacking works by combining individual estimators, but the combiner is an estimator itself, instead of just an operation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using ensemble techniques is an effective way to maximize the performance of our predictive model. However, a major drawback of this approach is model interpretability. While its performance is good, explaining how features interact with each other to form the result could be challenging. This is &lt;a href="https://www.technologyreview.com/2017/04/11/5113/the-dark-secret-at-the-heart-of-ai/"&gt;the black box problem&lt;/a&gt; that happens when the model is too complex that we don’t know how it actually works. If a single model performs relatively well, going for it would be a good compromise. The simpler the model is, the easier for us to explain the result to our audience. Rather than going full technical, we should consider perspectives of the audience as well to make our work accessible. As always, thank you very much for reading! Have a good one!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>d8361145b69fd0e6d13645148059b1ef</distill:md5>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-09-ensemble</guid>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-09-ensemble/robot.png" medium="image" type="image/png" width="626" height="528"/>
    </item>
    <item>
      <title>Examining PISA 2018 Data Set with Statistical Learning Approach</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-02-27-statlearning</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;It has been a while since my last post as I am swamped with manuscript writings, but a good thing is I got more ideas of what to write for my blog as well. As I learned more about data mining, I became aware about the overlapping space between Statistics and Machine Learning in how they involve data as a primary part in their work. Statistics, which is the study that concerns the understanding, analyzing, and interpreting empirical data, is known as the underlying mechanism of machine learning as algorithms such as Linear Regression or Logistic Regression usually operate under a set of equations; for that, it is reasonable to say that statistics is a prerequisite for researchers to learn before dicing into machine learning &lt;a href="https://www.taylorfrancis.com/books/mono/10.4324/9780203137819/introduction-statistical-concepts-debbie-hahs-vaughn-richard-lomax"&gt;(Lomax and Hahs-Vaughn, 2012)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Statistical Learning (SL) is a sub-field of Machine Learning (ML) research that seeks to explain relationship between variables with statistical models before extending their capability to predict the outcome of unseen data points &lt;a href="https://books.google.ca/books?id=sna9BaxVbj8C&amp;amp;lpg=PR7&amp;amp;ots=oqKdHSpld7&amp;amp;dq=Vladimir%20Naumovich%20Vapnik.%20Statistical%20learning%20theory&amp;amp;lr&amp;amp;pg=PR7#v=onepage&amp;amp;q=Vladimir%20Naumovich%20Vapnik.%20Statistical%20learning%20theory&amp;amp;f=false"&gt;(Vapnik, 1999)&lt;/a&gt;. Some may say that SL and ML are the same thing; that is true to an extent as statistics is the science that works behind the development of ML, but there are also differences if you really look into the technical part of it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The traditional statistical approach focuses on inferring relationships between variables with explicitly laid out instructions or equations such as Linear Regression, while the ML approach to research focuses on developing algorithms that can recognize patterns in the data set to accurately predict the unseen data without much emphasis on assumptions behind the data set and interpretability of the model (e.g., Random Forest, Neural Network) &lt;a href="https://www.datasciencecentral.com/machine-learning-vs-statistics-in-one-picture/"&gt;(Glen, 2019)&lt;/a&gt;. Statistical Learning positions itself in the intersection of the two fields by focusing on understanding relationships between variables while at the same time seeking to develop a meaningful model that can be used to predict unseen data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In other words, Statistics focuses on learning the meaning of data, typically the low-dimensionality one, via statistical inferences while ML focuses on the application aspect by developing a complex model that is accurate and usable in the real application. Statistical Learning aims to cover both purposes by focusing on understanding meaning of the data with a meaningful model while also aiming to use that model in the real-world application (&lt;a href="https://pubmed.ncbi.nlm.nih.gov/27406289/"&gt;Iniesta et al., 2016&lt;/a&gt;; &lt;a href="https://onlinestats.canr.udel.edu/machine-learning-vs-statistics/"&gt;University of Delaware, 2021&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In this post, I will be developing two statistical learning models, namely Linear Regression and Polynomial Regression, and apply it to Thai student data from the Programme for International Student Assessment (PISA) 2018 data set to examine the impact of classroom competition and cooperation to students’ academic performance. PISA is a large-scale educational survey that is collected internationally every three years on school-related topics such as student achievement, student school well-being, and teachers’ instructional support on students (&lt;a href="https://www.oecd.org/education/pisa-2018-results-volume-i-5f07c754-en.htm"&gt;OECD, 2019&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-02-27-statlearningstatlearn.jpeg" style="width:50.0%" alt="" /&gt;
&lt;p class="caption"&gt;Image from &lt;a href="https://marketbusinessnews.com/financial-glossary/statistical-learning" class="uri"&gt;https://marketbusinessnews.com/financial-glossary/statistical-learning&lt;/a&gt;. No copyright infringement is intended&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="setting-up-r-environment"&gt;Setting up R environment&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;As always, we will first set up the working directory, loading in the packages, and importing the data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;setwd(&amp;quot;D:/Program/Private_project/DistillSite/_posts/2022-02-27-statlearning&amp;quot;)

library(foreign) #To read SPSS data
library(psych) #For descriptive stat
library(tidyverse) #for data manipulation
library(DataExplorer)
library(ggcorrplot) #for correlation matrix
library(tidymodels) #for model building
library(kableExtra) #for kable table
library(visdat) #for overall missingness visualization&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We will Import the data set with &lt;code&gt;read_csv&lt;/code&gt; and subset our variables of interest with &lt;code&gt;select&lt;/code&gt;. We will also recode factor variables such as gender to make it dichotomous. That is, 1 as an indicator of that variable and 0 as anything that is not. See the &lt;a href="https://stats.oarc.ucla.edu/spss/faq/coding-systems-for-categorical-variables-in-regression-analysis/"&gt;Coding Systems for Categorical Variables&lt;/a&gt; page of University of California, Los Angeles.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#Import the data set
PISA_TH &amp;lt;-read_csv(&amp;quot;PISA2018TH.csv&amp;quot;, col_names = TRUE)

PISA_Subsetted &amp;lt;-  PISA_TH %&amp;gt;% 
  select(FEMALE = ST004D01T, READING_SCORE = PV1READ, CLASSCOMP = PERCOMP,
         CLASSCOOP = PERCOOP, ATTCOMP = COMPETE)

PISA_Subsetted$FEMALE &amp;lt;- recode_factor(PISA_Subsetted$FEMALE, 
                                       &amp;quot;1&amp;quot; = &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot; = &amp;quot;0&amp;quot;)
kbl(head(PISA_Subsetted)) %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;responsive&amp;quot;),
                full_width = TRUE, position = &amp;quot;left&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table table-striped table-hover table-condensed table-responsive" style&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:left;"&gt;
FEMALE
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
READING_SCORE
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
CLASSCOMP
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
CLASSCOOP
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
ATTCOMP
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
480.756
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1.5630
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1.6762
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4352
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
502.610
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
476.744
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0866
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
-0.5358
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4352
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
489.858
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.6912
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.6012
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
-0.2661
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
536.178
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1.2903
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1.6762
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4352
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
566.755
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2020
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.6012
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3709
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="missing-data-handling"&gt;Missing Data Handling&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Before we dive in further, we will be examining the data set for missing data ratio and patterns that could interfere with our analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;visdat::vis_miss(PISA_Subsetted, sort_miss = TRUE, cluster = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f823693b12_files/figure-html/unnamed-chunk-4-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;naniar::gg_miss_upset(PISA_Subsetted, nsets = 5, nintersects = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f823693b12_files/figure-html/unnamed-chunk-4-2.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It turns out we only have 1.8% of missing data. The missingness pattern indicates that the missing pair of perceived classroom cooperation and perceived classroom competition has the most frequency among all missing variables. There are also 91 cases that do not have any variable values. Given the discovered pattern, it makes more sense to remove them with listwise deletion instead of performing an imputation (aka plugging in possible numbers with an educated guess). Note that there is no rule of thumb in handling missing data. It depends on judgement of the researcher themselves. I tried imputing the data set as well and it gives no noticeable change, but I just want to try something different this time.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_Subsetted &amp;lt;- na.omit(PISA_Subsetted)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="exploratory-data-analysis-and-assumption-check"&gt;Exploratory Data Analysis and Assumption Check&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To perform analyses using statistical learning, it is important to confirm that all statistical assumptions of the models were met to ensure that the results are meaningful. The assumptions that we will check in this section are normality distribution, multicollinearity, and influential outliers. There are other assumptions we need to check as well such as variable independence andhomoscedasticity (for the equality of variance throughout the data set), but we would have to create a linear regression model with the whole data set without teaching the machine. We won’t be doing that here because I aim to automate the process with the &lt;code&gt;tidymodel&lt;/code&gt; approach instead of performing regression manually every time.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will first examine structure of the data set with &lt;code&gt;plot_str&lt;/code&gt; to plot data structure and &lt;code&gt;plot_intro&lt;/code&gt; to plot basic infornmation of the data set.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_intro(PISA_Subsetted, title = &amp;quot;Types of Variables&amp;quot;)+
  theme(plot.title = element_text(hjust = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f823693b12_files/figure-html/unnamed-chunk-6-1.png" width="672" /&gt;&lt;img src="file15f823693b12_files/figure-html/unnamed-chunk-6-2.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This plot shows us how many discrete and continuous variables that we have in the data set. Mext, we will plot structure of the data to check which variable is numerical and which variable is factor.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_str(PISA_Subsetted)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Next, we can check for normality distribution with histogram, skewness/kurtosis with descriptive statistics, and Quantile-Quantile Plot (Q-Q Plot).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;describe(PISA_Subsetted)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;              vars    n   mean    sd median trimmed   mad    min
FEMALE*          1 8158   1.46  0.50   1.00    1.45  0.00   1.00
READING_SCORE    2 8158 411.26 88.42 402.43  408.00 91.53 146.33
CLASSCOMP        3 8158   0.20  0.87   0.20    0.19  0.73  -1.99
CLASSCOOP        4 8158   0.20  0.92   0.60    0.22  1.07  -2.14
ATTCOMP          5 8158   0.03  0.79   0.20    0.00  0.65  -2.35
                 max  range  skew kurtosis   se
FEMALE*         2.00   1.00  0.18    -1.97 0.01
READING_SCORE 720.09 573.76  0.32    -0.37 0.98
CLASSCOMP       2.04   4.03  0.02    -0.04 0.01
CLASSCOOP       1.68   3.82 -0.39    -0.47 0.01
ATTCOMP         2.01   4.35  0.13     1.11 0.01&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_histogram(PISA_Subsetted)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f823693b12_files/figure-html/unnamed-chunk-8-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;set.seed(123)
plot_qq(PISA_Subsetted, sampled_rows = 1000L)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f823693b12_files/figure-html/unnamed-chunk-8-2.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The results of skewness and kurtosis indicate no departure from normality; that is, the data is normally distributed with all skewness stays between the range of -0.5 and 0.5, and all kurtosis stays between the range of -3 and 3 &lt;a href="https://www.taylorfrancis.com/books/mono/10.4324/9780203137819/introduction-statistical-concepts-debbie-hahs-vaughn-richard-lomax"&gt;(Lomax and Hahs-Vaughn, 2012)&lt;/a&gt;. The normality assumption is further confirmed with bell shape of the histogram and visualization from the Q-Q plots. Next, we will examine bivariate correlation between variables to investigate their relationships and confirm the absence of multicollinearity assumption.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Multicollinearity refers to the condition where relationships between our variables of interest is high; this which may influence the result of our analysis because we are having two variables that investigate the same thing &lt;a href="https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wics.84?casa_token=CDV_J-e2DHsAAAAA%3AY7_DJTK8-IXnH4b1BsSLHpmSq6Peg1i4t8sanKhqtF-OAtQzQ_7JrGZpiNSf8Tn2B5xXm4OcvVlM"&gt;(Alin, 2010)&lt;/a&gt;. For example, when you investigate the influence of person’s weight to their height and included both weight in pound and weight in kilogram, the results can be messed up because you include variables that measure to much of the same thing.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_correlation(PISA_Subsetted, type = &amp;quot;continuous&amp;quot;, title = &amp;quot;Correlation Matrix&amp;quot;) +
  theme(plot.title = element_text(hjust = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f823693b12_files/figure-html/unnamed-chunk-9-1.png" width="672" /&gt;&lt;img src="file15f823693b12_files/figure-html/unnamed-chunk-9-2.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All variables exhibit positive correlation to each other. The matrix shows no multicollinearity as the highest correlation coefficient between variables is 0.26, which is far from extreme in terms of magnitude (.6 to .7 coefficients should be flagged). For your reference, the report of data exploration can be generated with a simple function of &lt;code&gt;create_report(PISA_Subsetted)&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="data-preprocessing"&gt;Data Preprocessing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To test out our models, we need to split the data set into a training and a testing set to make sure that the testing is fair. It is similar to the situation where we do not want our students to take a peek at the exam before their test.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;set.seed(456)

# Put 3/4 of the data into the training set 
data_split &amp;lt;- initial_split(PISA_Subsetted, prop = 3/4)

# Create data frames for the two sets:
train_data &amp;lt;- training(data_split) #3/4
test_data  &amp;lt;- testing(data_split)  #1/4&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="linear-regression-model"&gt;Linear Regression Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Now we have a training set that has 75% of the whole data and a testing set that has 25% of the whole data. Next, we will create a Linear Regression Model to predict the value of students’ reading score (&lt;code&gt;READING_SCORE&lt;/code&gt;) with the degree of classroom cooperation, the degree of classroom competition, and students’ attitude to competition. First, we will request for a linear regression model with &lt;code&gt;linear_reg()&lt;/code&gt; set the mode to regression and set the engine to “lm”, which stands for linear model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;lm_mod &amp;lt;- 
  linear_reg() %&amp;gt;% 
  set_mode(&amp;quot;regression&amp;quot;) %&amp;gt;%
  set_engine(&amp;quot;lm&amp;quot;)

lm_mod&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Linear Regression Model Specification (regression)

Computational engine: lm &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Next, we will do some data preprocessing by removing variables that we do not need in the analysis with &lt;code&gt;recipe&lt;/code&gt; package. We will specify the formula as &lt;code&gt;READING_SCORE ~ .&lt;/code&gt;, which means we will be predicting students’ reading score with all other variables. The &lt;code&gt;step_rm()&lt;/code&gt; argument removes the variable we specified; for our case, it is students’ gender. For your reference, there are functions that assist us in other ways of data preprocessing as well such as &lt;a href="https://recipes.tidymodels.org/reference/step_dummy.html"&gt;&lt;code&gt;step_dummy()&lt;/code&gt;&lt;/a&gt; to dummy code the variable or &lt;a href="https://recipes.tidymodels.org/reference/step_string2factor.html"&gt;&lt;code&gt;step_string2factor&lt;/code&gt;&lt;/a&gt; that converts string variables into factor variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_linear_recipe &amp;lt;- 
  recipe(READING_SCORE ~ ., data = train_data) %&amp;gt;%
  step_rm(FEMALE)

PISA_linear_recipe&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Recipe

Inputs:

      role #variables
   outcome          1
 predictor          4

Operations:

Variables removed FEMALE&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We have the model. We have the formula. Now, we can combine them both into a workflow with &lt;code&gt;workflow()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_workflow &amp;lt;- 
  workflow() %&amp;gt;% 
  add_model(lm_mod) %&amp;gt;% 
  add_recipe(PISA_linear_recipe)

PISA_workflow&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;== Workflow ==========================================================
Preprocessor: Recipe
Model: linear_reg()

-- Preprocessor ------------------------------------------------------
1 Recipe Step

* step_rm()

-- Model -------------------------------------------------------------
Linear Regression Model Specification (regression)

Computational engine: lm &lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="fit-the-linear-regression-model-to-the-trainingtesting-dataset"&gt;Fit the Linear Regression Model to the Training/Testing Dataset&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We will train (or teach) our Linear Regression model to learn characteristics of our data with the training set. This way, the machine will be able to predict values of the testing data with what it learned.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;set.seed(678)

PISA_linear_fit &amp;lt;- 
  PISA_workflow %&amp;gt;% 
  fit(data = train_data)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Now that we have taught the model, we can examine its performance by comparing its predicted value with the actual value of our targeted variable (i.e., students’ reading score). We will try extracting coefficients of the model first.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_linear_fit %&amp;gt;% 
  extract_fit_parsnip() %&amp;gt;% 
  broom::tidy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 4 x 5
  term        estimate std.error statistic  p.value
  &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
1 (Intercept)   406.        1.15    355.   0       
2 CLASSCOMP       3.77      1.33      2.84 4.50e- 3
3 CLASSCOOP      19.7       1.23     16.0  1.91e-56
4 ATTCOMP         3.08      1.45      2.12 3.38e- 2&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Coefficient interpretation is actually the same as interpreting linear regression results. First, we look at the &lt;em&gt;p&lt;/em&gt;-value to see if any of these predictors are significant. Then, we interpret the coefficient of each prtedictor to the targeted variable. For example, the coefficient of 19.77 for the classroom cooperation variable means that every one-unit increase in the degree of classroom cooperation, students’ score will increase by 19.7. Next, we can evaluate performance of the model by comparing the predicted value with the actual value, as well as extracting values that can summarize overall performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_linear_train_pred &amp;lt;- 
  predict(PISA_linear_fit, train_data) %&amp;gt;% 
  bind_cols(train_data %&amp;gt;% select(READING_SCORE)) 

PISA_linear_train_pred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 6,118 x 2
   .pred READING_SCORE
   &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;
 1  427.          471.
 2  421.          239.
 3  451.          546.
 4  405.          248.
 5  407.          457.
 6  398.          491.
 7  389.          308.
 8  421.          320.
 9  394.          473.
10  404.          507.
# ... with 6,108 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;.pred&lt;/code&gt; column is the predicted value and the &lt;code&gt;READING_SCORE&lt;/code&gt; column is the actual value. We can see that some predicted numbers are off from the actual number. Let us see overall performance of the model with Mean-Squared Error (MSE), Root Mean-Squared Error (RMSE), and R-Squared (R^2). The first two values indicate how poor our model performs, and the third value indicates how many percent of the variation of our targeted variable is explained by the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mse_vec &amp;lt;- function(truth, estimate, na_rm = TRUE, ...) {
  
  mse_impl &amp;lt;- function(truth, estimate) {
    mean((truth - estimate) ^ 2)
  }
  
  metric_vec_template(
    metric_impl = mse_impl,
    truth = truth, 
    estimate = estimate,
    na_rm = na_rm,
    cls = &amp;quot;numeric&amp;quot;,
    ...
  )
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;mse_vec(
  truth = PISA_linear_train_pred$READING_SCORE, 
  estimate = PISA_linear_train_pred$.pred
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 7389.577&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;rmse(PISA_linear_train_pred, 
     truth = READING_SCORE,
     estimate = .pred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 3
  .metric .estimator .estimate
  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
1 rmse    standard        86.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;rsq(PISA_linear_train_pred, 
     truth = READING_SCORE,
     estimate = .pred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 3
  .metric .estimator .estimate
  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
1 rsq     standard      0.0499&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We have MSE = 7389.577, RMSE = 86, and R-Squared = 0.0499 (or 4% of the targeted variable). Let us to the same for the testing round to ‘test’ out how well our model can predict the targeted variable.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_linear_test_pred &amp;lt;- 
  predict(PISA_linear_fit, test_data) %&amp;gt;% 
  bind_cols(test_data %&amp;gt;% select(READING_SCORE)) 

PISA_linear_test_pred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 2,040 x 2
   .pred READING_SCORE
   &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;
 1  398.          477.
 2  427.          542.
 3  415.          519.
 4  422.          597.
 5  421.          467.
 6  434.          573.
 7  397.          522.
 8  425.          436.
 9  423.          440.
10  399.          543.
# ... with 2,030 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;mse_vec(
  truth = PISA_linear_test_pred$READING_SCORE, 
  estimate = PISA_linear_test_pred$.pred
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 7472.221&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;rmse(PISA_linear_test_pred, 
     truth = READING_SCORE,
     estimate = .pred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 3
  .metric .estimator .estimate
  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
1 rmse    standard        86.4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;rsq(PISA_linear_test_pred, 
     truth = READING_SCORE,
     estimate = .pred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 3
  .metric .estimator .estimate
  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
1 rsq     standard      0.0584&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;In the testing round, We have MSE = 7472.221, RMSE = 86.4, and R-Squared = 0.0584 (or 5% of the targeted variable). The performance of both training and testing round are similar, meaning that there is no overfit (the machine memorized its lesson too much / too strict) or underfit (the machine did not learn as much as it is supposed to and therefore unable to know the relationship between variables). Now that we know what Linear Regression can do, let us see what can we accomplish with Polynomial Regression.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="polynomial-regression-model"&gt;Polynomial Regression Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Polynomial Regression is similar to Linear Regression, with the difference in its ability to capture non-linear relationship like the picture below. This ability allows the model to drop its assumption that the relationship between the independent and dependent variables has to be in linear shape.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-02-27-statlearningpolynomial.png" alt="" /&gt;
&lt;p class="caption"&gt;Image from &lt;a href="https://towardsdatascience.com/polynomial-regression-an-alternative-for-neural-networks-c4bd30fa6cf6" class="uri"&gt;https://towardsdatascience.com/polynomial-regression-an-alternative-for-neural-networks-c4bd30fa6cf6&lt;/a&gt;. No copyright infringement is intended&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We can customize ability to which our model can capture the pattern of our data by increasing its degree as demonstrated by the picture above. We can see that the red line (3rd degree) misses to capture a lot of patterns while the blue line (5th degree) captures a fair amount of patterns while the green line travels to almost every possible patterns. However, it is not necessarily true that the model with the highest degree is the best since it could capture noises that will not appear in the new data we will test the machine on (as well as the real-world data).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;So, after all we have discussed so far, let’s try creating a 3rd degree polynomial regression to see if we can capture the relationship any better than our linear regression. We will include &lt;code&gt;step_poly&lt;/code&gt; with degree = 3 into the &lt;code&gt;recipe&lt;/code&gt; function to make our model able to capture a degree of non-linearity.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_poly_recipe &amp;lt;- 
  recipe(READING_SCORE ~ CLASSCOMP + CLASSCOOP + ATTCOMP, data = train_data) %&amp;gt;%
  step_poly(CLASSCOMP, degree = 3) %&amp;gt;%
  step_poly(CLASSCOOP, degree = 3)

PISA_poly_recipe&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Recipe

Inputs:

      role #variables
   outcome          1
 predictor          3

Operations:

Orthogonal polynomials on CLASSCOMP
Orthogonal polynomials on CLASSCOOP&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Then, we will add the 3rd degree polynomial recipe to a workflow using the same model as our linear regression part (&lt;code&gt;lm_mod&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_poly_wf &amp;lt;- workflow() %&amp;gt;%
  add_model(lm_mod) %&amp;gt;%
  add_recipe(PISA_poly_recipe)

PISA_poly_wf&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;== Workflow ==========================================================
Preprocessor: Recipe
Model: linear_reg()

-- Preprocessor ------------------------------------------------------
2 Recipe Steps

* step_poly()
* step_poly()

-- Model -------------------------------------------------------------
Linear Regression Model Specification (regression)

Computational engine: lm &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Finally, we can fit our model to the training data to teach it to recognize patterns of our training data set before using it to predict the testing data. While we are at it, we can use &lt;code&gt;extract_fit_parsnip()&lt;/code&gt; to extract coefficients of our model as well to see which polynomial degree of which variable is significant in capturing patterns of the training data.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;set.seed(999)

PISA_poly_fit &amp;lt;- 
  PISA_poly_wf %&amp;gt;%
  fit(data = train_data)

PISA_poly_fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;== Workflow [trained] ================================================
Preprocessor: Recipe
Model: linear_reg()

-- Preprocessor ------------------------------------------------------
2 Recipe Steps

* step_poly()
* step_poly()

-- Model -------------------------------------------------------------

Call:
stats::lm(formula = ..y ~ ., data = data)

Coefficients:
     (Intercept)           ATTCOMP  CLASSCOMP_poly_1  
         411.053             2.729           271.369  
CLASSCOMP_poly_2  CLASSCOMP_poly_3  CLASSCOOP_poly_1  
         295.526            19.960          1400.633  
CLASSCOOP_poly_2  CLASSCOOP_poly_3  
         -55.306           304.765  &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_poly_fit %&amp;gt;% 
  extract_fit_parsnip() %&amp;gt;% 
  broom::tidy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 8 x 5
  term             estimate std.error statistic  p.value
  &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
1 (Intercept)        411.        1.10   374.    0       
2 ATTCOMP              2.73      1.45     1.88  6.00e- 2
3 CLASSCOMP_poly_1   271.       90.4      3.00  2.69e- 3
4 CLASSCOMP_poly_2   296.       88.4      3.34  8.31e- 4
5 CLASSCOMP_poly_3    20.0      87.2      0.229 8.19e- 1
6 CLASSCOOP_poly_1  1401.       88.7     15.8   4.97e-55
7 CLASSCOOP_poly_2   -55.3      88.6     -0.625 5.32e- 1
8 CLASSCOOP_poly_3   305.       87.2      3.50  4.75e- 4&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;After we trained our model, we can use it to predict the unseen test data and compare it with our actual data to assess its accuracy.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_poly_pred &amp;lt;- 
  predict(PISA_poly_fit, test_data) %&amp;gt;% 
  bind_cols(test_data %&amp;gt;% select(READING_SCORE)) 

kbl(head(PISA_poly_pred)) %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;responsive&amp;quot;),
                full_width = TRUE, position = &amp;quot;left&amp;quot;)  &lt;/code&gt;&lt;/pre&gt;
&lt;table class="table table-striped table-hover table-condensed table-responsive" style&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:right;"&gt;
.pred
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
READING_SCORE
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
398.7327
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
476.744
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
426.4006
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
542.238
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
410.2957
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
518.744
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
417.5398
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
597.441
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
415.7663
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
466.964
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
431.5449
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
572.923
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;To check if our 3rd degree polynomial model performed well, we can extract MSE, RMSE, and R^2 as our performance metrics.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mse_vec(
  truth = PISA_poly_pred$READING_SCORE, 
  estimate = PISA_poly_pred$.pred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 7470.233&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;rmse(PISA_poly_pred, 
     truth = READING_SCORE,
     estimate = .pred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 3
  .metric .estimator .estimate
  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
1 rmse    standard        86.4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;rsq(PISA_poly_pred, 
     truth = READING_SCORE,
     estimate = .pred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 3
  .metric .estimator .estimate
  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
1 rsq     standard      0.0584&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The metrics show that our polynomial slightly outperform our linear regression model, with higher R-squared and lower MSE. The problem is, how do we know if 3rd degree is the most appropriate degree we can make to predict our data. For polynomial regression, we can look for the most appropriate polynomial degree via cross-validation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="model-tuning-with-cross-validation"&gt;Model Tuning with Cross Validation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Cross-Validation is a way to evaluate performance of our machine learning models by dividing our data into smaller groups and use them to estimate how the model will perform in when used to make predictions on data that are not in our training set. Basically, we test our machines with smaller tests to see which of them qualifies for the final test. Another way we can see this is when we tune an old ratio to make it able to receive the clearest frequency of the broadcast, but we use the machines to automatically do it instead to save our time and effort.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will begin by setting up our recipe with &lt;code&gt;tune()&lt;/code&gt; and add it to a workflow.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_poly_tuned_recipe &amp;lt;- 
  recipe(READING_SCORE ~ CLASSCOMP + CLASSCOOP + ATTCOMP, data = train_data) %&amp;gt;%
  step_poly(CLASSCOMP, CLASSCOOP, degree = tune())

PISA_poly_wf &amp;lt;- workflow() %&amp;gt;%
  add_model(lm_mod) %&amp;gt;%
  add_recipe(PISA_poly_tuned_recipe)

PISA_poly_wf&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;== Workflow ==========================================================
Preprocessor: Recipe
Model: linear_reg()

-- Preprocessor ------------------------------------------------------
1 Recipe Step

* step_poly()

-- Model -------------------------------------------------------------
Linear Regression Model Specification (regression)

Computational engine: lm &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Then, we will create smaller data sets from our training set. We will create 10 sets (or 10 folds) to test 10 candidates of our model, which are models with polynomial degree from 1 (equivalent to linear regression), 2 (quadratic regression), 3 (cubic regression),…, to 10 (decic regression). In other words, we are looking for the model that performs the best among 10 candidates using RMSE and R-Squared as our performance metrics.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;train_data_subsetted = subset(train_data, select = c(&amp;#39;READING_SCORE&amp;#39;,&amp;#39;CLASSCOMP&amp;#39;,&amp;#39;CLASSCOOP&amp;#39;,&amp;#39;ATTCOMP&amp;#39;))

PISA_folds &amp;lt;- vfold_cv(train_data, v = 10)

degree_grid &amp;lt;- grid_regular(degree(range = c(1, 10)), levels = 10)

tune_resample &amp;lt;- tune_grid(
  object = PISA_poly_wf, 
  resamples = PISA_folds, 
  grid = degree_grid)

autoplot(tune_resample)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f823693b12_files/figure-html/unnamed-chunk-34-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We want RMSE to be as low as possible and R-Squared to be as high as possible. However, our models can only do so much. Their performance will remain somewhat steady when it reaches a certain point. In our case, it is from the 6th polynomial degree onward. While the plot suggests that the 10th polynomial degree performed the best, we do not want our model to be too complex as it can be harder to interpret. We want the model that performs the best and is easiest to interpret. For that, we might want to pick the model that comes right after a significant performance boost to not make it too complex while able to bring out its maximum performance, which is the 6th degree polynomial model (Sextic Regression).&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-02-27-statlearning10FCV.jpg" alt="" /&gt;
&lt;p class="caption"&gt;Data Tuning Results with 10-Folds Cross Validation&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;To dive into the numbers, we can have the software shows and selects the best model candidate to include it in our final model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;show_best(tune_resample, metric = &amp;quot;rmse&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 5 x 7
  degree .metric .estimator  mean     n std_err .config              
   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                
1     10 rmse    standard    85.4    10   0.852 Preprocessor10_Model1
2      6 rmse    standard    85.5    10   0.845 Preprocessor06_Model1
3      9 rmse    standard    85.5    10   0.860 Preprocessor09_Model1
4      8 rmse    standard    85.5    10   0.857 Preprocessor08_Model1
5      7 rmse    standard    85.5    10   0.843 Preprocessor07_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;select_by_one_std_err(tune_resample, degree, metric = &amp;quot;rsq&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 9
  degree .metric .estimator   mean     n std_err .config  .best .bound
   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
1      6 rsq     standard   0.0616    10 0.00730 Prepro~ 0.0647 0.0563&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="final-model"&gt;Final Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We will then assign the best model candidate as our &lt;code&gt;best_degree&lt;/code&gt; to include it in the final workflow. Then, as usual, we teach (fit) it and test it with our test data.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;best_degree &amp;lt;- select_by_one_std_err(tune_resample, degree, metric = &amp;quot;rsq&amp;quot;)

final_wf &amp;lt;- finalize_workflow(PISA_poly_wf, best_degree)

final_fit &amp;lt;- fit(final_wf, train_data)

final_fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;== Workflow [trained] ================================================
Preprocessor: Recipe
Model: linear_reg()

-- Preprocessor ------------------------------------------------------
1 Recipe Step

* step_poly()

-- Model -------------------------------------------------------------

Call:
stats::lm(formula = ..y ~ ., data = data)

Coefficients:
     (Intercept)           ATTCOMP  CLASSCOMP_poly_1  
         411.050             2.836           279.070  
CLASSCOMP_poly_2  CLASSCOMP_poly_3  CLASSCOMP_poly_4  
         300.125            19.260          -167.827  
CLASSCOMP_poly_5  CLASSCOMP_poly_6  CLASSCOOP_poly_1  
        -197.978          -477.187          1381.076  
CLASSCOOP_poly_2  CLASSCOOP_poly_3  CLASSCOOP_poly_4  
         -56.933           281.590          -251.840  
CLASSCOOP_poly_5  CLASSCOOP_poly_6  
        -130.564           389.358  &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Now, we will test our model and extract its performance metrics to evaluate it.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_poly_test_pred &amp;lt;- 
  predict(final_fit, test_data) %&amp;gt;% 
  bind_cols(test_data %&amp;gt;% select(READING_SCORE))

mse_vec(
  truth = PISA_poly_test_pred$READING_SCORE, 
  estimate = PISA_poly_test_pred$.pred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 7382.151&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;rmse(PISA_poly_test_pred, 
     truth = READING_SCORE,
     estimate = .pred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 3
  .metric .estimator .estimate
  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
1 rmse    standard        85.9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;rsq(PISA_poly_test_pred, 
     truth = READING_SCORE,
     estimate = .pred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 1 x 3
  .metric .estimator .estimate
  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
1 rsq     standard      0.0697&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;To compare the performance of both Linear Regression and 6th Degree Polynomial Regression, we will create a comparison table to compare their performance for our discussion.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;tabl &amp;lt;- &amp;quot;
| Model         | R-Square      | MSE   | RMSE |
|---------------|:-------------:|------:|-----:|
| Linear Reg    | 0.0584        | 7472.2| 86.4 |
| Sextic Reg    | 0.0697        | 7382.1| 85.9 |
&amp;quot;
cat(tabl)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th align="center"&gt;R-Square&lt;/th&gt;
&lt;th align="right"&gt;MSE&lt;/th&gt;
&lt;th align="right"&gt;RMSE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;Linear Reg&lt;/td&gt;
&lt;td align="center"&gt;0.0584&lt;/td&gt;
&lt;td align="right"&gt;7472.2&lt;/td&gt;
&lt;td align="right"&gt;86.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;Sextic Reg&lt;/td&gt;
&lt;td align="center"&gt;0.0697&lt;/td&gt;
&lt;td align="right"&gt;7382.1&lt;/td&gt;
&lt;td align="right"&gt;85.9&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;The table shows that our sextic regression slightly outperform our linear regression model as seen from its higher R-Squared and Lower Error Value (i.e., MSE, RMSE).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="final-remarks-and-conclusion"&gt;Final Remarks and Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;What we have done so far is developing two predictive models of linear regression and polynomial regression to predict students’ reading score from the degree of classroom cooperation, the degree of classroom competition, and students’ attitude to competition. While the traditional statistical approach examines the entire data set to make retrospective inferences, the statistical learning approach that we use both examines the relationship between the predictors and the targeted variables as well as testing predictive power of our models with the hold-out testing set, so that it can be used with new data we may receive in the future.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, considering the differences in performance metrics between the two algorithm, linear regression could be more appropriate with the variables examined in this study as its performance is slightly lower than sextic Regression at the trade-off of model interpretability. In other words, it is easier to interpret the model to non-technical audience and therefore easier to put the result into practice.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can incorporate the model into an &lt;a href="https://doi.org/10.1145/3027385.3027405"&gt;early warning system&lt;/a&gt; to inform teachers in their feedback provision to students in addition to other information such as students’ score and their class history. However, note that the models we developed in this post is far from perfect. The data set used in this study is relatively outdated as the new data of PISA 2022 cycle will be released in the future. Performance of the models leaves much to be desired as only 6% of the dependent variable was accounted for at most.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The thing is, we have accomplished what we are here for. We have tried our hands on developing a couple models with a large scale data set and user it to predict unseen data with statistical learning approach. Instead of going for complex models like Random Forest or Artificial Neural Networks, sticking to the ground with basic models could also be feasible in the actual practice as well. This is the point I am trying to make in this post. Thank you very much for reading. Have a good one!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>b2033900d5f03f16e91b1582152a1e58</distill:md5>
      <category>R</category>
      <category>Statistics</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-02-27-statlearning</guid>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-02-27-statlearning/statlearn.jpeg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Classical Test Theory in R</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-01-15-ctt</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Classical Test Theory (CTT - also known as weak theory or true-score theory) is a psychometric framework that analyzes psychological tests based on the three concepts of True score (T), Observed score (X), and Error score (E) &lt;a href="https://doi.org/10.1177/0013164498058003001"&gt;(Fan, 1998)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;CTT assumes that each person has a true score that could be obtained if there were no errors in measurement. This approach to psychometric employs simple mathematical estimates such as averages, proportions, and correlation as methods to measure test takers’ level of construct regardless of their individual attribute.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The major advantage of CTT are its relatively weak theoretical assumptions, which make it easy to apply in many testing situations such as classroom assessment without much constraints. Relatively weak theoretical assumptions not only characterize CTT but also its extensions (e.g., generalizability theory) as well.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;knitr::opts_chunk$set(error = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We will load essential libraries as usual and import test item data and its answer key.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(psych) #for general psychometric functions
library(CTT) #for CTT-based item analysis
library(tidyverse) #data toolbox for R
library(kableExtra) #neat tables for R markdown
library(psychometric) #for item difficulty&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The dataset we will use for this task is an assessment dataset for 241 students N = (241). The assessment comprises of 20 multiple-choice exam items. Each item has 4 response options.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will begin by loading in the unscored answer data as seen below. &lt;code&gt;i001&lt;/code&gt; represents item 1, &lt;code&gt;i002&lt;/code&gt; represents item 2 and so forth until item 20.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#Importing test data
data &amp;lt;- read_csv(&amp;quot;sample.score.csv&amp;quot;, col_names = T)

kbl(head(data)) %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;responsive&amp;quot;),
                full_width = TRUE, position = &amp;quot;left&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table table-striped table-hover table-condensed table-responsive" style&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:left;"&gt;
i001
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i002
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i003
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i004
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i005
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i006
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i007
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i008
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i009
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i010
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i011
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i012
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i013
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i014
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i015
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i016
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i017
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i018
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i019
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i020
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Next, we will import answer key data for all 20 items. Note that the answer keys need to match with the response options in the unscored response data; that is, if the answer for &lt;code&gt;i001&lt;/code&gt; is A, the key has to be “A”, not “a” or “B” or other responses.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#Importing test key
key &amp;lt;- read_csv(&amp;quot;sample.key.csv&amp;quot;, col_names = T) 
key &amp;lt;- as.matrix(key)


kbl(key) %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;responsive&amp;quot;),
                full_width = TRUE, position = &amp;quot;left&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table table-striped table-hover table-condensed table-responsive" style&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:left;"&gt;
i001
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i002
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i003
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i004
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i005
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i006
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i007
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i008
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i009
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i010
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i011
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i012
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i013
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i014
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i015
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i016
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i017
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i018
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i019
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
i020
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
B
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
A
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
D
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
C
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We will then use the function &lt;code&gt;score()&lt;/code&gt; function to score multiple choice responses by specifying our unscored response and answer key dataframe. What makes this function useful is that we can also request for reliability analysis with &lt;code&gt;rel = TRUE&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When scored, each item will be binarily coded, with 0 as incorrect and 1 as correct. We can also requested for item parameter such as mean of that item (&lt;code&gt;itemMean&lt;/code&gt;), point-biserial and biserial correlation (we will get there soon), Coefficient Alpha reliability (or Cronbach’s Alpha) if that item is removed from the exam, and we can even requested for the function to flag if that item has low reliability value.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#scoring
myScore &amp;lt;- score(data, key, output.scored=TRUE, rel = TRUE)

scored_item &amp;lt;- myScore$scored %&amp;gt;% as.data.frame()

kbl(head(scored_item)) %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;responsive&amp;quot;),
                full_width = TRUE, position = &amp;quot;left&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table table-striped table-hover table-condensed table-responsive" style&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:right;"&gt;
V1
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
V2
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
V3
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
V4
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
V5
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
V6
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
V7
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
V8
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
V9
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
V10
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
V11
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
V12
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
V13
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
V14
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
V15
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
V16
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
V17
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
V18
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
V19
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
V20
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class="r"&gt;&lt;code&gt;item_parameter &amp;lt;- itemAnalysis(scored_item, itemReport=TRUE, NA.Delete=TRUE, pBisFlag = T,  bisFlag = T, flagStyle = c(&amp;quot;X&amp;quot;,&amp;quot;&amp;quot;))

str(item_parameter)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;List of 6
 $ nItem     : int 20
 $ nPerson   : int 241
 $ alpha     : num 0.54
 $ scaleMean : num 9.44
 $ scaleSD   : num 3
 $ itemReport:&amp;#39;data.frame&amp;#39;: 20 obs. of  7 variables:
  ..$ itemName      : chr [1:20] &amp;quot;V1&amp;quot; &amp;quot;V2&amp;quot; &amp;quot;V3&amp;quot; &amp;quot;V4&amp;quot; ...
  ..$ itemMean      : num [1:20] 0.759 0.734 0.257 0.68 0.365 ...
  ..$ pBis          : num [1:20] 0.1148 0.1036 0.0506 0.0591 0.1111 ...
  ..$ bis           : num [1:20] 0.1576 0.1395 0.0686 0.0771 0.1423 ...
  ..$ alphaIfDeleted: num [1:20] 0.535 0.537 0.545 0.545 0.537 ...
  ..$ lowPBis       : chr [1:20] &amp;quot;X&amp;quot; &amp;quot;X&amp;quot; &amp;quot;X&amp;quot; &amp;quot;X&amp;quot; ...
  ..$ lowBis        : chr [1:20] &amp;quot;X&amp;quot; &amp;quot;X&amp;quot; &amp;quot;X&amp;quot; &amp;quot;X&amp;quot; ...
 - attr(*, &amp;quot;class&amp;quot;)= chr &amp;quot;itemAnalysis&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The output also comes with scores of each examinee on the exam, which we can perform a descriptive analysis with &lt;code&gt;psych::describe()&lt;/code&gt; on to learn how students of this section perform on this test with mean, median, mode, standard deviation, and so forth.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;kbl(head(myScore$score)) %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;responsive&amp;quot;),
                full_width = TRUE, position = &amp;quot;left&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table table-striped table-hover table-condensed table-responsive" style&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:left;"&gt;
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
x
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
P1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
P2
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
P3
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
13
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
P4
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
P5
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
14
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
P6
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
12
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class="r"&gt;&lt;code&gt;describe(myScore$score)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   vars   n mean sd median trimmed  mad min max range skew kurtosis
X1    1 241 9.44  3      9    9.34 2.97   1  19    18 0.26    -0.14
     se
X1 0.19&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can also transform the score into a new scale by setting a new mean, SD, and normality with score.transform(). We will refer to the IQ score, with its mean = 100 and its SD = 15. We can even transform the score into a Z score (mean = 0, SD = 1), T score (mean = 50, SD = 10), or even Stanine score (mean = 5, SD = 2).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#Score transformation
IQ &amp;lt;- score.transform(myScore$score, mu.new = 100, sd.new = 15, normalize = TRUE)

#new.scores is the transformed score
kbl(head(IQ$new.scores)) %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;responsive&amp;quot;),
                full_width = FALSE, position = &amp;quot;left&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table table-striped table-hover table-condensed table-responsive" style="width: auto !important; "&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:left;"&gt;
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
x
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
P1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
99.29768
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
P2
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
82.70691
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
P3
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
116.70018
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
P4
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
99.29768
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
P5
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
121.19395
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
P6
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
112.44734
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class="r"&gt;&lt;code&gt;#p.scores is the percentile rank of every examinee
kbl(head(IQ$p.scores)) %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;responsive&amp;quot;),
                full_width = FALSE, position = &amp;quot;left&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table table-striped table-hover table-condensed table-responsive" style="width: auto !important; "&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:left;"&gt;
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
x
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
P1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4813278
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
P2
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1244813
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
P3
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.8672199
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
P4
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4813278
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
P5
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.9211618
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:left;"&gt;
P6
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.7966805
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;We can also visualize Item Characteristic Curve (ICC) of each item from the scoring output. The x-axis indicates total test score of examinees while the y-axis indicates the proportion of examinee that gets that item right. For example, there are 70% (or 0.7) of examinees who have 4 as their total score got item 1 correctly. The curve goes up as total score of the examinee increases, with the proportion of examinee that gets the item correctly at 100% in examinees with the total score of 16 and above.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;cttICC(score = myScore$score, itemVector = myScore$scored[,1], 
       xlab = &amp;quot;Total test score&amp;quot;,
       ylab = &amp;quot;Proportion of the examinee&amp;quot;,
       plotTitle = &amp;quot;Item Characteristic Curve of item 1&amp;quot;,
       colTheme=&amp;quot;dukes&amp;quot;, cex=1.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f8a977b99_files/figure-html/unnamed-chunk-10-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We will convert our score data into matrix for for further implementation with item analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#extract responses only as a matrix
responses &amp;lt;- as.matrix(myScore$scored) &lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="reliability-analysis"&gt;Reliability Analysis&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Reliability of a test is an extent to which a test can produce consistent scores for a particular sample of examinees; that is, a test should yield similar, if not the same, scores for an examinee when taken multiple times, given that there is minimal interference such as practice effect. Test reliability can be measured in different ways such as test-retest, internal consistency, or alternate forms.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="internal-consistency"&gt;Internal Consistency&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;For starter, we will compute internal consistency of the test, which includes Coefficient Alpha (or Cronbach’s Alpha), Guttman’s Lambda-6, Kuder-Richardson 20, and Kuder-Richardson 21. The output also yields the proportion of students who got the item correctly to students who got the item incorrectly as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;psych::alpha(myScore$scored, check.keys = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Reliability analysis   
Call: psych::alpha(x = myScore$scored, check.keys = T)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
      0.55      0.55    0.58     0.057 1.2 0.041 0.48 0.15    0.045

    95% confidence boundaries 
         lower alpha upper
Feldt     0.47  0.55  0.63
Duhachek  0.47  0.55  0.63

 Reliability if an item is dropped:
     raw_alpha std.alpha G6(smc) average_r  S/N alpha se  var.r med.r
V1        0.55      0.54    0.58     0.059 1.18    0.042 0.0081 0.045
V2        0.56      0.55    0.58     0.060 1.21    0.041 0.0079 0.048
V3        0.56      0.55    0.59     0.060 1.22    0.041 0.0080 0.048
V4        0.56      0.56    0.59     0.062 1.25    0.040 0.0080 0.052
V5        0.55      0.54    0.58     0.059 1.20    0.041 0.0081 0.048
V6        0.51      0.50    0.54     0.050 1.00    0.046 0.0071 0.041
V7        0.56      0.55    0.59     0.062 1.25    0.040 0.0078 0.051
V8        0.55      0.54    0.58     0.059 1.19    0.041 0.0080 0.048
V9        0.55      0.54    0.58     0.058 1.17    0.042 0.0080 0.048
V10-      0.56      0.56    0.59     0.062 1.25    0.040 0.0079 0.052
V11       0.54      0.54    0.57     0.057 1.15    0.042 0.0081 0.045
V12       0.55      0.54    0.58     0.058 1.17    0.042 0.0080 0.044
V13       0.53      0.52    0.56     0.055 1.10    0.043 0.0074 0.044
V14       0.53      0.52    0.56     0.055 1.10    0.043 0.0076 0.043
V15       0.54      0.53    0.57     0.057 1.14    0.042 0.0076 0.044
V16       0.51      0.51    0.55     0.051 1.03    0.045 0.0068 0.044
V17       0.49      0.48    0.52     0.047 0.93    0.048 0.0056 0.039
V18       0.51      0.50    0.53     0.050 1.00    0.046 0.0060 0.044
V19       0.55      0.54    0.58     0.059 1.19    0.042 0.0079 0.045
V20       0.55      0.55    0.58     0.060 1.20    0.041 0.0077 0.048

 Item statistics 
       n raw.r std.r r.cor r.drop mean   sd
V1   241  0.26  0.28 0.181  0.125 0.76 0.43
V2   241  0.23  0.24 0.127  0.083 0.73 0.44
V3   241  0.22  0.23 0.115  0.082 0.26 0.44
V4   241  0.20  0.20 0.066  0.043 0.68 0.47
V5   241  0.26  0.26 0.152  0.106 0.37 0.48
V6   241  0.51  0.50 0.497  0.370 0.54 0.50
V7   241  0.19  0.20 0.074  0.045 0.28 0.45
V8   241  0.27  0.27 0.163  0.113 0.48 0.50
V9   241  0.26  0.29 0.199  0.143 0.83 0.38
V10- 241  0.21  0.20 0.067  0.043 0.56 0.50
V11  241  0.32  0.32 0.228  0.169 0.34 0.47
V12  241  0.31  0.30 0.205  0.148 0.56 0.50
V13  241  0.39  0.38 0.323  0.240 0.39 0.49
V14  241  0.40  0.38 0.327  0.247 0.45 0.50
V15  241  0.33  0.33 0.254  0.183 0.31 0.46
V16  241  0.47  0.47 0.452  0.337 0.64 0.48
V17  241  0.60  0.59 0.647  0.478 0.44 0.50
V18  241  0.52  0.50 0.526  0.387 0.33 0.47
V19  241  0.25  0.27 0.176  0.123 0.19 0.39
V20  241  0.26  0.25 0.152  0.097 0.44 0.50

Non missing response frequency for each item
         0    1 miss
 [1,] 0.24 0.76    0
 [2,] 0.27 0.73    0
 [3,] 0.74 0.26    0
 [4,] 0.32 0.68    0
 [5,] 0.63 0.37    0
 [6,] 0.46 0.54    0
 [7,] 0.72 0.28    0
 [8,] 0.52 0.48    0
 [9,] 0.17 0.83    0
[10,] 0.56 0.44    0
[11,] 0.66 0.34    0
[12,] 0.44 0.56    0
[13,] 0.61 0.39    0
[14,] 0.55 0.45    0
[15,] 0.69 0.31    0
[16,] 0.36 0.64    0
[17,] 0.56 0.44    0
[18,] 0.67 0.33    0
[19,] 0.81 0.19    0
[20,] 0.56 0.44    0&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="kuder-richardson"&gt;Kuder-Richardson&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;For Kuder-Richardson formula 20 (KR20) and Kuder-Richardson formula 21 (KR21), we can write functions that compute them as follows:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;KR20 &amp;lt;-
  function(X)
    {
    X &amp;lt;- data.matrix(X)
    k &amp;lt;- ncol(X)
    
    # Person total score variances
    SX &amp;lt;- var(rowSums(X))
    
    # item means
    IM &amp;lt;- colMeans(X)
    
    return(((k/(k - 1))*((SX - sum(IM*(1 - IM)))/SX)))
  }

KR20(responses)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.541653&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;KR21 &amp;lt;-
  function(X)
    {
    X &amp;lt;- data.matrix(X)
    n &amp;lt;- ncol(X)
    
    return((n/(n-1))*((var(rowSums(X)) - n*(sum(colMeans(X))/n) * 
                         (1-(sum(colMeans(X))/n))))/var(rowSums(X)))
  }

KR21(responses)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.4700428&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="split-half-test-retest-reliability"&gt;Split-half (Test-Retest) Reliability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The test-retest reliability is an estimation of reliability based on the correlation of two equivalent forms of the tests. It is not recommended to use the same test form twice to avoid practice effect, which could artifically increase reliability coefficient of the test.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;psych::splitHalf(scored_item, raw = TRUE, check.keys = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Split half reliabilities  
Call: psych::splitHalf(r = scored_item, raw = TRUE, check.keys = TRUE)

Maximum split half reliability (lambda 4) =  0.69
Guttman lambda 6                          =  0.58
Average split half reliability            =  0.55
Guttman lambda 3 (alpha)                  =  0.55
Guttman lambda 2                          =  0.57
Minimum split half reliability  (beta)    =  0.32
Average interitem r =  0.06  with median =  0.04
                                             2.5% 50% 97.5%
 Quantiles of split half reliability      =  0.44 0.55 0.63&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="spearman-brown-reliability"&gt;Spearman-Brown Reliability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;To compute Spearman-Brown reliability, we need to use Cronbach’s Alpha as a base computation; for that, we need to export the function as a separate file first.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#With a written function
cronbachs.alpha &amp;lt;-
  function(X)
    {
    
    X &amp;lt;- data.matrix(X)
    n &amp;lt;- ncol(X) # Number of items
    k &amp;lt;- nrow(X) # Number of examinees
    
    # Cronbachs alpha
    alpha &amp;lt;- (n/(n - 1))*(1 - sum(apply(X, 2, var))/var(rowSums(X)))
    
    return(list(&amp;quot;Crombach&amp;#39;s alpha&amp;quot; = alpha,
                &amp;quot;Number of items&amp;quot; = n,
                &amp;quot;Number of examinees&amp;quot; = k))
    }

#Dump &amp;quot;cronbach.alpha&amp;quot; function for further use
dump(&amp;quot;cronbachs.alpha&amp;quot;, file = &amp;quot;cronbachs.alpha.R&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The Spearman-Brown prophecy formula is a formula that predicts reliability of the test through the modification of test length. This formula is one way can use to answer questions such as “how short can I make my assessment?” or “how many items should I write for my test to have adequate reliability?”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Reliability of a test usually decreases when we shorten the test length and increases when we add more test items as well. However, there is no magic number of how long a test should be, so one should consider their context such as nature of the test content, ability of the examinee, and average time for a student to complete the assessment.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;SpearmanBrown &amp;lt;- 
  function(x, n1, n2)
    {
    
    source(&amp;quot;cronbachs.alpha.R&amp;quot;)
    
    x &amp;lt;- as.matrix(x)
    N &amp;lt;- n2/n1
    
    # cronbach&amp;#39;s alpha for the original test
    alpha &amp;lt;- cronbachs.alpha(x)[[1]]
    predicted.alpha &amp;lt;- N * alpha / (1 + (N - 1) * alpha)
    
    return(list(original.reliability = alpha,
                original.sample.size = n1,
                predicted.reliability = predicted.alpha,
                predicted.sample.size = n2))
  }

# predict reliability by Spearman-Brown formula
# if the number of items is reduced from 25 to 15
SpearmanBrown(responses, n1 = 20, n2 = 15)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$original.reliability
[1] 0.5395239

$original.sample.size
[1] 20

$predicted.reliability
[1] 0.4677309

$predicted.sample.size
[1] 15&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# predict reliability by Spearman-Brown formula
# if the number of items is increased from 25 to 35
SpearmanBrown(responses, n1 = 20, n2 = 35)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$original.reliability
[1] 0.5395239

$original.sample.size
[1] 20

$predicted.reliability
[1] 0.6721757

$predicted.sample.size
[1] 35&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# predict reliability by Spearman-Brown formula
# if the number of items is doubled
SpearmanBrown(responses, n1 = 20, n2 = 40)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$original.reliability
[1] 0.5395239

$original.sample.size
[1] 20

$predicted.reliability
[1] 0.7008971

$predicted.sample.size
[1] 40&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="guttmans-lambda"&gt;Guttman’s Lambda&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Guttman’s Lambda is also another reliability coefficient we can use in similar way as Coefficient Alpha (or Cronbach’s Alpha).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;psych::guttman(responses)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Call: psych::guttman(r = responses)

Alternative estimates of reliability

Guttman bounds 
L1 =  0.51 
L2 =  0.56 
L3 (alpha) =  0.53 
L4 (max) =  0.7 
L5 =  0.55 
L6 (smc) =  0.57 
TenBerge bounds 
mu0 =  0.53 mu1 =  0.56 mu2 =  0.56 mu3 =  0.56 

alpha of first PC =  0.64 
estimated greatest lower bound based upon communalities=  0.69 

beta found by splitHalf  =  0.33 &lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="pearson-product-moment-correlation-coefficient"&gt;Pearson product-moment correlation coefficient&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Another way we can compute reliability of the test is via Pearson product-moment correlation between two halves of the test (aka Split-half method). However, comparing only a pair or two of the test half might not be enough. We can use Bootstrap resampling method to randomly split the examinees into two halves 1000 times, so that we can make sure that the computation is as exhaustive as possible.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#Split data (variables-item) into two equally and randomly.

split.items &amp;lt;- 
  function(X, seed = NULL)
    {
    # optional fixed seed
    if (!is.null(seed)) {set.seed(seed)} 
    
    X &amp;lt;- as.matrix(X)
    
    # if n = 2x, then lengths Y1 = Y2
    # if n = 2x+1, then lenths Y1 = Y2+1
    n &amp;lt;- ncol(X)
    index &amp;lt;- sample(1:n, ceiling(n/2))
    Y1 &amp;lt;- X[, index ]
    Y2 &amp;lt;- X[, -index]
    return(list(Y1, Y2)) 
  }

dump(&amp;quot;split.items&amp;quot;, file = &amp;quot;split.items.R&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;pearson &amp;lt;- 
  function(X, seed = NULL, n = NULL)
    {
    source(&amp;quot;split.items.R&amp;quot;)
    
    # optional fixed seed
    if (!is.null(seed)) {set.seed(seed)}
    
    # the number of bootstrap replicates. 1e3 = 1000
    if (is.null(n)) {n &amp;lt;- 1e3}   
    
    X &amp;lt;- as.matrix(X)
    r &amp;lt;- rep(NA, n)
    
    for (i in 1:n) {
      # split items
      Y &amp;lt;- split.items(X)
      
      # total scores
      S1 &amp;lt;- as.matrix(rowSums(Y[[1]]))
      S2 &amp;lt;- as.matrix(rowSums(Y[[2]]))
      
      # residual scores
      R1 &amp;lt;- S1 - mean(S1)
      R2 &amp;lt;- S2 - mean(S2)
      
      # Pearson product-moment correlation coefficient
      r[i] &amp;lt;- (t(R1) %*% R2) / (sqrt((t(R1) %*% R1)) * sqrt((t(R2) %*% R2)))
    }
    
    return(mean(r))
  }

# compute the Pearson product-moment correlation coefficient
pearson(responses, seed = 456, n = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.3499066&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can also double check if the function is right by splitting response of the examinee into two halves and compute a correlation coefficient by ourselves.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# compare
# split items
set.seed(456)
Y &amp;lt;- split.items(responses)

# total scores
Set1 &amp;lt;- as.matrix(rowSums(Y[[1]]))
Set2 &amp;lt;- as.matrix(rowSums(Y[[2]]))

cor(Set1, Set2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          [,1]
[1,] 0.3499066&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Note that the split-half method can both overestimate or underestimate ability of the examinees when two halves of the test are not parallel.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="standard-error-of-measurement"&gt;Standard Error of Measurement&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Standard error of measurement is a measure of the spread of observed score around the “true” score. The standard error of measurement that uses Coefficient’s alpha as its reliability statistics can be computed as follows:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;SEM &amp;lt;-
  function(X){
    source(&amp;quot;cronbachs.alpha.R&amp;quot;)
    X &amp;lt;- data.matrix(X)
    
    return(sd(rowSums(X)) * sqrt(1 - cronbachs.alpha(X)[[1]]))
  }

SEM(responses)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2.036401&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;As a result, the range in which the true score of an examinee might stay in could be ± 2.036401.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="confidence-intervals-for-true-scores"&gt;Confidence Intervals for True Scores&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We can also compute confidence interval for true scores of each item via the code below. Note that one limitation of CTT is that it is population dependent, so the confidence interval may change when the test is administered to a different group of examinees.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# 90% confidence interval for the true score
head(cbind(lower_bound = round(rowSums(responses)-1.65* sd(rowSums(responses))*
                                 sqrt(1-KR20(responses)), 2), observed = rowSums(responses),
           upper_bound = round(rowSums(responses)+1.65* sd(rowSums(responses))*
                                 sqrt(1-KR20(responses)), 2)), 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      lower_bound observed upper_bound
 [1,]        5.65        9       12.35
 [2,]        2.65        6        9.35
 [3,]        9.65       13       16.35
 [4,]        5.65        9       12.35
 [5,]       10.65       14       17.35
 [6,]        8.65       12       15.35
 [7,]        3.65        7       10.35
 [8,]        5.65        9       12.35
 [9,]        4.65        8       11.35
[10,]        7.65       11       14.35
[11,]        2.65        6        9.35
[12,]        4.65        8       11.35
[13,]        1.65        5        8.35
[14,]        9.65       13       16.35
[15,]        4.65        8       11.35
[16,]        1.65        5        8.35
[17,]       11.65       15       18.35
[18,]        4.65        8       11.35
[19,]        6.65       10       13.35
[20,]        9.65       13       16.35&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="item-analysis"&gt;Item Analysis&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;Item Analysis of the Classical Test Theory approach relies on two statistics to evaluate an item, P-value (not to be confused with &lt;em&gt;p&lt;/em&gt;-value in statistical tests) and point-biserial correlation coefficient (p-Bis).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;P-value in this context represents the proportion of examinees responding in the keyed direction It is typically referred to as &lt;strong&gt;item difficulty&lt;/strong&gt;. Point-biserial corrrelation coefficient is the correlation between a particular item and other items; this index is typically referred to as &lt;strong&gt;item discrimination&lt;/strong&gt; that indicates the degree to which that item can distinguish high ability examinee or examinees who actually know that construct from examinees who do not possess adequate knowledge to get that item correctly.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="item-discrimination."&gt;Item Discrimination.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;As mentioned above, item discrimination refers to how well an item discriminates high-ability examinees from those with low-ability. Items that are very hard (i.e., p &amp;lt; 0.20) or very easy (p &amp;gt; 0.90) usually have lower item discrimination values than items with medium difficulty.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We usually examine point-biserial correlation coefficient (p-Bis) of the item. If p-Bis is lower than 0.20, the item can be flagged for low discrimination, while 0.20 to 0.39 indicates good discrimination, and 0.4 and above indicates excellent discrimination. If p-Bis is negative, then the item doesn’t seem to measure the same construct that the other items are measuring. It could also mean that the item is mis-keyed.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;item.analysis &amp;lt;- 
  function(responses)
    {
    # CRITICAL VALUES
    cvpb = 0.20
    cvdl = 0.15
    cvdu = 0.85
    
    require(CTT, warn.conflicts = FALSE, quietly = TRUE)
    (ctt.analysis &amp;lt;- CTT::reliability(responses, itemal = TRUE, NA.Delete = TRUE))
    
    # Mark items that are potentially problematic
    item.analysis &amp;lt;- data.frame(item = seq(1:ctt.analysis$nItem),
                                r.pbis = ctt.analysis$pBis,
                                bis = ctt.analysis$bis,
                                item.mean = ctt.analysis$itemMean,
                                alpha.del = ctt.analysis$alphaIfDeleted)
  
    if (TRUE) {
      item.analysis$check &amp;lt;- 
        ifelse(item.analysis$r.pbis &amp;lt; cvpb |
                 item.analysis$item.mean &amp;lt; cvdl |
                 item.analysis$item.mean &amp;gt; cvdu, &amp;quot;X&amp;quot;, &amp;quot;&amp;quot;)
    }
    
    return(item.analysis)
  }

kbl(item.analysis(responses)) %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;responsive&amp;quot;),
                full_width = FALSE, position = &amp;quot;left&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table table-striped table-hover table-condensed table-responsive" style="width: auto !important; "&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:right;"&gt;
item
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
r.pbis
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
bis
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
item.mean
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
alpha.del
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
check
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
1
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1147596
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1683232
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.7593361
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5353385
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
X
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
2
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1035530
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1409292
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.7344398
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5372914
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
X
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
3
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0506175
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0685887
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2572614
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5452346
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
X
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
4
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0591143
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0785356
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.6804979
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5450389
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
X
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
5
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1111202
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1406669
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3651452
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5369402
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
X
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
6
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3656502
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4640379
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5394191
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4909063
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
7
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0700444
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0920897
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2780083
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5426513
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
X
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
8
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1112357
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1394688
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4813278
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5373705
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
X
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
9
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1515853
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2230714
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.8298755
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5299043
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
X
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
10
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
-0.0434185
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
-0.0549382
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4356846
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5634988
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
X
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
11
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1696824
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2157830
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3360996
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5269375
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
X
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
12
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1279600
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1627060
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5601660
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5343645
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
X
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
13
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2312726
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2933878
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3900415
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5161192
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
14
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2134979
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2656683
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4481328
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5191567
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
15
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1947141
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2547864
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3112033
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5227779
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
X
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
16
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3353941
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4428850
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.6390041
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4977310
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
17
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4609036
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5799529
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4356846
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4727918
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
18
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4013261
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5080745
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3319502
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4865119
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
19
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0956187
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1323704
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1908714
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5375545
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
X
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
20
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1054536
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1323520
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4356846
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5382775
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
X
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="item-difficulty"&gt;Item Difficulty&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Under CTT, item difficulty is simply the percentages of examinees obtaining the correct answer. Item difficulty ranges from 0 to 1, with higher values indicate easier items as more examinees are able to correctly answer this item. This index is useful in assessing whether it is appropriate for the level of the students taking the test.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The desired range of item difficulty index is between 0.3 to 0.9 (by approximate), while the number close to 0 or 1 offers little information on measuring students’ level of the construct. In plain words, we wouldn’t want to have test items that are too easy that everybody get it right, or items that are too hard that no one can answer it correctly. However, the extreme cut-off for item difficulty could apply to measurements that are designed for extreme groups.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;Item_Difficulty &amp;lt;- item.exam(x = responses, y = NULL, discrim = T)

kbl(Item_Difficulty) %&amp;gt;%
  kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;, &amp;quot;hover&amp;quot;, &amp;quot;condensed&amp;quot;, &amp;quot;responsive&amp;quot;),
                full_width = FALSE, position = &amp;quot;left&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table table-striped table-hover table-condensed table-responsive" style="width: auto !important; "&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align:right;"&gt;
Sample.SD
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
Item.total
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
Item.Tot.woi
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
Difficulty
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
Discrimination
&lt;/th&gt;
&lt;th style="text-align:left;"&gt;
Item.Criterion
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
Item.Reliab
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
Item.Rel.woi
&lt;/th&gt;
&lt;th style="text-align:right;"&gt;
Item.Validity
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
0.4283763
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2544665
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1147596
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.7593361
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2125
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1087810
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0490582
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
0.4425501
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2483214
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1035530
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.7344398
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2875
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1096664
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0457322
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
0.4380344
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1956677
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0506175
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2572614
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2375
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0855312
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0221262
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
0.4672541
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2135535
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0591143
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.6804979
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2125
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0995765
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0275640
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
0.4824729
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2684805
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1111202
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3651452
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3125
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1292655
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0535011
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
0.4994811
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5054235
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3656502
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5394191
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5500
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2519252
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1822561
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
0.4489499
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2181283
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0700444
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2780083
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1625
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0977253
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0313811
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
0.5006911
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2744754
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1112357
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4813278
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3125
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1371420
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0555790
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
0.3765241
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2730001
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1515853
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.8298755
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2375
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1025776
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0569570
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
0.4968782
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1224407
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
-0.0434185
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4356846
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1375
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0607118
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
-0.0215289
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
0.4733565
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3208135
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1696824
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3360996
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3125
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1515438
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0801535
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
0.4973999
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2892525
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1279600
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5601660
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2625
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1435753
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0635151
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
0.4887744
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3825120
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2312726
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3900415
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4000
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1865738
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1128054
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
0.4983375
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3691600
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2134979
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4481328
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4750
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1835842
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1061730
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
0.4639493
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3412014
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1947141
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3112033
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3625
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1579714
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0901499
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
0.4812889
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4738815
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3353941
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.6390041
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4750
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2276002
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1610862
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
0.4968782
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5863010
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4609036
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4356846
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.6125
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2907152
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2285373
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
0.4718933
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5290626
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4013261
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3319502
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.5000
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2491426
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1889898
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
0.3938058
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2248264
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0956187
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1908714
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1500
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0883541
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0375770
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align:right;"&gt;
0.4968782
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.2677464
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1054536
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.4356846
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.3125
&lt;/td&gt;
&lt;td style="text-align:left;"&gt;
NA
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.1327610
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
0.0522888
&lt;/td&gt;
&lt;td style="text-align:right;"&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="visualizing-item-discrimination"&gt;Visualizing Item Discrimination&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We can use the functions below to visualize for items that should be revised for low discrimination power&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;item.discrimination &amp;lt;-
  function(responses)
    {
    # CRITICAL VALUES
    cvpb = 0.20
    cvdl = 0.15
    cvdu = 0.85
    
    require(CTT, warn.conflicts = FALSE, quietly = TRUE)
    ctt.analysis &amp;lt;- CTT::reliability(responses, itemal = TRUE, NA.Delete = TRUE)
    
    item.discrimination &amp;lt;- data.frame(item = 1:ctt.analysis$nItem , 
                                      discrimination = ctt.analysis$pBis)
    
    plot(item.discrimination,
         type = &amp;quot;p&amp;quot;,
         pch = 1,
         cex = 3,
         col = &amp;quot;purple&amp;quot;,
         ylab = &amp;quot;point-Biserial correlation&amp;quot;,
         xlab = &amp;quot;Item Number&amp;quot;,
         ylim = c(0, 1),
         main = &amp;quot;Test Item Discriminations&amp;quot;)
    
    abline(h = cvpb, col = &amp;quot;red&amp;quot;)
    
    outlier &amp;lt;- data.matrix(subset(item.discrimination,
                                  subset = (item.discrimination[, 2] &amp;lt; cvpb)))
    
    text(outlier, paste(&amp;quot;i&amp;quot;, outlier[,1], sep = &amp;quot;&amp;quot;), col = &amp;quot;red&amp;quot;, cex = .7)
    
    return(item.discrimination[order(item.discrimination$discrimination),])
  }

item.discrimination(responses)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f8a977b99_files/figure-html/unnamed-chunk-28-1.png" width="672" data-distill-preview=1 /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   item discrimination
10   10    -0.04341855
3     3     0.05061749
4     4     0.05911432
7     7     0.07004435
19   19     0.09561872
2     2     0.10355300
20   20     0.10545357
5     5     0.11112015
8     8     0.11123568
1     1     0.11475958
12   12     0.12795997
9     9     0.15158531
11   11     0.16968243
15   15     0.19471413
14   14     0.21349786
13   13     0.23127262
16   16     0.33539410
6     6     0.36565023
18   18     0.40132606
17   17     0.46090362&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="visualizing-item-total-correlation-bis"&gt;Visualizing Item Total-Correlation (Bis)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The item total correlation is a correlation between the question score (e.g., 0 or 1 for multiple choice) and the overall assessment score. The assumption is that if an examinee gets a question correctly they should, in general, have higher overall assessment scores than participants who get that question wrong.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;test_item.total &amp;lt;-
  function(responses)
    {
    # CRITICAL VALUES
    cvpb = 0.20
    cvdl = 0.15
    cvdu = 0.85
    
    require(CTT, warn.conflicts = FALSE, quietly = TRUE)
    ctt.analysis &amp;lt;- CTT::reliability(responses, itemal = TRUE, NA.Delete = TRUE)
    
    test_item.total &amp;lt;- data.frame(item = 1:ctt.analysis$nItem , 
                                  biserial = ctt.analysis$bis)
    
    plot(test_item.total,
         main = &amp;quot;Test Item-Total Correlation&amp;quot;,
         type = &amp;quot;p&amp;quot;,
         pch = 1,
         cex = 2.8,
         col = &amp;quot;purple&amp;quot;,
         ylab = &amp;quot;Biserial correlation&amp;quot;,
         xlab = &amp;quot;Item Number&amp;quot;,
         ylim = c(0, 1),
         xlim = c(0, ctt.analysis$nItem))
    
    abline(h = cvpb, col = &amp;quot;red&amp;quot;)
    
    outlier &amp;lt;- data.matrix(subset(test_item.total,
                                  subset = test_item.total[,2] &amp;lt; cvpb))
    
    text(outlier, paste(&amp;quot;i&amp;quot;, outlier[,1], sep = &amp;quot;&amp;quot;), col = &amp;quot;red&amp;quot;, cex = .7)
    
    return(test_item.total[order(test_item.total$biserial),])
    }

test_item.total(responses)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f8a977b99_files/figure-html/unnamed-chunk-29-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   item    biserial
10   10 -0.05493825
3     3  0.06858871
4     4  0.07853557
7     7  0.09208970
20   20  0.13235198
19   19  0.13237037
8     8  0.13946879
5     5  0.14066694
2     2  0.14092920
12   12  0.16270597
1     1  0.16832321
11   11  0.21578295
9     9  0.22307140
15   15  0.25478638
14   14  0.26566826
13   13  0.29338781
16   16  0.44288497
6     6  0.46403793
18   18  0.50807452
17   17  0.57995293&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="distractoroption-analysis"&gt;Distractor/Option Analysis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In distractor analysis, examinees are divided into three ability levels (i.e., lower, middle and upper) based on their total test score. The proportions of examinees who mark each option in each of the three ability levels are compared. In the lower ability level, we would expect to see a smaller proportion of examinees choosing the correct option and a larger proportion of them choosing the incorrect options (known as distractors).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ideally, good distractors would attract about the same proportion of examinees. Distractors that don’t attract any or attract a very small proportion of examinees relative to other distractors should be considered for revision. We do not want response options that are too obvious.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In those with higher ability level, we would expect to see the majority of examinees choose the correct option. If distractors are more appealing than the correct option to high ability examineess, then it should be eliminated or revised.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;distractorAnalysis(items = data, key = key, nGroups = 4, pTable = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$i001
  correct key   n       rspP       pBis     discrim      lower
A       *   A 183 0.75933610  0.1147596  0.22334218 0.70769231
B           B  16 0.06639004 -0.2888067 -0.13846154 0.13846154
C           C  22 0.09128631 -0.1975788 -0.05968170 0.07692308
D           D  20 0.08298755 -0.1841578 -0.02519894 0.07692308
       mid50      mid75      upper
A 0.63636364 0.78846154 0.93103448
B 0.06060606 0.05769231 0.00000000
C 0.16666667 0.09615385 0.01724138
D 0.13636364 0.05769231 0.05172414

$i002
  correct key   n       rspP        pBis     discrim      lower
A           A   8 0.03319502 -0.01743924  0.01909814 0.01538462
B           B  40 0.16597510 -0.28940212 -0.16180371 0.23076923
C       *   C 177 0.73443983  0.10355300  0.26392573 0.61538462
D           D  16 0.06639004 -0.28880672 -0.12122016 0.13846154
       mid50      mid75      upper
A 0.01515152 0.07692308 0.03448276
B 0.16666667 0.19230769 0.06896552
C 0.74242424 0.71153846 0.87931034
D 0.07575758 0.01923077 0.01724138

$i003
  correct key  n      rspP        pBis     discrim     lower
A           A 61 0.2531120 -0.21045253 -0.05278515 0.2769231
B           B 74 0.3070539 -0.32875754 -0.22387268 0.4307692
C       *   C 62 0.2572614  0.05061749  0.16021220 0.1846154
D           D 44 0.1825726 -0.04787523  0.11644562 0.1076923
      mid50     mid75     upper
A 0.2878788 0.2115385 0.2241379
B 0.2878788 0.2884615 0.2068966
C 0.1818182 0.3461538 0.3448276
D 0.2424242 0.1538462 0.2241379

$i004
  correct key   n       rspP        pBis     discrim      lower
A           A  11 0.04564315 -0.16015191 -0.09045093 0.10769231
B           B  24 0.09958506 -0.15640748 -0.04244032 0.07692308
C           C  42 0.17427386 -0.28460760 -0.15437666 0.29230769
D       *   D 164 0.68049793  0.05911432  0.28726790 0.52307692
      mid50      mid75      upper
A 0.0000000 0.05769231 0.01724138
B 0.1515152 0.13461538 0.03448276
C 0.1363636 0.11538462 0.13793103
D 0.7121212 0.69230769 0.81034483

$i005
  correct key  n       rspP       pBis     discrim      lower
A           A 84 0.34854772 -0.2097597 -0.02997347 0.32307692
B       *   B 88 0.36514523  0.1111202  0.27294430 0.26153846
C           C 61 0.25311203 -0.3237027 -0.19681698 0.36923077
D           D  8 0.03319502 -0.1706663 -0.04615385 0.04615385
       mid50      mid75     upper
A 0.42424242 0.34615385 0.2931034
B 0.22727273 0.48076923 0.5344828
C 0.30303030 0.13461538 0.1724138
D 0.04545455 0.03846154 0.0000000

$i006
  correct key   n       rspP       pBis    discrim     lower
A           A  61 0.25311203 -0.3587149 -0.2503979 0.3538462
B       *   B 130 0.53941909  0.3656502  0.6466844 0.2153846
C           C  34 0.14107884 -0.3800902 -0.2615385 0.2615385
D           D  16 0.06639004 -0.2994444 -0.1347480 0.1692308
       mid50      mid75      upper
A 0.34848485 0.17307692 0.10344828
B 0.45454545 0.69230769 0.86206897
C 0.16666667 0.11538462 0.00000000
D 0.03030303 0.01923077 0.03448276

$i007
  correct key  n      rspP        pBis      discrim     lower
A       *   A 67 0.2780083  0.07004435  0.211936340 0.1846154
B           B 63 0.2614108 -0.17772394  0.027851459 0.2307692
C           C 47 0.1950207 -0.13004489  0.003183024 0.1692308
D           D 64 0.2655602 -0.32387947 -0.242970822 0.4153846
      mid50     mid75     upper
A 0.2424242 0.3076923 0.3965517
B 0.3333333 0.2115385 0.2586207
C 0.1818182 0.2692308 0.1724138
D 0.2424242 0.2115385 0.1724138

$i008
  correct key   n       rspP        pBis     discrim      lower
A           A  61 0.25311203 -0.45040855 -0.34641910 0.41538462
B           B  53 0.21991701 -0.11191555  0.01061008 0.23076923
C       *   C 116 0.48132780  0.11123568  0.36286472 0.29230769
D           D  11 0.04564315 -0.08820493 -0.02705570 0.06153846
       mid50      mid75      upper
A 0.34848485 0.13461538 0.06896552
B 0.16666667 0.25000000 0.24137931
C 0.42424242 0.59615385 0.65517241
D 0.06060606 0.01923077 0.03448276

$i009
  correct key   n       rspP       pBis     discrim      lower
A           A   8 0.03319502 -0.1554692 -0.04244032 0.07692308
B       *   B 200 0.82987552  0.1515853  0.28488064 0.64615385
C           C  19 0.07883817 -0.2169620 -0.15013263 0.18461538
D           D  14 0.05809129 -0.2866682 -0.09230769 0.09230769
       mid50      mid75      upper
A 0.01515152 0.00000000 0.03448276
B 0.87878788 0.88461538 0.93103448
C 0.01515152 0.07692308 0.03448276
D 0.09090909 0.03846154 0.00000000

$i010
  correct key   n       rspP        pBis     discrim     lower
A           A  94 0.39004149 -0.13702229  0.02360743 0.3384615
B           B  23 0.09543568 -0.16743006 -0.05596817 0.1076923
C           C  19 0.07883817 -0.27622610 -0.13474801 0.1692308
D       *   D 105 0.43568465 -0.04341855  0.16710875 0.3846154
       mid50      mid75      upper
A 0.43939394 0.42307692 0.36206897
B 0.07575758 0.15384615 0.05172414
C 0.04545455 0.05769231 0.03448276
D 0.43939394 0.36538462 0.55172414

$i011
  correct key   n       rspP       pBis     discrim     lower
A           A 117 0.48547718 -0.3887571 -0.29151194 0.5846154
B           B  23 0.09543568 -0.1398436 -0.02148541 0.1076923
C       *   C  81 0.33609959  0.1696824  0.38435013 0.1846154
D           D  20 0.08298755 -0.1548240 -0.07135279 0.1230769
       mid50      mid75      upper
A 0.59090909 0.44230769 0.29310345
B 0.12121212 0.05769231 0.08620690
C 0.21212121 0.42307692 0.56896552
D 0.07575758 0.07692308 0.05172414

$i012
  correct key   n       rspP       pBis     discrim     lower
A       *   A 135 0.56016598  0.1279600  0.32413793 0.4000000
B           B  59 0.24481328 -0.2253473 -0.03925729 0.2461538
C           C  24 0.09958506 -0.2766477 -0.16737401 0.1846154
D           D  23 0.09543568 -0.2673278 -0.11750663 0.1692308
       mid50      mid75      upper
A 0.50000000 0.65384615 0.72413793
B 0.31818182 0.19230769 0.20689655
C 0.09090909 0.09615385 0.01724138
D 0.09090909 0.05769231 0.05172414

$i013
  correct key  n      rspP       pBis    discrim     lower     mid50
A           A 27 0.1120332 -0.2781623 -0.1501326 0.1846154 0.1363636
B           B 46 0.1908714 -0.1753162 -0.1100796 0.2307692 0.1212121
C           C 74 0.3070539 -0.3778479 -0.2564987 0.4461538 0.3333333
D       *   D 94 0.3900415  0.2312726  0.5167109 0.1384615 0.4090909
       mid75      upper
A 0.07692308 0.03448276
B 0.30769231 0.12068966
C 0.23076923 0.18965517
D 0.38461538 0.65517241

$i014
  correct key   n      rspP       pBis    discrim     lower     mid50
A       *   A 108 0.4481328  0.2134979  0.5358090 0.1538462 0.4393939
B           B  31 0.1286307 -0.2535706 -0.1445623 0.2307692 0.1060606
C           C  58 0.2406639 -0.3415163 -0.2774536 0.4153846 0.2121212
D           D  44 0.1825726 -0.2361190 -0.1137931 0.2000000 0.2424242
       mid75     upper
A 0.55769231 0.6896552
B 0.07692308 0.0862069
C 0.17307692 0.1379310
D 0.19230769 0.0862069

$i015
  correct key  n       rspP       pBis     discrim      lower
A           A 15 0.06224066 -0.1403108 -0.02148541 0.10769231
B           B 74 0.30705394 -0.3315040 -0.24111406 0.43076923
C       *   C 75 0.31120332  0.1947141  0.40769231 0.09230769
D           D 77 0.31950207 -0.2661525 -0.14509284 0.36923077
       mid50      mid75     upper
A 0.03030303 0.01923077 0.0862069
B 0.31818182 0.26923077 0.1896552
C 0.30303030 0.38461538 0.5000000
D 0.34848485 0.32692308 0.2241379

$i016
  correct key   n       rspP       pBis     discrim      lower
A       *   A 154 0.63900415  0.3353941  0.59071618 0.32307692
B           B  39 0.16182573 -0.4136149 -0.31750663 0.36923077
C           C  11 0.04564315 -0.1861371 -0.06153846 0.06153846
D           D  37 0.15352697 -0.3545356 -0.21167109 0.24615385
       mid50      mid75      upper
A 0.56060606 0.82692308 0.91379310
B 0.13636364 0.05769231 0.05172414
C 0.09090909 0.01923077 0.00000000
D 0.21212121 0.09615385 0.03448276

$i017
  correct key   n      rspP       pBis    discrim     lower
A           A  82 0.3402490 -0.4829386 -0.4350133 0.5384615
B           B  26 0.1078838 -0.3113845 -0.1538462 0.1538462
C       *   C 105 0.4356846  0.4609036  0.7716180 0.1076923
D           D  28 0.1161826 -0.2706021 -0.1827586 0.2000000
       mid50      mid75      upper
A 0.36363636 0.32692308 0.10344828
B 0.21212121 0.03846154 0.00000000
C 0.33333333 0.48076923 0.87931034
D 0.09090909 0.15384615 0.01724138

$i018
  correct key  n       rspP       pBis    discrim      lower
A           A 96 0.39834025 -0.4250997 -0.3180371 0.50769231
B           B 43 0.17842324 -0.2755281 -0.1925729 0.26153846
C           C 22 0.09128631 -0.2623175 -0.1366048 0.15384615
D       *   D 80 0.33195021  0.4013261  0.6472149 0.07692308
       mid50      mid75      upper
A 0.48484848 0.38461538 0.18965517
B 0.18181818 0.19230769 0.06896552
C 0.09090909 0.09615385 0.01724138
D 0.24242424 0.32692308 0.72413793

$i019
  correct key   n       rspP        pBis     discrim     lower
A           A  17 0.07053942 -0.14630133 -0.07135279 0.1230769
B           B  48 0.19917012 -0.23586731 -0.09469496 0.2153846
C           C 130 0.53941909 -0.22016521 -0.05570292 0.5384615
D       *   D  46 0.19087137  0.09561872  0.22175066 0.1230769
       mid50      mid75      upper
A 0.06060606 0.03846154 0.05172414
B 0.22727273 0.23076923 0.12068966
C 0.57575758 0.55769231 0.48275862
D 0.13636364 0.17307692 0.34482759

$i020
  correct key   n       rspP       pBis     discrim      lower
A           A  21 0.08713693 -0.2194626 -0.07506631 0.09230769
B           B  49 0.20331950 -0.1948577 -0.07374005 0.24615385
C       *   C 105 0.43568465  0.1054536  0.32838196 0.29230769
D           D  66 0.27385892 -0.2973162 -0.17957560 0.36923077
      mid50      mid75      upper
A 0.1515152 0.07692308 0.01724138
B 0.1969697 0.19230769 0.17241379
C 0.3939394 0.46153846 0.62068966
D 0.2575758 0.26923077 0.18965517&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="concluding-remark"&gt;Concluding remark&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;While we have discussed a lot about characteristics of test items such as Reliability, Item Difficulty, and Item Discrimination, those concepts are building blocks of a bigger concept known as test validity. Test Validity is the foundational concept in measurement that concerns the evidential support to the interpretation and use score of a test score in a particular context &lt;a href="https://www.apa.org/science/programs/testing/standards"&gt;(AERA et al., 2014)&lt;/a&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For example, in order to use &lt;a href="https://www.pearsonassessments.com/store/usassessments/en/Store/Professional-Assessments/Cognition-%26-Neuro/Wechsler-Adult-Intelligence-Scale-%7C-Fourth-Edition/p/100000392.html"&gt;WAIS-IV&lt;/a&gt; for diagnostic purposes in Thailand, an array of validity evidence needs to be established such as evidence based on test content (i.e., is content of the test known to the Thai population?), evidence based on internal structure of the test (i.e., how do we know if items of the test measure the same psychological attribute?), or even evidence based on consequences of the test (i.e., are we sure that claims made from the test such as learning disability diagnosis are for that purpose only and for no other unrelated claims such as stigmatization?).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;CTT is relatively weak in its theoretical assumption, which makes it applicable to many testing situations. This framework also does not require a complex theoretical model to assess psychological attributes of examinees. However, CTT falls short in its sample dependency, which makes it less preferable in test development scenarios that require associations with other population such as test equating and computerized adaptive testing. Item Response Theory is able to address this limitation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With online assessment becoming more implemented, especially in this pandemic time where most activities take place in online environments, we can take advantage of the technology by having computers score the exam for us for improved efficiency. We can also improve property of our test by using information from item analysis as demonstrated above as well. While testing brings about anxiety in a lot of students (me included. I hate being tested), we can hardly deny that it is an important part of education and other settings (e.g., clinical, legal). For that, it is important that every decision made on the test needs to be supported by as much evidence as possible. As always, thank you very much for checking this post out. Good day, everyone!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>76b518c0a94ddfd28cece43d8bf5be5f</distill:md5>
      <category>R</category>
      <category>Psychometric</category>
      <guid>https://taridwong.github.io/posts/2022-01-15-ctt</guid>
      <pubDate>Sat, 15 Jan 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-01-15-ctt/ctt_files/figure-html5/unnamed-chunk-28-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Examining the Big 5 personality Dataset with factor analysis</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-01-01-efacfa</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The concept of Big 5 personality traits represents tendency of individuals to possess five personality characteristics of extraversion, agreeableness, openness, conscientiousness, and neuroticism &lt;a href="https://doi.org/10.1002/job.742"&gt;(Neal et al., 2012)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Here, we will examine the data set with Exploratory Data Analysis (EFA) to see if we can identify any other structures aside from the original five groups as indicated by the concept of big 5 personality traits; then, we will verify those structures with Confirmatory Factor Analysis (CFA) to assess and compare their statistical characteristic (i.e., model fit).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will begin by loading essential packages for data preprocessing and statistical modeling as indicated below.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(parameters) #for parameter processing
library(tidymodels) #for data splitting
library(tidyverse) #toolbox for R
library(psych) #for descriptive statistics and the data set
library(ggcorrplot) #for correlation matrix
library(see) #add-on for ggplot2
library(lavaan) #for SEM
library(performance) #assessment of Regression Models Performance
library(semPlot) #to plot path model for CFA
library(dlookr) #missing data diagnosis
library(mice) #missing data imputation&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="dataset"&gt;Dataset&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The data set we use is a built-in data set from &lt;code&gt;psych&lt;/code&gt; package, which was collected in the United States as a part of the Synthetic Aperture Personality Assessment (SAPA) project &lt;a href="https://link.springer.com/chapter/10.1007/978-1-4419-1210-7_2"&gt;(Revelle et al., 2010)&lt;/a&gt;. There are 2800 observations and 25 variables of all 5 personality traits.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will begin by loading in the data set and check for its missing value.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Load the data
data &amp;lt;- psych::bfi[, 1:25]

head(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1
61617  2  4  3  4  4  2  3  3  4  4  3  3  3  4  4  3  4  2  2  3  3
61618  2  4  5  2  5  5  4  4  3  4  1  1  6  4  3  3  3  3  5  5  4
61620  5  4  5  4  4  4  5  4  2  5  2  4  4  4  5  4  5  4  2  3  4
61621  4  4  6  5  5  4  4  3  5  5  5  3  4  4  4  2  5  2  4  1  3
61622  2  3  3  4  5  4  4  5  3  2  2  2  5  4  5  2  3  4  4  3  3
61623  6  6  5  6  5  6  6  6  1  3  2  1  6  5  6  3  5  2  2  3  4
      O2 O3 O4 O5
61617  6  3  4  3
61618  2  4  3  3
61620  2  5  5  2
61621  3  4  3  5
61622  3  4  3  3
61623  3  5  6  1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;#diagnose for missing value
dlookr::diagnose(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 25 x 6
   variables types   missing_count missing_percent unique_count
   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;           &amp;lt;int&amp;gt;           &amp;lt;dbl&amp;gt;        &amp;lt;int&amp;gt;
 1 A1        integer            16           0.571            7
 2 A2        integer            27           0.964            7
 3 A3        integer            26           0.929            7
 4 A4        integer            19           0.679            7
 5 A5        integer            16           0.571            7
 6 C1        integer            21           0.75             7
 7 C2        integer            24           0.857            7
 8 C3        integer            20           0.714            7
 9 C4        integer            26           0.929            7
10 C5        integer            16           0.571            7
# ... with 15 more rows, and 1 more variable: unique_rate &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;visdat::vis_miss(data, sort_miss = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f8656aba2_files/figure-html/unnamed-chunk-5-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are some degree of missingness in the dataset as indicated by the missingness map. For good measure, we will impute it with the predictive mean matchmaking method by the &lt;code&gt;mice&lt;/code&gt; package.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="data-preprocessing"&gt;Data preprocessing&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;#imputation
mice_model &amp;lt;- mice(data, method=&amp;#39;pmm&amp;#39;, seed = 123)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 iter imp variable
  1   1  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  1   2  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  1   3  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  1   4  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  1   5  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  2   1  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  2   2  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  2   3  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  2   4  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  2   5  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  3   1  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  3   2  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  3   3  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  3   4  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  3   5  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  4   1  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  4   2  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  4   3  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  4   4  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  4   5  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  5   1  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  5   2  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  5   3  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  5   4  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  5   5  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;data_complete &amp;lt;- complete(mice_model)

visdat::vis_miss(data_complete, sort_miss = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f8656aba2_files/figure-html/unnamed-chunk-6-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;There is no missing value present in the dataset after the imputation. We can proceed with factor structure checking to assess whether the dataset is appropriate for factor analysis with &lt;code&gt;check_factorstructure()&lt;/code&gt;. Two existing methods are the &lt;em&gt;Bartlett’s Test of Sphericity&lt;/em&gt; and the &lt;em&gt;Kaiser, Meyer, Olkin (KMO)&lt;/em&gt; Measure of Sampling Adequacy (MSA).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The former tests whether a matrix is significantly different from an identity matrix. This statistical test for the presence of correlations among variables, providing the statistical probability that the correlation matrix has significant correlations among at least some of variables. As for factor analysis to work, some relationships between variables are needed, thus, a significant Bartlett’s test of sphericity is required to be significant.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The latter method, ranging from 0 to 1, indicates the degree to which each variable in the dataset is predicted without error by the other variables. A value of 0 indicates that the sum of partial correlations is large relative to the sum correlations, indicating factor analysis is likely to be inappropriate. A KMO value close to 1 indicates that the sum of partial correlations is not large relative to the sum of correlations and so factor analysis should yield distinct and reliable factors.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#check for factor structure
check_factorstructure(data_complete)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Is the data suitable for Factor Analysis?

  - KMO: The Kaiser, Meyer, Olkin (KMO) measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.84).
  - Sphericity: Bartlett&amp;#39;s test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(300) = 20158.27, p &amp;lt; .001).&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The Barlett’s test suggested that there is sufficient significant correlation in the data for factor analysis. Speaking of correlation, let us generate a correlation matrix to check for relationship between variables in the dataset as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;variables.to.use&amp;lt;-c(&amp;quot;A1&amp;quot;, &amp;quot;A2&amp;quot;, &amp;quot;A3&amp;quot;, &amp;quot;A4&amp;quot;, &amp;quot;A5&amp;quot;,
                    &amp;quot;C1&amp;quot;, &amp;quot;C2&amp;quot;, &amp;quot;C3&amp;quot;, &amp;quot;C4&amp;quot;, &amp;quot;C5&amp;quot;,
                    &amp;quot;E1&amp;quot;, &amp;quot;E2&amp;quot;, &amp;quot;E3&amp;quot;, &amp;quot;E4&amp;quot;, &amp;quot;E5&amp;quot;,
                    &amp;quot;N1&amp;quot;, &amp;quot;N2&amp;quot;, &amp;quot;N3&amp;quot;, &amp;quot;N4&amp;quot;, &amp;quot;N5&amp;quot;,
                    &amp;quot;O1&amp;quot;, &amp;quot;O2&amp;quot;, &amp;quot;O3&amp;quot;, &amp;quot;O4&amp;quot;, &amp;quot;O5&amp;quot;)

data.corr&amp;lt;-cor(data_complete[variables.to.use],
                 method = &amp;quot;pearson&amp;quot;,
                 use=&amp;#39;all.obs&amp;#39;)
ggcorrplot(data.corr,
           p.mat=cor_pmat(data_complete[variables.to.use]),
           hc.order=TRUE, 
           type=&amp;#39;lower&amp;#39;,
           color=c(&amp;#39;red3&amp;#39;, &amp;#39;white&amp;#39;, &amp;#39;green3&amp;#39;),
           outline.color = &amp;#39;darkgoldenrod1&amp;#39;, 
           lab=FALSE, #omit the correlation coefficient
           legend.title=&amp;#39;Correlation&amp;#39;,
           pch=4, 
           pch.cex=4, #size of the cross mark for non-significant indicator
           lab_size=6)+ 
  labs(title=&amp;quot;Correlation Matrix&amp;quot;)+
  theme(plot.title=element_text(face=&amp;#39;bold&amp;#39;,size=14,hjust=0.5,colour=&amp;quot;darkred&amp;quot;))+
  theme(legend.position=c(0.10,0.80), legend.box.just = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f8656aba2_files/figure-html/unnamed-chunk-8-1.png" width="1440" data-distill-preview=1 /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The green panel indicates positive relationships while the red panel indicates negative relationship. The cross symbol suggests that the relationship between two variables is not statistically significant.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Next, we would need to split the dataset into two to perform EFA and CFA, so that we can make sure to test the model on an unseen set of data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Establish two sets of indices to split the dataset
N &amp;lt;- nrow(data_complete)
indices &amp;lt;- seq(1, N)
indices_EFA &amp;lt;- sample(indices, floor((.5*N)))
indices_CFA &amp;lt;- indices[!(indices %in% indices_EFA)]

# Use those indices to split the dataset into halves for your EFA and CFA
bfi_EFA &amp;lt;- data_complete[indices_EFA, ]
bfi_CFA &amp;lt;- data_complete[indices_CFA, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;check_factorstructure(bfi_EFA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Is the data suitable for Factor Analysis?

  - KMO: The Kaiser, Meyer, Olkin (KMO) measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.84).
  - Sphericity: Bartlett&amp;#39;s test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(300) = 10442.00, p &amp;lt; .001).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;check_factorstructure(bfi_CFA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Is the data suitable for Factor Analysis?

  - KMO: The Kaiser, Meyer, Olkin (KMO) measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.84).
  - Sphericity: Bartlett&amp;#39;s test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(300) = 9970.81, p &amp;lt; .001).&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The two datasets that we splitted are appropriate for factor analysis, so we can proceed with EFA as the first analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="exploratory-factor-analysis"&gt;Exploratory Factor Analysis&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Exploratory Factor Analysis is a statistical technique in social science to explain the variance between several measured variables as a smaller set of latent variables. EFA is often used to consolidate survey data by revealing the groupings (factors) that underly individual questions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An EFA provides information on each item’s relationship to a single factor that is hypothesized to be represented by each of the items. EFA results give you basic information about how well items relate to that hypothesized construct.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The common application of EFA is to investigate relationships between observed variable and latent variables (factor) such as measurement piloting.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="scree-plot"&gt;Scree plot&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To empirically determine the dimensionality of your data, a common strategy is to examine the eigenvalues and scree plot.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The scree plot is a visual representation of eigenvalues that determines potential dimensionality of the dataset. Eigenvalues can be generated from a principal component analysis or a factor analysis, and the &lt;code&gt;scree()&lt;/code&gt; function calculates and plots both by default. Since &lt;code&gt;eigen()&lt;/code&gt; finds eigenvalues via principal components analysis, we will use &lt;code&gt;factors = FALSE&lt;/code&gt; so our scree plot will only display the values corresponding to those results.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Calculate the correlation matrix first
bfi_EFA_cor &amp;lt;- cor(bfi_EFA, use = &amp;quot;pairwise.complete.obs&amp;quot;) 

# Then use that correlation matrix to calculate eigenvalues
eigenvals &amp;lt;- eigen(bfi_EFA_cor)

# Look at the eigenvalues returned
eigenvals$values&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] 4.9971246 2.8223485 2.1038601 1.8779429 1.5744386 1.1297133
 [7] 0.9125106 0.8288279 0.7267445 0.6883978 0.6700299 0.6551319
[13] 0.6260697 0.5834295 0.5417429 0.5243013 0.4984783 0.4935423
[19] 0.4685089 0.4314514 0.4170720 0.3972532 0.3913737 0.3763374
[25] 0.2633686&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Then use the correlation matrix to create the scree plot

scree(bfi_EFA_cor, factors = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f8656aba2_files/figure-html/unnamed-chunk-13-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;From the above plot, The point where the slope of the curve is clearly leveling off (the “elbow) indicates that the number of factors that should be retained. For this case, the plot indicated that six factors should be retained.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, the dataset we use is for Big 5 personality traits with 5 factors, so we can investigate both 5 and 6 factors model to compare them both.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="fit-efa-models"&gt;Fit EFA models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We can initially explore the factor structure of 5 groups first as intended by the big 5 personality theory.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Fit an EFA
efa &amp;lt;- psych::fa(data, nfactors = 5) %&amp;gt;% 
  model_parameters(sort = TRUE, threshold = &amp;quot;max&amp;quot;)

efa&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Rotated loadings from Factor Analysis (oblimin-rotation)

Variable | MR2  |  MR1  |  MR3  |  MR5  |  MR4  | Complexity | Uniqueness
-------------------------------------------------------------------------
N1       | 0.81 |       |       |       |       |    1.08    |    0.35   
N2       | 0.78 |       |       |       |       |    1.04    |    0.40   
N3       | 0.71 |       |       |       |       |    1.07    |    0.45   
N5       | 0.49 |       |       |       |       |    1.96    |    0.65   
N4       | 0.47 |       |       |       |       |    2.27    |    0.51   
E2       |      | -0.68 |       |       |       |    1.07    |    0.46   
E4       |      | 0.59  |       |       |       |    1.49    |    0.47   
E1       |      | -0.56 |       |       |       |    1.21    |    0.65   
E5       |      | 0.42  |       |       |       |    2.60    |    0.60   
E3       |      | 0.42  |       |       |       |    2.55    |    0.56   
C2       |      |       | 0.67  |       |       |    1.17    |    0.55   
C4       |      |       | -0.61 |       |       |    1.18    |    0.55   
C3       |      |       | 0.57  |       |       |    1.11    |    0.68   
C5       |      |       | -0.55 |       |       |    1.44    |    0.57   
C1       |      |       | 0.55  |       |       |    1.19    |    0.67   
A3       |      |       |       | 0.66  |       |    1.07    |    0.48   
A2       |      |       |       | 0.64  |       |    1.04    |    0.55   
A5       |      |       |       | 0.53  |       |    1.49    |    0.54   
A4       |      |       |       | 0.43  |       |    1.74    |    0.72   
A1       |      |       |       | -0.41 |       |    1.97    |    0.81   
O3       |      |       |       |       | 0.61  |    1.17    |    0.54   
O5       |      |       |       |       | -0.54 |    1.21    |    0.70   
O1       |      |       |       |       | 0.51  |    1.13    |    0.69   
O2       |      |       |       |       | -0.46 |    1.75    |    0.74   
O4       |      |       |       |       | 0.37  |    2.69    |    0.75   

The 5 latent factors (oblimin rotation) accounted for 41.48% of the total variance of the original data (MR2 = 10.28%, MR1 = 8.80%, MR3 = 8.12%, MR5 = 7.94%, MR4 = 6.34%).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;summary(efa)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# (Explained) Variance of Components

Parameter                       |   MR2 |   MR1 |   MR3 |   MR5 |   MR4
-----------------------------------------------------------------------
Eigenvalues                     | 4.493 | 2.249 | 1.505 | 1.188 | 0.934
Variance Explained              | 0.103 | 0.088 | 0.081 | 0.079 | 0.063
Variance Explained (Cumulative) | 0.103 | 0.191 | 0.272 | 0.351 | 0.415
Variance Explained (Proportion) | 0.248 | 0.212 | 0.196 | 0.191 | 0.153&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;As we can see, the 25 items nicely spread on the 5 latent factors as the theory suggests. Based on this model, we can now predict back the scores for each individual for these new variables:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;predict_result &amp;lt;- predict(efa, names = c(&amp;quot;Neuroticism&amp;quot;, &amp;quot;Conscientiousness&amp;quot;, &amp;quot;Extraversion&amp;quot;, &amp;quot;Agreeableness&amp;quot;, &amp;quot;Opennness&amp;quot;))

head(predict_result)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  Neuroticism Conscientiousness Extraversion Agreeableness  Opennness
1 -0.21410935        0.06924675  -1.33208860   -0.85364725 -1.5809244
2  0.15008464        0.48139729  -0.59950262   -0.08478873 -0.1876070
3  0.62827949        0.10964162  -0.04800816   -0.55616873  0.2502735
4 -0.09425827        0.03836489  -1.05089539   -0.10394941 -1.1000032
5 -0.16368420        0.44253657  -0.10519669   -0.71857460 -0.6612203
6  0.18984314        1.08439177   1.40730835    0.39278790  0.6222356&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="how-many-factors-should-we-retain"&gt;How many factors should we retain?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;When running a factor analysis (FA), one often needs to specify how many components (or latent variables) to retain or to extract. This decision is often supported by some statistical indices and procedures aiming at finding the optimal number of factors, e.g., scree plot from (&lt;code&gt;scree()&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Interestingly, a huge amount of methods exist to statistically address this issue. These methods can sometimes contradict with each other in terms of retained factor. As a result, seeking the number that is supported by most methods is a reasonable compromise.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="the-method-agreement-procedure"&gt;The Method Agreement procedure&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The Method Agreement procedure, first implemented in the &lt;code&gt;psycho&lt;/code&gt; package, proposes to rely on the consensus of methods, rather than on one method in particular. This procedure can be used through the &lt;code&gt;n_factors()&lt;/code&gt; by providing a dataframe, and the function will run a large number of routines and return the optimal number of factors based on the higher consensus.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;n_factor &amp;lt;- parameters::n_factors(data_complete)

n_factor&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Method Agreement Procedure:

The choice of 6 dimensions is supported by 4 (21.05%) methods out of 19 (Optimal coordinates, Parallel analysis, Kaiser criterion, Scree (SE)).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;as.data.frame(n_factor)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   n_Factors              Method              Family
1          1 Acceleration factor               Scree
2          3                 CNG                 CNG
3          4                beta Multiple_regression
4          4          Scree (R2)            Scree_SE
5          4    VSS complexity 1                 VSS
6          5    VSS complexity 2                 VSS
7          5       Velicer&amp;#39;s MAP        Velicers_MAP
8          6 Optimal coordinates               Scree
9          6   Parallel analysis               Scree
10         6    Kaiser criterion               Scree
11         6          Scree (SE)            Scree_SE
12         7                   t Multiple_regression
13         7                   p Multiple_regression
14         8                 BIC                 BIC
15        12      BIC (adjusted)                 BIC
16        22             Bentler             Bentler
17        24            Bartlett             Barlett
18        24            Anderson             Barlett
19        24              Lawley             Barlett&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;For more details, a summary table can be obtained&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;summary(n_factor)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   n_Factors n_Methods
1          1         1
2          3         1
3          4         3
4          5         2
5          6         4
6          7         2
7          8         1
8         12         1
9         22         1
10        24         3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(n_factor) + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f8656aba2_files/figure-html/unnamed-chunk-19-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interestingly, most methods also suggest six factors from the model, which is consistent with the &lt;a href="https://en.wikipedia.org/wiki/HEXACO_model_of_personality_structure"&gt;HEXACO model&lt;/a&gt; of personalities that is similar with the Big 5 personality theory.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="confirmatory-factor-analysis"&gt;Confirmatory Factor Analysis&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We’ve seen above that while an EFA with 5 latent variables works great on our dataset, a structure with 6 latent factors may be statistically viable as well. This topic can be statistically tested with a CFA to bridge factor analysis with Structural Equation Modelling (SEM).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, in order to do that cleanly, EFA should be independent from CFA in the sense that the factor structure should be explored on a &lt;strong&gt;training&lt;/strong&gt; set, and then tested (or “confirmed”) on a &lt;strong&gt;test&lt;/strong&gt; set.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="train-test-split"&gt;Train test split&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The data can be easily split into two sets with the &lt;code&gt;initial_split()&lt;/code&gt; of the tidymodel package, through which we will use 70% of the sample as the training and the rest as the test dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;set.seed(999)

# Put 3/4 of the data into the training set 
data_split &amp;lt;- initial_split(bfi_CFA, prop = 0.7)

# Create data frames for the two sets:
training &amp;lt;- training(data_split) #3/4
test  &amp;lt;- testing(data_split)  #1/4&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="create-cfa-structures-out-of-efa-models"&gt;Create CFA structures out of EFA models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In the next step, we will run two EFA models on the training set and specifying 5 and 6 latent factors respectively. We will also request for path diagram of the models as well as their density plot of factor score.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;First, we will examine structure of the 5 factors model.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;structure_big5 &amp;lt;- psych::fa(training, nfactors = 5)
fa.diagram(structure_big5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f8656aba2_files/figure-html/unnamed-chunk-21-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(density(structure_big5$scores, na.rm = TRUE), 
     main = &amp;quot;Factor Scores&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f8656aba2_files/figure-html/unnamed-chunk-22-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Then, for the structure of the 6 factors model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;structure_big6 &amp;lt;- psych::fa(training, nfactors = 6) 
fa.diagram(structure_big6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f8656aba2_files/figure-html/unnamed-chunk-23-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(density(structure_big6$scores, na.rm = TRUE), 
     main = &amp;quot;Factor Scores&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f8656aba2_files/figure-html/unnamed-chunk-24-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We will then transform both EFA models into &lt;code&gt;lavaan&lt;/code&gt; syntax to perform CFA.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#Converting EFA into a lavaan-ready syntax
cfa_big5 &amp;lt;- efa_to_cfa(structure_big5)

#Investigate how the model looks
cfa_big5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Latent variables
MR2 =~ N1 + N2 + N3 + N4 + N5
MR1 =~ E1 + E2 + E3 + E4 + E5
MR3 =~ C1 + C2 + C3 + C4 + C5
MR5 =~ A1 + A2 + A3 + A4 + A5
MR4 =~ O1 + O2 + O3 + O4 + O5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;cfa_big6 &amp;lt;- efa_to_cfa(structure_big6)
cfa_big6&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Latent variables
MR2 =~ N1 + N2 + N3 + N5
MR1 =~ E1 + E2 + E3 + E4 + E5 + N4
MR3 =~ C1 + C2 + C3 + C4 + C5
MR4 =~ O1 + O2 + O3 + O4 + O5
MR5 =~ A1 + A2 + A3
MR6 =~ A4 + A5&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="fit-and-compare-models"&gt;Fit and Compare models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Next, we will fit both models with lavaan package before requesting for model fit measure with &lt;code&gt;fitmeasures&lt;/code&gt;. We can also compare both models head-to-head with &lt;code&gt;compare_performance&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;model_big5 &amp;lt;- lavaan::cfa(cfa_big5, data = test)
model_big6 &amp;lt;- lavaan::cfa(cfa_big6, data = test)

fitmeasures(model_big5, fit.measures = &amp;quot;all&amp;quot;, output = &amp;quot;text&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Model Test User Model:

  Test statistic                              1090.069
  Degrees of freedom                               265
  P-value                                        0.000

Model Test Baseline Model:

  Test statistic                              3470.893
  Degrees of freedom                               300
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.740
  Tucker-Lewis Index (TLI)                       0.705
  Bentler-Bonett Non-normed Fit Index (NNFI)     0.705
  Bentler-Bonett Normed Fit Index (NFI)          0.686
  Parsimony Normed Fit Index (PNFI)              0.606
  Bollen&amp;#39;s Relative Fit Index (RFI)              0.644
  Bollen&amp;#39;s Incremental Fit Index (IFI)           0.743
  Relative Noncentrality Index (RNI)             0.740

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)             -17119.026
  Loglikelihood unrestricted model (H1)     -16573.992
                                                      
  Akaike (AIC)                               34358.052
  Bayesian (BIC)                             34600.610
  Sample-size adjusted Bayesian (BIC)        34410.212

Root Mean Square Error of Approximation:

  RMSEA                                          0.086
  90 Percent confidence interval - lower         0.081
  90 Percent confidence interval - upper         0.091
  P-value RMSEA &amp;lt;= 0.05                          0.000

Standardized Root Mean Square Residual:

  RMR                                            0.173
  RMR (No Mean)                                  0.173
  SRMR                                           0.087

Other Fit Indices:

  Hoelter Critical N (CN) alpha = 0.05         118.397
  Hoelter Critical N (CN) alpha = 0.01         125.159
                                                      
  Goodness of Fit Index (GFI)                    0.809
  Adjusted Goodness of Fit Index (AGFI)          0.765
  Parsimony Goodness of Fit Index (PGFI)         0.659
                                                      
  McDonald Fit Index (MFI)                       0.375
                                                      
  Expected Cross-Validation Index (ECVI)         2.874&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;fitmeasures(model_big6, fit.measures = &amp;quot;all&amp;quot;, output = &amp;quot;text&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Model Test User Model:

  Test statistic                              1105.900
  Degrees of freedom                               260
  P-value                                        0.000

Model Test Baseline Model:

  Test statistic                              3470.893
  Degrees of freedom                               300
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.733
  Tucker-Lewis Index (TLI)                       0.692
  Bentler-Bonett Non-normed Fit Index (NNFI)     0.692
  Bentler-Bonett Normed Fit Index (NFI)          0.681
  Parsimony Normed Fit Index (PNFI)              0.591
  Bollen&amp;#39;s Relative Fit Index (RFI)              0.632
  Bollen&amp;#39;s Incremental Fit Index (IFI)           0.737
  Relative Noncentrality Index (RNI)             0.733

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)             -17126.942
  Loglikelihood unrestricted model (H1)     -16573.992
                                                      
  Akaike (AIC)                               34383.884
  Bayesian (BIC)                             34646.655
  Sample-size adjusted Bayesian (BIC)        34440.390

Root Mean Square Error of Approximation:

  RMSEA                                          0.088
  90 Percent confidence interval - lower         0.083
  90 Percent confidence interval - upper         0.093
  P-value RMSEA &amp;lt;= 0.05                          0.000

Standardized Root Mean Square Residual:

  RMR                                            0.187
  RMR (No Mean)                                  0.187
  SRMR                                           0.091

Other Fit Indices:

  Hoelter Critical N (CN) alpha = 0.05         114.677
  Hoelter Critical N (CN) alpha = 0.01         121.285
                                                      
  Goodness of Fit Index (GFI)                    0.804
  Adjusted Goodness of Fit Index (AGFI)          0.755
  Parsimony Goodness of Fit Index (PGFI)         0.643
                                                      
  McDonald Fit Index (MFI)                       0.366
                                                      
  Expected Cross-Validation Index (ECVI)         2.936&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;model_comparison &amp;lt;-performance::compare_performance(model_big5, model_big6)

rmarkdown::paged_table(model_comparison)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable="false"&gt;
&lt;script data-pagedtable-source type="application/json"&gt;
{"columns":[{"label":["Name"],"name":[1],"type":["chr"],"align":["left"]},{"label":["Model"],"name":[2],"type":["chr"],"align":["left"]},{"label":["Chi2"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Chi2_df"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["p_Chi2"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["Baseline"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["Baseline_df"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["p_Baseline"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["GFI"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["AGFI"],"name":[10],"type":["dbl"],"align":["right"]},{"label":["NFI"],"name":[11],"type":["dbl"],"align":["right"]},{"label":["NNFI"],"name":[12],"type":["dbl"],"align":["right"]},{"label":["CFI"],"name":[13],"type":["dbl"],"align":["right"]},{"label":["RMSEA"],"name":[14],"type":["dbl"],"align":["right"]},{"label":["RMSEA_CI_low"],"name":[15],"type":["dbl"],"align":["right"]},{"label":["RMSEA_CI_high"],"name":[16],"type":["dbl"],"align":["right"]},{"label":["p_RMSEA"],"name":[17],"type":["dbl"],"align":["right"]},{"label":["RMR"],"name":[18],"type":["dbl"],"align":["right"]},{"label":["SRMR"],"name":[19],"type":["dbl"],"align":["right"]},{"label":["RFI"],"name":[20],"type":["dbl"],"align":["right"]},{"label":["PNFI"],"name":[21],"type":["dbl"],"align":["right"]},{"label":["IFI"],"name":[22],"type":["dbl"],"align":["right"]},{"label":["RNI"],"name":[23],"type":["dbl"],"align":["right"]},{"label":["Loglikelihood"],"name":[24],"type":["dbl"],"align":["right"]},{"label":["AIC"],"name":[25],"type":["dbl"],"align":["right"]},{"label":["AIC_wt"],"name":[26],"type":["dbl"],"align":["right"]},{"label":["BIC"],"name":[27],"type":["dbl"],"align":["right"]},{"label":["BIC_wt"],"name":[28],"type":["dbl"],"align":["right"]},{"label":["BIC_adjusted"],"name":[29],"type":["dbl"],"align":["right"]}],"data":[{"1":"model_big5","2":"lavaan","3":"1090.069","4":"265","5":"0","6":"3470.893","7":"300","8":"0","9":"0.8086236","10":"0.7652931","11":"0.6859399","12":"0.705433","13":"0.7397992","14":"0.08599657","15":"0.08072575","16":"0.09133459","17":"5.417888e-14","18":"0.1728950","19":"0.08680462","20":"0.6444603","21":"0.6059136","22":"0.7426399","23":"0.7397992","24":"-17119.03","25":"34358.05","26":"9.999975e-01","27":"34600.61","28":"1.000000e+00","29":"34410.21"},{"1":"model_big6","2":"lavaan","3":"1105.900","4":"260","5":"0","6":"3470.893","7":"300","8":"0","9":"0.8039676","10":"0.7549595","11":"0.6813787","12":"0.692188","13":"0.7332296","14":"0.08790871","15":"0.08260383","16":"0.09328222","17":"5.262457e-14","18":"0.1866024","19":"0.09099409","20":"0.6323601","21":"0.5905282","22":"0.7365529","23":"0.7332296","24":"-17126.94","25":"34383.88","26":"2.459084e-06","27":"34646.66","28":"1.003556e-10","29":"34440.39"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The model comparison indicated that both model are empirically viable, but we would need a different dataset to re-assess the 6 factor model with appropriate item distribution (i.e., each factor has its own dedicated variables).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Another thing that should be noted is our conclusion should be theoretically permissible; that is, whether we use the big 5 or big 6 models, we should have theories and evidence to support our decision. Otherwise, if we only rely on statistical results, we can use 24 personality factors models because the result said it has the second-most approval rate by method consensus, but that would not make any theoretical sense.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For social science research where we care about the “how”, our decisions should be theoretically-driven, so that we can explain the way our model works in both statistical and theoretical sense.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Anyway, this is all for this post. Thank you so much for reading this as always. The next semester is on around the corner, but I still have some time to write my blog before going back to the regularly-scheduled paper writing. Also, Happy New Year, everyone! Let us do our best in both personal and professional endeavor.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>91399b5a2be494d92b5d7d707afca65d</distill:md5>
      <category>R</category>
      <category>Statistics</category>
      <guid>https://taridwong.github.io/posts/2022-01-01-efacfa</guid>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-01-01-efacfa/corrmatrix.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Measuring Text Similarity with Movie plot data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-29-movie-similarity</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;One characteristic of textual data in the real world setting is that most of them possess meaning that to convey to their intended audience. The meaning of one message could be similar to another when they are crafted for similar purposes. With the right tool, we can identify such similarities and visualize them to extract insights from textual data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The data set I will use in this post are movie plot summaries available on IMDb and Wikipedia. Here, I will quantify the similarity of movies based on their plot and separate them into groups before plotting them on a dendrogram to represent how closely the movies are related to each other.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As always, we will begin by importing necessary modules and dataset.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Import modules
import numpy as np
import pandas as pd
import nltk

# Set seed for reproducibility
np.random.seed(5)

# Read in IMDb and Wikipedia movie data (both in the same file)
movies_df = pd.read_csv(&amp;quot;movies.csv&amp;quot;)

print(&amp;quot;Number of movies loaded: %s &amp;quot; % (len(movies_df)))

# Display the data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Number of movies loaded: 100 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;movies_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    rank  ...                                          imdb_plot
0      0  ...  In late summer 1945, guests are gathered for t...
1      1  ...  In 1947, Andy Dufresne (Tim Robbins), a banker...
2      2  ...  The relocation of Polish Jews from surrounding...
3      3  ...  The film opens in 1964, where an older and fat...
4      4  ...  In the early years of World War II, December 1...
..   ...  ...                                                ...
95    95  ...  Shortly after moving to Los Angeles with his p...
96    96  ...  L.B. &amp;quot;Jeff&amp;quot; Jeffries (James Stewart) recuperat...
97    97  ...  Sights of Vienna, Austria, flash across the sc...
98    98  ...  At the end of an ordinary work day, advertisin...
99    99  ...                                                NaN

[100 rows x 5 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="combine-wikipedia-and-imdb-plot-summaries"&gt;Combine Wikipedia and IMDb plot summaries&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The dataset we imported currently contains two columns titled &lt;code&gt;wiki_plot&lt;/code&gt; and &lt;code&gt;imdb_plot&lt;/code&gt;. They are the plot found for the movies on Wikipedia and IMDb, respectively. The text in the two columns is similar, however, they are often written in different tones and thus provide context on a movie in a different manner of linguistic expression. Further, sometimes the text in one column may mention a feature of the plot that is not present in the other column. For example, consider the following plot extracts from &lt;em&gt;The Godfather&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Wikipedia: “On the day of his only daughter’s wedding, Vito Corleone…”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IMDb: “In late summer 1945, guests are gathered for the wedding reception of Don Vito Corleone’s daughter Connie”&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;While the Wikipedia plot only mentions it is the day of the daughter’s wedding, the IMDb plot also mentions the year of the scene and the name of the daughter. We can combine them to avoid the overheads in computation associated with extra columns to process.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Combine wiki_plot and imdb_plot into a single column
movies_df[&amp;quot;plot&amp;quot;] = movies_df[&amp;quot;wiki_plot&amp;quot;].astype(str) + &amp;quot;\n&amp;quot; + \
                    movies_df[&amp;quot;imdb_plot&amp;quot;].astype(str)
                    
movies_df.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   rank  ...                                               plot
0     0  ...  On the day of his only daughter&amp;#39;s wedding, Vit...
1     1  ...  In 1947, banker Andy Dufresne is convicted of ...
2     2  ...  In 1939, the Germans move Polish Jews into the...
3     3  ...  In a brief scene in 1964, an aging, overweight...
4     4  ...  It is early December 1941. American expatriate...

[5 rows x 6 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="tokenization"&gt;Tokenization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Tokenization is the process by which we break down articles into individual sentences or words, as needed. We can also use the regular expression (Regex) method to remove tokens that are entirely numeric values or punctuation to retain only words with meaning.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As an example, we will perform tokenization on a part of Godfather’s plot. Notice that quotation marks and numbers were removed in the output.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Tokenize a paragraph into sentences and store in sent_tokenized
sent_tokenized = [sent for sent in nltk.sent_tokenize(&amp;quot;&amp;quot;&amp;quot;
                        Today (May 19, 2016) is his only daughter&amp;#39;s wedding. 
                        Vito Corleone is the Godfather.
                        &amp;quot;&amp;quot;&amp;quot;)]
                        
# Word Tokenize first sentence from sent_tokenized, save as words_tokenized

words_tokenized = [word for word in nltk.word_tokenize(sent_tokenized[0])]

# Remove tokens that do not contain any letters from words_tokenized
import re

filtered = [word for word in words_tokenized if re.search(&amp;#39;[a-zA-Z]&amp;#39;, word)]

# Display filtered words to observe words after tokenization
filtered&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;Today&amp;#39;, &amp;#39;May&amp;#39;, &amp;#39;is&amp;#39;, &amp;#39;his&amp;#39;, &amp;#39;only&amp;#39;, &amp;#39;daughter&amp;#39;, &amp;quot;&amp;#39;s&amp;quot;, &amp;#39;wedding&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="stemming"&gt;Stemming&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Stemming is the process by which we bring down a word from its different forms to the root word (or to stem). This helps us establish meaning to different forms of the same words without having to deal with each form separately. For example, the words ‘fishing’, ‘fished’, and ‘fisher’ all get stemmed to the word ‘fish’.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Consider the following sentences:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;“Young William Wallace witnesses the treachery of Longshanks” - &lt;em&gt;Gladiator&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“escapes to the city walls only to witness Cicero’s death” - &lt;em&gt;Braveheart&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Instead of building separate dictionary entries for both witnesses and witness, which mean the same thing outside of quantity, stemming them reduces them to ‘wit’.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There are different algorithms available for stemming such as the Porter Stemmer and Snowball Stemmer. Here, we will use Snowball Stemmer.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Import the SnowballStemmer to perform stemming
from nltk.stem.snowball import SnowballStemmer

# Create an English language SnowballStemmer object
stemmer = SnowballStemmer(&amp;quot;english&amp;quot;)

# Print filtered to observe words without stemming
print(&amp;quot;Without stemming: &amp;quot;, filtered)

# Stem the words from filtered and store in stemmed_words&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Without stemming:  [&amp;#39;Today&amp;#39;, &amp;#39;May&amp;#39;, &amp;#39;is&amp;#39;, &amp;#39;his&amp;#39;, &amp;#39;only&amp;#39;, &amp;#39;daughter&amp;#39;, &amp;quot;&amp;#39;s&amp;quot;, &amp;#39;wedding&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;stemmed_words = [stemmer.stem(t) for t in filtered]

# Print the stemmed_words to observe words after stemming
print(&amp;quot;After stemming:   &amp;quot;, stemmed_words)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;After stemming:    [&amp;#39;today&amp;#39;, &amp;#39;may&amp;#39;, &amp;#39;is&amp;#39;, &amp;#39;his&amp;#39;, &amp;#39;onli&amp;#39;, &amp;#39;daughter&amp;#39;, &amp;quot;&amp;#39;s&amp;quot;, &amp;#39;wed&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="tokenization-and-stemming-together"&gt;Tokenization and Stemming together&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We are now able to tokenize and stem sentences. But we may have to use the two functions repeatedly one after the other to handle a large amount of data, hence we can think of wrapping them in a function and passing the text to be tokenized and stemmed as the function argument. Then we can pass the new wrapping function, which shall perform both tokenizing and stemming instead of just tokenizing, as the tokenizer argument while creating the TF-IDF vector of the text (we will get there to what TF-IDF means).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All the words are in their root form, which will lead to a better establishment of meaning as some of the non-root forms may not be present in the NLTK training corpus.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Define a function to perform both stemming and tokenization
def tokenize_and_stem(text):
    
    # Tokenize by sentence, then by word
    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]
    
    # Filter out raw tokens to remove noise
    filtered_tokens = [token for token in tokens if re.search(&amp;#39;[a-zA-Z]&amp;#39;, token)]
    
    # Stem the filtered_tokens
    stems = [stemmer.stem(t) for t in filtered_tokens]
    
    return stems

words_stemmed = tokenize_and_stem(&amp;quot;Today (May 19, 2016) is his only daughter&amp;#39;s wedding.&amp;quot;)
print(words_stemmed)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;today&amp;#39;, &amp;#39;may&amp;#39;, &amp;#39;is&amp;#39;, &amp;#39;his&amp;#39;, &amp;#39;onli&amp;#39;, &amp;#39;daughter&amp;#39;, &amp;quot;&amp;#39;s&amp;quot;, &amp;#39;wed&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="create-tf-idf-vectorizer"&gt;Create TF-IDF Vectorizer&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Computers do not &lt;em&gt;understand&lt;/em&gt; text. These are machines only capable of understanding numbers and performing numerical computation. Hence, we must convert our textual plot summaries to numbers for the computer to be able to extract meaning from them. One simple method of doing this would be to count all the occurrences of each word in the entire vocabulary and return the counts in a vector. This method is called &lt;code&gt;CountVectorizer.&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Consider the word ‘the’. It appears quite frequently in almost all movie plots and will have a high count in each case. However, “the” could hardly be counted as the movie plot itself. For that, &lt;a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf"&gt;Term Frequency-Inverse Document Frequency&lt;/a&gt; (TF-IDF) is one method that overcomes the shortcomings of &lt;code&gt;CountVectorizer&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In TF-IDF, frequency of a word is the measure of how often it appears in a document, while the Inverse Document Frequency is the parameter which reduces the importance of a word if it frequently appears in several documents. In simplest terms, TF-IDF recognizes words which are unique and important to any given document.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Import TfidfVectorizer to create TF-IDF vectors
from sklearn.feature_extraction.text import TfidfVectorizer

# Instantiate TfidfVectorizer object with stopwords and tokenizer
# parameters for efficient processing of text
tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,
                                 min_df=0.2, stop_words=&amp;#39;english&amp;#39;,
                                 use_idf=True, tokenizer=tokenize_and_stem,
                                 ngram_range=(1,3))&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="fit-transform-tf-idf-vectorizer"&gt;Fit transform TF-IDF Vectorizer&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Once we create a TF-IDF Vectorizer, we must fit the text to it and then transform the text to produce the corresponding numeric form of the data which the computer will be able to understand and derive meaning from. To do this, we use the &lt;code&gt;fit_transform()&lt;/code&gt; method of the &lt;code&gt;TfidfVectorizer&lt;/code&gt; object.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the TF-IDF object, there is a parameter called &lt;code&gt;stopwords&lt;/code&gt;. Stopwords are those words in a given text which do not contribute considerably towards the meaning of the sentence and are generally grammatical filler words. For example, in the sentence ‘Dorothy Gale lives with her dog Toto on the farm of her Aunt Em and Uncle Henry’, we could drop the words ‘her’ and ‘the’, and still have a similar overall meaning to the sentence. Thus, ‘her’ and ‘the’ are stopwords and can be conveniently dropped from the sentence.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On setting the stopwords to ‘english’, we direct the vectorizer to drop all stopwords from a pre-defined list of English language stopwords present in the nltk module. Another parameter, &lt;code&gt;ngram_range&lt;/code&gt;, defines the length of the ngrams to be formed while vectorizing the text.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Fit and transform the tfidf_vectorizer with the &amp;quot;plot&amp;quot; of each movie
# to create a vector representation of the plot summaries
tfidf_matrix = tfidf_vectorizer.fit_transform([x for x in movies_df[&amp;quot;plot&amp;quot;]])

print(tfidf_matrix.shape)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(100, 564)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="import-k-means-and-create-clusters"&gt;Import K-Means and create clusters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To determine how closely one movie is related to the other by the help of unsupervised machine learning, we can use clustering techniques. Clustering is the method of grouping together a number of items such that they exhibit similar properties. According to the measure of similarity desired, a given sample of items can have one or more clusters.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A good basis of clustering in our data set could be the genre of the movies. Say we could have a cluster ‘0’ which holds movies of the ‘Drama’ genre, and ‘1’ for the ‘Adventure’ genre.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;K-means is an algorithm which helps us to implement clustering in Python. The name derives from its method of implementation: the given sample is divided into &lt;strong&gt;&lt;em&gt;K&lt;/em&gt;&lt;/strong&gt; clusters where each cluster is denoted by the &lt;strong&gt;&lt;em&gt;mean&lt;/em&gt;&lt;/strong&gt; of all the items lying in that cluster.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Here, we will examine how many movies we have in each of the five clusters we specified; then, we will visualize them with a category plot.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Import k-means to perform clustering
from sklearn.cluster import KMeans

# Create a KMeans object with 5 clusters and save as km
km = KMeans(n_clusters=5)

# Fit the k-means object with tfidf_matrix
km.fit(tfidf_matrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;KMeans(n_clusters=5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;clusters = km.labels_.tolist()

# Create a column cluster to denote the generated cluster for each movie
movies_df[&amp;quot;cluster&amp;quot;] = clusters

# Display number of films per cluster (clusters from 0 to 4)
movies_df[&amp;#39;cluster&amp;#39;].value_counts() &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3    31
1    27
0    22
4    13
2     7
Name: cluster, dtype: int64&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;import seaborn as sns
import matplotlib.pyplot as plt

#convert the cluster list into a dataframe
clusters_df = pd.DataFrame(clusters, columns = [&amp;#39;cluster_group&amp;#39;])

sns.set_theme(style=&amp;quot;whitegrid&amp;quot;)
sns.catplot(x=&amp;quot;cluster_group&amp;quot;, kind=&amp;quot;count&amp;quot;, data=clusters_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;seaborn.axisgrid.FacetGrid object at 0x000000000C1AA250&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="calculate-similarity-distance"&gt;Calculate similarity distance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;By using &lt;code&gt;countvectorizer&lt;/code&gt;, we can turn a sentence into numbers for the computer to calculate similarity distance with the cosine similarity measurement (it is basically a number that indicates how closely related the two sets of words are).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Import cosine_similarity to calculate similarity of movie plots
from sklearn.metrics.pairwise import cosine_similarity

# Calculate the similarity distance
similarity_distance = 1 - cosine_similarity(tfidf_matrix)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="import-matplotlib-linkage-and-dendrograms"&gt;Import Matplotlib, Linkage, and Dendrograms&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We will then create a dendrogram of the movie title based on its plot similarity to visualize the level of similarity between our data points.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Dendrograms help visualize the results of hierarchical clustering, which is an alternative to k-means clustering. Two pairs of movies at the same level of hierarchical clustering are expected to have similar strength of similarity between the corresponding pairs of movies.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The more similar the two movies are, the closer they will be together as they travel down the dendrogram path. The plot is a little large to accommodate the number of data points, so you might need to zoom in to see which movie is similar to which.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Import modules necessary to plot dendrogram
from scipy.cluster.hierarchy import linkage, dendrogram

# Create mergings matrix 
mergings = linkage(similarity_distance, method=&amp;#39;complete&amp;#39;)

# Plot the dendrogram, using title as label column
dendrogram_ = dendrogram(mergings,
               labels=[x for x in movies_df[&amp;quot;title&amp;quot;]],
               leaf_rotation=90,
               leaf_font_size=16,
)

# Adjust the plot
fig = plt.gcf()
_ = [lbl.set_color(&amp;#39;r&amp;#39;) for lbl in plt.gca().get_xmajorticklabels()]
fig.set_size_inches(120, 50)

# Show the plotted dendrogram
plt.grid(False)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f861bc1099_files/figure-html/unnamed-chunk-13-1.png" width="11520" /&gt;&lt;/p&gt;
&lt;h2 id="concluding-remark"&gt;Concluding remark&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;While I am not an expert in movie critique, the movie plot data is a good venue to practice text cleaning with tokenization and stemming. The TF-IDF method is also widely implemented to extract meaningful information from textual data in general. Lastly, clustering is also a useful exploratory machine learning method to gain insights from unlabeled data to inform our decisions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This post combines both Narutal Language Processing and Machine Learning techniques to calculate similarity score between sets of words. This method can be used to establish a groundwork for a recommendation system that we often seen in popular sites such as Netflix or Spotify by grouping movies or musics together to recommend them to users. As always, thank you very much for reading!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>97ed681d6820c89f6bb644175087be55</distill:md5>
      <category>Python</category>
      <category>Natural Language Processing</category>
      <category>Unsupervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-29-movie-similarity</guid>
      <pubDate>Wed, 29 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-29-movie-similarity/movie-similarity_files/figure-html5/unnamed-chunk-11-1.png" medium="image" type="image/png" width="484" height="483"/>
    </item>
    <item>
      <title>Missing Data Analysis</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-27-missingdata</link>
      <description>


&lt;h2 id="looking-for-what-that-is-not-there"&gt;Looking for what that is not there&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;“If I had eight hours to chop down a tree, I’d spend six sharpening my axe.” - Abraham Lincoln via &lt;a href="https://www.guilford.com/books/Principles-and-Practice-of-Structural-Equation-Modeling/Rex-Kline/9781462523344"&gt;Kline (2016)&lt;/a&gt;. This adage is appropriate to set the tone for this post, as well as applicable to most things in general, including working with data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;My professors taught me that real data never works, and my experience attested to their statement countless times as I iterated over the data work procedure of importing, cleaning, model building, model tuning, and communicating results. One thing about it that I used to find frustrating is the data I got if oftentimes incomplete (or partly missing).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Missing data are values that should have been recorded but were not. The best way to treat missing data is not to have them, but unfortunately, real data is oftentimes &lt;del&gt;ugly&lt;/del&gt; unorganized. Missing data could potentially caused by nonresponse in surveys, or technical issues with data-collecting equipment.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;My previous posts were about visualizing data that we have, but this time, we will be visualizing things that we ‘do not’ have (aka missing data), as well as discussing about ways we can deal with them via complete case analysis or imputation.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2021-12-27-missingdatamissingpic.jpg" style="width:40.0%" alt="" /&gt;
&lt;p class="caption"&gt;Photo by Adam Lingelbach&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="import-and-read-the-data-set"&gt;Import and read the data set&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;As usual, we will begin by importing essential libraries and load in the data set to preprocess it.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;knitr::opts_chunk$set(error = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(foreign) #To read SPSS data
library(tidyverse) #datawork toolbox
library(dlookr) #for missing data diagnosis
library(visdat) #for overall missingness visualization
library(naniar) #for missingness visualization
library(VIM) #for donor-based imputation
library(simputation) #for model-based imputation
library(mice) #for multiple imputation&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;#Import the data set
PISA_CAN &amp;lt;-read.spss(&amp;quot;PISA2018CAN.sav&amp;quot;,to.data.frame = TRUE, use.value.labels = FALSE)

#Subset and rename the variables
PISA_Subsetted &amp;lt;-  PISA_CAN %&amp;gt;% 
  select(REPEAT, FEMALE = ST004D01T, ESCS, DAYSKIP = ST062Q01TA,
         CLASSSKIP = ST062Q02TA, LATE = ST062Q03TA,
         BEINGBULLIED, DISCLIMA, ADAPTIVITY)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in select(., REPEAT, FEMALE = ST004D01T, ESCS, DAYSKIP = ST062Q01TA, : unused arguments (REPEAT, FEMALE = ST004D01T, ESCS, DAYSKIP = ST062Q01TA, CLASSSKIP = ST062Q02TA, LATE = ST062Q03TA, BEINGBULLIED, DISCLIMA, ADAPTIVITY)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;#Recode variables into factor
PISA_Subsetted$DAYSKIP &amp;lt;-as.factor(PISA_Subsetted$DAYSKIP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in is.factor(x): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_Subsetted$CLASSSKIP &amp;lt;-as.factor(PISA_Subsetted$CLASSSKIP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in is.factor(x): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_Subsetted$LATE &amp;lt;-as.factor(PISA_Subsetted$LATE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in is.factor(x): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_Subsetted$FEMALE &amp;lt;-as.factor(PISA_Subsetted$FEMALE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in is.factor(x): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_Subsetted$REPEAT &amp;lt;-as.factor(PISA_Subsetted$REPEAT)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in is.factor(x): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Renaming factor levels with dplyr
PISA_Subsetted$FEMALE &amp;lt;- recode_factor(PISA_Subsetted$FEMALE, 
                                       &amp;quot;1&amp;quot; = &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot; = &amp;quot;0&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in recode(.x, !!!values, .default = .default, .missing = .missing): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;glimpse(PISA_Subsetted)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in glimpse(PISA_Subsetted): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The data set in this post was a Canadian student data subsetted from the Programme for Internal Student Assessment (PISA), which is an international assessment that measures 15-year-old students’ reading, mathematics, and science literacy every three years.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;From the &lt;code&gt;glimpse&lt;/code&gt; call above, our dataset has 9 variables and 22,653 data points.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="check-for-missing-data"&gt;Check for missing data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First, we will use the &lt;code&gt;dlookr&lt;/code&gt; package to diagnose missingness of the data set, as well as plot missing data map with &lt;code&gt;vis_miss&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The plot provides a specific visualization of the amount of missing data, showing in black the location of missing values, and also providing information on the overall percentage of missing values overall (in the legend), and in each variable.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;dlookr::diagnose(PISA_Subsetted)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in dlookr::diagnose(PISA_Subsetted): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;visdat::vis_miss(PISA_Subsetted, sort_miss = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in test_if_dataframe(x): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;If we are curious about the proportion of missing data by groups, we can also group the dataset by our categorical variable of interest, say, gender, before examining the missingness ratio.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_Subsetted %&amp;gt;% group_by (FEMALE) %&amp;gt;%
  miss_var_summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in group_by(., FEMALE): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="types-of-missing-data"&gt;Types of Missing Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Yes, we know now that our data is missing, but not all missing data are created (or not created, pun wholeheartedly intended) equal. There are three types of missing data, MCAR, MAR, and MNAR.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Missing Completely at Random (MCAR)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Locations of missing values in the dataset are purely random. they do not depend on any other data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For example, if a doctor forgets to record the age of every tenth patient entering an ICU, the presence of missing value would not depend on the characteristic of the patients.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Missing at Random (MAR)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Locations of missing values in the dataset depend on some other, observed data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Data are considered as MAR if the probability of missingness is unrelated to the actual value on that variable after controlling for the other variables in the dataset&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In survey data, high-income respondents are less likely to inform the researcher about the number of properties owned.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Below is an example of MAR missingness. See that &lt;code&gt;sea_temp&lt;/code&gt; and &lt;code&gt;air_temp&lt;/code&gt; are missing at a certain part of the year. Maybe the measuring tools broke down or something before they got them fixed.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;oceanbuoys %&amp;gt;% arrange(year) %&amp;gt;% vis_miss()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file15f816de2c3a_files/figure-html/unnamed-chunk-8-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Missing Not at Random (MNAR)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If it is not MCAR or MAR, it is probably MNAR. This is the most tricky type of missingness to handle.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The missing values depend on both characteristics of the data and also on missing values. In this case, determining the mechanism of the generation of missing value is difficult.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Missing values for a variable like blood pressure may partially depend on the values of blood pressure as patients who have low blood pressure are less likely to get their blood pressure checked at frequently.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="visualize-missing-data"&gt;Visualize Missing data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Okay, now we know what missing data is, and what are types of missing data, here are some ways we can visualize them so that we know their patterns and what they are up to.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="missing-pattern-wupset-plot"&gt;Missing pattern w/Upset plot&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An upset plot from the &lt;code&gt;UpSetR&lt;/code&gt; package can be used to visualize the patterns of missingness, or rather the combinations of missingness across cases and variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;gg_miss_upset(PISA_Subsetted, nsets = 9, nintersects = 15)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in is.data.frame(x): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The small bar plot to the left indicated the amount of missingness in variables. Consistent with the missingness diagnosis, the variable &lt;code&gt;BEINGBULLIED&lt;/code&gt; has the most missing data, following by &lt;code&gt;DAYSKIP&lt;/code&gt; and &lt;code&gt;CLASSKIP&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The dot plot to the right showed combinations of variable that are missing in the data set. For example, there are 1,234 cases that have missing data in the variable &lt;code&gt;LATE&lt;/code&gt;, &lt;code&gt;CLASSKIP&lt;/code&gt;, &lt;code&gt;DAYSKIP&lt;/code&gt;, and &lt;code&gt;BEINGBULLIED&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The parameter &lt;code&gt;nsets&lt;/code&gt; looks at 9 sets of variables, while the parameter &lt;code&gt;nintersects&lt;/code&gt; looks at 15 variable combinations.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="general-visual-summaries-of-missing-data"&gt;General visual summaries of missing data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;This section demonstrates numerous ways to visualize missing data to determine their patterns.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="missingness-in-variables-with-gg_miss_var"&gt;Missingness in variables with &lt;code&gt;gg_miss_var&lt;/code&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;This plot shows the number of missing values in each variable in a dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_Subsetted %&amp;gt;% miss_var_table()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in test_if_null(data): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;gg_miss_var(PISA_Subsetted, show_pct = TRUE) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in test_if_dataframe(x): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="missingness-in-cases-with-gg_miss_case"&gt;Missingness in cases with &lt;code&gt;gg_miss_case&lt;/code&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;This plot shows the number of missing values in each case. For example, the table showed that there are 2 cases with 9 missing variables (i.e., no data in all variables), and there are 1050 cases with 8 missing variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_Subsetted %&amp;gt;% miss_case_table()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in test_if_null(data): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;gg_miss_case(PISA_Subsetted)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in test_if_null(data): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="missingness-across-factors-with-gg_miss_fct"&gt;Missingness across factors with &lt;code&gt;gg_miss_fct&lt;/code&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;This plot shows the number of missingness in each column, broken down by a categorical variable from the dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;gg_miss_fct(x = PISA_Subsetted, fct = REPEAT) + 
  labs(title = &amp;quot;Missing data by the History of Class Repetition&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in tbl_vars_dispatch(x): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;gg_miss_fct(x = PISA_Subsetted, fct = LATE) + 
  labs(title = &amp;quot;Missing data by Lateness History&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in tbl_vars_dispatch(x): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The heatmap above showed the proportion of missing data we have in each response of the selected categorical variable; for example, the history of class repetition (&lt;code&gt;REPEAT&lt;/code&gt;), with &lt;code&gt;0&lt;/code&gt; as no, &lt;code&gt;1&lt;/code&gt; as yes, and &lt;code&gt;NA&lt;/code&gt; as missing.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="missingness-along-a-repeating-span-with-gg_miss_span"&gt;Missingness along a repeating span with &lt;code&gt;gg_miss_span&lt;/code&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;This plot showed the number of missings in a given span, or breaksize, for a single selected variable.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;gg_miss_span(PISA_Subsetted, REPEAT, span_every = 2000) +
  theme_dark()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in test_if_null(data): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The plot went over the data and showed us how many missing data we have every 2000 data points that it went through.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="cumulative-missing"&gt;Cumulative missing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;This plot showed the cumulative amount of missing value over the data set. A sharp increase in cumulative missing value could indicate missing patterns to be discovered.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_Subsetted %&amp;gt;% gg_miss_case_cumsum(breaks = 2000) + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in test_if_null(data): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;This plot showed the cumulative amount of missing value over the variable. We could examine the relative proportion of missing values across variables via this plot.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_Subsetted %&amp;gt;% gg_miss_var_cumsum() + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in test_if_null(data): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="what-should-we-do-with-the-missing-data"&gt;What should we do with the missing data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Now that we know we have missing data, there are numerous ways we can deal with it such as disregarding them with complete case analysis, or making educated guesses with imputation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Anyway, dealing with missing data helps minimizing bias in the data, maximizing the use of available information (We don’t want to throw away any of our hard-earned data), and increasing the chance of getting a good reliability estimates such as standard errors, confidence intervals, and &lt;em&gt;p&lt;/em&gt;-values.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="complete-case-analysis"&gt;Complete Case Analysis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Listwise deletion&lt;/strong&gt; is the method of deleting all cases with missing value, so that we get a clean and complete data set as a result, at the expense of losing a chunk of data in the process.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Listwise deletion is often a default way to handle missing data (e.g., SPSS).&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;This often results in losing 20% to 50% of the data.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_Listwise &amp;lt;- PISA_Subsetted[complete.cases(PISA_Subsetted), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in eval(expr, envir, enclos): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;glimpse(PISA_Listwise)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in glimpse(PISA_Listwise): object &amp;#39;PISA_Listwise&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Notice that the size of our dataset got reduced to 18,327 cases. This happaned from deleting all cases with missing value.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Pairwise deletion&lt;/strong&gt; is the method that deletes cases only if they have missing data on variables involved in a particular computation, so we can still retain the data for other analyses that do not involve variables that are missing. However, the effective sample size can vary from one analysis to another.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As a demonstration, we will calculate a covariance matrix using pairwise complete observation method.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;pairwise_var &amp;lt;- c(&amp;quot;BEINGBULLIED&amp;quot;, &amp;quot;DISCLIMA&amp;quot;)
cov(PISA_Subsetted[pairwise_var], use=&amp;quot;pairwise.complete.obs&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in is.data.frame(x): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;HOWEVER&lt;/strong&gt;, the bias caused by using listwise/pairwise deletion has been shown in simulations to grossly exaggerate or underestimate some effects.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Despite giving valid estimates when data are MCAR, the statistical power will be severely reduced when there is a lot of missingness. If the missingness is MAR or MNAR, removing them introduces bias to models built on these data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="missing-data-imputation"&gt;Missing Data Imputation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Other than disregarding them, we can replace the missing value with our best guess with imputation. There are three approaches we can use, donor-based imputation, model-based imputation, and multiple imputation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="donor-based-imputation"&gt;Donor-based imputation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Donor-based imputation replaces missing values based on other complete observations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="mean-imputation"&gt;Mean Imputation&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Mean imputation replaces all missing values with the mean of that variable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;First, we will create a binary indicator for whether each value was originally missing.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_meanimp &amp;lt;- PISA_Subsetted %&amp;gt;%
  mutate(DISCLIMA_imp = ifelse(is.na(DISCLIMA), TRUE, FALSE)) %&amp;gt;%
  mutate(ADAPTIVITY_imp = ifelse(is.na(ADAPTIVITY), TRUE, FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in mutate(., DISCLIMA_imp = ifelse(is.na(DISCLIMA), TRUE, FALSE)): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_meanimp[c(&amp;quot;DISCLIMA_imp&amp;quot;,&amp;quot;ADAPTIVITY_imp&amp;quot;)] %&amp;gt;% head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in head(.): object &amp;#39;PISA_meanimp&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Replace missing values in DISCLIMA and ADAPTIVITY variables with their respective means.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_meanimp &amp;lt;- PISA_meanimp %&amp;gt;%
mutate(DISCLIMA = ifelse(is.na(DISCLIMA), mean(DISCLIMA, na.rm = TRUE), DISCLIMA)) %&amp;gt;%
mutate(ADAPTIVITY = ifelse(is.na(ADAPTIVITY), mean(ADAPTIVITY, na.rm = TRUE), ADAPTIVITY))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in mutate(., DISCLIMA = ifelse(is.na(DISCLIMA), mean(DISCLIMA, na.rm = TRUE), : object &amp;#39;PISA_meanimp&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_meanimp %&amp;gt;%select(DISCLIMA, ADAPTIVITY, DISCLIMA_imp, ADAPTIVITY_imp) %&amp;gt;%
head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in select(., DISCLIMA, ADAPTIVITY, DISCLIMA_imp, ADAPTIVITY_imp): unused arguments (DISCLIMA, ADAPTIVITY, DISCLIMA_imp, ADAPTIVITY_imp)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can try plotting the data on a margin plot to see the result of our mean imputation.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_meanimp %&amp;gt;% select(DISCLIMA, ADAPTIVITY, DISCLIMA_imp, ADAPTIVITY_imp) %&amp;gt;% marginplot(delimiter=&amp;quot;imp&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in select(., DISCLIMA, ADAPTIVITY, DISCLIMA_imp, ADAPTIVITY_imp): unused arguments (DISCLIMA, ADAPTIVITY, DISCLIMA_imp, ADAPTIVITY_imp)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You can see that all missing values were replaced by the mean of that variable. Yes, we got the data back, but what did it cost?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mean imputation destroys relationship between variables.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Models predicting one using the other will be fooled by the outlying imputed values and will produce biased results.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mean imputation &lt;del&gt;crushes&lt;/del&gt; takes away variance in the data, which could potentially underestimate all standard errors. This prevents reliable hypothesis testing and calculating confidence interval.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This method is not generally recommended, but to each their own. Use it at your own discretion. With the right justification from the literature, mean imputation can be a viable method in your analysis as well.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="k-nearest-neighborknn-imputation"&gt;K-Nearest Neighbor(kNN) Imputation&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For kNN imputation, we identify ‘k’ samples in the dataset that are similar or close in the space. Then we use these ‘k’ samples to estimate the value of the missing data points.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Basically, it is like you have a data point with missing values asks its neighbors what value do they have on the variable that it is missing. That data point then replace its missing values with the value of its nearest neighbor.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_kNNimp &amp;lt;- VIM::kNN(PISA_Subsetted, k = 6, variable = c(&amp;quot;DISCLIMA&amp;quot;, &amp;quot;ADAPTIVITY&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in check_data(data): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_kNNimp[c(&amp;quot;DISCLIMA&amp;quot;, &amp;quot;ADAPTIVITY&amp;quot;,&amp;quot;DISCLIMA_imp&amp;quot;,&amp;quot;ADAPTIVITY_imp&amp;quot;)] %&amp;gt;% head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in head(.): object &amp;#39;PISA_kNNimp&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Note that there are two more columns added, DISCLIMA_imp and ADAPTIVITY_imp. The two added columns tell us if our variables of interest were imputed with the kNN method or not, with &lt;code&gt;TRUE&lt;/code&gt; indicates that the value was imputed and &lt;code&gt;FALSE&lt;/code&gt; indicates otherwise.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="model-based-imputation"&gt;Model-based imputation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For model-based imputation, missing values are predicted with a statistical or machine learning model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The model that we used depends on the type of the missing variable:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Continuous variables - linear regression&lt;/li&gt;
&lt;li&gt;Binary variables - logistic regression&lt;/li&gt;
&lt;li&gt;Categorical variables - multinomial logistic regression&lt;/li&gt;
&lt;li&gt;Count variables - Poisson regression&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="linear-regression-imputation"&gt;Linear Regression Imputation&lt;/h5&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_lmreg &amp;lt;- impute_lm(PISA_Subsetted, DISCLIMA + ADAPTIVITY ~.)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in groups(dat, formula): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_lmreg %&amp;gt;% 
  is.na() %&amp;gt;%
  colSums()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in is.data.frame(x): object &amp;#39;PISA_lmreg&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Notice that we have managed to impute some cases of &lt;code&gt;DISCLIMA&lt;/code&gt; and &lt;code&gt;ADAPTIVITY&lt;/code&gt; based on the availability of other variables in the same case. However, if there is no other variable on that case (i.e., complete missing), the model won’t be able to predict the target value as there is no predictor available.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="logistic-regression-imputation"&gt;Logistic Regression Imputation&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Logistic regression imputation is similar to linear regression imputation, with a difference in the nature of missing value (Continuous vs Binary).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;missing_REPEAT &amp;lt;- is.na(PISA_Subsetted$REPEAT)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in eval(expr, envir, enclos): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;head(missing_REPEAT, 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in head(missing_REPEAT, 20): object &amp;#39;missing_REPEAT&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_logregimp &amp;lt;- PISA_Subsetted&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in eval(expr, envir, enclos): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;logreg_model &amp;lt;- glm(REPEAT ~ DAYSKIP + BEINGBULLIED + ESCS,
                data = PISA_Subsetted, family = binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in is.data.frame(data): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;preds &amp;lt;- predict(logreg_model, type = &amp;quot;response&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in h(simpleError(msg, call)): error in evaluating the argument &amp;#39;object&amp;#39; in selecting a method for function &amp;#39;predict&amp;#39;: object &amp;#39;logreg_model&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;preds &amp;lt;- ifelse(preds &amp;gt;= 0.5, 1, 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in ifelse(preds &amp;gt;= 0.5, 1, 0): object &amp;#39;preds&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_logregimp[missing_REPEAT, &amp;quot;REPEAT&amp;quot;] &amp;lt;- preds[missing_REPEAT]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in eval(expr, envir, enclos): object &amp;#39;preds&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;table(preds[missing_REPEAT])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in table(preds[missing_REPEAT]): object &amp;#39;preds&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;table(PISA_Subsetted$REPEAT)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in table(PISA_Subsetted$REPEAT): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_logregimp %&amp;gt;% 
  is.na() %&amp;gt;%
  colSums()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in is.data.frame(x): object &amp;#39;PISA_logregimp&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="multiple-imputation-by-chained-equation"&gt;Multiple Imputation by Chained Equation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Multiple Imputation by Chained Equation (MICE) - also known as sequential regression multiple imputation - is an emerging method in dealing with missing values by implementing the imputation multiple times as opposed to the single imputation methods mentioned above &lt;a href="https://onlinelibrary.wiley.com/doi/10.1002/mpr.329"&gt;(Azur et al., 2011)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With the right model, MICE was found to be effective in reducing bias, especially in a large data set with MCAR and MAR. The method basically imputed the missing value with a statistical model (say, linear regression) multiple times for different imputed values before pooling the results together for the final most likely value that the algorithm can come up.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The package &lt;code&gt;mice&lt;/code&gt; has several statistics and machine learning models we can use such as Predictive mean matching (pmm), Classification and Regression Tree (cart), and Random Forest Imputation (rf). Keep in mind that it is a best practice to justify our selected model in missing data imputation to make the analysis as less ‘black box’ as possible for explainability.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For this post, I will use the predictive mean matchmaking method that calculates the predicted value from a randomly drawn set of candidate donors that have the value closest to the missing entry. The assumption is the distribution of the missing cell is the same as the observed data of the candidate donors.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The rationale is that PMM produces little biased estimates when missing data is below 50% and not systematically missing in a large data set &lt;a href="https://stefvanbuuren.name/fimd/sec-pmm.html"&gt;(van Buuren, 2018)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mice_model &amp;lt;- mice(PISA_Subsetted, method=&amp;#39;pmm&amp;#39;, seed = 123)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in check.dataform(data): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_miceimp &amp;lt;- complete(mice_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in complete(mice_model): object &amp;#39;mice_model&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;psych::describe(PISA_Subsetted)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in psych::describe(PISA_Subsetted): object &amp;#39;PISA_Subsetted&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;psych::describe(PISA_miceimp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in psych::describe(PISA_miceimp): object &amp;#39;PISA_miceimp&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The message above shows that the algorithm went over the data set 5 times per iteration, with the total of 25 times in 5 iterations (5 x 5). In other words, the machine imputed the missing over and over again until the change becomes minimal to give us the most stable replacement value as possible.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There is no substantial difference in descriptive statistics of the pre-imputed and post imputed data set. Given that we gained 10% of our data back, it is a win for us.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="concluding-remark"&gt;Concluding Remark&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Data cleaning is a challenging, but necessary, process in data work. That is why it is important for us to know how to identify and deal with missing data appropriately before proceeding further into developing a statistical model and drawing conclusions from it. With a solid data preparation, combining with a thorough literature review, it is likely that we can draw meaningful conclusions from the data to inform our future decisions. The opposite is also true as well for poorly processed data sets. We wouldn’t want to waste our time and resources to know that the conclusion we draw is not well-supported.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A bit of controversial topic here. Non-methodologists might have some concerns that we cannot just make up the obtained scores. Like, what if the participants did not answer that question for a reason? How can we be sure that the number we generated will represent characteristics of the targeted population? The million-dollar question is, would you still do this, knowing that the number you generated might have some degree of error? Are you willing to trade authenticity of the data for the data point that might improve your statistical models? It is your task as a researcher and an informed individual to justify your choice in this matter, as well as other choices that you made in your endeavor.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Anyway, thank you so much for your read as always! Happy Holiday, everyone! I hope you have an awesome break! :)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>9e8924d0fa8f46fafe3590635e32c199</distill:md5>
      <category>R</category>
      <category>Data Visualization</category>
      <category>Statistics</category>
      <guid>https://taridwong.github.io/posts/2021-12-27-missingdata</guid>
      <pubDate>Mon, 27 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-27-missingdata/missingpic.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Applying Machine Learning to Audio Data: Visualization, Classification, and Recommendation</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data</link>
      <description>For this entry, I am trying my hands on audio data to extract its features for exploratory data analysis (EDA), using machine learning algorithms to perform music classification, and finally build up on that result to develop a recommendation system for music of similar characteristics.

(13 min read)</description>
      <category>Python</category>
      <category>Data Visualization</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data</guid>
      <pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data/applying-machine-learning-to-audio-data_files/figure-html5/unnamed-chunk-13-11.png" medium="image" type="image/png" width="3072" height="1152"/>
    </item>
    <item>
      <title>Interactive plots for Suicide Data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data</link>
      <description>For this entry, will be visualizing suicide data from 1958 to 2015 with interactive plots to communicate insights to non-technical audience.  

(14 min read)</description>
      <category>R</category>
      <category>Data Visualization</category>
      <category>Quantitative research</category>
      <guid>https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data</guid>
      <pubDate>Thu, 09 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data/preview.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Image Recognition with Artificial Neural Networks</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks</link>
      <description>In this entry, we will be developing a deep learning algorithm - a sub-field of machine learning inspired by the structure of human brain (neural networks) - to classify images of single digit number (0-9).  

(9 min read)</description>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <category>Deep Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks</guid>
      <pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks/deepnn.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Anomaly Detection with New York City taxi data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data</link>
      <description>In this entry, I will be conducting anomaly detection to identify points of anomaly in the taxi passengers data in New York City from July 2014 to January 2015 at half-hourly intervals.
 
 (4 min read)</description>
      <category>Unsupervised Machine Learning</category>
      <category>R</category>
      <guid>https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data</guid>
      <pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data/anomaly-detection-with-new-york-city-taxi-data_files/figure-html5/unnamed-chunk-9-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Crime mapping in San Francisco with police data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data</link>
      <description>In this entry, we will explore San Francisco crime data from 2016 to 2018 to understand the relationship between civilian-reported incidents of crime and police-reported incidents of crime. Along the way we will use table intersection methods to subset our data, aggregation methods to calculate important statistics, and simple visualizations to understand crime trends.  

(7 min read)</description>
      <category>Data Visualization</category>
      <category>R</category>
      <category>Quantitative research</category>
      <guid>https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data</guid>
      <pubDate>Tue, 30 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data/crime-mapping-in-san-francisco-with-police-data_files/figure-html5/unnamed-chunk-6-1.png" medium="image" type="image/png" width="1920" height="1920"/>
    </item>
    <item>
      <title>Exploring COVID-19 data from twitter with topic modeling</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds</link>
      <description>This entry focuses on the exploration of twitter data from Alberta's Chief Medical Officer of Health via word cloud and topic modeling to gain insights in characteristics of public health messaging during the COVID-19 pandemic.  

(7 min read)</description>
      <category>Python</category>
      <category>Natural Language Processing</category>
      <category>Unsupervised Machine Learning</category>
      <category>COVID-19</category>
      <guid>https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds</guid>
      <pubDate>Tue, 23 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds/word-cloud-pv.png" medium="image" type="image/png" width="1920" height="1920"/>
    </item>
    <item>
      <title>Finding a home among the paradigm push-back with Dialectical Pluralism</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-07-introducing-dialectical-pluralism</link>
      <description>


&lt;div class="figure"&gt;
&lt;img src="https://www.relevantinsights.com/wp-content/uploads/2020/05/Qualitative-Quantitative-Research-For-New-Product-Development.png" style="width:50.0%" alt="" /&gt;
&lt;p class="caption"&gt;Photo by Relevant Insight LLC&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="the-current-status-quo-of-paradigm-wars"&gt;The current status-quo of paradigm wars&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In learning of mixed methods research, the issue of &lt;em&gt;paradigm wars&lt;/em&gt; is usually brought up in how it affects us, scholars, and the academia as a whole. I occasionally came across the over-endorsement of one method (oftentimes quantitative) to another in my conversation with colleagues like, “&lt;strong&gt;Psychology is the field that is largely explained by quantitative research&lt;/strong&gt;”, “&lt;strong&gt;We do not buy the notion that &lt;em&gt;p&lt;/em&gt;-value can conclude anything beyond numbers&lt;/strong&gt;”; these statements are the byproduct of paradigm wars, a methodological preference that divided academic community to these days &lt;a href="https://www.abc.us.org/ojs/index.php/ajtp/article/view/507"&gt;(Williams, 2020)&lt;/a&gt;. Implicit as it may be, the remnant of this conflict is still there.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="then-what-should-we-do-about-it"&gt;Then, what should we do about it?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;At methodological level, Mixed Methods Research (MMR) seems to be a realistic answer to this problem by synergizing both qualitative and quantitative data into a greater whole than the sum of the two &lt;a href="https://journals.sagepub.com/doi/10.1177/1558689815581222"&gt;(Fetters &amp;amp; Freshwater, 2015)&lt;/a&gt;. One philosophical mechanism behind MMR is Dialectical Pluralism (DP), a process philosophy that serves as a middle ground for both qualitative and quantitative paradigms for equal acknowledgement while offers equal voice to paradigm with lesser recognition.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An interesting suggestion from &lt;a href="https://journals.sagepub.com/doi/full/10.1177/1558689815575861?casa_token=wOYWvNszEvQAAAAA%3AeNkxcE6l5kzRkjVIdQfMfmMzBQtqptRuHYwUv31K-EjnpEcbPr41BLOzdbIuQpHC38eHv12-85zE"&gt;Shannon-Baker (2016)&lt;/a&gt; is that qualitative or quantitative view should be used as our regard to data rather than the whole research; for that, we should consider our research framework as a fluid “stance” to research instead of an archaic notion of static concept.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="what-about-the-how"&gt;What about the how?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;As I realize the existence of multiple realities in social science, the next step could be to move beyond our current paradigm as &lt;a href="https://journals.sagepub.com/doi/full/10.1177/1558689815607692?casa_token=qS0WXE4b3tIAAAAA%3AuB66Vn274U-7lhn5lr6IRO6ysiQtIEIwhZcyP3qpzQUpXYnRsI8p91GfSyZK3ILa2Fy-p7S8foVl"&gt;Johnson (2017)&lt;/a&gt; suggests, “&lt;strong&gt;the next theoretical step in the paradigms dialog is to articulate a metaparadigm&lt;/strong&gt;” (p.159). Researchers are encouraged to apply the dialectic worldview in their practice, both &lt;em&gt;inter&lt;/em&gt;personally and &lt;em&gt;intra&lt;/em&gt;personally.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Interpersonally, researchers could seek a collaborative space with heterogeneous team members, and conclusions should be made based on the evidence of shared values. Ultimately, practical truths should be emphasized instead of absolute truths.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Intrapersonally, researchers could have internal dialogues that reconcile differences of their perspective through personal reflection. The more ideas a researcher has in their toolbox, the more versatile they can be, as &lt;a href="https://www.esquire.com/lifestyle/a4310/the-crack-up/"&gt;Fitzgerald (1936)&lt;/a&gt; said, “&lt;strong&gt;the test of a first-rate intelligence is the ability to hold two opposed ideas in the mind at the same time and still retain the ability to function&lt;/strong&gt;”.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="final-remarks-and-food-for-thought"&gt;Final remarks and food for thought&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To me, learning about DP is quite impactful as it offers a middle ground for seemingly conflicting ideas to collaborate equally to further the body of knowledge than wresting internally over the some-old quarrel. That, my friend, could be a promising way for us to grow up as a versatile individuals (and researchers). Have a good one!&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>e30fd61efb53344445856c9ea37666ed</distill:md5>
      <category>Mixed methods research</category>
      <guid>https://taridwong.github.io/posts/2021-11-07-introducing-dialectical-pluralism</guid>
      <pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://www.relevantinsights.com/wp-content/uploads/2020/05/Qualitative-Quantitative-Research-For-New-Product-Development.png" medium="image" type="image/png"/>
    </item>
  </channel>
</rss>
