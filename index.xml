<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>Tarid Wongvorachan</title>
    <link>https://taridwong.github.io/</link>
    <atom:link href="https://taridwong.github.io/index.xml" rel="self" type="application/rss+xml"/>
    <description>Welcome to my data science blog!
</description>
    <generator>Distill</generator>
    <lastBuildDate>Sat, 31 Dec 2022 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Data Exploration with </title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-12-31-ggstat</link>
      <description>


&lt;h1 id="its-been-a-while-eh"&gt;It’s been a while, eh?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Hi everyone. It has been a while since my last post. I have been
busy with finishing my conference submissions and manuscripts. While
working, one of my professors asked me and my colleague to perform a
t-test for one of our projects. It was that time that I was introduced
to a very helpful package called &lt;a
href="https://indrajeetpatil.github.io/ggstatsplot/index.html"&gt;&lt;code&gt;ggstatsplot&lt;/code&gt;&lt;/a&gt;.
I usually do t-test with the &lt;code&gt;t.test&lt;/code&gt; function from the base
&lt;code&gt;stats&lt;/code&gt; package. It still works, but I found that
&lt;code&gt;ggstatsplot&lt;/code&gt; is more user-friendly as it provides visual of
the results instead of just texts and numbers. Plus, I like graphing. It
is fun and beautiful.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The mentioned reason is why I decided to write this post as a way
to introduce how useful &lt;code&gt;ggstatsplot&lt;/code&gt; is, as well as brushing
up the basic knowledge of data exploration techniques that maybe simple
but foundational to a lot of existing quantitative analysis techniques
out there. It is almost a mandatory for every researchers to learn about
these techniques in their introduction to research course. That is why I
hope this post would serve as a useful reference for you all. I also
want to keep things simple as a change to discussing about advance
techniques like genetic algorithm and machine learning.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In this post, I will be performing and visualizing data
exploration techniques such as Pearson’s correlation test, Chi-square
Goodness of Fit test, Chi-square Test of Independence, One-sample
t-test, and Paired-sample t-test.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;About the data, we will be using the student alcohol consumption
data set, which is a survey data of students’ mathematics score from
Portuguese secondary schools in 2005-2006 school year. The data is
available in the &lt;a
href="https://archive.ics.uci.edu/ml/datasets.php"&gt;University of
California-Irvine machine learning repository&lt;/a&gt; and &lt;a
href="https://www.kaggle.com/datasets/uciml/student-alcohol-consumption?select=student-mat.csv"&gt;Kaggle&lt;/a&gt;.
The data was collected and used by &lt;a
href="https://repositorium.sdum.uminho.pt/handle/1822/8024"&gt;Cortez and
Silva (2008)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="data-preprocessing-and-assumption-check"&gt;Data Preprocessing and
Assumption Check&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;There are four packages that I used, &lt;code&gt;ggplot2&lt;/code&gt; for base
data visualization, &lt;code&gt;dplyr&lt;/code&gt; for data manipulation,
&lt;code&gt;DataExplorer&lt;/code&gt; for data summarization, &lt;code&gt;psych&lt;/code&gt; for
descriptive statistics, &lt;code&gt;rstatix&lt;/code&gt; for preliminary analysis,
and &lt;code&gt;ggstatsplot&lt;/code&gt;, the main package that we will play around
today, for data visualization with statistical details.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(ggplot2)
library(dplyr)
library(DataExplorer)
library(psych)
library(rstatix)
library(tidyr)
library(ggstatsplot)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;First, we will read the data set with &lt;code&gt;read.csv&lt;/code&gt;, convert
the categorical variables into factors with &lt;code&gt;as.factor&lt;/code&gt;, and
subset relevant variables out with &lt;code&gt;select&lt;/code&gt;. It makes the
data much easier to work with when we filter the variables that we want
into a smaller data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;df &amp;lt;- read.csv(&amp;quot;student-mat.csv&amp;quot;, header = TRUE)

df &amp;lt;- df %&amp;gt;% 
  as.data.frame() %&amp;gt;%
  mutate(across(c(sex, address, schoolsup, famsup), as.factor))

#drop variable

df_clean &amp;lt;- df %&amp;gt;% select (c(sex, address, schoolsup, famsup, G1, G3))&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Next, we will explore variable type in the data set and missing
value with plot_intro and proportion of categorical variables with
&lt;code&gt;plot_bar&lt;/code&gt;. Both packages are from the
&lt;code&gt;DataExplorer&lt;/code&gt; package.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_intro(df_clean)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34903e1337a_files/figure-html/unnamed-chunk-5-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The introduction bar plot shows no missing data. Majority of the
variables are discrete, which are &lt;code&gt;sex&lt;/code&gt; - students’
biological sex, &lt;code&gt;address&lt;/code&gt; - student’s home address type
(binary: ‘U’ - urban or ‘R’ - rural), &lt;code&gt;schoolsup&lt;/code&gt; - whether
that student enrolls in extra educational support, and
&lt;code&gt;famsup&lt;/code&gt; - whether that student gets educational support from
their family. The remaining variables are continuous; those variables
are &lt;code&gt;G1&lt;/code&gt; - students’ score in the first data collection
period, and &lt;code&gt;G3&lt;/code&gt; - students’ final score.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot_bar(df_clean)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34903e1337a_files/figure-html/unnamed-chunk-6-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The bar plots above show that the proportion between male and female
students is rather balance. The proportion between students who receive
educational support from family is skewed, and the proportion between
students who receive extra educational support from school and students’
home address type are severely skewed.&lt;/li&gt;
&lt;li&gt;Next, we will perform preliminary analysis to check if our data
follows assumptions of the test we will perform with the
&lt;code&gt;ggstatsplot&lt;/code&gt; package. We will use descriptive statistics
with the &lt;code&gt;describe&lt;/code&gt; function to examine characteristics of
our variable of interest, and &lt;code&gt;identify_outliers&lt;/code&gt; to check
for extreme outliers.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;describe(df_clean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           vars   n  mean   sd median trimmed  mad min max range
sex*          1 395  1.47 0.50      1    1.47 0.00   1   2     1
address*      2 395  1.78 0.42      2    1.85 0.00   1   2     1
schoolsup*    3 395  1.13 0.34      1    1.04 0.00   1   2     1
famsup*       4 395  1.61 0.49      2    1.64 0.00   1   2     1
G1            5 395 10.91 3.32     11   10.80 4.45   3  19    16
G3            6 395 10.42 4.58     11   10.84 4.45   0  20    20
            skew kurtosis   se
sex*        0.11    -1.99 0.03
address*   -1.33    -0.24 0.02
schoolsup*  2.20     2.86 0.02
famsup*    -0.46    -1.79 0.02
G1          0.24    -0.71 0.17
G3         -0.73     0.37 0.23&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The descriptive results shows that most variables except
&lt;code&gt;schoolsup&lt;/code&gt; have normal distribution, with skewness and
kurtosis values within ± 2 (&lt;a
href="https://www.routledge.com/An-Introduction-to-Statistical-Concepts/Hahs-Vaughn-Lomax/p/book/9781138650558"&gt;Lomax
&amp;amp; Hahs-Vaughn, 2020&lt;/a&gt;). We can visually confirm normality with
Quantile-quantile plot.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;set.seed(123)
plot_qq(df_clean)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34903e1337a_files/figure-html/unnamed-chunk-8-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Q-Q plots show no departure from normality in our continuous
variables (i.e., test scores from the first period and final score).
Next, we will examine the presence of outliers.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;df_clean %&amp;gt;% 
  group_by(sex) %&amp;gt;%
  identify_outliers(G1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] sex        address    schoolsup  famsup     G1         G3        
[7] is.outlier is.extreme
&amp;lt;0 rows&amp;gt; (or 0-length row.names)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;No outlier detected. We are good to perform Pearson’s correlation
test, Chi-square test, One-sample t-test, and Dependent-sample t-test
with &lt;code&gt;ggstatplot&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="scatter-plot---pearsons-correlation-test"&gt;Scatter plot -
Pearson’s correlation test&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The first function I really like is &lt;a
href="https://indrajeetpatil.github.io/ggstatsplot/articles/web_only/ggscatterstats.html"&gt;&lt;code&gt;ggscatterstat&lt;/code&gt;&lt;/a&gt;.
It provides a scatter plot with statistical details to show association
between two continuous variables. The plot can be used to check whether
the association between two contuinuous variables is linear, as well as
their distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;set.seed(123)

ggstatsplot::ggscatterstats(
  data = df_clean, 
  x = G1, 
  y = G3, 
  type = &amp;quot;parametric&amp;quot;,            # type of test that needs to be run
  conf.level = 0.95,
  
  title = &amp;quot;Correlation Test&amp;quot;,
  xlab = &amp;quot;First period grade&amp;quot;,       # label for x axis
  ylab = &amp;quot;Final grade&amp;quot;,              # label for y axis 
  line.color = &amp;quot;blue&amp;quot;, 
  messages = FALSE,
  
  label.var = famsup,
  label.expression = G3 &amp;gt;= 19)+
  
  ggplot2::theme(plot.title = element_text(hjust = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34903e1337a_files/figure-html/unnamed-chunk-10-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The scatter plot shows a strong positive correlation between
first period scores and final scores among 395 pairs of students
(&lt;em&gt;p&lt;/em&gt; &amp;lt; 0.001). The meaning is that students’ score tend to go
the same way; that is, if a student scores well early in their course,
it is likely that their final score will be high as well. The
distribution of both scores also seems normal as shown by the bar
plot.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can also flag cases with extreme value with
&lt;code&gt;label.expression&lt;/code&gt; and &lt;code&gt;label.var&lt;/code&gt; as well for
further investigation. In our case, we flag student whose final score
equal to or greater than 19 and ask the function to identify whether
that student gets educational support from their family.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="pie-chart---chi-square-test"&gt;Pie Chart - Chi-Square test&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The second function, &lt;a
href="https://indrajeetpatil.github.io/ggstatsplot/articles/web_only/ggpiestats.html"&gt;&lt;code&gt;ggpiestats&lt;/code&gt;&lt;/a&gt;,
can be used to examine “Goodness of fit” to see whether the proportions
of our variable matches with what we hypothesized. We can first specify
the proportion that we think our variable will have using
&lt;code&gt;ratio&lt;/code&gt; argument. Here, I hypothesized that the proportion
between students who receive- and did not receive educational support
from family are 50/50; therefore ,the value in &lt;code&gt;ratio&lt;/code&gt;
argument is c(0.5, 0.5).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;set.seed(123)

ggstatsplot::ggpiestats(
  data = df_clean,
  x = famsup,
  type = &amp;quot;parametric&amp;quot;,
  ratio = c(0.50,0.50),
  messages = FALSE,
  paired = FALSE, #Logical indicating whether data came from a within-subjects or repeated measures design study
  conf.level = 0.95,
  title = &amp;quot;Chi-Square Goodness of Fit Test&amp;quot;
)+
  ggplot2::theme(plot.title = element_text(hjust = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34903e1337a_files/figure-html/unnamed-chunk-11-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The pie chart shows that the proportion between students who
receive- and did not receive educational support from family differ from
what I hypothesized (&lt;em&gt;p&lt;/em&gt; &amp;lt; 0.001). In fact, the pie chart
shows that 61% of students from our sample receive educational support
from family and 39% of students did not receive educational support from
family.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Pearson’s C value indicates effect size, which is magnitude
of the result reported by the plot. In our case, the effect size is
0.22, indicating a small magnitude of difference between our
hypothesized proportion and the actual proportion of our variable of
interest. In plain language, it means that our hypothesized proportion
(i.e., 50/50) is somewhat close to the actual proportion (i.e.,
39/61).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can also add another variable in the function to request for a
Chi-Square test of independence that examines whether two variables are
related. We will try examining if whether that student enrolls in extra
educational support (&lt;code&gt;schoolsup&lt;/code&gt;) is associated with whether
that student gets educational support from their family
(&lt;code&gt;famsup&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;set.seed(123)

ggstatsplot::ggpiestats(
  data = df_clean,
  x = famsup,
  y = schoolsup,
  type = &amp;quot;parametric&amp;quot;,
  ratio = c(0.50,0.50),
  messages = FALSE,
  paired = FALSE, #Logical indicating whether data came from a within-subjects or repeated measures design study
  conf.level = 0.95,
  title = &amp;quot;Chi-Square Test of Independence&amp;quot;
)+
  ggplot2::theme(plot.title = element_text(hjust = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34903e1337a_files/figure-html/unnamed-chunk-12-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The pie chart shows a small difference between schoolsup and
famsup (&lt;em&gt;p&lt;/em&gt; &amp;lt; 0.05). The Cramer’s V effect size of 0.09
indicating a small magnitude of difference between the two variables.
The interpretation is that there is a weak association between whether a
student enrolls in extra educational support and whether that a student
gets educational support from their family.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The plot also shows that the proportion within the two variables
differn from 50/50 as indicated by the provided Chi-square goodness of
fit results.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="histogram---one-sample-t-test"&gt;Histogram - One Sample
t-test&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The third function, &lt;a
href="https://indrajeetpatil.github.io/ggstatsplot/articles/web_only/gghistostats.html"&gt;&lt;code&gt;gghistostats&lt;/code&gt;&lt;/a&gt;,
can be used to examine distribution of a continuous variable and to test
if the mean of a sample variable is different from a specified value
(population parameter) with one-sample t-test. In the function, I
hypothesized that the population mean for students’ final score
(&lt;code&gt;G3&lt;/code&gt;) is 10 as specified in &lt;code&gt;test.value&lt;/code&gt;
argument.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;set.seed(123)

ggstatsplot::gghistostats(
  data = df_clean,
  x = G3,
  title = &amp;quot;Distribution of Final Grade&amp;quot;,
  centrality.type = &amp;quot;parametric&amp;quot;,    # one sample t-test
  test.value = 10,                    
  effsize.type = &amp;quot;d&amp;quot;,         
  xlab = &amp;quot;Students&amp;#39; Final score&amp;quot;,
  ylab = &amp;quot;Number of Student&amp;quot;,
  centrality.para = &amp;quot;mean&amp;quot;,          # which measure of central tendency is to be plotted
  normal.curve = TRUE,
  binwidth = 1,                   # binwidth value (needs to be toyed around with until you find the best one)
  messages = FALSE                   # turn off the messages
) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34903e1337a_files/figure-html/unnamed-chunk-13-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The histogram shows that the sample mean does not significantly
differ from the hypothesized (or population) mean (&lt;em&gt;p&lt;/em&gt; &amp;gt;
0.05), meaning that the actual mean in our sample (i.e., 10.42) is close
to the mean that we thought students’ population would have (i.e., 10).
The histogram also shows that our data is normally distributed as seen
from the normality curve. We will not interpret effect size (i.e.,
Cohen’s d) as there is no statistical significance in the results.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="violin-plot---dependent-sample-t-test"&gt;Violin plot - Dependent
Sample t-test&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The fourth and final function we will play around is &lt;a
href="https://indrajeetpatil.github.io/ggstatsplot/articles/web_only/ggwithinstats.html"&gt;&lt;code&gt;ggwithinstats&lt;/code&gt;&lt;/a&gt;,
that performs a paired-sample t-test or dependent sample t-test to
determine whether the mean the two measurement time point is zero. In
our case, we are examining students’ mathematics score between their
first period and their final score. The function requires a long-format
data set, so we will transform the data with &lt;code&gt;pivot_longer&lt;/code&gt;
from &lt;code&gt;tidyr&lt;/code&gt; package.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;df_long &amp;lt;- df_clean %&amp;gt;% pivot_longer(cols=c(&amp;#39;G1&amp;#39;, &amp;#39;G3&amp;#39;),
                        names_to=&amp;#39;Measure_point&amp;#39;,
                        values_to=&amp;#39;Scores&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# for reproducibility
set.seed(123)

ggstatsplot::ggwithinstats(
  data = df_long, 
  x = Measure_point, 
  y = Scores,
  type = &amp;quot;parametric&amp;quot;, # type of statistical test
  xlab = &amp;quot;Period of testing&amp;quot;,
  ylab = &amp;quot;Students&amp;#39; score&amp;quot;,
  pairwise.comparisons = TRUE, 
  sphericity.correction = FALSE, ## don&amp;#39;t display sphericity corrected dfs and p-values
  
  outlier.tagging = TRUE, ## whether outliers should be flagged
  outlier.label = address, ## label to attach to outlier values
  outlier.label.color = &amp;quot;green&amp;quot;, ## outlier point label color
  
  mean.plotting = TRUE, ## whether the mean is to be displayed
  mean.color = &amp;quot;darkblue&amp;quot;, ## color for mean
  
  title = &amp;quot;Comparison of students&amp;#39; score&amp;quot;)+
  
  ggplot2::theme(plot.title = element_text(hjust = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34903e1337a_files/figure-html/unnamed-chunk-15-1.png" width="672" data-distill-preview=1 /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Results of the paired sample t-test are displayed along with
violin plot that shows both mean and distribution density of both
conditions (i.e., &lt;code&gt;G1&lt;/code&gt; and &lt;code&gt;G3&lt;/code&gt;). The results show
that the mean of students’ score between two time points are different
(&lt;em&gt;p&lt;/em&gt; &amp;lt; 0.001). The Hedges’s g effect size of 0.18 indicating a
small magnitude of difference between the two variables. The
interpretation is that there is a small difference between students’
first period and final mathematics scores.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The dotted lines in the plot show the pair of students between
the two time points. Note that there are some students who took the
first exam but did not take the final exam as shown from outliers in the
final exam scores (i.e., the orange part). There are some students who
scored zero in the final exam.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="conclusion"&gt;Conclusion&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Both descriptive and inferential statistics techniques that we
used in this post are mainly for data exploration. Despite their
simplicity, they serve as foundations for several research that
underlies products such as data driven COVID-19 policy (&lt;a
href="https://www.mdpi.com/668636"&gt;Li et al., 2020&lt;/a&gt;), validation
method for machine learning algorithm (&lt;a
href="https://dx.plos.org/10.1371/journal.pone.0224365"&gt;Vabalas et al.,
2019&lt;/a&gt;), and more complex statistical analysis such as structural
equation modeling (&lt;a
href="https://journal.psych.ac.cn/acps/EN/abstract/abstract2415.shtml"&gt;Zhonglin
et al., 2004&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;My point for this post is that simpler techniques should not be
overlooked. In some cases, they are enough to answer research questions
without relying on complex techniques that are more challenging to
perform, yet provide results that are harder to interpret. Basically, if
it makes sense for your work to use just a simple t-test, then there is
no need to overthink. In &lt;a
href="https://doi.org/10.1111/bjet.13276"&gt;one of our articles&lt;/a&gt;, we
used step-wise regression instead of random forest algorithm because it
performs better with a smaller sample size and is easier to interpret.
Staying simple when we can is the point that I think we, as researchers,
should consider in our work. Thank you very much for reading this. Happy
new year 2023!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>74aa2936e1f3e576ef5639e3dbd7a2c5</distill:md5>
      <category>R</category>
      <category>Statistics</category>
      <guid>https://taridwong.github.io/posts/2022-12-31-ggstat</guid>
      <pubDate>Sat, 31 Dec 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-12-31-ggstat/ggstat_files/figure-html5/unnamed-chunk-15-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Test Shortening with Genetic Algorithm and Ant Colony Optimization</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-08-14-ga-aco</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Hi, Everyone. It’s been really hot here in Summer. I have some
more time before my next school year (2023) begins. As I learned more
about Psychometric, I came to know about techniques that can be used to
reduce the number of questions in a questionnaire for efficiency. This
post was inspired by &lt;a
href="https://okan.cloud/posts/2021-01-19-how-to-shorten-a-measurement-instrument-automatically-part-ii/"&gt;my
supervisor’s post of the same technique&lt;/a&gt;. He applied the data to the
Experiences in Close Relationships (ECR) scale of Adult Romantic
Attachment Measure (&lt;a
href="https://psycnet.apa.org/record/1997-36873-002?source=post_page---------------------------"&gt;Brennan
et al., 1998&lt;/a&gt;). I want to try following his footstep and replicate
the same techniques to a different data set for my practice. In this
post, I will use Genetic Algorithm and Ant Colony Optimization Algorithm
to automatically shorten the length of a test.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will be using the International Personality Item Pool data set
(&lt;a
href="https://www.sciencedirect.com/science/article/pii/S0092656605000553"&gt;Goldberg
et al., 2006&lt;/a&gt;). The data set has polytomous test items; that is,
answers from test takers can be more than two values. Instead of “right”
and “wrong”, the answer can be “strongly disagree”, “disagree”,
“neutral”, “agree”, and “strongly agree”. The test measures person’s
characteristics of extraversion, emotional stability, agreeableness,
conscientiousness, and openness to experience.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;First, we will import the data set and subset a portion of the
data for feasibility. The full data set has 176,380 cases. We will
subset only 5000 of them.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;setwd(&amp;#39;D:/Class_and_Work/U_Alberta/Research projects/Response time&amp;#39;)
library(tidyverse)
library(data.table)
RANDOM_STATE = 123

df &amp;lt;- fread(&amp;quot;ipip.csv&amp;quot;, header = TRUE, fill = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The data set contains responses of individuals to 50 personality
test questions. Each response is on ordinal scale, meaning that the
response can be categorized and ranked (i.e., disagree - neutral -
agree). The correlation plot below indicates relationships between items
measuring the same dimension (e.g., extraversion). Items with negative
correlation are reverse-coded, meaning that the items are rephrased to
have an opposite meaning. For example, a question measuring extraversion
may ask “I see myself as someone who is talkative”. The reverse-coded
version maybe “I see myself as someone who tends to be quiet”.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;df_survey &amp;lt;- df[1:5000, 1:50]

head(df_survey)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   EXT1 EXT2 EXT3 EXT4 EXT5 EXT6 EXT7 EXT8 EXT9 EXT10 EST1 EST2 EST3
1:    4    1    5    2    5    1    5    2    4     1    1    4    4
2:    3    5    3    4    3    3    2    5    1     5    2    3    4
3:    2    3    4    4    3    2    1    3    2     5    4    4    4
4:    2    2    2    3    4    2    2    4    1     4    3    3    3
5:    3    3    3    3    5    3    3    5    3     4    1    5    5
6:    3    3    4    2    4    2    2    3    3     4    3    4    3
   EST4 EST5 EST6 EST7 EST8 EST9 EST10 AGR1 AGR2 AGR3 AGR4 AGR5 AGR6
1:    2    2    2    2    2    3     2    2    5    2    4    2    3
2:    1    3    1    2    1    3     1    1    4    1    5    1    5
3:    2    2    2    2    2    1     3    1    4    1    4    2    4
4:    2    3    2    2    2    4     3    2    4    3    4    2    4
5:    3    1    1    1    1    3     2    1    5    1    5    1    3
6:    2    2    1    2    1    2     2    2    3    1    4    2    3
   AGR7 AGR8 AGR9 AGR10 CSN1 CSN2 CSN3 CSN4 CSN5 CSN6 CSN7 CSN8 CSN9
1:    2    4    3     4    3    4    3    2    2    4    4    2    4
2:    3    4    5     3    3    2    5    3    3    1    3    3    5
3:    1    4    4     3    4    2    2    2    3    3    4    2    4
4:    2    4    3     4    2    4    4    4    1    2    2    3    1
5:    1    5    5     3    5    1    5    1    3    1    5    1    5
6:    2    3    4     4    3    2    4    1    3    2    4    3    4
   CSN10 OPN1 OPN2 OPN3 OPN4 OPN5 OPN6 OPN7 OPN8 OPN9 OPN10
1:     4    5    1    4    1    4    1    5    3    4     5
2:     3    1    2    4    2    3    1    4    2    5     3
3:     2    5    1    2    1    4    2    5    3    4     4
4:     4    4    2    5    2    3    1    4    4    3     3
5:     5    5    1    5    1    5    1    5    3    5     5
6:     3    5    1    5    1    3    1    5    4    5     2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;DataExplorer::plot_correlation(df_survey)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34908957537_files/figure-html/unnamed-chunk-3-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The test has 10 items (or questions) in each domain, totalling 50
test items. This number seems reasonable, but we could further reduce
the number of items to avoid fatiguing our examinee. One way we can
reduce the number is through the use of coefficient alpha reliability.
The &lt;code&gt;alpha&lt;/code&gt; function in &lt;code&gt;psych&lt;/code&gt; package can be
used to examine how much reliability the test will have after dropping
some items.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;alpha &amp;lt;- psych::alpha(df_survey, check.keys=TRUE)
alpha$alpha.drop&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       raw_alpha std.alpha   G6(smc) average_r      S/N    alpha se
EXT1-  0.8696207 0.8679394 0.9137837 0.1182655 6.572282 0.002611411
EXT2   0.8691535 0.8675726 0.9134115 0.1179326 6.551309 0.002621739
EXT3-  0.8668282 0.8651111 0.9121684 0.1157390 6.413506 0.002671484
EXT4   0.8683708 0.8667878 0.9129267 0.1172256 6.506819 0.002638125
EXT5-  0.8673617 0.8656936 0.9123513 0.1162518 6.445660 0.002659426
EXT6   0.8685602 0.8667696 0.9132939 0.1172093 6.505797 0.002632815
EXT7-  0.8682855 0.8668348 0.9129689 0.1172677 6.509470 0.002640926
EXT8   0.8712934 0.8695949 0.9150056 0.1197881 6.668412 0.002577510
EXT9-  0.8704641 0.8687061 0.9143138 0.1189665 6.616502 0.002593862
EXT10  0.8683898 0.8669061 0.9133637 0.1173317 6.513491 0.002638721
EST1   0.8703855 0.8687731 0.9141121 0.1190281 6.620390 0.002594302
EST2-  0.8715179 0.8698555 0.9152856 0.1200308 6.683770 0.002572223
EST3   0.8713890 0.8698611 0.9149559 0.1200360 6.684096 0.002575067
EST4-  0.8722369 0.8705305 0.9161060 0.1206634 6.723826 0.002558568
EST5   0.8712487 0.8695326 0.9155456 0.1197302 6.664750 0.002576908
EST6   0.8697921 0.8682577 0.9140230 0.1185557 6.590577 0.002606440
EST7   0.8698114 0.8682822 0.9134175 0.1185780 6.591988 0.002605601
EST8   0.8695089 0.8680298 0.9130377 0.1183477 6.577465 0.002612008
EST9   0.8692737 0.8677100 0.9136737 0.1180571 6.559153 0.002617047
EST10  0.8680675 0.8667208 0.9130501 0.1171656 6.503046 0.002644521
AGR1   0.8729617 0.8708241 0.9162004 0.1209403 6.741382 0.002540781
AGR2-  0.8696206 0.8674465 0.9136713 0.1178184 6.544122 0.002609287
AGR3   0.8720020 0.8701186 0.9156558 0.1202767 6.699333 0.002559864
AGR4-  0.8718810 0.8699519 0.9145632 0.1201208 6.689461 0.002561684
AGR5   0.8713817 0.8694433 0.9147959 0.1196472 6.659506 0.002572622
AGR6-  0.8740795 0.8722052 0.9166081 0.1222577 6.825042 0.002520054
AGR7   0.8695310 0.8674391 0.9132795 0.1178118 6.543703 0.002610839
AGR8-  0.8712084 0.8692263 0.9151725 0.1194462 6.646796 0.002576807
AGR9-  0.8719947 0.8700279 0.9149316 0.1201919 6.693960 0.002559508
AGR10- 0.8692295 0.8669770 0.9138922 0.1173954 6.517496 0.002617962
CSN1-  0.8715010 0.8696873 0.9151554 0.1198740 6.673849 0.002569307
CSN2   0.8743915 0.8722266 0.9165440 0.1222783 6.826354 0.002512017
CSN3-  0.8729250 0.8712122 0.9165431 0.1213081 6.764712 0.002542491
CSN4   0.8695425 0.8679183 0.9139521 0.1182463 6.571073 0.002609660
CSN5-  0.8713024 0.8695137 0.9151010 0.1197126 6.663639 0.002573481
CSN6   0.8723390 0.8703796 0.9153758 0.1205215 6.714834 0.002551467
CSN7-  0.8739940 0.8724664 0.9170480 0.1225097 6.841070 0.002522233
CSN8   0.8704663 0.8686382 0.9148756 0.1189042 6.612566 0.002591175
CSN9-  0.8724726 0.8705899 0.9156435 0.1207194 6.727375 0.002550163
CSN10- 0.8718644 0.8699681 0.9157181 0.1201359 6.690420 0.002563072
OPN1-  0.8731489 0.8713227 0.9152167 0.1214131 6.771378 0.002536994
OPN2   0.8723907 0.8704576 0.9152106 0.1205948 6.719479 0.002551733
OPN3-  0.8742745 0.8725471 0.9161203 0.1225876 6.846034 0.002516526
OPN4   0.8733534 0.8715435 0.9160396 0.1216236 6.784738 0.002533974
OPN5-  0.8710467 0.8687933 0.9141883 0.1190467 6.621561 0.002579241
OPN6   0.8723086 0.8703180 0.9150081 0.1204637 6.711173 0.002553227
OPN7-  0.8718728 0.8699217 0.9156417 0.1200926 6.687679 0.002563265
OPN8-  0.8752372 0.8730645 0.9163104 0.1230898 6.878014 0.002497109
OPN9   0.8760793 0.8755095 0.9191497 0.1255113 7.032742 0.002491151
OPN10- 0.8716998 0.8695968 0.9144305 0.1197899 6.668526 0.002565750
            var.r      med.r
EXT1-  0.02247896 0.08151262
EXT2   0.02242356 0.08166635
EXT3-  0.02239961 0.07907977
EXT4   0.02233050 0.08041782
EXT5-  0.02230091 0.07914325
EXT6   0.02288756 0.07930301
EXT7-  0.02235567 0.08146699
EXT8   0.02281218 0.08201026
EXT9-  0.02271017 0.08153477
EXT10  0.02258192 0.08041782
EST1   0.02257745 0.08146699
EST2-  0.02293955 0.08210536
EST3   0.02261406 0.08151262
EST4-  0.02326988 0.08180150
EST5   0.02327337 0.08157208
EST6   0.02262279 0.08082590
EST7   0.02258286 0.08146699
EST8   0.02248911 0.08132409
EST9   0.02280470 0.08041782
EST10  0.02268410 0.08002237
AGR1   0.02320503 0.08201026
AGR2-  0.02287127 0.08108114
AGR3   0.02323923 0.08151262
AGR4-  0.02249784 0.08225522
AGR5   0.02279661 0.08201026
AGR6-  0.02261039 0.08321513
AGR7   0.02275238 0.08092384
AGR8-  0.02309551 0.08162526
AGR9-  0.02265465 0.08180150
AGR10- 0.02322872 0.07874079
CSN1-  0.02314059 0.08166635
CSN2   0.02285444 0.08249577
CSN3-  0.02337612 0.08195375
CSN4   0.02299208 0.08082590
CSN5-  0.02315868 0.08157208
CSN6   0.02302597 0.08225522
CSN7-  0.02294173 0.08385676
CSN8   0.02335693 0.07998131
CSN9-  0.02305508 0.08195375
CSN10- 0.02344565 0.08162526
OPN1-  0.02302069 0.08295971
OPN2   0.02315918 0.08201026
OPN3-  0.02266237 0.08249577
OPN4   0.02310353 0.08225522
OPN5-  0.02321234 0.08082590
OPN6   0.02315973 0.08201026
OPN7-  0.02343876 0.08146699
OPN8-  0.02269568 0.08336518
OPN9   0.02248538 0.08407870
OPN10- 0.02292938 0.08162526&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;summary(alpha)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Reliability analysis   
 raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r
      0.87      0.87    0.92      0.12 6.8 0.0025  2.7 0.44    0.082&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="genetic-algorithm"&gt;Genetic Algorithm&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Another way we can reduce the number of items is through the use
of Genetic Algorithm, which is an optimization method inspired from the
natural selection theory (&lt;a
href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0167110"&gt;Schroeders
et al., 2016&lt;/a&gt;). This is how it works:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First, the algorithm randomly selects several item sets from the
item pool. These item sets act as parents.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Second, the algorithm picks items from each parent to form
several sets of items as children (or an offspring).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Third, some items in the children set were exchanged with items
from the item pool as a mutation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Fourth, the children were evaluated. Children who produce better
results were kept while those that did not do well are discarded. The
process continues until a certain criterion is met.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Figure 1 below from visualizes how the process is done.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-08-14-ga-acoGA.png" style="width:75.0%" alt="" /&gt;
&lt;p class="caption"&gt;Illustration of the Genetic Algorithm. Figure from &lt;a
href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0167110"&gt;Schroeders
et al. (2016)&lt;/a&gt; .No copyright infringement is intended&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;We will use the &lt;code&gt;GAabbreviate&lt;/code&gt; package to perform test
abbreviation with the Genetic Algorithm (&lt;a
href="https://www.frontiersin.org/articles/10.3389/fpsyg.2016.00189/full"&gt;Sahdra
et al., 2016&lt;/a&gt;). To prepare the data, we will first create summations
of scores in each domain for every examinee as the scale score. We will
also transform the data frame into a matrix.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(GAabbreviate)

scales = cbind(rowSums(df_survey[, 1:10]), 
               rowSums(df_survey[, 11:20]),
               rowSums(df_survey[, 21:30]),
               rowSums(df_survey[, 31:40]),
               rowSums(df_survey[, 41:50]))

df_survey &amp;lt;- as.data.frame(sapply(df_survey, as.integer))

df_survey &amp;lt;- matrix(as.integer(unlist(df_survey)), nrow=nrow(df_survey))&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We will create the Genetic Algorithm object with the
&lt;code&gt;GAabbreviate&lt;/code&gt; function (&lt;a
href="https://www.frontiersin.org/articles/10.3389/fpsyg.2016.00189/full"&gt;Sahdra
et al, 2016&lt;/a&gt;). We will set the cost of each item to 0.001 so that the
algorithm can produce results that explains the most variance (&lt;a
href="https://www.sciencedirect.com/science/article/pii/S0092656610000036"&gt;Yarkoni,
2010&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;ipip_ga &amp;lt;- GAabbreviate(items = df_survey, # Matrix of item responses
                      verbose = FALSE,
                      scales = scales, # Scale scores
                      itemCost = 0.001, # The cost of each item
                      maxItems = 5, # Max number of items per dimension
                      maxiter = 1000, # Max number of iterations
                      run = 100, # Number of runs
                      crossVal = TRUE, # Cross-validation
                      seed = RANDOM_STATE) # Seed for reproducibility&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can request for a summary of the algorithm below. The algorithm
ran 301 times before achieving the best results. The number of items in
the final set is 25, meaning that we reduced the test in half.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;summary(ipip_ga)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;── Genetic Algorithm ─────────────────── 

GA settings: 
Type                  =  binary 
Population size       =  50 
Number of generations =  1000 
Elitism               =  2 
Crossover probability =  0.8 
Mutation probability  =  0.1 

GA results: 
Iterations             = 301 
Total cost             = 1.74582
Number of items in initial set = 50
Number of items in final set = 25
Mean coefficient alpha = 0.3212
Mean convergent correlation (training) = 0.8069
Mean convergent correlation (validation) = 0.8029&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can call for coefficient alpha reliability of the test dimensions
below. Reliability of the emotional stability, agreeableness, and
openness to experience dimension are satisfactory (0.80 for emotional
stability, 0.73 for agreeableness), meaning that the subtest we created
for the mentioned two dimensions are usable (&lt;a
href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4205511/"&gt;Tavakol
&amp;amp; Dennick, 2011&lt;/a&gt;). However, items in the extraversion,
conscientiousness, openness to experience dimension should be revised as
indicated by their reliability (-0.21 for extraversion, -0.39 for
conscientiousness, and 0.67 for openness) (&lt;a
href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4205511/"&gt;Tavakol
&amp;amp; Dennick, 2011&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;ipip_ga$measure$alpha&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          Scale1    Scale2    Scale3     Scale4    Scale5
alpha -0.2131873 0.8071732 0.7320936 -0.3923633 0.6721945&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can also request for the list of items in each dimension as
demonstrated below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;ext &amp;lt;- which(ipip_ga$measure$key[,1]==1)
est &amp;lt;- which(ipip_ga$measure$key[,2]==1)
agr &amp;lt;- which(ipip_ga$measure$key[,3]==1)
csn &amp;lt;- which(ipip_ga$measure$key[,4]==1)
opn &amp;lt;- which(ipip_ga$measure$key[,5]==1)

ipip_ga$measure$items[ext]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;x1 x2 x3 x6 x7 
 1  2  3  6  7 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;ipip_ga$measure$items[est]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;x11 x15 x16 x17 x18 
 11  15  16  17  18 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;ipip_ga$measure$items[agr]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;x22 x26 x28 x29 x30 
 22  26  28  29  30 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;ipip_ga$measure$items[csn]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;x32 x33 x35 x36 x37 
 32  33  35  36  37 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;ipip_ga$measure$items[opn]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;x41 x45 x48 x49 x50 
 41  45  48  49  50 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can also request for a summary plot of the Genetic Algorithm
below. The plot has total cost, test length, mean of the overall
explained variance, explained variance in each dimension, and pattern of
items included in each iteration.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(ipip_ga)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34908957537_files/figure-html/unnamed-chunk-11-1.png" width="672" data-distill-preview=1 /&gt;&lt;/p&gt;
&lt;h2 id="ant-colony-algorithm"&gt;Ant Colony Algorithm&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;One more way we can shorten our test is using the Ant Colony
Optimization (ACO) algorithm. ACO is an optimization method inspired
from the foraging behavior of Argentine ants by using virtual ants to
find the shortest path to a destination, which is the optimal set of
test items for our case (&lt;a href="10.1109/MCI.2006.329691"&gt;Doringo et
al., 2006&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;See Figure 2 below for the visual illustration of ACO. There are
two routes that lead to the same food source. As ants travel randomly to
the food source, they all leave pheromone for others to follow
them.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, given that the upper route is shorter, it receives more
pheromone as ants travel back and forth from their nest to the food
source more often. The route with more pheromone (i.e., the shorter
route) is chosen more by the ants while the longer route gets chosen
less as the pheromone evaporates from having less ant.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-08-14-ga-acoACO.png" style="width:75.0%" alt="" /&gt;
&lt;p class="caption"&gt;Illustration of the Any Colony Optimization
Algorithm. Figure from &lt;a href="10.1109/MCI.2006.329691"&gt;Doringo et
al. (2006)&lt;/a&gt;. No copyright infringement is intended&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;We will use the &lt;code&gt;Shortform&lt;/code&gt; package to perform test
abbreviation with ACO (&lt;a
href="https://cran.r-project.org/web/packages/ShortForm/index.html"&gt;Raborn
&amp;amp; Leite, 2020&lt;/a&gt;). We will load the package and subset the data for
feasibility. ACO is quite computationally expensive with a large data
set. To save time, I subsetted only the first 100 cases of IPIP response
data.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(ShortForm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  #####                             #######                      
 #     # #    #  ####  #####  ##### #        ####  #####  #    # 
 #       #    # #    # #    #   #   #       #    # #    # ##  ## 
  #####  ###### #    # #    #   #   #####   #    # #    # # ## # 
       # #    # #    # #####    #   #       #    # #####  #    # 
 #     # #    # #    # #   #    #   #       #    # #   #  #    # 
  #####  #    #  ####  #    #   #   #        ####  #    # #    # 
 
         Version 0.4.6
             (o&amp;lt;
             //\
             V_/_ &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;df_survey_aco &amp;lt;- df[1:100, 1:50] #ACO uses dataframe format&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We will also define model structure of the measurement with
&lt;code&gt;lavaan&lt;/code&gt; syntax. Here, I defined five dimensions that are
measured by 10 items each. In addition to the measurement model, I will
also indicate the list of items that measure each dimension.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;model &amp;lt;- &amp;quot; ext =~ EXT1+EXT2+EXT3+EXT4+EXT5+EXT6+EXT7+EXT8+EXT9+EXT10
           est =~ EST1+EST2+EST3+EST4+EST5+EST6+EST7+EST8+EST9+EST10
           agr =~ AGR1+AGR2+AGR3+AGR4+AGR5+AGR6+AGR7+AGR8+AGR9+AGR10
           csn =~ CSN1+CSN2+CSN3+CSN4+CSN5+CSN6+CSN7+CSN8+CSN9+CSN10
           opn =~ OPN1+OPN2+OPN3+OPN4+OPN5+OPN6+OPN7+OPN8+OPN9+OPN10
         &amp;quot;

items &amp;lt;- list(c(paste0(&amp;quot;EXT&amp;quot;, seq(1, 10))),
              c(paste0(&amp;quot;EST&amp;quot;, seq(1, 10))),
              c(paste0(&amp;quot;AGR&amp;quot;, seq(1, 10))),
              c(paste0(&amp;quot;CSN&amp;quot;, seq(1, 10))),
              c(paste0(&amp;quot;OPN&amp;quot;, seq(1, 10)))
              )

items&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
 [1] &amp;quot;EXT1&amp;quot;  &amp;quot;EXT2&amp;quot;  &amp;quot;EXT3&amp;quot;  &amp;quot;EXT4&amp;quot;  &amp;quot;EXT5&amp;quot;  &amp;quot;EXT6&amp;quot;  &amp;quot;EXT7&amp;quot;  &amp;quot;EXT8&amp;quot; 
 [9] &amp;quot;EXT9&amp;quot;  &amp;quot;EXT10&amp;quot;

[[2]]
 [1] &amp;quot;EST1&amp;quot;  &amp;quot;EST2&amp;quot;  &amp;quot;EST3&amp;quot;  &amp;quot;EST4&amp;quot;  &amp;quot;EST5&amp;quot;  &amp;quot;EST6&amp;quot;  &amp;quot;EST7&amp;quot;  &amp;quot;EST8&amp;quot; 
 [9] &amp;quot;EST9&amp;quot;  &amp;quot;EST10&amp;quot;

[[3]]
 [1] &amp;quot;AGR1&amp;quot;  &amp;quot;AGR2&amp;quot;  &amp;quot;AGR3&amp;quot;  &amp;quot;AGR4&amp;quot;  &amp;quot;AGR5&amp;quot;  &amp;quot;AGR6&amp;quot;  &amp;quot;AGR7&amp;quot;  &amp;quot;AGR8&amp;quot; 
 [9] &amp;quot;AGR9&amp;quot;  &amp;quot;AGR10&amp;quot;

[[4]]
 [1] &amp;quot;CSN1&amp;quot;  &amp;quot;CSN2&amp;quot;  &amp;quot;CSN3&amp;quot;  &amp;quot;CSN4&amp;quot;  &amp;quot;CSN5&amp;quot;  &amp;quot;CSN6&amp;quot;  &amp;quot;CSN7&amp;quot;  &amp;quot;CSN8&amp;quot; 
 [9] &amp;quot;CSN9&amp;quot;  &amp;quot;CSN10&amp;quot;

[[5]]
 [1] &amp;quot;OPN1&amp;quot;  &amp;quot;OPN2&amp;quot;  &amp;quot;OPN3&amp;quot;  &amp;quot;OPN4&amp;quot;  &amp;quot;OPN5&amp;quot;  &amp;quot;OPN6&amp;quot;  &amp;quot;OPN7&amp;quot;  &amp;quot;OPN8&amp;quot; 
 [9] &amp;quot;OPN9&amp;quot;  &amp;quot;OPN10&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We will create the ACO object with the &lt;code&gt;antcolony.lavaan&lt;/code&gt;
function. We will set the number of ants to 20, pheromone evaporation
rate to 0.5, and fit indices of comparative fit index (CFI),
Tucker-Lewis index (TLI), and root mean square error of approximation
(RMSEA).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;ipip_ACO &amp;lt;- antcolony.lavaan(data = df_survey_aco, # Response data set
                            ants = 20, # Number of ants
                            evaporation = 0.5, #  % of the pheromone retained after evaporation
                            antModel = model, # Factor model for IPIP
                            list.items = items, # Items for each dimension
                            full = 50, # The total number of unique items in the IPIP scale
                            i.per.f = c(5, 5, 5, 5, 5), # The desired number of items per dimension. The number has to match the number of dimension
                            factors = c(&amp;#39;EXT&amp;#39;,&amp;#39;EST&amp;#39;,&amp;#39;AGR&amp;#39;,&amp;#39;CSN&amp;#39;,&amp;#39;OPN&amp;#39;), # Names of dimensions
                            
                            # lavaan settings - Change estimator to WLSMV
                            lavaan.model.specs = list(model.type = &amp;quot;cfa&amp;quot;, auto.var = T, estimator = &amp;quot;WLSMV&amp;quot;,
                                                      ordered = NULL, int.ov.free = TRUE, int.lv.free = FALSE, 
                                                      auto.fix.first = TRUE, auto.fix.single = TRUE, 
                                                      auto.cov.lv.x = TRUE, auto.th = TRUE, auto.delta = TRUE,
                                                      auto.cov.y = TRUE, std.lv = F),
                            
                            steps = 50, # The number of ants in a row for which the model does not change
                            fit.indices = c(&amp;#39;cfi&amp;#39;, &amp;#39;tli&amp;#39;, &amp;#39;rmsea&amp;#39;), # Fit statistics to use
                            fit.statistics.test = &amp;quot;(cfi &amp;gt; 0.95)&amp;amp;(tli &amp;gt; 0.95)&amp;amp;(rmsea &amp;lt; 0.06)&amp;quot;,
                            max.run = 300) # The maximum number of ants to run before the algorithm stops&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 Run number 1.           
 Run number 2.           
 Run number 3.           
 Run number 4.           
 Run number 5.           
 Run number 6.           
 Run number 7.           
 Run number 8.           
 Run number 9.           
 Run number 10.           
 Run number 11.           
 Run number 12.           
 Run number 13.           
 Run number 14.           
 Run number 15.           
 Run number 16.           
 Run number 17.           
 Run number 18.           
 Run number 19.           
 Run number 20.           
 Run number 21.           [1] &amp;quot;Compiling results.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;After getting the result, we can check our fit indices and the list
of items retained by ACO. Item that has “1” indicates that it is
selected by the algorithm.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;ipip_ACO[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          cfi       tli      rmsea mean_gamma EXT1 EXT2 EXT3 EXT4
[1,] 0.968031 0.9638086 0.03537734      0.529    1    0    1    0
     EXT5 EXT6 EXT7 EXT8 EXT9 EXT10 EST1 EST2 EST3 EST4 EST5 EST6
[1,]    1    0    1    0    1     0    0    0    1    0    1    1
     EST7 EST8 EST9 EST10 AGR1 AGR2 AGR3 AGR4 AGR5 AGR6 AGR7 AGR8
[1,]    0    1    0     1    0    1    1    1    1    0    0    1
     AGR9 AGR10 CSN1 CSN2 CSN3 CSN4 CSN5 CSN6 CSN7 CSN8 CSN9 CSN10
[1,]    0     0    1    1    0    0    1    0    0    0    1     1
     OPN1 OPN2 OPN3 OPN4 OPN5 OPN6 OPN7 OPN8 OPN9 OPN10
[1,]    1    0    0    0    1    0    0    1    1     1&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can also request for the summary of lavaan model from the subtest
extracted by ACO.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;ipip_ACO$best.model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;lavaan 0.6-12 ended normally after 71 iterations

  Estimator                                       DWLS
  Optimization method                           NLMINB
  Number of model parameters                        60

  Number of observations                           100

Model Test User Model:
                                              Standard      Robust
  Test Statistic                               283.962     324.326
  Degrees of freedom                               265         265
  P-value (Chi-square)                           0.202       0.007
  Scaling correction factor                                  1.883
  Shift parameter                                          173.490
    simple second-order correction                                &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Next, we can also request for an output of which item is loaded onto
which dimension in the new model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;cat(ipip_ACO$best.syntax)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;EXT =~ EXT7 + EXT3 + EXT5 + EXT1 + EXT9
EST =~ EST3 + EST10 + EST6 + EST8 + EST5
AGR =~ AGR8 + AGR4 + AGR3 + AGR2 + AGR7
CSN =~ CSN2 + CSN5 + CSN9 + CSN1 + CSN10
OPN =~ OPN5 + OPN8 + OPN1 + OPN9 + OPN10
         &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;When checking coefficient alpha reliability of the items chosen by
ACO, we can see below that the dimension of extraversion, emotional
stability, agreeableness, and conscientiousness have satisfactory
reliability values.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#check reliability of each subscale

#EXT
psych::alpha(df_survey_aco[, c(&amp;quot;EXT1&amp;quot;, &amp;quot;EXT3&amp;quot;, &amp;quot;EXT5&amp;quot;, &amp;quot;EXT4&amp;quot;, &amp;quot;EXT7&amp;quot;)], check.keys=TRUE)$total&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; raw_alpha std.alpha   G6(smc) average_r      S/N        ase  mean
 0.8854019  0.886472 0.8729437 0.6096311 7.808396 0.01810261 3.038
       sd  median_r
 1.032285 0.5956059&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;#EST
psych::alpha(df_survey_aco[, c(&amp;quot;EST1&amp;quot;, &amp;quot;EST5&amp;quot;, &amp;quot;EST6&amp;quot;, &amp;quot;EST7&amp;quot;, &amp;quot;EST8&amp;quot;)], check.keys=TRUE)$total&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; raw_alpha std.alpha  G6(smc) average_r      S/N        ase  mean
  0.819672 0.8179554 0.816637 0.4733049 4.493158 0.02866376 2.962
        sd  median_r
 0.9705544 0.4724666&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;#AGR
psych::alpha(df_survey_aco[, c(&amp;quot;AGR2&amp;quot;, &amp;quot;AGR4&amp;quot;, &amp;quot;AGR5&amp;quot;, &amp;quot;AGR9&amp;quot;, &amp;quot;AGR10&amp;quot;)], check.keys=TRUE)$total&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; raw_alpha std.alpha G6(smc) average_r      S/N        ase  mean
 0.7890755  0.788927 0.76396 0.4277669 3.737698 0.03327975 3.794
        sd  median_r
 0.7776967 0.4230731&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;#CSN
psych::alpha(df_survey_aco[, c(&amp;quot;CSN3&amp;quot;, &amp;quot;CSN5&amp;quot;, &amp;quot;CSN7&amp;quot;, &amp;quot;CSN9&amp;quot;, &amp;quot;CSN10&amp;quot;)], check.keys=TRUE)$total&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; raw_alpha std.alpha   G6(smc) average_r      S/N        ase  mean
 0.7009179 0.6928163 0.6664406 0.3108564 2.255382 0.04580886 3.284
        sd  median_r
 0.7914595 0.2909734&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;#OPN
psych::alpha(df_survey_aco[, c(&amp;quot;OPN5&amp;quot;, &amp;quot;OPN7&amp;quot;, &amp;quot;OPN8&amp;quot;, &amp;quot;OPN9&amp;quot;, &amp;quot;OPN10&amp;quot;)], check.keys=TRUE)$total&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; raw_alpha std.alpha   G6(smc) average_r      S/N        ase  mean
 0.5971511 0.6095035 0.5874082 0.2379028 1.560843 0.06264639 3.802
        sd  median_r
 0.6006697 0.2483103&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The explained variance plot below shows that ACO was able to
increase the amount of variance explained by the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(ipip_ACO, type = &amp;quot;variance&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$Pheromone
NULL

$Gamma
NULL

$Beta
NULL

$Variance&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34908957537_files/figure-html/unnamed-chunk-19-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can also request for the pheromone plot with the argument
&lt;code&gt;type=""pheromone&lt;/code&gt;. This will allow us to see the amount of
pheromone used by our virtual ants.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(ipip_ACO, type = &amp;quot;pheromone&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$Pheromone&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34908957537_files/figure-html/unnamed-chunk-20-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
$Gamma
NULL

$Beta
NULL

$Variance
NULL&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;This post demonstrated several ways we can reduce the number of
test items whether through the traditional method of reliability
examination, Genetic Algorithm, or ACO. Many psychological assessment
provides helpful insights to clinicians, educators, and the examinee
themselves with helpful information. However, some tests are quite
lengthy that they fatigue the examinee out before; for example, the &lt;a
href="https://www.upress.umn.edu/test-division/MMPI-2-RF"&gt;Minnesota
Multiphasic Personality Inventory-2 Restructured Form (MMPI-2-RF)&lt;/a&gt;
has 338 items. Genetic Algorithm and ACO may be useful to reduce the
number of test items to the optimal level while retaining accuracy and
representativeness of the test.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, note that the two algorithms are not silver bullets. We
cannot just apply them to every test without consulting the literature,
test developers, test users, and other relevant stakeholders first.
After reducing the number of test items, researchers should check if the
remaining items are representative to the measured construct and perform
a pilot testing to examine if the test serves its intended purpose. As
always, thank you very much for reading this!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>c46ab128857e41863a9767e1dac6d118</distill:md5>
      <category>R</category>
      <category>Psychometric</category>
      <guid>https://taridwong.github.io/posts/2022-08-14-ga-aco</guid>
      <pubDate>Sun, 14 Aug 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-08-14-ga-aco/ga-aco_files/figure-html5/unnamed-chunk-11-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Leveraging a Large-Scale Educational Data Set with Educational Data Mining</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-08-06-edm</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Hi Everyone. It’s been awhile since my last blog post. I have
been occupied with writing and research meeting, among other things. I
have had the opportunity to work with several large-scale data sets from
start to finish (i.e., planning research ideas, data cleaning,
interpreting patterns, and translating insights for the audience. That
is why I want to post some of my ideas to this blog to share with you
with it is like to work with data from one end to another. In this post,
I will be predicting students’ high school dropout rate through the
usage of a large-scale educational data set.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;My work is largely in the field of educational data mining (EDM),
which is the method of knowledge discovery from educational databases
(&lt;a
href="https://www.wiley.com/en-gb/Data+Mining+and+Learning+Analytics%3A+Applications+in+Educational+Research-p-9781118998236"&gt;Elatia
et al., 2016&lt;/a&gt;). Such data is usually extracted from sources such as
students’ interactive learning environment, computerized testing, and
large-scale assessment data repository (&lt;a
href="https://educationaldatamining.org/"&gt;International Educational Data
Mining Society, 2022&lt;/a&gt;). The data set I use in this posting is the
High School Longitudinal Study of 2009, which is a longitudinal data set
that tracks the transition of American youth from secondary schooling to
subsequent education and work roles.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The original data set has 4014 variables and 23,503 cases that
were collected from students’ base year (2009), first follow-up (2012),
2013 update collection (2013), high school transcripts (2013–2014), and
second follow-up (2016). First, I chose a handful of variables based on
theories that are relevant to the prediction of students’ school
dropout.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;After the initial screening, we have 67 variables left. Then, I
further removed responses that were not answered by students or their
parents to preserve data representation. I use &lt;a
href="https://boxuancui.github.io/DataExplorer/"&gt;&lt;code&gt;dataexplorer&lt;/code&gt;&lt;/a&gt;
package to examine types and missingness of the variables. Figure 1
below shows that the data set largely consists of categorical variables
then continuous variables. The data also has a bit of missing
data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-08-06-edmMissing.png" style="width:50.0%" alt="" /&gt;
&lt;p class="caption"&gt;Figure 1. Variable Type and Missing Data&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="data-preprocessing"&gt;Data Preprocessing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To further clean up the data, variables with more than 30%
missingness were removed, and the rest missing data was imputed with
Random Forest algorithm based on the &lt;a
href="https://cran.r-project.org/web/packages/mice/mice.pdf"&gt;multivariate
imputation by chained equation&lt;/a&gt; method. I have done everything in
advance to save time. Here is the cleaned data. I will also load the
following packages for data preprocessing.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tidyverse)
library(readxl)
library(Hmisc)
library(corrplot)

hsls_30_rf &amp;lt;-read_csv(&amp;quot;hsls_30percent_imputed_rf.csv&amp;quot;, col_names = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;At this point, the data set has 51 variables and 16137 cases. I
converted some variables into factors to reflect their nature with
&lt;code&gt;as.factor&lt;/code&gt; function. I also mapped correlation matrix of the
data set to examine variables that are not related to one another.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;hsls_30_rf &amp;lt;- hsls_30_rf %&amp;gt;% 
  as.data.frame() %&amp;gt;%
  mutate(across(c(X1SEX, X1RACE, X1MOMRESP, 
                  X1MOMEDU, X1MOMRACE, X1DADRESP, 
                  X1DADEDU, X1DADRACE, X1HHNUMBER, 
                  X1STUEDEXPCT, X1PAREDEXPCT, X1TMRACE, 
                  X1TMCERT, 
                  X1LOCALE, X1REGION, S1NOHWDN, 
                  S1NOPAPER, S1NOBOOKS, S1LATE, 
                  S1PAYOFF, S1GETINTOCLG, S1AFFORD, 
                  S1WORKING, S1FRNDGRADES, S1FRNDSCHOOL, 
                  S1FRNDCLASS, S1FRNDCLG, S1HRMHOMEWK, 
                  S1HRSHOMEWK, S1SUREHSGRAD, P1BEHAVE, 
                  P1ATTEND, P1PERFORM, P1HWOFTEN, 
                  X4EVERDROP, X4PSENRSTLV), as.factor))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;correlation_30_rf &amp;lt;-rcorr(as.matrix(hsls_30_rf))

corrplot(correlation_30_rf$r, type = &amp;quot;upper&amp;quot;, order = &amp;quot;hclust&amp;quot;, 
         p.mat = correlation_30_rf$P, insig = &amp;quot;pch&amp;quot;, pch = 4, pch.cex = 1,
         tl.col = &amp;quot;black&amp;quot;, tl.cex = 0.5, tl.srt = 90)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2022-08-06-edmcor-unclean.png" style="width:75.0%" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Based on the above correlation matrix and theoretical relevance,
variables that are recommended for removal are: &lt;code&gt;X1RACE&lt;/code&gt;,
&lt;code&gt;X1MOMRACE&lt;/code&gt;, &lt;code&gt;X1DADRACE&lt;/code&gt;, &lt;code&gt;X1LOCALE&lt;/code&gt;,
&lt;code&gt;P1HWOFTEN&lt;/code&gt;, &lt;code&gt;X1HHNUMBER&lt;/code&gt;, &lt;code&gt;X1TMCERT&lt;/code&gt;,
&lt;code&gt;X1REGION&lt;/code&gt;, &lt;code&gt;X1MOMRESP&lt;/code&gt;, &lt;code&gt;X1DADRESP&lt;/code&gt;,
&lt;code&gt;X1SEX&lt;/code&gt;, &lt;code&gt;X1TMRACE&lt;/code&gt;, &lt;code&gt;X1TSRACE&lt;/code&gt;,
&lt;code&gt;X1MTHUTI.&lt;/code&gt; They are removed because 1) they are not
theoretically related to the prediction of high school dropout and 2)
they have insignificant correlation that might negatively impact the
prediction result.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;hsls_30_rf_final &amp;lt;- hsls_30_rf %&amp;gt;% select(!c(X1RACE, X1MOMRACE, X1DADRACE, X1LOCALE, P1HWOFTEN, X1HHNUMBER, X1TMCERT, X1REGION, X1MOMRESP, X1DADRESP, X1SEX, X1TMRACE, X1MTHUTI))&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;After uncorrelated variables were removed, we have 38 variables
left. The new correlation matrix is as follows:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;correlation_30_rf_final &amp;lt;-rcorr(as.matrix(hsls_30_rf_final))

corrplot(correlation_30_rf_final$r, type = &amp;quot;upper&amp;quot;, order = &amp;quot;hclust&amp;quot;, 
         p.mat = correlation_30_rf_final$P, insig = &amp;quot;pch&amp;quot;, pch = 4, pch.cex = 1,
         tl.col = &amp;quot;black&amp;quot;, tl.cex = 0.5, tl.srt = 90)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2022-08-06-edmcor-clean.png" style="width:75.0%" /&gt;&lt;/p&gt;
&lt;h2 id="data-augmentation"&gt;Data Augmentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;After the initial data preprocessing in R, I will use Python to
perform machine learning. I personally use R for data
exploration/statistical analysis and Python for machine learning. First,
I will initiate Python environment and import necessary modules.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

from sklearn.manifold import TSNE

import warnings
warnings.filterwarnings(&amp;quot;ignore&amp;quot;)

RANDOM_STATE = 123&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Then, I will transfer the data set to Python environment because the
two languages run in parallel instead of on the same ground.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;df = r.hsls_30_rf_final

df[&amp;#39;X4EVERDROP&amp;#39;] = np.where(df[&amp;#39;X4EVERDROP&amp;#39;] == &amp;quot;0&amp;quot;, 0, 1)

df.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  X1MOMEDU X1DADEDU   X1SES  ...  P1PERFORM  X4EVERDROP  X4PSENRSTLV
0        5        5  1.5644  ...          1           0            1
1        3        2 -0.3699  ...          1           0            0
2        7        0  1.2741  ...          1           0            1
3        4        0  0.1495  ...          1           1            2
4        3        3  1.0639  ...          1           0            1

[5 rows x 38 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;From the data set, I will extract the predictors (X) and the
targeted variable (y). I will also check class proportion of the
targeted variable to see if they are balanced.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;X_extreme = df.drop(&amp;#39;X4EVERDROP&amp;#39;, axis=1)
y_extreme = df[&amp;#39;X4EVERDROP&amp;#39;]

print(&amp;quot;The proportion of target variable&amp;#39;s class :&amp;quot;, Counter(y_extreme))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The proportion of target variable&amp;#39;s class : Counter({0: 14133, 1: 2004})&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can see that the data is imbalanced. We have 14133 cases of
student who did not and 2004 student who dropped out of their high
school. Class imbalance problem in educational data sets could hamper
the accuracy of predictive models as many of them are designed on the
assumption that the predicted class is balanced (&lt;a
href="https://www.wiley.com/en-us/Imbalanced+Learning%3A+Foundations%2C+Algorithms%2C+and+Applications-p-9781118074626"&gt;He
&amp;amp; Ma, 2013&lt;/a&gt;). This problem is especially prevalent in the
prediction of high-stakes educational issues such as such as school
dropout or grade repetition, where discrepancy between two classes is
high due to its rare occurrence (&lt;a
href="https://www.mdpi.com/2227-7102/9/4/275"&gt;Barros et al.,
2019&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;I will visualize the imbalance with a t-Distributed Stochastic
Neighbor Embedding (tSNE) plot and a count plot.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;tsne = TSNE(n_components=2, random_state=RANDOM_STATE)

TSNE_result = tsne.fit_transform(X_extreme)

plt.figure(figsize=(12,8))
sns.scatterplot(TSNE_result[:,0], TSNE_result[:,1], hue=y_extreme, legend=&amp;#39;full&amp;#39;, palette=&amp;quot;hls&amp;quot;)

plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file349051826594_files/figure-html/unnamed-chunk-10-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;
sns.set_theme(style=&amp;quot;darkgrid&amp;quot;)
sns.countplot(x=&amp;quot;X4EVERDROP&amp;quot;, data = df)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file349051826594_files/figure-html/unnamed-chunk-11-3.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In &lt;a href="https://taridwong.github.io/posts/2022-04-28-xai/"&gt;my
previous post&lt;/a&gt;, I used the combination of Synthetic Minority
Oversampling TEchnique (SMOTE) and Edited Nearest Neighbor (ENN). The
thing is, SMOTE+ENN only works with numerical variables. We have a lot
of categorical variables in this data set, so we need to find a
workaround for that. I will use SMOTE for nominal and continuous
variable (SMOTE-NC) and random undersampling instead instead.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;from imblearn.over_sampling import SMOTENC
from imblearn.under_sampling import RandomUnderSampler 
from sklearn.model_selection import train_test_split

smote_nc = SMOTENC(random_state=RANDOM_STATE, sampling_strategy=0.8,
                    categorical_features=[0, 1, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36])

rus_hybrid = RandomUnderSampler(random_state=RANDOM_STATE, sampling_strategy=&amp;#39;not minority&amp;#39;)

X_smote_extreme, y_smote_extreme = smote_nc.fit_resample(X_extreme, y_extreme)

X_hybrid_extreme, y_hybrid_extreme = rus_hybrid.fit_resample(X_smote_extreme, y_smote_extreme)

print(&amp;quot;For Y extreme :&amp;quot;, Counter(y_extreme))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;For Y extreme : Counter({0: 14133, 1: 2004})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;For Y smote extreme :&amp;quot;, Counter(y_smote_extreme))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;For Y smote extreme : Counter({0: 14133, 1: 11306})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;For Y hybrid extreme :&amp;quot;, Counter(y_hybrid_extreme))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;For Y hybrid extreme : Counter({0: 11306, 1: 11306})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;X_train_hybrid_ext, X_test_hybrid_ext, y_train_hybrid_ext, y_test_hybrid_ext = train_test_split(X_hybrid_extreme, y_hybrid_extreme, test_size = 0.30, random_state = RANDOM_STATE)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;After we finished augmenting the data, below is the result. We have
much more instances of student who dropped out of their high school as
seen from the tSNE plot and the count plot below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;TSNE_result = tsne.fit_transform(X_hybrid_extreme)

plt.figure(figsize=(12,8))
sns.scatterplot(TSNE_result[:,0], TSNE_result[:,1], hue=y_hybrid_extreme, legend=&amp;#39;full&amp;#39;, palette=&amp;quot;hls&amp;quot;)

plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file349051826594_files/figure-html/unnamed-chunk-13-5.png" width="1152" /&gt;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;sns.set_theme(style=&amp;quot;darkgrid&amp;quot;)
sns.countplot(y_hybrid_extreme)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file349051826594_files/figure-html/unnamed-chunk-14-7.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Next, we will proceed to the classification stage. For many
classification algorithms such as XGBoost or Random Forest, you need to
transform categorical variables into numerical variables with label
encoding or one-hot encoding first. However, we have a lot of
categorical variables that may hamper the process. To circumvent this,
we will use &lt;a href="https://catboost.ai/"&gt;CatBoost&lt;/a&gt;, which is a
gradient boosting decision tree that supports categorical variables
without the need for data transformation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="classification"&gt;Classification&lt;/h2&gt;
&lt;pre class="python"&gt;&lt;code&gt;from sklearn.feature_selection import RFECV
from catboost import CatBoostClassifier
from sklearn.model_selection import RandomizedSearchCV&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;First, we will create a CatBoost object as well as a list of
hyperparameters to tune. We can use the default mode of CatBoost, but
tuning the algorithm makes the algorithm perform better. I will tune
tree depth, learning rate, and the number of iteration that the machine
learns. I use randomized grid search to tune the algorithm to save
time.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;CBC = CatBoostClassifier(random_state=RANDOM_STATE)

parameters = {&amp;#39;depth&amp;#39;         : [4,5,6,7,8,9,10],
              &amp;#39;learning_rate&amp;#39; : [0.01,0.02,0.03,0.04,0.05],
              &amp;#39;iterations&amp;#39;    : [10,20,30,40,50,60,70,80,90,100]
             }

cat_features = [0, 1, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36]

Cat_random = RandomizedSearchCV(estimator = CBC, 
                                param_distributions = parameters, 
                                n_iter = 10, cv = 3, verbose=0, 
                                random_state = RANDOM_STATE, error_score=&amp;#39;raise&amp;#39;)

Cat_random.fit(X_train_hybrid_ext, y_train_hybrid_ext, cat_features = cat_features)
&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}&lt;/style&gt;&lt;div id="sk-container-id-1" class="sk-top-container"&gt;&lt;div class="sk-text-repr-fallback"&gt;&lt;pre&gt;RandomizedSearchCV(cv=3, error_score=&amp;#x27;raise&amp;#x27;,
                   estimator=&amp;lt;catboost.core.CatBoostClassifier object at 0x0000017E09E4C130&amp;gt;,
                   param_distributions={&amp;#x27;depth&amp;#x27;: [4, 5, 6, 7, 8, 9, 10],
                                        &amp;#x27;iterations&amp;#x27;: [10, 20, 30, 40, 50, 60,
                                                       70, 80, 90, 100],
                                        &amp;#x27;learning_rate&amp;#x27;: [0.01, 0.02, 0.03,
                                                          0.04, 0.05]},
                   random_state=123)&lt;/pre&gt;&lt;b&gt;In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. &lt;br /&gt;On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.&lt;/b&gt;&lt;/div&gt;&lt;div class="sk-container" hidden&gt;&lt;div class="sk-item sk-dashed-wrapped"&gt;&lt;div class="sk-label-container"&gt;&lt;div class="sk-label sk-toggleable"&gt;&lt;input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" &gt;&lt;label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow"&gt;RandomizedSearchCV&lt;/label&gt;&lt;div class="sk-toggleable__content"&gt;&lt;pre&gt;RandomizedSearchCV(cv=3, error_score=&amp;#x27;raise&amp;#x27;,
                   estimator=&amp;lt;catboost.core.CatBoostClassifier object at 0x0000017E09E4C130&amp;gt;,
                   param_distributions={&amp;#x27;depth&amp;#x27;: [4, 5, 6, 7, 8, 9, 10],
                                        &amp;#x27;iterations&amp;#x27;: [10, 20, 30, 40, 50, 60,
                                                       70, 80, 90, 100],
                                        &amp;#x27;learning_rate&amp;#x27;: [0.01, 0.02, 0.03,
                                                          0.04, 0.05]},
                   random_state=123)&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="sk-parallel"&gt;&lt;div class="sk-parallel-item"&gt;&lt;div class="sk-item"&gt;&lt;div class="sk-label-container"&gt;&lt;div class="sk-label sk-toggleable"&gt;&lt;input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" &gt;&lt;label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow"&gt;estimator: CatBoostClassifier&lt;/label&gt;&lt;div class="sk-toggleable__content"&gt;&lt;pre&gt;&amp;lt;catboost.core.CatBoostClassifier object at 0x0000017E09E4C130&amp;gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="sk-serial"&gt;&lt;div class="sk-item"&gt;&lt;div class="sk-estimator sk-toggleable"&gt;&lt;input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" &gt;&lt;label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow"&gt;CatBoostClassifier&lt;/label&gt;&lt;div class="sk-toggleable__content"&gt;&lt;pre&gt;&amp;lt;catboost.core.CatBoostClassifier object at 0x0000017E09E4C130&amp;gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;After the tuning, I will print out the grid search result. The best
hyperparameter values we have are learning_rate = 0.05, iterations = 80,
and depth = 8. I will then let the machine learn from the data by
fitting the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot; Results from Grid Search &amp;quot; )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; Results from Grid Search &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;\n The best estimator across ALL searched params:\n&amp;quot;,Cat_random.best_estimator_)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 The best estimator across ALL searched params:
 &amp;lt;catboost.core.CatBoostClassifier object at 0x0000017E018FE5B0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;\n The best score across ALL searched params:\n&amp;quot;,Cat_random.best_score_)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 The best score across ALL searched params:
 0.8718094516047511&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;\n The best parameters across ALL searched params:\n&amp;quot;,Cat_random.best_params_)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 The best parameters across ALL searched params:
 {&amp;#39;learning_rate&amp;#39;: 0.05, &amp;#39;iterations&amp;#39;: 80, &amp;#39;depth&amp;#39;: 8}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;CBC_tuned = CatBoostClassifier(learning_rate = 0.05, iterations = 80, depth = 8, random_state=RANDOM_STATE)

CBC_tuned.fit(X_train_hybrid_ext, y_train_hybrid_ext, cat_features = cat_features)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0:  learn: 0.6665327    total: 70.3ms   remaining: 5.55s
1:  learn: 0.6464428    total: 145ms    remaining: 5.66s
2:  learn: 0.6274255    total: 232ms    remaining: 5.94s
3:  learn: 0.6038900    total: 311ms    remaining: 5.91s
4:  learn: 0.5853056    total: 395ms    remaining: 5.92s
5:  learn: 0.5711453    total: 477ms    remaining: 5.89s
6:  learn: 0.5518932    total: 580ms    remaining: 6.05s
7:  learn: 0.5371312    total: 661ms    remaining: 5.95s
8:  learn: 0.5246082    total: 773ms    remaining: 6.1s
9:  learn: 0.5108871    total: 867ms    remaining: 6.07s
10: learn: 0.5017540    total: 953ms    remaining: 5.98s
11: learn: 0.4914030    total: 1.04s    remaining: 5.88s
12: learn: 0.4829017    total: 1.14s    remaining: 5.86s
13: learn: 0.4753740    total: 1.24s    remaining: 5.85s
14: learn: 0.4684517    total: 1.33s    remaining: 5.78s
15: learn: 0.4595894    total: 1.44s    remaining: 5.74s
16: learn: 0.4516042    total: 1.53s    remaining: 5.68s
17: learn: 0.4452736    total: 1.63s    remaining: 5.62s
18: learn: 0.4382162    total: 1.72s    remaining: 5.53s
19: learn: 0.4326054    total: 1.81s    remaining: 5.44s
20: learn: 0.4268372    total: 1.9s remaining: 5.35s
21: learn: 0.4202041    total: 2s   remaining: 5.29s
22: learn: 0.4167642    total: 2.11s    remaining: 5.23s
23: learn: 0.4122437    total: 2.2s remaining: 5.14s
24: learn: 0.4093793    total: 2.31s    remaining: 5.08s
25: learn: 0.4039919    total: 2.43s    remaining: 5.05s
26: learn: 0.4004563    total: 2.52s    remaining: 4.95s
27: learn: 0.3975197    total: 2.65s    remaining: 4.92s
28: learn: 0.3932144    total: 2.76s    remaining: 4.86s
29: learn: 0.3904852    total: 2.87s    remaining: 4.79s
30: learn: 0.3880708    total: 2.98s    remaining: 4.71s
31: learn: 0.3859222    total: 3.1s remaining: 4.65s
32: learn: 0.3831005    total: 3.2s remaining: 4.56s
33: learn: 0.3810144    total: 3.32s    remaining: 4.49s
34: learn: 0.3785504    total: 3.42s    remaining: 4.39s
35: learn: 0.3761624    total: 3.51s    remaining: 4.29s
36: learn: 0.3737088    total: 3.61s    remaining: 4.2s
37: learn: 0.3712488    total: 3.72s    remaining: 4.11s
38: learn: 0.3685311    total: 3.85s    remaining: 4.04s
39: learn: 0.3662991    total: 3.96s    remaining: 3.96s
40: learn: 0.3634880    total: 4.08s    remaining: 3.88s
41: learn: 0.3606838    total: 4.19s    remaining: 3.79s
42: learn: 0.3587348    total: 4.31s    remaining: 3.71s
43: learn: 0.3567398    total: 4.42s    remaining: 3.62s
44: learn: 0.3548707    total: 4.54s    remaining: 3.53s
45: learn: 0.3520839    total: 4.66s    remaining: 3.44s
46: learn: 0.3492787    total: 4.77s    remaining: 3.35s
47: learn: 0.3467724    total: 4.87s    remaining: 3.25s
48: learn: 0.3430803    total: 4.97s    remaining: 3.15s
49: learn: 0.3411632    total: 5.06s    remaining: 3.04s
50: learn: 0.3396453    total: 5.16s    remaining: 2.93s
51: learn: 0.3377685    total: 5.26s    remaining: 2.83s
52: learn: 0.3362602    total: 5.36s    remaining: 2.73s
53: learn: 0.3349568    total: 5.45s    remaining: 2.63s
54: learn: 0.3336092    total: 5.55s    remaining: 2.52s
55: learn: 0.3318186    total: 5.66s    remaining: 2.42s
56: learn: 0.3287766    total: 5.75s    remaining: 2.32s
57: learn: 0.3268753    total: 5.84s    remaining: 2.21s
58: learn: 0.3251920    total: 5.93s    remaining: 2.11s
59: learn: 0.3227292    total: 6.03s    remaining: 2.01s
60: learn: 0.3213152    total: 6.13s    remaining: 1.91s
61: learn: 0.3192716    total: 6.22s    remaining: 1.81s
62: learn: 0.3183261    total: 6.32s    remaining: 1.71s
63: learn: 0.3171622    total: 6.42s    remaining: 1.6s
64: learn: 0.3159563    total: 6.52s    remaining: 1.5s
65: learn: 0.3143367    total: 6.61s    remaining: 1.4s
66: learn: 0.3133019    total: 6.7s remaining: 1.3s
67: learn: 0.3122973    total: 6.8s remaining: 1.2s
68: learn: 0.3103480    total: 6.89s    remaining: 1.1s
69: learn: 0.3093429    total: 7.01s    remaining: 1s
70: learn: 0.3081943    total: 7.12s    remaining: 902ms
71: learn: 0.3067472    total: 7.22s    remaining: 802ms
72: learn: 0.3058445    total: 7.33s    remaining: 703ms
73: learn: 0.3047133    total: 7.43s    remaining: 603ms
74: learn: 0.3035925    total: 7.54s    remaining: 502ms
75: learn: 0.3022633    total: 7.63s    remaining: 402ms
76: learn: 0.3011552    total: 7.72s    remaining: 301ms
77: learn: 0.2998250    total: 7.82s    remaining: 200ms
78: learn: 0.2985347    total: 7.91s    remaining: 100ms
79: learn: 0.2977599    total: 8.01s    remaining: 0us
&amp;lt;catboost.core.CatBoostClassifier object at 0x0000017E06ACAD60&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We have 38 variables. We can use all of them, but we can also
further reduce them for to look for the most relevant variables to the
model. We can trim the variable with recursive feature elimination
(RFE), which is a feature selection method that fits the model and
remove the weakest feature (or predictor) iteratively until the optimal
number of features is found (&lt;a
href="https://link.springer.com/content/pdf/10.1023/A:1012487302797.pdf"&gt;Guyon
et al., 2022&lt;/a&gt;). Note that this process is entirely data-driven,
meaning that the machine decides which variable solely based on the
data, not the theory. In this post, I use a variant of RFE called RFE
with cross validation (RFECV) that selects the best subset of features
based on the cross-validation score of the model. RFECV is a
bit&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I have computed RFECV in advance to save time. Below is the
result. Performance of the model jumped at 20 features and fluctuated
after that, meaning that the optimal number of features is 20.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;rfecv_model = RFECV(estimator=CBC_tuned, step=1, cv=5 ,scoring=&amp;#39;accuracy&amp;#39;)
rfecv = rfecv_model.fit(X_train_hybrid_ext, y_train_hybrid_ext)

print(&amp;#39;Optimal number of features :&amp;#39;, rfecv.n_features_)
print(&amp;#39;Best features :&amp;#39;, X_train_hybrid_ext.columns[rfecv.support_])
print(&amp;#39;Original features :&amp;#39;, X_train_hybrid_ext.columns)

plt.figure(figsize=(10, 15), dpi=800)
plt.xlabel(&amp;quot;Number of features selected&amp;quot;)
plt.ylabel(&amp;quot;Cross validation score \n of number of selected features&amp;quot;)
plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2022-08-06-edmrfecv.PNG" style="width:50.0%" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The optimal set of features are &lt;code&gt;X1MOMEDU&lt;/code&gt;,
&lt;code&gt;X1DADEDU&lt;/code&gt;, &lt;code&gt;X1MTHEFF&lt;/code&gt;, &lt;code&gt;X1SCIUTI&lt;/code&gt;,
&lt;code&gt;X1SCIEFF&lt;/code&gt;, &lt;code&gt;X1SCHOOLBEL&lt;/code&gt;,
&lt;code&gt;X1SCHOOLENG&lt;/code&gt;, &lt;code&gt;X1STUEDEXPCT&lt;/code&gt;,
&lt;code&gt;X1SCHOOLCLI&lt;/code&gt;, &lt;code&gt;X1COUPERCOU&lt;/code&gt;,
&lt;code&gt;X1COUPERPRI&lt;/code&gt;, &lt;code&gt;X3TGPA9TH&lt;/code&gt;, &lt;code&gt;S1NOHWDN&lt;/code&gt;,
&lt;code&gt;S1NOPAPER&lt;/code&gt;, &lt;code&gt;S1GETINTOCLG&lt;/code&gt;,
&lt;code&gt;S1WORKING&lt;/code&gt;, &lt;code&gt;S1HRMHOMEWK&lt;/code&gt;,
&lt;code&gt;S1HRSHOMEWK&lt;/code&gt;, &lt;code&gt;S1HROTHHOMWK&lt;/code&gt;,
&lt;code&gt;X4PSENRSTLV&lt;/code&gt;. I will reduce the number of variable based on
the RFECV result and create a training and a testing data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;X_hybrid_extreme_trim = X_hybrid_extreme[[&amp;#39;X1MOMEDU&amp;#39;, &amp;#39;X1DADEDU&amp;#39;, &amp;#39;X1MTHEFF&amp;#39;, &amp;#39;X1SCIUTI&amp;#39;, &amp;#39;X1SCIEFF&amp;#39;,&amp;#39;X1SCHOOLBEL&amp;#39;, &amp;#39;X1SCHOOLENG&amp;#39;, &amp;#39;X1STUEDEXPCT&amp;#39;, &amp;#39;X1SCHOOLCLI&amp;#39;,
&amp;#39;X1COUPERCOU&amp;#39;, &amp;#39;X1COUPERPRI&amp;#39;, &amp;#39;X3TGPA9TH&amp;#39;, &amp;#39;S1NOHWDN&amp;#39;, &amp;#39;S1NOPAPER&amp;#39;,
&amp;#39;S1GETINTOCLG&amp;#39;, &amp;#39;S1WORKING&amp;#39;, &amp;#39;S1HRMHOMEWK&amp;#39;, &amp;#39;S1HRSHOMEWK&amp;#39;, &amp;#39;S1HROTHHOMWK&amp;#39;, &amp;#39;X4PSENRSTLV&amp;#39;]]

X_train_hybrid_ext, X_test_hybrid_ext, y_train_hybrid_ext, y_test_hybrid_ext = train_test_split(X_hybrid_extreme_trim, y_hybrid_extreme, test_size = 0.30, random_state = RANDOM_STATE)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Then, I will fit the CatBoost model I created earlier with this new
data set and use it to predict the testing data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;cat_features_post_trim = [0, 1, 7, 12, 13, 14, 15,16, 17, 19]

CBC_tuned.fit(X_train_hybrid_ext, y_train_hybrid_ext, cat_features = cat_features_post_trim)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0:  learn: 0.6630968    total: 60.7ms   remaining: 4.8s
1:  learn: 0.6347421    total: 128ms    remaining: 5s
2:  learn: 0.6135360    total: 194ms    remaining: 4.98s
3:  learn: 0.5958557    total: 263ms    remaining: 5s
4:  learn: 0.5789939    total: 333ms    remaining: 4.99s
5:  learn: 0.5568907    total: 411ms    remaining: 5.07s
6:  learn: 0.5378680    total: 489ms    remaining: 5.1s
7:  learn: 0.5226018    total: 562ms    remaining: 5.06s
8:  learn: 0.5087498    total: 632ms    remaining: 4.99s
9:  learn: 0.5009270    total: 711ms    remaining: 4.98s
10: learn: 0.4841922    total: 784ms    remaining: 4.92s
11: learn: 0.4746953    total: 858ms    remaining: 4.86s
12: learn: 0.4681778    total: 932ms    remaining: 4.8s
13: learn: 0.4612541    total: 1s   remaining: 4.74s
14: learn: 0.4532992    total: 1.08s    remaining: 4.68s
15: learn: 0.4450863    total: 1.18s    remaining: 4.72s
16: learn: 0.4375271    total: 1.26s    remaining: 4.67s
17: learn: 0.4308885    total: 1.35s    remaining: 4.67s
18: learn: 0.4254128    total: 1.46s    remaining: 4.68s
19: learn: 0.4213425    total: 1.53s    remaining: 4.6s
20: learn: 0.4177958    total: 1.61s    remaining: 4.53s
21: learn: 0.4131347    total: 1.69s    remaining: 4.45s
22: learn: 0.4089653    total: 1.77s    remaining: 4.38s
23: learn: 0.4055672    total: 1.84s    remaining: 4.3s
24: learn: 0.4030118    total: 1.94s    remaining: 4.27s
25: learn: 0.3997708    total: 2.02s    remaining: 4.18s
26: learn: 0.3945504    total: 2.09s    remaining: 4.11s
27: learn: 0.3909571    total: 2.17s    remaining: 4.03s
28: learn: 0.3874681    total: 2.25s    remaining: 3.96s
29: learn: 0.3842159    total: 2.33s    remaining: 3.88s
30: learn: 0.3817295    total: 2.41s    remaining: 3.81s
31: learn: 0.3793464    total: 2.49s    remaining: 3.73s
32: learn: 0.3760956    total: 2.58s    remaining: 3.68s
33: learn: 0.3744325    total: 2.67s    remaining: 3.61s
34: learn: 0.3711698    total: 2.74s    remaining: 3.53s
35: learn: 0.3690638    total: 2.82s    remaining: 3.45s
36: learn: 0.3659461    total: 2.9s remaining: 3.37s
37: learn: 0.3637044    total: 2.97s    remaining: 3.28s
38: learn: 0.3621688    total: 3.05s    remaining: 3.2s
39: learn: 0.3601269    total: 3.12s    remaining: 3.12s
40: learn: 0.3580019    total: 3.2s remaining: 3.04s
41: learn: 0.3567636    total: 3.3s remaining: 2.99s
42: learn: 0.3544761    total: 3.39s    remaining: 2.92s
43: learn: 0.3505973    total: 3.47s    remaining: 2.84s
44: learn: 0.3495149    total: 3.54s    remaining: 2.76s
45: learn: 0.3476525    total: 3.62s    remaining: 2.67s
46: learn: 0.3462577    total: 3.69s    remaining: 2.59s
47: learn: 0.3445400    total: 3.76s    remaining: 2.51s
48: learn: 0.3427600    total: 3.84s    remaining: 2.43s
49: learn: 0.3408402    total: 3.91s    remaining: 2.35s
50: learn: 0.3381710    total: 3.99s    remaining: 2.27s
51: learn: 0.3364197    total: 4.06s    remaining: 2.19s
52: learn: 0.3339518    total: 4.14s    remaining: 2.11s
53: learn: 0.3311018    total: 4.24s    remaining: 2.04s
54: learn: 0.3294152    total: 4.33s    remaining: 1.97s
55: learn: 0.3280687    total: 4.42s    remaining: 1.9s
56: learn: 0.3269127    total: 4.5s remaining: 1.81s
57: learn: 0.3252681    total: 4.57s    remaining: 1.73s
58: learn: 0.3243089    total: 4.65s    remaining: 1.65s
59: learn: 0.3212165    total: 4.72s    remaining: 1.57s
60: learn: 0.3201323    total: 4.81s    remaining: 1.5s
61: learn: 0.3189120    total: 4.88s    remaining: 1.42s
62: learn: 0.3178599    total: 4.96s    remaining: 1.34s
63: learn: 0.3169985    total: 5.03s    remaining: 1.26s
64: learn: 0.3154376    total: 5.14s    remaining: 1.19s
65: learn: 0.3142858    total: 5.21s    remaining: 1.1s
66: learn: 0.3130732    total: 5.29s    remaining: 1.03s
67: learn: 0.3115498    total: 5.37s    remaining: 948ms
68: learn: 0.3106751    total: 5.45s    remaining: 868ms
69: learn: 0.3104327    total: 5.49s    remaining: 784ms
70: learn: 0.3094918    total: 5.54s    remaining: 703ms
71: learn: 0.3084260    total: 5.62s    remaining: 624ms
72: learn: 0.3070031    total: 5.69s    remaining: 546ms
73: learn: 0.3052639    total: 5.76s    remaining: 468ms
74: learn: 0.3043254    total: 5.84s    remaining: 389ms
75: learn: 0.3038159    total: 5.88s    remaining: 310ms
76: learn: 0.3029093    total: 5.95s    remaining: 232ms
77: learn: 0.3014591    total: 6.06s    remaining: 155ms
78: learn: 0.2992314    total: 6.13s    remaining: 77.6ms
79: learn: 0.2984094    total: 6.21s    remaining: 0us
&amp;lt;catboost.core.CatBoostClassifier object at 0x0000017E06ACAD60&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Results from classification report are satisfactory as seen from the
macro average of precision, recall, and f1-score. I also show the
receiver operating characteristic curve below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;from sklearn.metrics import roc_auc_score
from sklearn.metrics import classification_report

pred_ext = CBC_tuned.predict(X_test_hybrid_ext)

print(classification_report(y_test_hybrid_ext, pred_ext))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;              precision    recall  f1-score   support

           0       0.86      0.92      0.88      3351
           1       0.91      0.85      0.88      3433

    accuracy                           0.88      6784
   macro avg       0.88      0.88      0.88      6784
weighted avg       0.88      0.88      0.88      6784&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;roc_auc_score(y_test_hybrid_ext, pred_ext)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.8827700805886101&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;from sklearn import metrics

y_pred_proba_cat = CBC_tuned.predict_proba(X_test_hybrid_ext)[::,1]
fpr_cat, tpr_cat, _ = metrics.roc_curve(y_test_hybrid_ext,  y_pred_proba_cat)

auc_cat = metrics.roc_auc_score(y_test_hybrid_ext, y_pred_proba_cat)

#create ROC curve
plt.plot(fpr_cat,tpr_cat, label=&amp;quot;ROC_AUC=&amp;quot;+str(auc_cat.round(3)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;lt;matplotlib.lines.Line2D object at 0x0000017E0748CA30&amp;gt;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.legend(loc=&amp;quot;lower right&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.legend.Legend object at 0x0000017E0748CA00&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;True Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;True Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.xlabel(&amp;#39;False Positive Rate&amp;#39;)

# displaying the title&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;False Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Area Under Curve&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Area Under Curve&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file349051826594_files/figure-html/unnamed-chunk-23-9.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I also visualize feature importance of the model below. The most
impactful predictor to students’ high school dropout is their last year
GPA, followed by hours spent doing homework on typical school days, and
their self-efficacy in mathematics.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;from matplotlib.pyplot import figure

importances_cat = pd.Series(CBC_tuned.feature_importances_, index = X_hybrid_extreme_trim.columns)

sorted_importance_cat = importances_cat.sort_values()

#Horizontal bar plot
sorted_importance_cat.plot(kind=&amp;#39;barh&amp;#39;, color=&amp;#39;lightgreen&amp;#39;); 
plt.xlabel(&amp;#39;Feature Importance Score&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;Feature Importance Score&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;Features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;Features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Visualizing Important Features&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Visualizing Important Features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.rcParams[&amp;quot;figure.figsize&amp;quot;] = (8, 4)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file349051826594_files/figure-html/unnamed-chunk-24-11.png" width="1152" data-distill-preview=1 /&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The point of this post is to demonstrate how EDM can be used with
large-scale educational data to derive insights and potentially apply it
to practice. We started out with a lot of variables (4014), then we
reduce it based on the relevant theory to 67, based on missing data to
51, based on correlation coefficient to 38, and based on RFECV to
20.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We might want to select variables that are actionable for the
model to be meaningful. For example, saying that a student is likely to
dropout of their high school because of their socio-economic status
might not be as helpful because you cannot change their family income in
a matter of days or months. However, saying that their GPA and hours
spent on home work are influencing factors might allow students to
adjust their learning behavior.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With a meaningful model, an early warning system can be developed
to alert teachers of potential under-performing students for an early
intervention. However, I do not mean that results from the model is
perfect. It should be used in conjunction with other indicators such as
student record, parents’ observation, and behavior note. As education
goes online or semi-online, records of student data can be leveraged to
better understand them and ultimately benefit the teaching
practice.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>53861e41352b60b38bf303d190c7b01f</distill:md5>
      <category>R</category>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-08-06-edm</guid>
      <pubDate>Sat, 06 Aug 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-08-06-edm/edm_files/figure-html5/unnamed-chunk-24-11.png" medium="image" type="image/png" width="2304" height="1536"/>
    </item>
    <item>
      <title>Item Response Theory</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-05-15-irt</link>
      <description>


&lt;h2 id="introduction-to-item-response-theory"&gt;Introduction to Item
Response Theory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://taridwong.github.io/posts/2022-01-15-ctt/"&gt;My
previous post on Classical Test Theory (CTT)&lt;/a&gt; discussed how it has
several disadvantages that limit its interpretation to a certain group
of population and therefore reduces its utility to test development.
Specifically, generalizability of the test scores from CTT is quite
limited due to item/test dependency; item parameters such as item
difficulty, item discrimination, and reliability estimates are dependent
upon test scores, which are derived from a group of sample (&lt;a
href="https://www.jstor.org/stable/41219505?casa_token=PZl4ti6FHboAAAAA%3AEsuXjTVftCbh7pnyqQVhGv5cCEG9nAWMNV36MC7nXp5svyUaJhXBQG6jYtcmYNNf3S3b7HkhG48gjFTDo_ftMvXvm6vwRAuQwiEretfpuoKpwFiUQw&amp;amp;seq=1"&gt;DeVellis,
2006&lt;/a&gt;). If we change our sample, those parameters might change. Also,
If we administer two forms of the same test (i.e., form A and form B) to
the same examinee, we still cannot guarantee that they will obtain the
same score on both tests. Raw scores of a CTT-based test do not reflect
learning progress of an examinee as CTT-based scores are not comparable
across time (&lt;a
href="https://www.jstor.org/stable/41219505?casa_token=PZl4ti6FHboAAAAA%3AEsuXjTVftCbh7pnyqQVhGv5cCEG9nAWMNV36MC7nXp5svyUaJhXBQG6jYtcmYNNf3S3b7HkhG48gjFTDo_ftMvXvm6vwRAuQwiEretfpuoKpwFiUQw&amp;amp;seq=1"&gt;DeVellis,
2006&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Such limitations can be addressed by Item Response Theory (IRT).
IRT is able to link information from test items to examinee performance
on the same scale to provide information on the specific domain of
interest and ability of the examinee (θ) (&lt;a
href="https://www.routledge.com/Handbook-of-Item-Response-Theory-Volume-3-Applications/Linden/p/book/9780367221188"&gt;Hambleton
&amp;amp; Zenisky, 2018&lt;/a&gt;). The relationship between observable items and
examinee performance is explained through &lt;em&gt;Item Characteristic Curve
(ICC)&lt;/em&gt;, which explains the probability of getting an item(s)
correctly given the current ability level and parameter (&lt;a
href="https://doi.org/10.1111/j.1745-3992.1993.tb00543.x"&gt;Hambleton
&amp;amp; Jones, 1993&lt;/a&gt;). Therefore, IRT allows researchers to predict
examinees’ expected test score given their ability level. In this post,
I will be examining characteristics of test items based on the IRT
framework. The R packages I will be using are &lt;a
href="https://cran.r-project.org/web/packages/ltm/ltm.pdf"&gt;&lt;code&gt;ltm&lt;/code&gt;&lt;/a&gt;
and &lt;a
href="https://cran.r-project.org/web/packages/mirt/mirt.pdf"&gt;&lt;code&gt;mirt&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(ltm)
library(mirt)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;First, let’s load in a data set. I will be using the data from Law
School Admission Test (LSAT), N = 1000, 5 items. The data can be called
with &lt;code&gt;data(LSAT).&lt;/code&gt;As an initial step, we can use
&lt;code&gt;ltm::descript&lt;/code&gt; for descriptive statistics.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;data(LSAT)
ltm::descript(LSAT)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Descriptive statistics for the &amp;#39;LSAT&amp;#39; data-set

Sample:
 5 items and 1000 sample units; 0 missing values

Proportions for each level of response:
           0     1  logit
Item 1 0.076 0.924 2.4980
Item 2 0.291 0.709 0.8905
Item 3 0.447 0.553 0.2128
Item 4 0.237 0.763 1.1692
Item 5 0.130 0.870 1.9010


Frequencies of total scores:
     0  1  2   3   4   5
Freq 3 20 85 237 357 298


Point Biserial correlation with Total Score:
       Included Excluded
Item 1   0.3620   0.1128
Item 2   0.5668   0.1532
Item 3   0.6184   0.1728
Item 4   0.5344   0.1444
Item 5   0.4354   0.1216


Cronbach&amp;#39;s alpha:
                  value
All Items        0.2950
Excluding Item 1 0.2754
Excluding Item 2 0.2376
Excluding Item 3 0.2168
Excluding Item 4 0.2459
Excluding Item 5 0.2663


Pairwise Associations:
   Item i Item j p.value
1       1      5   0.565
2       1      4   0.208
3       3      5   0.113
4       2      4   0.059
5       1      2   0.028
6       2      5   0.009
7       1      3   0.003
8       4      5   0.002
9       3      4   7e-04
10      2      3   4e-04&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;From the output above, inspection of non significant results can be
used to reveal ‘problematic’ items in pairwise association. Latent
variable models assume that the high associations between items can be
explained by a set of latent variables, so any pair of items that is not
related to each other violates this assumption. Additionally, Item 1
seems to be the easiest item as seen from its highest proportion of
correct response.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="dichotomous-item"&gt;Dichotomous Item&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We will be performing IRT analyses on dichotomous items, which are
items that only have two possible scores of incorrect (0) and correct
(1). The three most common dichotomous IRT models are Rasch/1-parameter
logistics model (1PL), 2-parameter logistics model (2PL), and
3-parameter logistics model(3PL).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="rasch-model-1pl"&gt;Rasch Model (1PL)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We will fit the original Rasch model, which fixes the item
discrimination (aka &lt;em&gt;a&lt;/em&gt; parameter) of all items to 1 to the data.
The 1PL (also called &lt;em&gt;Rasch model&lt;/em&gt;) model describes test items in
terms of only one parameter, &lt;em&gt;item difficulty&lt;/em&gt; (aka &lt;em&gt;b&lt;/em&gt;
parameter). Item difficulty is simply how hard an item is (how high does
one’s latent ability level need to be in order to have a 50% chance of
getting the item right?). &lt;em&gt;b-parameter&lt;/em&gt; is estimated for each
item of the test.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;ltm::rasch()&lt;/code&gt; assumes equal a-parameter across
items with an estimated value. In order to impose the constraint = 1,
the &lt;code&gt;constraint&lt;/code&gt; argument is used. This argument accepts a
two-column matrix where the first column denotes the parameter and the
second column indicates the value at which the corresponding parameter
should be fixed.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_rasch &amp;lt;- rasch(LSAT, constraint = cbind(length(LSAT) + 1, 1))

summary(mod_rasch)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = cbind(length(LSAT) + 1, 1))

Model Summary:
   log.Lik      AIC      BIC
 -2473.054 4956.108 4980.646

Coefficients:
                value std.err   z.vals
Dffclt.Item 1 -2.8720  0.1287 -22.3066
Dffclt.Item 2 -1.0630  0.0821 -12.9458
Dffclt.Item 3 -0.2576  0.0766  -3.3635
Dffclt.Item 4 -1.3881  0.0865 -16.0478
Dffclt.Item 5 -2.2188  0.1048 -21.1660
Dscrmn         1.0000      NA       NA

Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 6.3e-05 
quasi-Newton: BFGS &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The results of the descriptive analysis are also validated by the
model fit, where items 3 and 1 are the most difficult and the easiest
respectively (the lower the &lt;em&gt;b&lt;/em&gt;-parameter value, the easier). The
parameter estimates can be transformed to probability estimates using
the &lt;code&gt;coef()&lt;/code&gt; method&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;coef(mod_rasch, prob = TRUE, order = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           Dffclt Dscrmn P(x=1|z=0)
Item 1 -2.8719712      1  0.9464434
Item 5 -2.2187785      1  0.9019232
Item 4 -1.3880588      1  0.8002822
Item 2 -1.0630294      1  0.7432690
Item 3 -0.2576109      1  0.5640489&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The last column denotes the probability of a positive response
(getting the item correctly) to the &lt;em&gt;i&lt;/em&gt;th item for the average
individual. The argument &lt;code&gt;order = TRUE&lt;/code&gt; indicates the output
to sort the items according to the difficulty estimates. In order to
check the fit of the model to the data, the argument
&lt;code&gt;GoF.rasch()&lt;/code&gt; and &lt;code&gt;margins()&lt;/code&gt; can be used. The
former argument performs a parametric Bootstrap goodness-of-fit test
using Pearson’s Chi-square statistics, while the latter examines the
two- and three-way chi-square residual analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;GoF.rasch(mod_rasch, B = 199) # B = Bootstrap sample&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Bootstrap Goodness-of-Fit using Pearson chi-squared

Call:
rasch(data = LSAT, constraint = cbind(length(LSAT) + 1, 1))

Tobs: 30.6 
# data-sets: 200 
p-value: 0.265 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Based on 200 data-points, the non significant p-value suggests an
acceptable fit of the model. The null hypothesis states that the
observed data and the model fit with each other (no differences). Now,
for two-way margin analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_rasch)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = cbind(length(LSAT) + 1, 1))

Fit on the Two-Way Margins

Response: (0,0)
  Item i Item j Obs   Exp (O-E)^2/E  
1      2      4  81 98.69      3.17  
2      1      5  12 18.45      2.25  
3      3      5  67 80.04      2.12  

Response: (1,0)
  Item i Item j Obs    Exp (O-E)^2/E  
1      3      5  63  51.62      2.51  
2      2      4 156 139.78      1.88  
3      3      4 108  99.42      0.74  

Response: (0,1)
  Item i Item j Obs    Exp (O-E)^2/E  
1      2      4 210 193.47      1.41  
2      2      3 135 125.07      0.79  
3      1      4  53  47.24      0.70  

Response: (1,1)
  Item i Item j Obs    Exp (O-E)^2/E  
1      2      4 553 568.06      0.40  
2      3      5 490 501.43      0.26  
3      2      3 418 427.98      0.23  &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;From the above output, using the 3.5 rule of thumb, the value of all
two-way combinations are below the cut-off (same way of how statistical
hypothesis works) and therefore indicate a good fit to the two-way
margins. Next, we will examine the fit to the three-way margins.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_rasch, type = &amp;quot;three-way&amp;quot;, nprint = 2) #nprint returns 2 highest residual values for each combinations&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = cbind(length(LSAT) + 1, 1))

Fit on the Three-Way Margins

Response: (0,0,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E    
1      2      3      4  48 66.07      4.94 ***
2      1      3      5   6 13.58      4.23 ***

Response: (1,0,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      2      4  70 82.01      1.76  
2      2      4      5  28 22.75      1.21  

Response: (0,1,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      2      5   3  7.73      2.90  
2      3      4      5  37 45.58      1.61  

Response: (1,1,0)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      3      4      5  48  36.91      3.33  
2      1      2      4 144 126.35      2.47  

Response: (0,0,1)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      3      5  41 34.58      1.19  
2      2      4      5  64 72.26      0.94  

Response: (1,0,1)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      1      2      4 190 174.87      1.31  
2      1      2      3 126 114.66      1.12  

Response: (0,1,1)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      2      5  42 34.35      1.70  
2      1      4      5  46 38.23      1.58  

Response: (1,1,1)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      3      4      5 397 416.73      0.93  
2      2      3      4 343 361.18      0.91  

&amp;#39;***&amp;#39; denotes a chi-squared residual greater than 3.5 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The three-way margins suggest a problematic fit for two triplets of
items, both containing item 3. We can try fitting the unconstrained
version of Rasch model (not fixing the &lt;em&gt;a&lt;/em&gt;-parameter to 1) to see
the difference. This time, no need for the &lt;code&gt;constraint&lt;/code&gt;
argument.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_1pl &amp;lt;- rasch(LSAT, constraint = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;After fitting the 1PL model, we will request for Item
Characteristics Curve (ICC), Item Information Curve (IIC), Test
Information Function (TIF), Latent Ability Curve of the examinees, and
Uni-dimensionality Plot of the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mfrow = c(2, 3))

plot(mod_1pl, type=c(&amp;quot;ICC&amp;quot;), 
     legend = TRUE, cx = &amp;quot;bottomright&amp;quot;, lwd = 2, 
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;1PL Item Characteristics Curve&amp;quot;)

plot(mod_1pl,type=c(&amp;quot;IIC&amp;quot;),
     legend = TRUE, cx = &amp;quot;topright&amp;quot;, lwd = 2,
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;1PL Item Information Curve&amp;quot;)

plot(mod_1pl,type=c(&amp;quot;IIC&amp;quot;),items=c(0), lwd = 2,
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;Test Information Function&amp;quot;)

plot(0:1, 0:1, type = &amp;quot;n&amp;quot;, ann = FALSE, axes = FALSE)
info1 &amp;lt;- information(mod_1pl, c(-4, 0)) #Item information on the negative area of theta (-4 to 0)
info2 &amp;lt;- information(mod_1pl, c(0, 4)) #Item information on the positive area of theta (0 to 4)
text(0.5, 0.5, labels = paste(&amp;quot;Total Information:&amp;quot;, round(info1$InfoTotal, 3),
                               &amp;quot;\n\nInformation in (-4, 0):&amp;quot;, round(info1$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info1$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;),
                               &amp;quot;\n\nInformation in (0, 4):&amp;quot;, round(info2$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info2$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;)))

theta.rasch&amp;lt;-ltm::factor.scores(mod_1pl)
summary(theta.rasch$score.dat$z1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-1.9104 -0.9594 -0.4660 -0.6867 -0.4660  0.5930 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(theta.rasch, main = &amp;quot;Latent Ability of the Examinee&amp;quot;)

unitest_1pl &amp;lt;- unidimTest(mod_1pl,LSAT)
plot(unitest_1pl, type = &amp;quot;b&amp;quot;, pch = 1:2, main = &amp;quot;Modified Parallel Analysis Plot&amp;quot;)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Real Data&amp;quot;, &amp;quot;Average Simulated Data&amp;quot;), lty = 1, 
    pch = 1:2, col = 1:2, bty = &amp;quot;n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file349070062f97_files/figure-html/unnamed-chunk-11-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The first plot is ICC of out 1PL model. ICC shows the
relationship between examinee ability (θ) and the probability of
examinees answering an item correctly based on their ability. On ICC,
item discrimination is represented by the steepness of the curve, and
item difficulty is represented by the position where the probability of
getting the item correct is 0.5.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Try comparing item 1 and item 3. For item 1, the point where the
probability of getting the item correctly is 0.5 matches with the
ability point -4 on the X-axis. However, for item 3, the point where the
probability of getting the item correctly is 0.5 matches with the
ability point 0 on the X-axis. In other words, you need more ability to
get item 3 correct than item 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The second plot is IIC. IIC shows how much “information” about
the latent trait ability an item can provide. Item information curves
peak at the point of difficulty value, where the item has the highest
discrimination and the probability of answering the item correctly is
0.5. To put it in plain language, a very difficult item will provide
very little information about persons with low ability (because the item
is already too hard), and very easy items will provide little
information about persons with high ability levels (because it is too
easy).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The third plot is TIF of the whole test. This is simply the sum
of the individual IICs above. The curves shows how much information this
test offers in terms of ability level of examinees. Ideally, we want a
test which provides fairly good coverage of a wide range of latent
ability levels. Otherwise, the test is only good at identifying a
limited range of examinees. The current TIF shows that more information
is yielded around examinees with -2 ability level. The test could use
more items for people with high ability (more difficult item is needed).
In particular, the amount of Test Information for ability levels in the
interval (-4 - 0) is almost 60%, and the item that seems to distinguish
between respondents with higher ability levels is item 3 as it is the
most difficult item.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Latent Ability Curve shows the distribution of examinee’s
latent ability level. The plot shows that most examinees are located
around 0 to 1 ability level.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;unitest_1pl&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Unidimensionality Check using Modified Parallel Analysis

Call:
rasch(data = LSAT, constraint = NULL)

Matrix of tertachoric correlations
       Item 1 Item 2 Item 3 Item 4 Item 5
Item 1 1.0000 0.1703 0.2275 0.1072 0.0665
Item 2 0.1703 1.0000 0.1891 0.1111 0.1724
Item 3 0.2275 0.1891 1.0000 0.1867 0.1055
Item 4 0.1072 0.1111 0.1867 1.0000 0.2009
Item 5 0.0665 0.1724 0.1055 0.2009 1.0000

Alternative hypothesis: the second eigenvalue of the observed data is substantially larger 
            than the second eigenvalue of data under the assumed IRT model

Second eigenvalue in the observed data: 0.2254
Average of second eigenvalues in Monte Carlo samples: 0.2541
Monte Carlo samples: 100
p-value: 0.5941&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The last plot is a test for unidimensionality of the test with
modified parallel analysis &lt;a
href="https://psycnet.apa.org/record/1983-31736-001"&gt;(Drasgpw &amp;amp;
Lissak, 1983&lt;/a&gt;). The output above shows that the result is
non-significant, meaning that the 1PL model fits the data well and we
are actually measuring a single trait here. The data is a law school
test, so it should be measuring contents about law, not maths or
English. The unidimensionality analysis shows that the test is measuring
what it is intended to measure.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;summary(mod_1pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = NULL)

Model Summary:
   log.Lik      AIC      BIC
 -2466.938 4945.875 4975.322

Coefficients:
                value std.err   z.vals
Dffclt.Item 1 -3.6153  0.3266 -11.0680
Dffclt.Item 2 -1.3224  0.1422  -9.3009
Dffclt.Item 3 -0.3176  0.0977  -3.2518
Dffclt.Item 4 -1.7301  0.1691 -10.2290
Dffclt.Item 5 -2.7802  0.2510 -11.0743
Dscrmn         0.7551  0.0694  10.8757

Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 2.9e-05 
quasi-Newton: BFGS &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;coef(mod_1pl, prob = TRUE, order = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           Dffclt    Dscrmn P(x=1|z=0)
Item 1 -3.6152665 0.7551347  0.9387746
Item 5 -2.7801716 0.7551347  0.8908453
Item 4 -1.7300903 0.7551347  0.7869187
Item 2 -1.3224208 0.7551347  0.7307844
Item 3 -0.3176306 0.7551347  0.5596777&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The output above suggests that the discrimination parameter of our
unconstrained model is different from 1, meaning that our constrained
and unconstrained Rasch models are different. The difference can be
tested with a likelihood ratio test using &lt;code&gt;anova().&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;anova(mod_rasch, mod_1pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 Likelihood Ratio Table
              AIC     BIC  log.Lik   LRT df p.value
mod_rasch 4956.11 4980.65 -2473.05                 
mod_1pl   4945.88 4975.32 -2466.94 12.23  1  &amp;lt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;By comparing model summary of the constrained and unconstrained
version, the latter is more suitable for the LSAT data due to its
smaller Akaike’s Information Criterion (AIC) and Bayesian Information
Criterion (BIC) values. AIC and BIC are measures of model performance
that account for model complexity. AIC is a measure that determines
which model fits the data better. The lower the score, the better fit
the model is. Similarly for BIC, the score measures complexity of the
model. BIC penalizes the model more for its complexity, meaning that
more complex models will have a worse (larger) score and will, in turn,
be less likely to be selected.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can double check the result by testing the unconstrained model
with the three-way margins, which yields a problematic fit with the
constrained model.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_1pl, type = &amp;quot;three-way&amp;quot;, nprint = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = NULL)

Fit on the Three-Way Margins

Response: (0,0,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      3      5   6  9.40      1.23  
2      3      4      5  30 25.85      0.67  

Response: (1,0,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      2      4      5  28 22.75      1.21  
2      2      3      4  81 74.44      0.58  

Response: (0,1,0)
  Item i Item j Item k Obs  Exp (O-E)^2/E  
1      1      2      5   3 7.58      2.76  
2      1      3      4   5 9.21      1.92  

Response: (1,1,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      2      4      5  51 57.49      0.73  
2      3      4      5  48 42.75      0.64  

Response: (0,0,1)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      1      3      5  41  33.07      1.90  
2      2      3      4 108 101.28      0.45  

Response: (1,0,1)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      2      3      4 210 218.91      0.36  
2      1      2      4 190 185.56      0.11  

Response: (0,1,1)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      3      5  23 28.38      1.02  
2      1      4      5  46 42.51      0.29  

Response: (1,1,1)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      1      2      4 520 526.36      0.08  
2      1      2      3 398 393.30      0.06  &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The new three-way margins suggests a good fit with the unconstrained
Rasch model. Finally, we investigate two more possible extensions of the
unconstrained Rasch model, the two-parameter logistic (2PL) model that
assumes a different discrimination parameter per item, and Rasch model
that incorporates a guessing parameter (3PL).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="parameter-logistics-model-2pl"&gt;2 Parameter Logistics Model
(2PL)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The 2PL model has the same equation as the 1PL model, but unlike
1PL, 2PL allows item discrimination and item difficulty to vary across
items instead of fixing it to a constant value.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The 2PL model can also be fitted with &lt;code&gt;ltm()&lt;/code&gt;.The
formula of &lt;code&gt;ltm()&lt;/code&gt; is two-sided, where its left is either a
data frame or a matrix, and its right allows only &lt;code&gt;z1&lt;/code&gt; and/or
&lt;code&gt;z2&lt;/code&gt;. Latent variables with &lt;code&gt;z2&lt;/code&gt; serves in the
case of interaction.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_2pl &amp;lt;- ltm(LSAT ~ z1)
summary(mod_2pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
ltm(formula = LSAT ~ z1)

Model Summary:
   log.Lik      AIC      BIC
 -2466.653 4953.307 5002.384

Coefficients:
                value std.err  z.vals
Dffclt.Item 1 -3.3597  0.8669 -3.8754
Dffclt.Item 2 -1.3696  0.3073 -4.4565
Dffclt.Item 3 -0.2799  0.0997 -2.8083
Dffclt.Item 4 -1.8659  0.4341 -4.2982
Dffclt.Item 5 -3.1236  0.8700 -3.5904
Dscrmn.Item 1  0.8254  0.2581  3.1983
Dscrmn.Item 2  0.7229  0.1867  3.8721
Dscrmn.Item 3  0.8905  0.2326  3.8281
Dscrmn.Item 4  0.6886  0.1852  3.7186
Dscrmn.Item 5  0.6575  0.2100  3.1306

Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 0.024 
quasi-Newton: BFGS &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The output above shows that the model estimated both item difficulty
and item discrimination. Next, we can try comparing our 1PL model with
the newly fitted 2PL model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#compare
anova(mod_1pl, mod_2pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 Likelihood Ratio Table
            AIC     BIC  log.Lik  LRT df p.value
mod_1pl 4945.88 4975.32 -2466.94                
mod_2pl 4953.31 5002.38 -2466.65 0.57  4   0.967&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The comparison suggested no significant difference between both
models. Next, we ca nrequest for ICC, IIC, TIF, Latent Ability
Distribution, and Unidimensionality Plot like we did with the 1PL
model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mfrow = c(2, 3))

plot(mod_2pl, type=c(&amp;quot;ICC&amp;quot;), 
     legend = TRUE, cx = &amp;quot;bottomright&amp;quot;, lwd = 2, 
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;2PL Item Characteristics Curve&amp;quot;)

plot(mod_2pl,type=c(&amp;quot;IIC&amp;quot;),
     legend = TRUE, cx = &amp;quot;topright&amp;quot;, lwd = 2,
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;2PL Item Information Curve&amp;quot;)

plot(mod_2pl,type=c(&amp;quot;IIC&amp;quot;),items=c(0), lwd = 2,
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;Test Information Function&amp;quot;)

plot(0:1, 0:1, type = &amp;quot;n&amp;quot;, ann = FALSE, axes = FALSE)
info1_2pl &amp;lt;- information(mod_2pl, c(-4, 0)) #Item information on the negative area of theta (-4 to 0)
info2_2pl &amp;lt;- information(mod_2pl, c(0, 4)) #Item information on the positive area of theta (0 to 4)
text(0.5, 0.5, labels = paste(&amp;quot;Total Information:&amp;quot;, round(info1_2pl$InfoTotal, 3),
                               &amp;quot;\n\nInformation in (-4, 0):&amp;quot;, round(info1_2pl$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info1_2pl$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;),
                               &amp;quot;\n\nInformation in (0, 4):&amp;quot;, round(info2_2pl$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info2_2pl$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;)))

theta.2pl&amp;lt;-ltm::factor.scores(mod_2pl)
summary(theta.2pl$score.dat$z1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-1.8953 -1.0026 -0.5397 -0.6629 -0.3572  0.6064 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(theta.2pl, main = &amp;quot;Latent ability scores of the participants 2PL&amp;quot;)

unitest_2pl &amp;lt;- unidimTest(mod_2pl,LSAT)
plot(unitest_2pl, type = &amp;quot;b&amp;quot;, pch = 1:2)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Real Data&amp;quot;, &amp;quot;Average Simulated Data&amp;quot;), lty = 1, 
    pch = 1:2, col = 1:2, bty = &amp;quot;n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file349070062f97_files/figure-html/unnamed-chunk-18-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ICC and IIC of the two models are different, meaning that when we
allow item discrimination to vary, characteristics and yielded
information of each item also changed accordingly.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;unitest_2pl&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Unidimensionality Check using Modified Parallel Analysis

Call:
ltm(formula = LSAT ~ z1)

Matrix of tertachoric correlations
       Item 1 Item 2 Item 3 Item 4 Item 5
Item 1 1.0000 0.1703 0.2275 0.1072 0.0665
Item 2 0.1703 1.0000 0.1891 0.1111 0.1724
Item 3 0.2275 0.1891 1.0000 0.1867 0.1055
Item 4 0.1072 0.1111 0.1867 1.0000 0.2009
Item 5 0.0665 0.1724 0.1055 0.2009 1.0000

Alternative hypothesis: the second eigenvalue of the observed data is substantially larger 
            than the second eigenvalue of data under the assumed IRT model

Second eigenvalue in the observed data: 0.2254
Average of second eigenvalues in Monte Carlo samples: 0.251
Monte Carlo samples: 100
p-value: 0.6634&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The above results of unidimensionality testing also suggests that
the test measures only one construct. When considering two models
together, it might be more preferable for us to choose the 1PL model as
there is no difference between both 1PL and 2PL; however, by nature, 1PL
model is more simple and easier to explain comparing to 2PL. Let us try
fitting the data to a 3PL model just in case.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="parameter-logistics-model-3pl"&gt;3 Parameter Logistics Model
(3PL)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The 3PL model is very similar to the 2PL model; however, the model
includes an additional parameter: lower asymptote (also known as the
guessing parameter). Under this model, individuals with zero ability
have a nonzero chance of correctly answering any item just by guessing
randomly. A 3PL model can be fitted with &lt;code&gt;tpm()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_3pl &amp;lt;- tpm(LSAT, type = &amp;quot;latent.trait&amp;quot;, max.guessing = 1)

summary(mod_3pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
tpm(data = LSAT, type = &amp;quot;latent.trait&amp;quot;, max.guessing = 1)

Model Summary:
  log.Lik      AIC      BIC
 -2466.66 4963.319 5036.935

Coefficients:
                value std.err  z.vals
Gussng.Item 1  0.0374  0.8650  0.0432
Gussng.Item 2  0.0777  2.5282  0.0307
Gussng.Item 3  0.0118  0.2815  0.0419
Gussng.Item 4  0.0353  0.5769  0.0612
Gussng.Item 5  0.0532  1.5596  0.0341
Dffclt.Item 1 -3.2965  1.7788 -1.8532
Dffclt.Item 2 -1.1451  7.5166 -0.1523
Dffclt.Item 3 -0.2490  0.7527 -0.3308
Dffclt.Item 4 -1.7658  1.6162 -1.0925
Dffclt.Item 5 -2.9902  4.0606 -0.7364
Dscrmn.Item 1  0.8286  0.2877  2.8797
Dscrmn.Item 2  0.7604  1.3774  0.5520
Dscrmn.Item 3  0.9016  0.4190  2.1516
Dscrmn.Item 4  0.7007  0.2574  2.7219
Dscrmn.Item 5  0.6658  0.3282  2.0284

Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Optimizer: optim (BFGS)
Convergence: 0 
max(|grad|): 0.028 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The model summary above suggested that all three item parameters of
item discrimination (&lt;em&gt;a&lt;/em&gt;), item difficulty (&lt;em&gt;b&lt;/em&gt;), and item
guessing (&lt;em&gt;c&lt;/em&gt;) are allowed to vary. Like what we did, we can try
requesting for ICC, IIC, TIF, latent ability distribution, and
unidimensionality plot.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mfrow = c(2, 3))

plot(mod_3pl, type=c(&amp;quot;ICC&amp;quot;), 
     legend = TRUE, cx = &amp;quot;bottomright&amp;quot;, lwd = 2, 
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;3PL Item Characteristics Curve&amp;quot;)

plot(mod_3pl,type=c(&amp;quot;IIC&amp;quot;),
     legend = TRUE, cx = &amp;quot;topright&amp;quot;, lwd = 2,
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;3PL Item Information Curve&amp;quot;)

plot(mod_3pl,type=c(&amp;quot;IIC&amp;quot;),items=c(0))

plot(0:1, 0:1, type = &amp;quot;n&amp;quot;, ann = FALSE, axes = FALSE)
info1_3pl &amp;lt;- information(mod_3pl, c(-4, 0)) #Item information on the negative area of theta (-4 to 0)
info2_3pl &amp;lt;- information(mod_3pl, c(0, 4)) #Item information on the positive area of theta (0 to 4)
text(0.5, 0.5, labels = paste(&amp;quot;Total Information:&amp;quot;, round(info1_3pl$InfoTotal, 3),
                               &amp;quot;\n\nInformation in (-4, 0):&amp;quot;, round(info1_3pl$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info1_3pl$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;),
                               &amp;quot;\n\nInformation in (0, 4):&amp;quot;, round(info2_3pl$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info2_3pl$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;)))

theta.3pl&amp;lt;-ltm::factor.scores(mod_3pl)
summary(theta.3pl$score.dat$z1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-1.8706 -0.9992 -0.5368 -0.6584 -0.3590  0.6116 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(theta.3pl, main = &amp;quot;Latent ability scores of the participants 3PL&amp;quot;)

unitest_3pl &amp;lt;- unidimTest(mod_3pl,LSAT)
plot(unitest_3pl, type = &amp;quot;b&amp;quot;, pch = 1:2)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Real Data&amp;quot;, &amp;quot;Average Simulated Data&amp;quot;), lty = 1, 
    pch = 1:2, col = 1:2, bty = &amp;quot;n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file349070062f97_files/figure-html/unnamed-chunk-21-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;unitest_3pl&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Unidimensionality Check using Modified Parallel Analysis

Call:
tpm(data = LSAT, type = &amp;quot;latent.trait&amp;quot;, max.guessing = 1)

Matrix of tertachoric correlations
       Item 1 Item 2 Item 3 Item 4 Item 5
Item 1 1.0000 0.1703 0.2275 0.1072 0.0665
Item 2 0.1703 1.0000 0.1891 0.1111 0.1724
Item 3 0.2275 0.1891 1.0000 0.1867 0.1055
Item 4 0.1072 0.1111 0.1867 1.0000 0.2009
Item 5 0.0665 0.1724 0.1055 0.2009 1.0000

Alternative hypothesis: the second eigenvalue of the observed data is substantially larger 
            than the second eigenvalue of data under the assumed IRT model

Second eigenvalue in the observed data: 0.2254
Average of second eigenvalues in Monte Carlo samples: 0.2482
Monte Carlo samples: 100
p-value: 0.5743&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The unidimensionality testing above suggested non-significant
result, meaning that the test measures one construct as intended.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;anova(mod_1pl, mod_3pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 Likelihood Ratio Table
            AIC     BIC  log.Lik  LRT df p.value
mod_1pl 4945.88 4975.32 -2466.94                
mod_3pl 4963.32 5036.94 -2466.66 0.56  9       1&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The comparison between the 1PL and the 3PL model also suggests no
statistical differences. The 3PL model also has larger AIC and BIC;
therefore, it is more preferable for us to use 1PL model with this data.
Finally, we can request for ability estimates for all response pattern
with the &lt;code&gt;factor.scores()&lt;/code&gt; function.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;factor.scores(mod_1pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = NULL)

Scoring Method: Empirical Bayes

Factor-Scores for observed response patterns:
   Item 1 Item 2 Item 3 Item 4 Item 5 Obs     Exp     z1 se.z1
1       0      0      0      0      0   3   2.364 -1.910 0.790
2       0      0      0      0      1   6   5.468 -1.439 0.793
3       0      0      0      1      0   2   2.474 -1.439 0.793
4       0      0      0      1      1  11   8.249 -0.959 0.801
5       0      0      1      0      0   1   0.852 -1.439 0.793
6       0      0      1      0      1   1   2.839 -0.959 0.801
7       0      0      1      1      0   3   1.285 -0.959 0.801
8       0      0      1      1      1   4   6.222 -0.466 0.816
9       0      1      0      0      0   1   1.819 -1.439 0.793
10      0      1      0      0      1   8   6.063 -0.959 0.801
11      0      1      0      1      1  16  13.288 -0.466 0.816
12      0      1      1      0      1   3   4.574 -0.466 0.816
13      0      1      1      1      0   2   2.070 -0.466 0.816
14      0      1      1      1      1  15  14.749  0.049 0.836
15      1      0      0      0      0  10  10.273 -1.439 0.793
16      1      0      0      0      1  29  34.249 -0.959 0.801
17      1      0      0      1      0  14  15.498 -0.959 0.801
18      1      0      0      1      1  81  75.060 -0.466 0.816
19      1      0      1      0      0   3   5.334 -0.959 0.801
20      1      0      1      0      1  28  25.834 -0.466 0.816
21      1      0      1      1      0  15  11.690 -0.466 0.816
22      1      0      1      1      1  80  83.310  0.049 0.836
23      1      1      0      0      0  16  11.391 -0.959 0.801
24      1      1      0      0      1  56  55.171 -0.466 0.816
25      1      1      0      1      0  21  24.965 -0.466 0.816
26      1      1      0      1      1 173 177.918  0.049 0.836
27      1      1      1      0      0  11   8.592 -0.466 0.816
28      1      1      1      0      1  61  61.235  0.049 0.836
29      1      1      1      1      0  28  27.709  0.049 0.836
30      1      1      1      1      1 298 295.767  0.593 0.862&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;By default, &lt;code&gt;factor.scores()&lt;/code&gt; produces ability estimates
for the observed response patterns (every combination available); if
ability estimates are required for non observed or specific response
patterns, these could be specified using the &lt;code&gt;resp.patterns&lt;/code&gt;
argument.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;factor.scores(mod_1pl, resp.patterns = rbind(c(1,1,1,1,1), c(0,0,0,0,0)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = NULL)

Scoring Method: Empirical Bayes

Factor-Scores for specified response patterns:
  Item 1 Item 2 Item 3 Item 4 Item 5 Obs     Exp     z1 se.z1
1      1      1      1      1      1 298 295.767  0.593 0.862
2      0      0      0      0      0   3   2.364 -1.910 0.790&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The specified response patterns above are for examinees who got
all items correctly and incorrectly respectively. The results suggested
that the examinee who got all item correctly has ability level of 0.50
and the examinee who got all item incorrectly has ability level of
-1.91.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The model we discussed so far, Rasch, 1PL,2PL, 3PL are all
suitable for dichotomous test items (True/False), but what if item
responses have more than 2 categories like in a survey (i.e., 1 =
Strongly disagree 2 = Disagree 3 = Agree 4 = Strongly Agree)? This is
when we use polytomous IRT models.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="polytomous-item"&gt;Polytomous Item&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The data we consider here comes from the Environment section of the
1990 British Social Attitudes Survey, N = 291, 6 items, 3 ordinal
response options. The data can be loaded with
&lt;code&gt;data(Environment)&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;data(Environment)
descript(Environment)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Descriptive statistics for the &amp;#39;Environment&amp;#39; data-set

Sample:
 6 items and 291 sample units; 0 missing values

Proportions for each level of response:
             very concerned slightly concerned not very concerned
LeadPetrol           0.6151             0.3265             0.0584
RiverSea             0.8007             0.1753             0.0241
RadioWaste           0.7457             0.1924             0.0619
AirPollution         0.6495             0.3196             0.0309
Chemicals            0.7491             0.1924             0.0584
Nuclear              0.5155             0.3265             0.1581


Frequencies of total scores:
      6  7  8  9 10 11 12 13 14 15 16 17 18
Freq 96 51 37 27 26 18 13  7  6  6  1  1  2


Cronbach&amp;#39;s alpha:
                        value
All Items              0.8215
Excluding LeadPetrol   0.8218
Excluding RiverSea     0.7990
Excluding RadioWaste   0.7767
Excluding AirPollution 0.7751
Excluding Chemicals    0.7790
Excluding Nuclear      0.8058


Pairwise Associations:
   Item i Item j p.value
1       1      2   0.001
2       1      3   0.001
3       1      4   0.001
4       1      5   0.001
5       1      6   0.001
6       2      3   0.001
7       2      4   0.001
8       2      5   0.001
9       2      6   0.001
10      3      4   0.001&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;From the descriptive output, the first response, “&lt;em&gt;very
concerned&lt;/em&gt;”, has the highest frequency. The p-values for the
pairwise associations indicate significant associations between all
items. An alternative method to explore the degree of association
between pairs of items can be done with &lt;code&gt;rcor.test()&lt;/code&gt; for
non-parametric correlation coefficients.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;rcor.test(Environment, method = &amp;quot;kendall&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
             LeadPetrol RiverSea RadioWaste AirPollution Chemicals
LeadPetrol    *****      0.385    0.260      0.457        0.305   
RiverSea     &amp;lt;0.001      *****    0.399      0.548        0.403   
RadioWaste   &amp;lt;0.001     &amp;lt;0.001    *****      0.506        0.623   
AirPollution &amp;lt;0.001     &amp;lt;0.001   &amp;lt;0.001      *****        0.504   
Chemicals    &amp;lt;0.001     &amp;lt;0.001   &amp;lt;0.001     &amp;lt;0.001        *****   
Nuclear      &amp;lt;0.001     &amp;lt;0.001   &amp;lt;0.001     &amp;lt;0.001       &amp;lt;0.001   
             Nuclear
LeadPetrol    0.279 
RiverSea      0.320 
RadioWaste    0.484 
AirPollution  0.382 
Chemicals     0.463 
Nuclear       ***** 

upper diagonal part contains correlation coefficient estimates 
lower diagonal part contains corresponding p-values&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;rcor.test()&lt;/code&gt; provides two options for nonparametric
calculation, Kendall’s &lt;em&gt;Tau&lt;/em&gt; and Spearman’s &lt;em&gt;rho&lt;/em&gt; in
&lt;code&gt;method&lt;/code&gt; argument. Initially, we will fit the partial credit
model (PCM), which is a Polytomous version of the Rasch model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="partial-credit-model"&gt;Partial Credit Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;PCM fixes the discrimination parameter of all item as 1 in the
same way as what Rasch’s model does. The threshold (or the parameter
that represents the trait level necessary for an examinee to have 50% to
pick a response category) of PCM is allowed to vary.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For the &lt;code&gt;gpcm()&lt;/code&gt; function, If
&lt;code&gt;constraint = "rasch"&lt;/code&gt;, then the discrimination parameter is
assumed equal for all items and fixed at one. If
&lt;code&gt;constraint = "1PL"&lt;/code&gt;, then the discrimination parameter βi is
assumed equal for all items but is estimated. Here, we are fixing all
discrimination parameter to 1.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_pcm_rasch &amp;lt;- gpcm(Environment, constraint = &amp;quot;rasch&amp;quot;)
summary(mod_pcm_rasch)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
gpcm(data = Environment, constraint = &amp;quot;rasch&amp;quot;)

Model Summary:
   log.Lik      AIC      BIC
 -1147.176 2318.351 2362.431

Coefficients:
$LeadPetrol
        value std.err z.value
Catgr.1 0.680   0.153   4.450
Catgr.2 2.785   0.292   9.541
Dscrmn  1.000      NA      NA

$RiverSea
        value std.err z.value
Catgr.1 1.822   0.180  10.149
Catgr.2 3.385   0.435   7.781
Dscrmn  1.000      NA      NA

$RadioWaste
        value std.err z.value
Catgr.1 1.542   0.174   8.879
Catgr.2 2.328   0.302   7.709
Dscrmn  1.000      NA      NA

$AirPollution
        value std.err z.value
Catgr.1 0.822   0.153   5.363
Catgr.2 3.517   0.376   9.343
Dscrmn  1.000      NA      NA

$Chemicals
        value std.err z.value
Catgr.1 1.555   0.174   8.949
Catgr.2 2.399   0.308   7.788
Dscrmn  1.000      NA      NA

$Nuclear
        value std.err z.value
Catgr.1 0.316   0.156   2.029
Catgr.2 1.498   0.208   7.218
Dscrmn  1.000      NA      NA


Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 0.0079 
optimizer: nlminb &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The model summary provides AIC and BIC of the model. Same as what we
did with our dichotomous data, we can request for parameter estimates of
each item.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;coef(mod_pcm_rasch, prob = TRUE, order = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;             Catgr.1 Catgr.2 Dscrmn
LeadPetrol     0.680   2.785      1
RiverSea       1.822   3.385      1
RadioWaste     1.542   2.328      1
AirPollution   0.822   3.517      1
Chemicals      1.555   2.399      1
Nuclear        0.316   1.498      1&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;In order to check the fit of the model to the data, the argument
&lt;code&gt;GoF.gpcm&lt;/code&gt; and &lt;code&gt;margins()&lt;/code&gt; can be used.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;GoF.gpcm(mod_pcm_rasch, B = 199) # B = Bootstrap sample&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Parametric Bootstrap Approximation to Pearson chi-squared Goodness-of-Fit Measure

Call:
gpcm(data = Environment, constraint = &amp;quot;rasch&amp;quot;)

Tobs: 1001.41 
# data-sets: 200 
p-value: 0.085 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Based on 200 data-points, the non significant p-value suggests an
acceptable fit of the model. The null hypothesis states that the
observed data and the model fit with each other (no differences). We can
move on to the two-way margin analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_pcm_rasch)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
gpcm(data = Environment, constraint = &amp;quot;rasch&amp;quot;)

Fit on the Two-Way Margins

             LeadPetrol RiverSea RadioWaste AirPollution Chemicals
LeadPetrol   &amp;lt;NA&amp;gt;       35.47     3.04      34.43         7.41    
RiverSea     ***        &amp;lt;NA&amp;gt;     20.07      77.50        18.75    
RadioWaste                       &amp;lt;NA&amp;gt;       44.64        68.12    
AirPollution ***        ***      ***        &amp;lt;NA&amp;gt;         33.29    
Chemicals                        ***        ***          &amp;lt;NA&amp;gt;     
Nuclear                          ***                              
             Nuclear
LeadPetrol    4.71  
RiverSea     10.36  
RadioWaste   43.35  
AirPollution 16.56  
Chemicals    27.68  
Nuclear      &amp;lt;NA&amp;gt;   

&amp;#39;***&amp;#39; denotes pairs of items with lack-of-fit&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The upper diagonal part of the output contains the residuals, and
the lower diagonal part indicates the pairs for which the residuals
exceed the threshold value. The two-way margin analysis above suggests
problematic fit of the data with the PCM model, meaning that PCM might
not be suitable for this data. We can try using the next model, the
Graded Response Model (GRM).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="graded-response-model"&gt;Graded Response Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;GRM is the polytomous version of the 2PL model (&lt;a
href="https://www.frontiersin.org/articles/10.3389/feduc.2021.721963/full"&gt;Dai
et al., 2021&lt;/a&gt;). Despite able to constrain the discrimination
parameter, GRM works differently than PCM. PCM estimates separate
category response parameters for each item, while the GRM model further
assumes that the thresholds for category response are also equal across
items (&lt;a
href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4520411/#:~:text=The%20difference%20between%20the%20two,are%20also%20equal%20across%20items."&gt;Nguyen
et al., 2014&lt;/a&gt;). Initially, we can try fitting the constrained version
of Graded Response Model (GRM) that assumes equal &lt;em&gt;a&lt;/em&gt; parameter
across items (similar to Rasch model). The model is fitted by
&lt;code&gt;grm()&lt;/code&gt; as follows&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_grm &amp;lt;- grm(Environment, constrained = TRUE)
summary(mod_grm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment, constrained = TRUE)

Model Summary:
   log.Lik      AIC      BIC
 -1106.193 2238.386 2286.139

Coefficients:
$LeadPetrol
        value
Extrmt1 0.395
Extrmt2 1.988
Dscrmn  2.218

$RiverSea
        value
Extrmt1 1.060
Extrmt2 2.560
Dscrmn  2.218

$RadioWaste
        value
Extrmt1 0.832
Extrmt2 1.997
Dscrmn  2.218

$AirPollution
        value
Extrmt1 0.483
Extrmt2 2.448
Dscrmn  2.218

$Chemicals
        value
Extrmt1 0.855
Extrmt2 2.048
Dscrmn  2.218

$Nuclear
        value
Extrmt1 0.062
Extrmt2 1.266
Dscrmn  2.218


Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 0.0049 
quasi-Newton: BFGS &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;If standard errors for the parameter estimates are required, you can
add the argument &lt;code&gt;Hessian = T&lt;/code&gt; to the function
&lt;code&gt;grm()&lt;/code&gt;. Similarly to our dichotomous case, the fit of the
model can be checked using &lt;code&gt;margins()&lt;/code&gt; for two-way
margins.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_grm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment, constrained = TRUE)

Fit on the Two-Way Margins

             LeadPetrol RiverSea RadioWaste AirPollution Chemicals
LeadPetrol   -          10.03     9.98       5.19         7.85    
RiverSea                -         5.06      17.12         2.56    
RadioWaste                       -           6.78        20.60    
AirPollution                                -             4.49    
Chemicals                                                -        
Nuclear                                                           
             Nuclear
LeadPetrol   16.93  
RiverSea      7.14  
RadioWaste   12.09  
AirPollution  4.57  
Chemicals     3.85  
Nuclear      -      &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The output above looks good as there is no indication of poor fit.
Next, we will try with the three-way margins.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_grm, type = &amp;quot;three&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment, constrained = TRUE)

Fit on the Three-Way Margins

   Item i Item j Item k (O-E)^2/E  
1       1      2      3     28.52  
2       1      2      4     34.26  
3       1      2      5     29.91  
4       1      2      6     42.74  
5       1      3      4     33.03  
6       1      3      5     66.72  
7       1      3      6     65.31  
8       1      4      5     25.48  
9       1      4      6     34.46  
10      1      5      6     39.49  
11      2      3      4     29.63  
12      2      3      5     37.74  
13      2      3      6     32.50  
14      2      4      5     27.08  
15      2      4      6     36.77  
16      2      5      6     19.49  
17      3      4      5     38.99  
18      3      4      6     26.91  
19      3      5      6     39.62  
20      4      5      6     22.25  &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Both the two- and three-way residuals show a good fit of the
constrained model to the data, but checking the fit of the model in the
margins does not correspond to an overall goodness-of-fit test. As a
result, we will fit the unconstrained version of the GRM as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_grm_unconstrained &amp;lt;- grm(Environment) #unconstrained GRM

summary(mod_grm_unconstrained)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment)

Model Summary:
   log.Lik      AIC      BIC
 -1090.404 2216.807 2282.927

Coefficients:
$LeadPetrol
        value
Extrmt1 0.487
Extrmt2 2.584
Dscrmn  1.378

$RiverSea
        value
Extrmt1 1.058
Extrmt2 2.499
Dscrmn  2.341

$RadioWaste
        value
Extrmt1 0.779
Extrmt2 1.793
Dscrmn  3.123

$AirPollution
        value
Extrmt1 0.457
Extrmt2 2.157
Dscrmn  3.283

$Chemicals
        value
Extrmt1 0.809
Extrmt2 1.868
Dscrmn  2.947

$Nuclear
        value
Extrmt1 0.073
Extrmt2 1.427
Dscrmn  1.761


Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 0.003 
quasi-Newton: BFGS &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can use a likelihood ratio test to check if the unconstrained
version GRM is better than its constrained one.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;anova(mod_grm, mod_grm_unconstrained)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 Likelihood Ratio Table
                          AIC     BIC  log.Lik   LRT df p.value
mod_grm               2238.39 2286.14 -1106.19                 
mod_grm_unconstrained 2216.81 2282.93 -1090.40 31.58  5  &amp;lt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The likelihood ratio test indicates that the unconstrained GRM is
preferable for the Environment data. We can plot the Item Characteristic
Curve (ICC) of all 6 items, Item Information Curve (IIC), and Test
Information Curve (TIC) below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mar=c(2,6,2,2), mfrow = c(3, 3))

plot(mod_grm_unconstrained, lwd = 2, cex = 0.6, legend = TRUE, cx = &amp;quot;left&amp;quot;, 
     xlab = &amp;quot;Latent Trait&amp;quot;, cex.main = 0.7, cex.lab = 0.7, cex.axis = 1)

##############################################################

plot(mod_grm_unconstrained, type = &amp;quot;IIC&amp;quot;, lwd = 2, cex = 0.6, legend = TRUE, cx = &amp;quot;topleft&amp;quot;,
     xlab = &amp;quot;Latent Trait&amp;quot;, cex.main = 0.7, cex.lab = 0.7, cex.axis = 1)

plot(mod_grm_unconstrained, type = &amp;quot;IIC&amp;quot;, items = 0, lwd = 2, xlab = &amp;quot;Latent Trait&amp;quot;,cex.main = 0.7, cex.lab = 0.7, cex.axis = 1)

info3 &amp;lt;- information(mod_grm_unconstrained, c(-4, 0))
info4 &amp;lt;- information(mod_grm_unconstrained, c(0, 4))

text(-1.9, 8, labels = paste(&amp;quot;Information in (-4, 0):&amp;quot;,
                             paste(round(100 * info3$PropRange, 1), &amp;quot;%&amp;quot;, sep = &amp;quot;&amp;quot;),
                             &amp;quot;\n\nInformation in (0, 4):&amp;quot;,
                             paste(round(100 * info4$PropRange, 1), &amp;quot;%&amp;quot;, sep = &amp;quot;&amp;quot;)), cex = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file349070062f97_files/figure-html/unnamed-chunk-37-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;From the Item Characteristic Curve, we observe that there is low
probability of endorsing the the first option, “very concerned”, for
relatively high latent trait levels, which means that the questions
asked are not considered as major environmental issues by the
respondent.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Test Information Curve also tells us that the test provides
89% of the total information for high latent trait levels.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally, the Item Information Curve indicates that items in
LeadPetrol and Nuclear provide little information in the whole latent
trait continuum. We can check this in detail using
&lt;code&gt;information()&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;information(mod_grm_unconstrained, c(-4, 4))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment)

Total Information = 26.97
Information in (-4, 4) = 26.7 (98.97%)
Based on all the items&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;For item 1 and item 6&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;information(mod_grm_unconstrained, c(-4, 4), items = c(1, 6)) #for item 1 and 6&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment)

Total Information = 5.36
Information in (-4, 4) = 5.17 (96.38%)
Based on items 1, 6&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We observe that item 1 and 6 provide only the 5.36% of the total
information (from the total of 26.97); Thus, they could probably be
excluded from a similar future study. Finally, a useful comparison is to
plot the ICC of each response option separately.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mar=c(2,6,2,2), mfrow = c(2, 2))

plot(mod_grm_unconstrained, category = 1, lwd = 2, cex = 0.7, legend = TRUE, cx = -4.5,
     cy = 0.85, xlab = &amp;quot;Latent Trait&amp;quot;, cex.main = 0.7, cex.lab = 0.7,
     cex.axis = 1)

for (ctg in 2:3) 
  {
  plot(mod_grm_unconstrained, category = ctg, lwd = 2, cex = 0.5, annot = FALSE,
      xlab = &amp;quot;Latent Trait&amp;quot;, cex.main = 0.7, cex.lab = 0.7,
      cex.axis = 1)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file349070062f97_files/figure-html/unnamed-chunk-40-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;From the plot, the response option for RadioWaste and Chemicals have
nearly identical characteristic curves for all categories, indicating
that these two items are probably regarded to have the same effect on
the environment.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;So far, we have discussed what IRT is, including its model for
dichotomous and polytomous test items. There are several models for us
to choose from, and each model has its parameter we can adjust to suit
our needs as well depending on how complex we want our model to be.
There is no right or wrong answer model selection. For example, if we
want the model to be as simple as possible with a dichotomous test (say,
a math test), we could go for Rasch model. If we want our model to be
more realistic, we may want to use 2PL or 3PL, which comes with expenses
such as the need for more sample size or more difficulty to fit the
model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, given how useful IRT is in analyzing test items with
several unique parameters, it doesn’t mean that we need to disregard the
concept of CTT in our practice. Both theories have its own contribution
and usefulness. For example, CTT is more practical to implement in the
classroom setting where there is small amount of students and there is
no need to investigate items at a deeper level. For example, if we want
to use a classroom assessment for formative purposes (i.e., a practice
quiz to help students prepare for the final exam), using CTT might be
sufficient. If we want to develop a large-scale test to measure students
at a national level, maybe IRT might be more appropriate to improve the
test with a more realistic model.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a
href="https://d1wqtxts1xzle7.cloudfront.net/47596077/ITEMS_Module_16-with-cover-page-v2.pdf?Expires=1652597267&amp;amp;Signature=dRiumBzQzxIiFeQvRCtF3cuBjpeaui-mWZXaskGDaQYlwenB7ZZVx0dBUxSnl47nyQnveBh0ZBHdY3IbzNnPHZqAhAOdIN~QIQVCfSQ11d0ZV3mef~kmo8nEzXj459sakTPeZCeTwZSVxklbbtIE52mzRfFItBw4ZF70kJMT9S3FC9YaI~lDDXN5YUbb4VL7ZgCH-lsBIthNbTSDUXxqGuwgeHgxKfotwPdlQRr5iLEEwlLkea2Fr1zow4uLrNg38yK31MdNg55m73x9iwJZA2s~6wlj9IfsWINVJlwPoIOjT5Zm0bIuyIfzGbUKRk~3AiXLjThGUjIi0wlc1Y6flQ__&amp;amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"&gt;Hambleton
and Jones (1993&lt;/a&gt;) did a really great job in comparing the difference
between CTT and IRT in their work. I have presented the table below for
your information.&lt;/p&gt;
&lt;table style="width:99%;"&gt;
&lt;caption&gt;The Comparison between CTT and IRT Models (&lt;a
href="https://d1wqtxts1xzle7.cloudfront.net/47596077/ITEMS_Module_16-with-cover-page-v2.pdf?Expires=1652597267&amp;amp;Signature=dRiumBzQzxIiFeQvRCtF3cuBjpeaui-mWZXaskGDaQYlwenB7ZZVx0dBUxSnl47nyQnveBh0ZBHdY3IbzNnPHZqAhAOdIN~QIQVCfSQ11d0ZV3mef~kmo8nEzXj459sakTPeZCeTwZSVxklbbtIE52mzRfFItBw4ZF70kJMT9S3FC9YaI~lDDXN5YUbb4VL7ZgCH-lsBIthNbTSDUXxqGuwgeHgxKfotwPdlQRr5iLEEwlLkea2Fr1zow4uLrNg38yK31MdNg55m73x9iwJZA2s~6wlj9IfsWINVJlwPoIOjT5Zm0bIuyIfzGbUKRk~3AiXLjThGUjIi0wlc1Y6flQ__&amp;amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"&gt;Hambleton
&amp;amp; Jones, 1993&lt;/a&gt;, p.43)&lt;/caption&gt;
&lt;colgroup&gt;
&lt;col width="24%" /&gt;
&lt;col width="34%" /&gt;
&lt;col width="40%" /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th&gt;Area&lt;/th&gt;
&lt;th&gt;Classical Test Theory&lt;/th&gt;
&lt;th&gt;Item Response Theory&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;Model&lt;/td&gt;
&lt;td&gt;Linear&lt;/td&gt;
&lt;td&gt;Non-linear&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;Level&lt;/td&gt;
&lt;td&gt;Test&lt;/td&gt;
&lt;td&gt;Item&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;Assumptions&lt;/td&gt;
&lt;td&gt;Weak (i.e., easy to fit with test data)&lt;/td&gt;
&lt;td&gt;Strong (i.e., more difficult to meet with test data)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;Item-Ability Relationship&lt;/td&gt;
&lt;td&gt;Not Specified&lt;/td&gt;
&lt;td&gt;Item Characteristics Function&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;Examinee Ability&lt;/td&gt;
&lt;td&gt;Represented by test scores or estimated true scores&lt;/td&gt;
&lt;td&gt;Represented by latent ability (Theta/θ)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;Invariance of item and person statistics&lt;/td&gt;
&lt;td&gt;Unavailable / item and person parameters are sample dependent&lt;/td&gt;
&lt;td&gt;Item and person parameters are sample dependent if the model fits
the data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;Item Statistics&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;em&gt;p&lt;/em&gt; = item difficulty&lt;/p&gt;
&lt;p&gt;&lt;em&gt;r&lt;/em&gt; = item discrimination&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;Item discrimination (&lt;em&gt;a&lt;/em&gt;), Item difficulty (&lt;em&gt;b&lt;/em&gt;),
guessing parameter (&lt;em&gt;c&lt;/em&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;Sample Size (for item parameter estimation)&lt;/td&gt;
&lt;td&gt;200-500&lt;/td&gt;
&lt;td&gt;More than 500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Similar to CTT, IRT can be used to develop tests, scales,
surveys, or other measurement tools. The model can analyze item-level
data of both dichotomous (i.e., exams with true/false) and polytomous
(i.e., surveys with no right/wrong answers) tests to provide information
on sensitivity of measurement across a range of latent trait. Knowing
information like item difficulty, item discrimination, and item guessing
is useful when building tests as we can examine which item is a good
item and which item is a not-so-useful one. For example, a test item
that is easy to guess might not be appropriate because everyone can do
it, so it doesn’t really measure anything.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IRT also allows us to put ability levels and difficulties of
items into the same scale to match the trait levels of a target
population. If we want a test to measure someone with “just enough”
knowledge to pass (say, a driver license exam), we can build a test to
measure people with low to medium knowledge. I mean, a taxi driver or a
racing driver know how to drive a car, and that is enough. However, if
we want to develop a test to select the best-of-the-best candidates for
scholarship selection, we might want to build a difficult test to
separate low-to-mid tier students to high performance students. Anyway,
that is all for this post. Thank you so much for reading this! Have a
good day!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>2530d599a3222722a8076a89ee971d4a</distill:md5>
      <category>R</category>
      <category>Psychometric</category>
      <guid>https://taridwong.github.io/posts/2022-05-15-irt</guid>
      <pubDate>Sun, 15 May 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-05-15-irt/IIC.png" medium="image" type="image/png" width="656" height="551"/>
    </item>
    <item>
      <title>Making Sense of Machine Learning with Explanable Artificial Intelligence</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-28-xai</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In my &lt;a
href="https://taridwong.github.io/posts/2022-04-09-ensemble/"&gt;previous
post&lt;/a&gt; on ensemble machine learning models, I mentioned that one major
drawback in the artificial intelligence (AI) field is &lt;a
href="https://www.technologyreview.com/2017/04/11/5113/the-dark-secret-at-the-heart-of-ai/"&gt;the
black box problem&lt;/a&gt;, which hampers interpretability of the results
from complex algorithms such as Random Forest or Extreme Gradient
Boosting. Not knowing how the algorithm works behind the prediction
could reduce applicability of the method itself as the audience can’t
fully comprehend the result and therefore unable to use it to inform
their decisions; this problem could therefore damage trust from the
stakeholders (users, policy makers, general audience) to the field as
well &lt;a href="https://doi.org/10.1175/BAMS-D-18-0195.1"&gt;(McGovern et
al., 2019)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On the developer’s side, fully understanding the machine learning
models through the explanable approach (aka the white-box approach)
allows developers to identify potential problems such as &lt;a
href="https://www.kaggle.com/code/alexisbcook/data-leakage/tutorial"&gt;data
leakage&lt;/a&gt; in the algorithm and fix (or debug) it with relative ease
(&lt;a
href="https://ieeexplore.ieee.org/abstract/document/8882211"&gt;Loyola-Gonzalez,
2019)&lt;/a&gt;. Further, knowing which variable affects the prediction the
most can inform feature engineering to reduce model complexity and
direct future data collection as well by focusing on collecting the
variables that matter &lt;a
href="https://www.kaggle.com/code/dansbecker/use-cases-for-model-insights"&gt;(Becker
&amp;amp; Cook, 2021)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On stakeholder’s side, it is important to emphasize model
explanability especially in industries such as healthcare, finances, and
military to foster trust between the people inside and outside of the
field that could lead to the extent that the result is used to inform
decisions made by humans such as financial credit approval &lt;a
href="https://www.kaggle.com/code/dansbecker/use-cases-for-model-insights"&gt;(Becker
&amp;amp; Cook, 2021&lt;/a&gt;; &lt;a
href="https://ieeexplore.ieee.org/abstract/document/8882211"&gt;Loyola-Gonzalez,
2019)&lt;/a&gt;. Clearly Understanding how, where, and why the model also
benefits the model itself as users are able to identify potential
problems in its performance and provide the develoeprs with their
feedback &lt;a
href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=9401991"&gt;(Velez
et al., 2021)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The above examples knowing how to extract human-understandable
insights from a complex machine learning model is important, especially
in social science data where the theoretical part is as important as the
methodological and the practical part. For that reason, I will be
applying the methods of Explanable Artificial Intelligence (XAI) to
extract interpretable insights from a classification model that predicts
students’ grade repetition. We will begin by setting up the environment
as usual.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

from imblearn.combine import SMOTEENN

from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score

import warnings
warnings.filterwarnings(&amp;quot;ignore&amp;quot;)

RANDOM_STATE = 123&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;I will be using the same data set as my &lt;a
href="https://taridwong.github.io/posts/2022-02-27-statlearning/"&gt;previous
post about statistical learning&lt;/a&gt;, namely the Programme for
International Student Assessment (PISA) 2018 (&lt;a
href="https://www.oecd.org/education/pisa-2018-results-volume-i-5f07c754-en.htm"&gt;OECD,
2019&lt;/a&gt;). However, the set of variables that I am examining will be
different as PISA contains several school-related variables that can be
shifted as the researcher sees fit. For this post, I will predict
students’ class repetition from 25 predictors (or features as called in
the field of machine learning) such as students’ socio-economic status,
history of bullying involvement, and their learning motivation. The data
is collected from Thai student in 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;df = pd.read_csv(&amp;quot;PISA_TH.csv&amp;quot;)

X = df.drop(&amp;#39;REPEAT&amp;#39;, axis=1)
y = df[&amp;#39;REPEAT&amp;#39;]

df.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   REPEAT    ESCS  DAYSKIP  ...  Invest_effort  WEALTH  Home_resource
0       0 -0.7914        1  ...              6  0.0721        -1.4469
1       0  0.8188        1  ...              8 -0.3429         1.1793
2       0  0.4509        1  ...             10  0.3031         1.1793
3       0  0.7086        1  ...             10 -0.5893        -0.1357
4       0  0.8361        1  ...             10  0.5406         1.1793

[5 rows x 25 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="addressing-sample-imbalance"&gt;Addressing Sample Imbalance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The problem is that our targeted variable is imbalance; that is,
the number of students who repeated a class is smaller than the number
of students who did not. This situation makes sense in the real-world
data as normal samples are usually more prevalent than the abnormal
ones, but it is undesirable in the machine learning scenario as the
model could recognize minority samples as unimportant and therefore
disregard them as noises &lt;a
href="https://arxiv.org/pdf/1106.1813.pdf"&gt;(Chawla et al., 2022)&lt;/a&gt;. As
a result, the model could give misleadingly optimistic performance on
classification datasets as it classifies only students who did not
repeat a class.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;See the t-Distributed Stochastic Neighbor Embedding (tSNE) plot
below for the visualization. There isn’t much samples of repeaters in
contrary to non-repeater students. Plus, the pattern is not prominent
enough as the blut dots (repeaters) stay very close to the red dots
(non-repeaters). This could make the pattern difficult to be learned by
the machine due to its ambiguity. One way we can mitigate this problem
is to perform data augmentation via oversampling and undersampling,
which synthesizes more minority samples and deletes or merges majority
samples to improve performance of the machine (&lt;a
href="https://ieeexplore.ieee.org/abstract/document/9034624?casa_token=P33Jkz0x1zEAAAAA:Xtz22PhKDSZ_ktb6X7w-Le7PHkxHwfzzRzvrL3qJcJIDwmyaAMizIr1lUBSK5Lpz1qyk4Ls"&gt;Budhiman
et al., 2019&lt;/a&gt;; &lt;a
href="https://ieeexplore.ieee.org/abstract/document/7797091"&gt;Wong et
al;., 2016&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;Counter(y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Counter({0: 8044, 1: 589})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;tsne = TSNE(n_components=2, random_state=RANDOM_STATE)

TSNE_result = tsne.fit_transform(X)

plt.figure(figsize=(12,8))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1200x800 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;sns.scatterplot(TSNE_result[:,0], TSNE_result[:,1], hue=y, legend=&amp;#39;full&amp;#39;, palette=&amp;quot;hls&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;AxesSubplot:&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To balance the data, I will use both oversampling and
undersampling. Normal oversampling methods duplicates minority samples
for more sample size; however, this approach does not add any more
information to the model (more of the same, basically). Instead, we can
&lt;em&gt;synthesize&lt;/em&gt; minority samples by creating samples that are
&lt;em&gt;similar&lt;/em&gt; to the existing minority samples; this technique is
named as &lt;strong&gt;Synthetic Minority Oversampling TEchnique
(SMOTE)&lt;/strong&gt; &lt;a
href="https://www.wiley.com/en-us/Imbalanced+Learning%3A+Foundations%2C+Algorithms%2C+and+Applications-p-9781118074626"&gt;(He
and Ma, 2013)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Also, we can further enhance the effectiveness of SMOTE by adding
undersampling into the process &lt;a
href="https://arxiv.org/pdf/1106.1813.pdf"&gt;(Chawla et al., 2022)&lt;/a&gt;.
Instead of randomly delete our majority samples, we will use the
&lt;strong&gt;Edited Nearest Neighbor (ENN)&lt;/strong&gt; method, which deletes
data points based on their neighbors to make the difference between
majority and minority samples &lt;a
href="https://www.researchgate.net/profile/Duke-T-J-Ludera/publication/348663430_Credit_Card_Fraud_Detection_by_Combining_Synthetic_Minority_Oversampling_and_Edited_Nearest_Neighbours/links/6009f844a6fdccdcb86fc68c/Credit-Card-Fraud-Detection-by-Combining-Synthetic-Minority-Oversampling-and-Edited-Nearest-Neighbours.pdf"&gt;(Ludera,
2021)&lt;/a&gt;. The combination of these two techniques is called &lt;a
href="https://imbalanced-learn.org/stable/auto_examples/combine/plot_comparison_combine.html#sphx-glr-auto-examples-combine-plot-comparison-combine-py"&gt;&lt;strong&gt;SMOTEENN&lt;/strong&gt;&lt;/a&gt;
See Figure 1 for the example of ENN from &lt;a
href="https://doi.org/10.1016/j.ins.2009.02.011"&gt;Guan et al.,
(2009)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-xaienn.png" style="width:70.0%" alt="" /&gt;
&lt;p class="caption"&gt;ENN Editing with 1-NN Classifier. No copyright
infringement is intended&lt;/p&gt;
&lt;/div&gt;
&lt;pre class="python"&gt;&lt;code&gt;smote_enn = SMOTEENN(random_state=RANDOM_STATE, sampling_strategy = &amp;#39;minority&amp;#39;, n_jobs=-1)

X_resampled, y_resampled = smote_enn.fit_resample(X, y)

Counter(y_resampled)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Counter({1: 8040, 0: 4794})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;tsne = TSNE(n_components=2, random_state=RANDOM_STATE)

TSNE_result = tsne.fit_transform(X_resampled)

plt.figure(figsize=(12,8))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1200x800 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;sns.scatterplot(TSNE_result[:,0], TSNE_result[:,1], hue=y_resampled, legend=&amp;#39;full&amp;#39;, palette=&amp;quot;hls&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;AxesSubplot:&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The second tSNE plot shows a more noticable pattern between student
repeaters and non-repeaters. The number of repeaters is increased while
the number of non-repeaters is decreased. Next, we can put our augmented
data into the Random Forest model for prediction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="random-forest-ensemble"&gt;Random Forest Ensemble&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We will be splitting the data set into a training and a testing set
as usual. For a quick recap, Random Forest is a machine learning model
that consists of several unique and uncorrelated decision trees; hence
the word Random in its name. Those trees work together to improve the
predictive accuracy of that dataset than a single decision tree &lt;a
href="https://mitpress.mit.edu/books/introduction-machine-learning-second-edition"&gt;(Kubat,
2017)&lt;/a&gt;. The model will be evaluated with the repeated stratified
10-folds technique to test our model prediction on different sets of
unseen data to ensure its accuracy, especially in the case of imbalanced
data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;CV = RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=RANDOM_STATE)


X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size = 0.30, 
                                                    random_state = RANDOM_STATE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;# random forest model creation
clf_rfc = RandomForestClassifier(random_state=RANDOM_STATE)
clf_rfc.fit(X_train, y_train)

# predictions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;RandomForestClassifier(random_state=123)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;rfc_predict = clf_rfc.predict(X_test)

rfc_cv_score = cross_val_score(clf_rfc, X_resampled, y_resampled, cv=CV, scoring=&amp;#39;roc_auc&amp;#39;)

print(&amp;quot;=== All AUC Scores ===&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;=== All AUC Scores ===&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(rfc_cv_score)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[0.98907675 0.99357898 0.99228597 0.99276793 0.99409399 0.99378369
 0.9931644  0.9901627  0.98967714 0.99287747 0.99384328 0.99252177
 0.99452348 0.99187137 0.99040419 0.99201929 0.9896265  0.99140778
 0.99115331 0.99457436]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;#39;\n&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;=== Mean AUC Score ===&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;=== Mean AUC Score ===&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;Mean AUC Score - RandForest: &amp;quot;, rfc_cv_score.mean())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean AUC Score - RandForest:  0.9921707177188173&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;#define metrics for normal RF
from sklearn import metrics

y_pred_proba_rf = clf_rfc.predict_proba(X_test)[::,1]
fpr_rf, tpr_rf, _ = metrics.roc_curve(y_test,  y_pred_proba_rf)

auc_rf = metrics.roc_auc_score(y_test, y_pred_proba_rf)
plt.plot(fpr_rf,tpr_rf, label=&amp;quot;AUC for Random Forest Classifier = &amp;quot;+str(auc_rf.round(3)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;lt;matplotlib.lines.Line2D object at 0x0000017E022C5400&amp;gt;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.legend(loc=&amp;quot;lower right&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.legend.Legend object at 0x0000017E01245490&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;True Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;True Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.xlabel(&amp;#39;False Positive Rate&amp;#39;)
           &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;False Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Receiver-Operator Curve (ROC)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Receiver-Operator Curve (ROC)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file3490688e4c33_files/figure-html/unnamed-chunk-10-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The results will be evaluated with the Receiver Operator
Characteristic (ROC) curve, which shows the diagnostic ability of binary
classifiers. One approach to use ROC is to evaluate its Area Under Curve
(AUC), which measures of the ability of a classifier to distinguish
between classes and is used as a summary of the ROC curve. The higher
the AUC, the better the performance of the model at distinguishing
between the positive and negative classes. The mean of 20 rounds of
testing (randomly splitting the data into 10 stratified parts, repeated
it for 2 times) looks good is around 0.99, meaning that there is a 99%
chance that the model is able to correctly predict which student is a
reapeater and which is not based on the data used to train the machine.
Now we know that the model works well with our data, let us move on to
interpreting it with XAI techniques.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="explaining-ai"&gt;Explaining AI&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;XAI is a set of methods that allows a machine learning model and its
results understandable to human in terms of how it works in terms of
prediction, including the impace of variables to the prediction results
&lt;a
href="https://link.springer.com/book/10.1007/978-3-030-68640-6"&gt;(Gianfagna
&amp;amp; Di Cecco, 2021)&lt;/a&gt;. The XAI methods that we will extract insights
are permutation importance, partial dependence plot, and Shapley
Additive explanations (SHAP) values.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="permutation-importance"&gt;Permutation Importance&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;One of the most basic questions we might ask of a model is: What
features have the biggest impact on predictions? This quention could be
answered through the examination of &lt;strong&gt;feature importance&lt;/strong&gt;.
There are multiple ways to measure feature importance. One way is to
extract the feature importance plot from the model itself as
demonstrated below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Create a pd.Series of features importances
importances_rf = pd.Series(clf_rfc.feature_importances_, index = X_resampled.columns)

# Sort importances_rf
sorted_importance_rf = importances_rf.sort_values()

#Horizontal bar plot
sorted_importance_rf.plot(kind=&amp;#39;barh&amp;#39;, color=&amp;#39;lightgreen&amp;#39;); 
plt.xlabel(&amp;#39;Feature Importance Score&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;Feature Importance Score&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;Features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;Features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Visualizing Important Features&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Visualizing Important Features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file3490688e4c33_files/figure-html/unnamed-chunk-11-3.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Another way, which we will focus on in this post, is to use the
permutation importance score from the area of XAI. Permutation
importance is calculated by asking the following question: “If I
randomly shuffle a single column of the validation data, leaving the
target and all other columns in place, how would that affect the
accuracy of predictions in that now-shuffled data?”. Randomly
re-ordering a single column should cause less accurate predictions,
since the resulting data no longer corresponds to anything observed in
the real world. Model accuracy especially suffers if we shuffle a column
that the model relied on heavily for predictions. In our case, if we
mess with the “BEINGBULLIED” variable, the model would be severely
affected by the reduced prediction accuracy. The same would happen to
the variable “Parent_emosup”, “Positive_feel” and so forth as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;import eli5
from eli5.sklearn import PermutationImportance

FEATURES = X_test.columns.tolist()

perm = PermutationImportance(clf_rfc, random_state=RANDOM_STATE).fit(X_test, y_test)
eli5.show_weights(perm, feature_names = FEATURES, top = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;IPython.core.display.HTML object&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-xaiper-imp.png" style="width:40.0%" alt="" /&gt;
&lt;p class="caption"&gt;Permutation Importance&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The permutation importance results are consistent with the
feature importance score we extracted from the model. The values towards
the top are the most important features, and those towards the bottom
matter least. The first number in each row shows how much model
performance decreased with a random shuffling (in this case, using
“accuracy” as the performance metric). Like most things in data science,
there is some randomness to the exact performance change from a
shuffling a column. We measure the amount of randomness in our
permutation importance calculation by repeating the process with
multiple shuffles. The number after the ± measures how performance
varied from one-reshuffling to the next.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In our example, the most important feature was “BEINGBULLIED”,
which is the index of exposure to bullying. The index was constructed
from questions that ask if students have experienced bullying in the
past 12 months from statements such as “Other students left me out of
things on purpose”; “Other students made fun of me”; “I was threatened
by other students”. Positive values on this scale indicate that the
student was more exposed to bullying at school than the average student
in OECD countries; negative values on this scale indicate that the
student was less exposed to bullying at school than the average student
across OECD countries. This result is consistent with the literature
that students’ grade repetition is associated with the likelihood of
being bullied (&lt;a
href="https://journals.plos.org/Plosmedicine/article?id=10.1371/journal.pmed.1003846"&gt;Lian
et al., 2021&lt;/a&gt;; &lt;a
href="https://www.tandfonline.com/doi/full/10.1080/21683603.2019.1699215?casa_token=OB1MKY8CNuMAAAAA%3A5gwLS94ZgeACsQtZhKmiDLGtJUCu_qUMTtNyy_ftGl14WekcRoUErjezdZcOvI1s6bJEg_HYFIo"&gt;Ozada
Nazim &amp;amp; Duyan, 2019&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="partial-dependence-plots"&gt;Partial Dependence Plots&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;While feature importance shows what variables most affect
predictions, partial dependence plots show how a feature affects
predictions. For our case, partial dependence plots can be used to
answer questions such as “Controlling for all variables, what impact
does the index of exposure to bullying have on the prediction of grade
repetition?”. The interpretation of partial dependence plot is somewhat
similar to the interpretation of linear or logistic regression. On this
plot, The y axis is interpreted as change in the prediction from what it
would be predicted at the baseline or leftmost value. A blue shaded area
indicates level of confidence.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The plot below indicates that being subjected to bullying (as
reflected by having positive value of the variable) increases the
likelihood of students to repeat a grade. Positive values in this index
indicate that the student is more exposed to bullying at school than the
average student in OECD countries. Negative values in this index
indicate that the student is less exposed to bullying at school than the
average student in OECD countries; therefore, having zero does not mean
students did not experience any form of bullying, but rather
experiencing bullying to some degree (i.e., being bullied a bit).
However, the predicting power does not change much after 0, meaning that
the amount of exposure to bullying does not matter in predicting
students’ grade repetition.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;from pdpbox import pdp

pdp_bullied = pdp.pdp_isolate(model=clf_rfc, dataset=X_test, model_features=FEATURES, feature=&amp;#39;BEINGBULLIED&amp;#39;)

pdp.pdp_plot(pdp_bullied, &amp;#39;BEINGBULLIED&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(&amp;lt;Figure size 1500x950 with 2 Axes&amp;gt;, {&amp;#39;title_ax&amp;#39;: &amp;lt;AxesSubplot:&amp;gt;, &amp;#39;pdp_ax&amp;#39;: &amp;lt;AxesSubplot:xlabel=&amp;#39;BEINGBULLIED&amp;#39;&amp;gt;})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file3490688e4c33_files/figure-html/unnamed-chunk-13-5.png" width="1440" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Partial Dependence Plots can also be used to examine interactions
between variables as well. The graph below shows predictions for any
combination of students’ exposure to bullying and the amount of
emotional support from parents. The prediction power is highest when
students score 0 in the index of exposure to bullying (i.e., being
bullied a bit) and having scores on the index of parents’ emotional
support between -1.7 to +0.5. Positive values on this scale mean that
students perceived greater levels of emotional support from their
parents than did the average student across OECD countries while
negative value means otherwise. Having higher exposure to bullying
reduces prediction power of the model as indicated by the changing color
from yellow to green, and when the score in the index of emotional
support reaches 1, the score of the exposure to bullying index becomes
less matter as the prediction power reduces.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;features_to_plot = [&amp;#39;BEINGBULLIED&amp;#39;, &amp;#39;Parent_emosup&amp;#39;]

inter1  =  pdp.pdp_interact(model=clf_rfc, dataset=X_test, model_features=FEATURES, features=features_to_plot)

pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type=&amp;#39;contour&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(&amp;lt;Figure size 750x950 with 3 Axes&amp;gt;, {&amp;#39;title_ax&amp;#39;: &amp;lt;AxesSubplot:&amp;gt;, &amp;#39;pdp_inter_ax&amp;#39;: &amp;lt;AxesSubplot:xlabel=&amp;#39;BEINGBULLIED&amp;#39;, ylabel=&amp;#39;Parent_emosup&amp;#39;&amp;gt;})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file3490688e4c33_files/figure-html/unnamed-chunk-14-7.png" width="720" data-distill-preview=1 /&gt;&lt;/p&gt;
&lt;h3 id="shap-values"&gt;SHAP Values&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Finally, SHAP value allows us to interpret the prediction at a
fine-grained level to the components of individual predictions to show
the impact of each feature. For our case, CHAP value can be used to
answer questions like “On what basis did the model predict that student
A is likely to repeat a grade?”. The plot is quite straightforward to
interpret. The red part shows what increases the likelihood of repeating
a grade, and the blue part shows what decreases the likelihood of
repeating a grade.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On the plot below, the prediction is at the base alue of 0.60,
meaning that it is the average of the model output. For this particular
student, their likelihood to repeat a grade is increased by being
exposed to bullying (BEINGBULLIED), having mediocre emotional support
from parents (Parent_emosup), and having poor overall social standing as
indicated by -0.9 the variable the index of socio-economic, social and
cultural status (ESCS). However, the likelihood is decreased by their
educational resources at home (Home_resource), having cooperative class
(Stu_coop), their parents’ occupational status (Parent_occupation), and
having low record of class skipping (CLASSKIP).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-xaishap_10th_java.png" style="width:85.0%" alt="" /&gt;
&lt;p class="caption"&gt;SHAP Value for a prediction&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id="shap-summary-plot"&gt;SHAP Summary Plot&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In addition to the breakdown of each individual prediction, we
can also visualize groups of SHAP values with SHAP summary plot and SHAP
dependence contribution plot. SHAP summary plots give us an overview of
feature importance and what is driving the prediction. This plot is made
of many dots. Each dot has three characteristics as follows: a)
horizontal location (the x-axis) that indicates whether the effect of
that value caused a higher or lower prediction; b) vertical location
(the y-axis) that indicates the variable name, in order of importance
from top to bottom. c) Gradient color indicates the original value for
that variable. In booleans (i.e., yes/no variable), it will take two
colors, but in number it can contain the whole spectrum.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For example, the left most point in the ‘Parent_emosup’ row is
red in color, meaning that for that particular student, having greater
levels of emotional support from their parents reduces their likelihood
of repeating a grade by roughly 0.3. Seeing variables have a wide spread
in range can be inferred that permutation importance is high; however,
it is best to use permutation importance to measure which variable is
important to the prediction.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Some features such as &lt;code&gt;Home_resource&lt;/code&gt; (educational
resource at home) have reasonably clear separation between the blue and
pink dots, which implies a straightforward meaning that the increase in
the variable value lower (i.e., more resource) the likelihood of
repeating a grade while the decrease in educational resource impacts the
variable in the other direction (higher chance to repeat a
grade).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, some variables such as &lt;code&gt;Stu_coop&lt;/code&gt; (the degree
of cooperativeness within classrooms) have blue and pink dots jumbled
together, suggesting that the increase in this variable leads to higher
predictions, and other times it leads to a lower prediction. In other
words, both high and low values of the variable can have both positive
and negative effects on the prediction. The most likely explanation for
this “jumbling” of effects is that the variable (in this case
&lt;code&gt;Stu_coop&lt;/code&gt;) has an interaction effect with other variables.
For example, there may be some situations where cooperating with other
students lead to &lt;a
href="https://www.simplypsychology.org/social-loafing.html"&gt;social
loafing&lt;/a&gt; - when stduents contribute less effort when working as a
group, and therefore learns less. This interaction needs further
investigation.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;shap_values_summary = explainer.shap_values(X_test)
shap.summary_plot(shap_values[1], X_test)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-xaishap_summary.png" style="width:45.0%" alt="" /&gt;
&lt;p class="caption"&gt;SHAP Summary Plot&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id="shap-dependence-contribution-plot"&gt;SHAP Dependence Contribution
Plot&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The earlier Partial Dependence Plots to show how a single feature
impacts predictions. This is insightful and relevant for many real-world
use cases. The interpretation is also friendly to non-technical audience
as well. However, there is a lot that we still don’t know; for example,
what is the distribution of effects? Is the effect of having a certain
value pretty constant, or does it vary a lot depending on the values of
other feaures. SHAP dependence contribution plots provide a similar
insight to the partial dependence plot, but they add a lot more detail.
The plot shows scatter dots that explain how the effect a single feature
has on the predictions made by the model. The plot can be read as
follows: a) The x-axis is the value of the feature; b) The y-axis is the
SHAP value for that feature, which represents how much knowing that
feature’s value changes the output of the model prediction; c) The color
corresponds to a second feature that may have an interaction effect with
the feature we are plotting.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The plot below shows the relatively flat trend of the
&lt;code&gt;BEINGBULLIED&lt;/code&gt; feature, meaning that this variable does
impact the prediction regardless of the value; this trend is consistent
with the partial dependence plot shown earlier in the post. However,
there is a sign of interaction as there are points with similar value
that produce different outcome. See the left of the 2D pane, for
example. For some students, being less exposed to bullying gives them
more chance to repeat a grade while some students got less chance. There
might be other features that interact with this variable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;While the primary trend is that being bullied increases the
chance of repeating a grade , there are some variations that can be
explained by the interaction of features as well. For a concrete
explanation, see the right of the 2D pane. Being positioned overthere
means that those students experience a lot of bullying, but their chance
of repeating a grade is relatively lower than those who experience less
bullying. One explanation is that some of those students have positive
feelings for themselves (indicated by the red color), which could make
them more resilient toward being bullied.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;shap.dependence_plot(&amp;#39;BEINGBULLIED&amp;#39;, shap_values_summary[1], X_test, interaction_index=&amp;quot;Positive_feel&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-xaishap_dependence_bulliedXpositivefeel.png" style="width:60.0%"
alt="" /&gt;
&lt;p class="caption"&gt;SHAP Dependence Contribution Plot&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;What we have done so far is making a prediction with a Random Forest
Ensemble model, which has high predictive power at the price of being
challenging to explain due to its complexity (&lt;a
href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2822360/"&gt;Zhang &amp;amp;
Wang, 2009&lt;/a&gt;). XAI tools such as permutation importance, partial
dependence plot, and SHAP values, allow us to understand outputs of the
model at various levels from the overall picture to fine-grained
individual cases. Knowing how predictions are made also allow
establishes venues for future studies as well. XAI results are important
to bridge the knowledge gap between technical (e.g., developers) and
non-technical (e.g., customers, users) audiences, which could build
trust and confidence when putting the AI models into the actual use. XAI
also helps an organization develop a responsible approach to AI
development by avoiding the reliance on results that we do not
understand to inform our decisions.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;However, note that XAI is not perfect. Its results are
context-dependent, meaning that if the context changes, so does the
result (&lt;a
href="https://www.sciencedirect.com/science/article/pii/S0740624X21001027"&gt;de
Brujin et al., 2021&lt;/a&gt;). The prediction and how it happens can only be
used as a factor to be considered along with other lines of evidence
such as expert opinion, counter explanations, and potential
consequences. Regardless, XAI is still a useful too to have in expanding
the knowledge we get from machine learning. Thank you very much for
reading!&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>f40f1e89b04d8b250adbd706645111a3</distill:md5>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-28-xai</guid>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-28-xai/xai_files/figure-html5/unnamed-chunk-14-11.png" medium="image" type="image/png" width="1440" height="1824"/>
    </item>
    <item>
      <title>Addressing Data Imbalance with Semi-Supervised Learning</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-28-semisupervised</link>
      <description>For this post, I will use semi-supervised learning approach to perform a classification task with a highly imbalance data.  

(7 min read)</description>
      <category>Python</category>
      <category>Unsupervised Machine Learning</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-28-semisupervised</guid>
      <pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-28-semisupervised/semi-ml.png" medium="image" type="image/png" width="900" height="450"/>
    </item>
    <item>
      <title>Examining Customer Cluster with Unsupervised Machine Learning</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-18-unsupervisedml</link>
      <description>In this post, I will be using two unsupervised learning techniques with a data set, namely K-means clustering and Hierarchical clustering, to determine groups of customers from their age, income, and spending behavior data.  

(8 min read)</description>
      <category>Python</category>
      <category>Unsupervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-18-unsupervisedml</guid>
      <pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-18-unsupervisedml/unsupervisedml_files/figure-html5/unnamed-chunk-14-19.png" medium="image" type="image/png" width="2880" height="1152"/>
    </item>
    <item>
      <title>Combining Multiple Machine Learning Models with the Ensemble Methods</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-09-ensemble</link>
      <description>This entry explores different ways to combine supervised machine learning models to maximize their predictive capability.  

(13 min read)</description>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-09-ensemble</guid>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-09-ensemble/robot.png" medium="image" type="image/png" width="626" height="528"/>
    </item>
    <item>
      <title>Examining PISA 2018 Data Set with Statistical Learning Approach</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-02-27-statlearning</link>
      <description>In this post, I will be developing two statistical learning models, namely Linear Regression and Polynomial Regression, and apply it to Thai student data from the Programme for International Student Assessment (PISA) 2018 data set to examine the impact of classroom competition and cooperation to students' academic performance.  

(14 min read)</description>
      <category>R</category>
      <category>Statistics</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-02-27-statlearning</guid>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-02-27-statlearning/statlearn.jpeg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Classical Test Theory in R</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-01-15-ctt</link>
      <description>For this post, I will be analyzing characteristics of test items based on the framework of Classical Test Theory (CTT).

(13 min read)</description>
      <category>R</category>
      <category>Psychometric</category>
      <guid>https://taridwong.github.io/posts/2022-01-15-ctt</guid>
      <pubDate>Sat, 15 Jan 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-01-15-ctt/ctt_files/figure-html5/unnamed-chunk-28-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Examining the Big 5 personality Dataset with factor analysis</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-01-01-efacfa</link>
      <description>For this entry, I will be examining the Big 5 personality Inventory data set with Exploratory Data Analysis to identify potential structures of personality trait and verify them with Confirmatory Factor Analysis.

(8 min read)</description>
      <category>R</category>
      <category>Statistics</category>
      <guid>https://taridwong.github.io/posts/2022-01-01-efacfa</guid>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-01-01-efacfa/corrmatrix.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Measuring Text Similarity with Movie plot data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-29-movie-similarity</link>
      <description>For this post, I will be analyzing textual data of movie plots to determine their similarity with TF-IDF and Clustering.

(7 min read)</description>
      <category>Python</category>
      <category>Natural Language Processing</category>
      <category>Unsupervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-29-movie-similarity</guid>
      <pubDate>Wed, 29 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-29-movie-similarity/movie-similarity_files/figure-html5/unnamed-chunk-11-1.png" medium="image" type="image/png" width="484" height="483"/>
    </item>
    <item>
      <title>Missing Data Analysis</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-27-missingdata</link>
      <description>For this post, I will examine missing data in a large-scale dataset and discuss about numerous ways we can clean them as a part of data preparation.

(10 min read)</description>
      <category>R</category>
      <category>Data Visualization</category>
      <category>Statistics</category>
      <guid>https://taridwong.github.io/posts/2021-12-27-missingdata</guid>
      <pubDate>Mon, 27 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-27-missingdata/missingpic.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Applying Machine Learning to Audio Data: Visualization, Classification, and Recommendation</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data</link>
      <description>For this entry, I am trying my hands on audio data to extract its features for exploratory data analysis (EDA), using machine learning algorithms to perform music classification, and finally build up on that result to develop a recommendation system for music of similar characteristics.

(13 min read)</description>
      <category>Python</category>
      <category>Data Visualization</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data</guid>
      <pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data/applying-machine-learning-to-audio-data_files/figure-html5/unnamed-chunk-13-11.png" medium="image" type="image/png" width="3072" height="1152"/>
    </item>
    <item>
      <title>Interactive plots for Suicide Data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data</link>
      <description>For this entry, will be visualizing suicide data from 1958 to 2015 with interactive plots to communicate insights to non-technical audience.  

(14 min read)</description>
      <category>R</category>
      <category>Data Visualization</category>
      <category>Quantitative research</category>
      <guid>https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data</guid>
      <pubDate>Thu, 09 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data/preview.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Image Recognition with Artificial Neural Networks</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks</link>
      <description>In this entry, we will be developing a deep learning algorithm - a sub-field of machine learning inspired by the structure of human brain (neural networks) - to classify images of single digit number (0-9).  

(9 min read)</description>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <category>Deep Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks</guid>
      <pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks/deepnn.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Anomaly Detection with New York City taxi data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data</link>
      <description>In this entry, I will be conducting anomaly detection to identify points of anomaly in the taxi passengers data in New York City from July 2014 to January 2015 at half-hourly intervals.
 
 (4 min read)</description>
      <category>Unsupervised Machine Learning</category>
      <category>R</category>
      <guid>https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data</guid>
      <pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data/anomaly-detection-with-new-york-city-taxi-data_files/figure-html5/unnamed-chunk-9-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Crime mapping in San Francisco with police data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data</link>
      <description>In this entry, we will explore San Francisco crime data from 2016 to 2018 to understand the relationship between civilian-reported incidents of crime and police-reported incidents of crime. Along the way we will use table intersection methods to subset our data, aggregation methods to calculate important statistics, and simple visualizations to understand crime trends.  

(7 min read)</description>
      <category>Data Visualization</category>
      <category>R</category>
      <category>Quantitative research</category>
      <guid>https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data</guid>
      <pubDate>Tue, 30 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data/crime-mapping-in-san-francisco-with-police-data_files/figure-html5/unnamed-chunk-6-1.png" medium="image" type="image/png" width="1920" height="1920"/>
    </item>
    <item>
      <title>Exploring COVID-19 data from twitter with topic modeling</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds</link>
      <description>


&lt;h2 id="covid-19-situation-in-alberta-canada"&gt;COVID-19 situation in
Alberta, Canada&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The province of Alberta, Canada, has suffered from the COVID-19
pandemic like all other places. Alberta has gone through cycles of
reopening and returning to provincial lock down since April 2020. The
province, however, has lifted almost all restrictions and enacted its
reopening plan on the recent &lt;a
href="https://calgary.ctvnews.ca/alberta-moves-to-stage-3-of-reopening-plan-on-canada-day-1.5475913"&gt;Canada
day&lt;/a&gt; when 70% of Alberta population has received at least one dose of
&lt;a
href="https://www.canada.ca/en/public-health/services/diseases/coronavirus-disease-covid-19/vaccines.html"&gt;approved
COVID-19 vaccination&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The navigation of the province through this pandemic was led by
the Alberta’s Chief Medical Officer of Health, &lt;a
href="https://www.alberta.ca/office-of-the-chief-medical-officer-of-health.aspx"&gt;Dr. Deena
Hinshaw&lt;/a&gt;. Dr.Hinshaw usually held public health briefings almost
every day during wave 1 to wave 3 of the pandemic, but her communication
channel has changed in wave 4 as less public health briefing was held
and more tweets were posted on the &lt;a
href="https://twitter.com/CMOH_Alberta"&gt;her account&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For that, I believe we could use &lt;a
href="https://en.wikipedia.org/wiki/Natural_language_processing"&gt;Natural
Language Processing (NLP)&lt;/a&gt; techniques to extract themes and
characteristics from Dr.Hinshaw’s tweet to examine the essence of public
health messages since the provincial reopening date, specifically from
July 1st to October 31st, 2021.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="text-mining-and-word-cloud-fundamentals"&gt;Text mining and word
cloud fundamentals&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For this post, we will use text mining and word clouds to
initially explore characteristics of the data set. Text mining is an
exploratory method for textual data under Natural Language Processing
(NLP), a branch of Artificial Intelligence concerning the understanding
of words and spoken texts. NLP is also a type of unsupervised machine
learning approach to discover hidden structures in the data to inform
decisions made by experts of the subject matter.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Word cloud is also a popular way to to communicate findings from
textual data in a visually engaging way. The more frequent a word appear
in the data set (or corpus) the bigger that word will be in the
cloud.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will use Python to perform this analysis on R platform with
&lt;code&gt;reticulate::repl_python()&lt;/code&gt;. First of all, we will be
importing necessary modules and twitter data set that we mined from
Dr. Hinshaw’s account with &lt;code&gt;pd.read_csv&lt;/code&gt;. There are 538
tweets in total, and we can print out examples of the tweets via
&lt;code&gt;tweets_df.Text.head(5)&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;#Import necessary modules

import numpy as np #for numpy array
import pandas as pd #for data reading and processing
import matplotlib.pyplot as plt #for plotting
import re #for Regex text cleaning
from wordcloud import WordCloud, STOPWORDS #for word clouds
from nltk.stem import WordNetLemmatizer #to reduce text to base form
from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation as LDA #for topic modeling
import warnings

warnings.filterwarnings(&amp;quot;ignore&amp;quot;) #suppress the warning that Python kindly gave me

tweets_df = pd.read_csv(&amp;quot;text-query-tweets.csv&amp;quot;)

tweets_df.shape

# Print out the first rows of papers&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(538, 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(tweets_df.Text.head(5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    We all have the ability to take small actions ...
1    As we head into Halloween weekend, I encourage...
2    Sadly, 9 new deaths related to COVID-19 were a...
3    Over the past 24 hours, we ID’d 603 new cases ...
4    Here is a summary of the latest #COVID19AB num...
Name: Text, dtype: object&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="lets-clean-the-text-first"&gt;Let’s clean the text first&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;After we imported our data into the system, we have to clean our
data to get rid of textual elements that we do not need such as
punctuation, numbers, as well as convert all words to lower case.
Painful as it may be, this has to be done. It took me days (not that
much, but I felt it that way) to clean all of this and make sure that no
junk is left behind (well, there could be. Do let me know if you find
any).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The phrase “Garbage in, garbage out” is really applicable here in
data work context. If you let any junk (corrupted data) in, the most you
will get is processed junk. After we cleaned the text, let us print them
out again to see what they look like. All numbers are gone. All texts
are in lowercase. All URLs and punctuation is gone. Good
riddance!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ATTN nerds: Note that in the code below, we will pass the
original &lt;code&gt;Text&lt;/code&gt; column in &lt;code&gt;tweets_df&lt;/code&gt; to the
&lt;code&gt;re.sub&lt;/code&gt; function only once. For the second cleaning function
onward, we will pass &lt;code&gt;tweets_df['Text_processed']&lt;/code&gt; instead to
stack our text cleaning results on the same column. Yes, I wrote this to
remind myself because I struggled on it for hours (half an hour,
actually).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;
#remove all numbers from the text with list comprehension
tweets_df[&amp;#39;Text_processed&amp;#39;] = tweets_df[&amp;#39;Text&amp;#39;].map(lambda x: re.sub(r&amp;#39;[0-9]+&amp;#39;, &amp;#39;&amp;#39;, x))

# Remove punctuation
tweets_df[&amp;#39;Text_processed&amp;#39;] = tweets_df[&amp;#39;Text_processed&amp;#39;].map(lambda x: re.sub(r&amp;#39;[^\w\s\,\.!?]&amp;#39;, &amp;#39;&amp;#39;, x))

# Convert the tweets to lowercase
tweets_df[&amp;#39;Text_processed&amp;#39;] = tweets_df[&amp;#39;Text_processed&amp;#39;].map(lambda x: x.lower())

#Clean out URLs
tweets_df[&amp;#39;Text_processed&amp;#39;] = tweets_df[&amp;#39;Text_processed&amp;#39;].map(lambda x: re.sub(r&amp;quot;http\S+&amp;quot;, &amp;quot;&amp;quot;, x))

# Print the processed titles of the first rows 
print(tweets_df[&amp;#39;Text_processed&amp;#39;].head())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    we all have the ability to take small actions ...
1    as we head into halloween weekend, i encourage...
2    sadly,  new deaths related to covid were also ...
3    over the past  hours, we idd  new cases amp co...
4    here is a summary of the latest covidab number...
Name: Text_processed, dtype: object&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="so-this-is-whats-happening-over-time"&gt;So this is what’s
happening over time&lt;/h2&gt;
&lt;pre class="python"&gt;&lt;code&gt;
#Change datetime format to datetime
tweets_df[&amp;#39;Datetime&amp;#39;] = pd.to_datetime(tweets_df[&amp;#39;Datetime&amp;#39;])

#Extract month from datetime
tweets_df[&amp;#39;Month&amp;#39;] = tweets_df[&amp;#39;Datetime&amp;#39;].dt.month

# Group the papers by year
groups = tweets_df.groupby(&amp;#39;Month&amp;#39;)

# Determine the size of each group
counts = groups.size()

# Visualize the counts as a bar plot

# Vertical lines
plt.axvline(x = 7.0, color = &amp;#39;forestgreen&amp;#39;, label = &amp;#39;The reopening date&amp;#39;, linestyle=&amp;#39;--&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.lines.Line2D object at 0x0000017E20915C10&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.axvline(x = 8.0, color = &amp;#39;firebrick&amp;#39;, label = &amp;#39;Wave 4 started&amp;#39;, linestyle=&amp;#39;--&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.lines.Line2D object at 0x0000017E209251F0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.legend(bbox_to_anchor = (1.0, 1), loc = &amp;#39;upper right&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.legend.Legend object at 0x0000017E20915BE0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Tweet count across months&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Tweet count across months&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;quot;Tweet count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;Tweet count&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.xlabel(&amp;quot;Month&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;Month&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;counts.plot()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;AxesSubplot:title={&amp;#39;center&amp;#39;:&amp;#39;Tweet count across months&amp;#39;}, xlabel=&amp;#39;Month&amp;#39;, ylabel=&amp;#39;Tweet count&amp;#39;&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file3490c204dc_files/figure-html/unnamed-chunk-5-1.png" width="720" style="display: block; margin: auto;" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The line plot above represents tweet counts across months after the
provincial reopening date. The x-axis indicates months and the y-axis
indicates the number of twitter post of Dr. Hinshaw. The number of tweet
dropped slightly from July to August as cases decreased, but wave 4 of
the pandemic started in August as cases were on the rise again. We can
see that the number of cases aligns with the number of tweets posted on
Dr. Hinshaw’s account.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="lets-see-the-big-picture-with-word-cloud"&gt;Let’s see the big
picture with word cloud&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Now that we know the frequency of tweets over months, we can plot a
word cloud from our processed text to see the big picture of twitter
data. There are 114,362 words in total after combining all 538 tweets
together. The word cloud below suggests that “covid” was mentioned the
most during the past four months, following by “vaccine”, “new cases”,
and “unvaccinated”.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;
text_all = &amp;quot; &amp;quot;.join(tweet for tweet in tweets_df.Text_processed)
print (&amp;quot;There are {} words in the combination of all tweets&amp;quot;.format(len(text_all)))

#lemmatize all words&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;There are 114362 words in the combination of all tweets&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;lemmatizer = WordNetLemmatizer()
text_all = &amp;quot;&amp;quot;.join([lemmatizer.lemmatize(i) for i in text_all])

# Create Stopword list:
stopwords_cloud = set(STOPWORDS)
stopwords_cloud.update([&amp;quot;https://&amp;quot;, &amp;quot;(/)&amp;quot;, &amp;quot;Online:&amp;quot;, 
                        &amp;quot;Twitter:&amp;quot;, &amp;quot;Join&amp;quot;, &amp;quot;us&amp;quot;, &amp;quot;virtually&amp;quot;,
                        &amp;quot;pm&amp;quot;, &amp;quot;:&amp;quot;, &amp;quot;https&amp;quot;, &amp;quot;t&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;co&amp;quot;, &amp;quot;amp&amp;quot;, &amp;quot;will&amp;quot;])
                      
#Generate a word cloud image
wordcloud_tweet = WordCloud(stopwords=stopwords_cloud, background_color=&amp;quot;white&amp;quot;,random_state=7).generate(text_all)

#Display the generated image:
#the matplotlib way:
plt.figure(figsize=[10,10])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1000x1000 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.imshow(wordcloud_tweet, interpolation=&amp;#39;bilinear&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage object at 0x0000017E1E4D79A0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.axis(&amp;quot;off&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(-0.5, 399.5, 199.5, -0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file3490c204dc_files/figure-html/unnamed-chunk-6-3.png" width="960" style="display: block; margin: auto;" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The thing is, word cloud can only provide a rough visual
presentation for the characteristics of our textual data. We would need
to dive a little bit deeper to graphs and numbers to examine what is
truly going on. Let us visualize them all on a bar plot.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2
id="common-word-bar-plot-and-text-preprocessing-for-topic-modeling"&gt;Common
word bar plot and text preprocessing for topic modeling&lt;/h2&gt;
&lt;pre class="python"&gt;&lt;code&gt;
# Helper function to count common words

def plot_10_most_common_words(count_data, tfidf_vectorizer):
    import matplotlib.pyplot as plt
    words = tfidf_vectorizer.get_feature_names()
    total_counts = np.zeros(len(words))
    for t in count_data:
        total_counts+=t.toarray()[0]
    
    count_dict = (zip(words, total_counts))
    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]
    words = [w[0] for w in count_dict]
    counts = [w[1] for w in count_dict]
    x_pos = np.arange(len(words)) 

    plt.bar(x_pos, counts,align=&amp;#39;center&amp;#39;)
    plt.xticks(x_pos, words, rotation=90) 
    plt.xlabel(&amp;#39;words&amp;#39;)
    plt.ylabel(&amp;#39;counts&amp;#39;)
    plt.title(&amp;#39;10 most common words&amp;#39;)
    plt.show()

#Make your own list of stop words
my_additional_stop_words = (&amp;quot;https://&amp;quot;, &amp;quot;(/)&amp;quot;, &amp;quot;➡Online:&amp;quot;, 
                        &amp;quot;➡Twitter:&amp;quot;, &amp;quot;Join&amp;quot;, &amp;quot;us&amp;quot;, &amp;quot;virtually&amp;quot;,
                        &amp;quot;pm&amp;quot;, &amp;quot;:&amp;quot;, &amp;quot;https&amp;quot;, &amp;quot;t&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;co&amp;quot;, &amp;quot;amp&amp;quot;, &amp;quot;today&amp;quot;, &amp;quot;new&amp;quot;, &amp;quot;covid&amp;quot;,
                        &amp;quot;covidab&amp;quot;, &amp;quot;hours&amp;quot;, &amp;quot;completed&amp;quot;)
                        
stop_words_lda = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)    

# Initialize the count vectorizer with the English stop words
tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words_lda)

# Fit and transform the processed titles
count_data = tfidf_vectorizer.fit_transform(tweets_df[&amp;#39;Text_processed&amp;#39;])

# Visualise the 10 most common words
plot_10_most_common_words(count_data, tfidf_vectorizer)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file3490c204dc_files/figure-html/unnamed-chunk-7-5.png" width="960" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The bar plot above gave us a more detailed information of which
word occurs more frequently than the others based on the term
frequency–inverse document frequency (TFIDF) statistics. TFIDF gives
each word a weight that reflects its importance to a document. For
TFIDF, words that occur too frequent like “the” provides little meaning
while words rarely occur doesn’t tell us much as well. We are taking
about the COVID-19 pandemic here, so it is obvious that “vaccinated” is
going to be mentioned the most in Dr. Hinshaw’s tweet. “Cases” and
“unvaccinated” seem to be reasonable to be mentioned as the second- and
third most important words as the government of Alberta has been putting
more effort in identifying more cases in the province and encourage
unvaccinated individuals to get their vaccine.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will also create a &lt;code&gt;tfidf_vectorizer&lt;/code&gt; model with
our own list of stopwords (or words that have little meaning such as
“is, am, are”) to prepare our data for Latent Dirichlet Allocation (LDA)
topic modeling.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2
id="finally-lets-see-potential-topics-from-dr.-hinshaws-tweet"&gt;Finally,
let’s see potential topics from Dr. Hinshaw’s tweet&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Latent Dirichlet Allocation is a powerful natural language
processing technique that discovers hidden patterns in topic from
unstructured textual data with statistical models &lt;a
href="https://link.springer.com/content/pdf/10.1007/s11042-018-6894-4.pdf"&gt;(Jelodar
et al., 2019)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Here, we can use LDA to discover potential topics among the sea
of tweets posted by Dr. Hinshaw to find out what she talked about since
the provincial reopening and wave 4 of the pandemic. I have specified
the model to extract 8 topics from the data, with 5 words per topics.
Note that these numbers are arbitrary chosen.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If we extracted too few topics, we might not be able to capture
the whole picture of the data. On the other hand, extracting too much
topics could just give us more of the same overlapping themes. We need
to find the middle ground.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;
# Helper function to print out the topics
def print_topics(model, tfidf_vectorizer, n_top_words):
    words = tfidf_vectorizer.get_feature_names()
    for topic_idx, topic in enumerate(model.components_):
        print(&amp;quot;\nTopic #%d:&amp;quot; % topic_idx)
        print(&amp;quot; &amp;quot;.join([words[i]
                        for i in topic.argsort()[:-n_top_words - 1:-1]]))
                        
#How many topic and words per topic we want to see
number_topics = 8
number_words = 5 
                      
# Create and fit the LDA model
lda = LDA(n_components=number_topics, random_state = 1)
lda.fit(count_data)

# Print the topics found by the LDA model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;LatentDirichletAllocation(n_components=8, random_state=1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print_topics(lda, tfidf_vectorizer, number_words)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Topic #0:
twitter online join video transcript

Topic #1:
possible information protection protect soon

Topic #2:
vaccines protect vaccine dose book

Topic #3:
cases tests partially unvaccinated idd

Topic #4:
reported deaths sadly condolences alberta

Topic #5:
oct age steps important dr

Topic #6:
matter pandemic report continue health

Topic #7:
ahs participating prevent book available&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="wrapping-up-here.-what-can-we-conclude"&gt;Wrapping up here. What
can we conclude?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The topics we discovered above can be inferred as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Topic 0: An invitation for the general population to join a live
update video on Twitter.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Topic 1: The availability of possible information on COVID-19
protection&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Topic 2: Encouragement to book for a vaccination for more
protection.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Topic 3: The proportion of unvaxxed vs vaxxed vs partiallyvaxxed
patients.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Topic 4: Covid-related death.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The insights that we gained could also be further supported by
opinion from public health experts as they could provide information at
a greater depth into their field.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;From what we have discussed so far, we can see that with the
right tool, LDA for our case, we could take advantage of the vast
availability of textual data that revolves around us in our everyday
lives and use that information to deepen our understanding of social
phenomena. We could explore how students opinion changed from pre- to
post-COVID era, or we could use this technique to media transcription of
social events such as political protests, election speech, or even
product review in the marketing field. Thank you for your
reading!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>150e32d23689517567eba0ddc47378ec</distill:md5>
      <category>Python</category>
      <category>Natural Language Processing</category>
      <category>Unsupervised Machine Learning</category>
      <category>COVID-19</category>
      <guid>https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds</guid>
      <pubDate>Tue, 23 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds/word-cloud-pv.png" medium="image" type="image/png" width="1920" height="1920"/>
    </item>
    <item>
      <title>Finding a home among the paradigm push-back with Dialectical Pluralism</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-07-introducing-dialectical-pluralism</link>
      <description>This entry discusses the reconciliation of quantitative and qualitative worldviews amidst the paradigm wars with pluralistic stance.

(2 min read)</description>
      <category>Mixed methods research</category>
      <guid>https://taridwong.github.io/posts/2021-11-07-introducing-dialectical-pluralism</guid>
      <pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://www.relevantinsights.com/wp-content/uploads/2020/05/Qualitative-Quantitative-Research-For-New-Product-Development.png" medium="image" type="image/png"/>
    </item>
  </channel>
</rss>
