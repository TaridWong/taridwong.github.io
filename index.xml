<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>Tarid Wongvorachan</title>
    <link>https://taridwong.github.io/</link>
    <atom:link href="https://taridwong.github.io/index.xml" rel="self" type="application/rss+xml"/>
    <description>Welcome to my data science blog!
</description>
    <generator>Distill</generator>
    <lastBuildDate>Sun, 15 May 2022 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Item Response Theory</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-05-15-irt</link>
      <description>


&lt;h2 id="introduction-to-item-response-theory"&gt;Introduction to Item
Response Theory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://taridwong.github.io/posts/2022-01-15-ctt/"&gt;My
previous post on Classical Test Theory (CTT)&lt;/a&gt; discussed how it has
several disadvantages that limit its interpretation to a certain group
of population and therefore reduces its utility to test development.
Specifically, generalizability of the test scores from CTT is quite
limited due to item/test dependency; item parameters such as item
difficulty, item discrimination, and reliability estimates are dependent
upon test scores, which are derived from a group of sample (&lt;a
href="https://www.jstor.org/stable/41219505?casa_token=PZl4ti6FHboAAAAA%3AEsuXjTVftCbh7pnyqQVhGv5cCEG9nAWMNV36MC7nXp5svyUaJhXBQG6jYtcmYNNf3S3b7HkhG48gjFTDo_ftMvXvm6vwRAuQwiEretfpuoKpwFiUQw&amp;amp;seq=1"&gt;DeVellis,
2006&lt;/a&gt;). If we change our sample, those parameters might change. Also,
If we administer two forms of the same test (i.e., form A and form B) to
the same examinee, we still cannot guarantee that they will obtain the
same score on both tests. Raw scores of a CTT-based test do not reflect
learning progress of an examinee as CTT-based scores are not comparable
across time (&lt;a
href="https://www.jstor.org/stable/41219505?casa_token=PZl4ti6FHboAAAAA%3AEsuXjTVftCbh7pnyqQVhGv5cCEG9nAWMNV36MC7nXp5svyUaJhXBQG6jYtcmYNNf3S3b7HkhG48gjFTDo_ftMvXvm6vwRAuQwiEretfpuoKpwFiUQw&amp;amp;seq=1"&gt;DeVellis,
2006&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Such limitations can be addressed by Item Response Theory (IRT).
IRT is able to link information from test items to examinee performance
on the same scale to provide information on the specific domain of
interest and ability of the examinee (θ) (&lt;a
href="https://www.routledge.com/Handbook-of-Item-Response-Theory-Volume-3-Applications/Linden/p/book/9780367221188"&gt;Hambleton
&amp;amp; Zenisky, 2018&lt;/a&gt;). The relationship between observable items and
examinee performance is explained through &lt;em&gt;Item Characteristic Curve
(ICC)&lt;/em&gt;, which explains the probability of getting an item(s)
correctly given the current ability level and parameter (&lt;a
href="https://doi.org/10.1111/j.1745-3992.1993.tb00543.x"&gt;Hambleton
&amp;amp; Jones, 1993&lt;/a&gt;). Therefore, IRT allows researchers to predict
examinees’ expected test score given their ability level. In this post,
I will be examining characteristics of test items based on the IRT
framework. The R packages I will be using are &lt;a
href="https://cran.r-project.org/web/packages/ltm/ltm.pdf"&gt;&lt;code&gt;ltm&lt;/code&gt;&lt;/a&gt;
and &lt;a
href="https://cran.r-project.org/web/packages/mirt/mirt.pdf"&gt;&lt;code&gt;mirt&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(ltm)
library(mirt)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;First, let’s load in a data set. I will be using the data from Law
School Admission Test (LSAT), N = 1000, 5 items. The data can be called
with &lt;code&gt;data(LSAT).&lt;/code&gt;As an initial step, we can use
&lt;code&gt;ltm::descript&lt;/code&gt; for descriptive statistics.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;data(LSAT)
ltm::descript(LSAT)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Descriptive statistics for the &amp;#39;LSAT&amp;#39; data-set

Sample:
 5 items and 1000 sample units; 0 missing values

Proportions for each level of response:
           0     1  logit
Item 1 0.076 0.924 2.4980
Item 2 0.291 0.709 0.8905
Item 3 0.447 0.553 0.2128
Item 4 0.237 0.763 1.1692
Item 5 0.130 0.870 1.9010


Frequencies of total scores:
     0  1  2   3   4   5
Freq 3 20 85 237 357 298


Point Biserial correlation with Total Score:
       Included Excluded
Item 1   0.3620   0.1128
Item 2   0.5668   0.1532
Item 3   0.6184   0.1728
Item 4   0.5344   0.1444
Item 5   0.4354   0.1216


Cronbach&amp;#39;s alpha:
                  value
All Items        0.2950
Excluding Item 1 0.2754
Excluding Item 2 0.2376
Excluding Item 3 0.2168
Excluding Item 4 0.2459
Excluding Item 5 0.2663


Pairwise Associations:
   Item i Item j p.value
1       1      5   0.565
2       1      4   0.208
3       3      5   0.113
4       2      4   0.059
5       1      2   0.028
6       2      5   0.009
7       1      3   0.003
8       4      5   0.002
9       3      4   7e-04
10      2      3   4e-04&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;From the output above, inspection of non significant results can be
used to reveal ‘problematic’ items in pairwise association. Latent
variable models assume that the high associations between items can be
explained by a set of latent variables, so any pair of items that is not
related to each other violates this assumption. Additionally, Item 1
seems to be the easiest item as seen from its highest proportion of
correct response.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="dichotomous-item"&gt;Dichotomous Item&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We will be performing IRT analyses on dichotomous items, which are
items that only have two possible scores of incorrect (0) and correct
(1). The three most common dichotomous IRT models are Rasch/1-parameter
logistics model (1PL), 2-parameter logistics model (2PL), and
3-parameter logistics model(3PL).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="rasch-model-1pl"&gt;Rasch Model (1PL)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We will fit the original Rasch model, which fixes the item
discrimination (aka &lt;em&gt;a&lt;/em&gt; parameter) of all items to 1 to the data.
The 1PL (also called &lt;em&gt;Rasch model&lt;/em&gt;) model describes test items in
terms of only one parameter, &lt;em&gt;item difficulty&lt;/em&gt; (aka &lt;em&gt;b&lt;/em&gt;
parameter). Item difficulty is simply how hard an item is (how high does
one’s latent ability level need to be in order to have a 50% chance of
getting the item right?). &lt;em&gt;b-parameter&lt;/em&gt; is estimated for each
item of the test.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;ltm::rasch()&lt;/code&gt; assumes equal a-parameter across
items with an estimated value. In order to impose the constraint = 1,
the &lt;code&gt;constraint&lt;/code&gt; argument is used. This argument accepts a
two-column matrix where the first column denotes the parameter and the
second column indicates the value at which the corresponding parameter
should be fixed.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_rasch &amp;lt;- rasch(LSAT, constraint = cbind(length(LSAT) + 1, 1))

summary(mod_rasch)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = cbind(length(LSAT) + 1, 1))

Model Summary:
   log.Lik      AIC      BIC
 -2473.054 4956.108 4980.646

Coefficients:
                value std.err   z.vals
Dffclt.Item 1 -2.8720  0.1287 -22.3066
Dffclt.Item 2 -1.0630  0.0821 -12.9458
Dffclt.Item 3 -0.2576  0.0766  -3.3635
Dffclt.Item 4 -1.3881  0.0865 -16.0478
Dffclt.Item 5 -2.2188  0.1048 -21.1660
Dscrmn         1.0000      NA       NA

Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 6.3e-05 
quasi-Newton: BFGS &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The results of the descriptive analysis are also validated by the
model fit, where items 3 and 1 are the most difficult and the easiest
respectively (the lower the &lt;em&gt;b&lt;/em&gt;-parameter value, the easier). The
parameter estimates can be transformed to probability estimates using
the &lt;code&gt;coef()&lt;/code&gt; method&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;coef(mod_rasch, prob = TRUE, order = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           Dffclt Dscrmn P(x=1|z=0)
Item 1 -2.8719712      1  0.9464434
Item 5 -2.2187785      1  0.9019232
Item 4 -1.3880588      1  0.8002822
Item 2 -1.0630294      1  0.7432690
Item 3 -0.2576109      1  0.5640489&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The last column denotes the probability of a positive response
(getting the item correctly) to the &lt;em&gt;i&lt;/em&gt;th item for the average
individual. The argument &lt;code&gt;order = TRUE&lt;/code&gt; indicates the output
to sort the items according to the difficulty estimates. In order to
check the fit of the model to the data, the argument
&lt;code&gt;GoF.rasch()&lt;/code&gt; and &lt;code&gt;margins()&lt;/code&gt; can be used. The
former argument performs a parametric Bootstrap goodness-of-fit test
using Pearson’s Chi-square statistics, while the latter examines the
two- and three-way chi-square residual analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;GoF.rasch(mod_rasch, B = 199) # B = Bootstrap sample&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Bootstrap Goodness-of-Fit using Pearson chi-squared

Call:
rasch(data = LSAT, constraint = cbind(length(LSAT) + 1, 1))

Tobs: 30.6 
# data-sets: 200 
p-value: 0.25 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Based on 200 data-points, the non significant p-value suggests an
acceptable fit of the model. The null hypothesis states that the
observed data and the model fit with each other (no differences). Now,
for two-way margin analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_rasch)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = cbind(length(LSAT) + 1, 1))

Fit on the Two-Way Margins

Response: (0,0)
  Item i Item j Obs   Exp (O-E)^2/E  
1      2      4  81 98.69      3.17  
2      1      5  12 18.45      2.25  
3      3      5  67 80.04      2.12  

Response: (1,0)
  Item i Item j Obs    Exp (O-E)^2/E  
1      3      5  63  51.62      2.51  
2      2      4 156 139.78      1.88  
3      3      4 108  99.42      0.74  

Response: (0,1)
  Item i Item j Obs    Exp (O-E)^2/E  
1      2      4 210 193.47      1.41  
2      2      3 135 125.07      0.79  
3      1      4  53  47.24      0.70  

Response: (1,1)
  Item i Item j Obs    Exp (O-E)^2/E  
1      2      4 553 568.06      0.40  
2      3      5 490 501.43      0.26  
3      2      3 418 427.98      0.23  &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;From the above output, using the 3.5 rule of thumb, the value of all
two-way combinations are below the cut-off (same way of how statistical
hypothesis works) and therefore indicate a good fit to the two-way
margins. Next, we will examine the fit to the three-way margins.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_rasch, type = &amp;quot;three-way&amp;quot;, nprint = 2) #nprint returns 2 highest residual values for each combinations&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = cbind(length(LSAT) + 1, 1))

Fit on the Three-Way Margins

Response: (0,0,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E    
1      2      3      4  48 66.07      4.94 ***
2      1      3      5   6 13.58      4.23 ***

Response: (1,0,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      2      4  70 82.01      1.76  
2      2      4      5  28 22.75      1.21  

Response: (0,1,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      2      5   3  7.73      2.90  
2      3      4      5  37 45.58      1.61  

Response: (1,1,0)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      3      4      5  48  36.91      3.33  
2      1      2      4 144 126.35      2.47  

Response: (0,0,1)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      3      5  41 34.58      1.19  
2      2      4      5  64 72.26      0.94  

Response: (1,0,1)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      1      2      4 190 174.87      1.31  
2      1      2      3 126 114.66      1.12  

Response: (0,1,1)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      2      5  42 34.35      1.70  
2      1      4      5  46 38.23      1.58  

Response: (1,1,1)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      3      4      5 397 416.73      0.93  
2      2      3      4 343 361.18      0.91  

&amp;#39;***&amp;#39; denotes a chi-squared residual greater than 3.5 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The three-way margins suggest a problematic fit for two triplets of
items, both containing item 3. We can try fitting the unconstrained
version of Rasch model (not fixing the &lt;em&gt;a&lt;/em&gt;-parameter to 1) to see
the difference. This time, no need for the &lt;code&gt;constraint&lt;/code&gt;
argument.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_1pl &amp;lt;- rasch(LSAT, constraint = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;After fitting the 1PL model, we will request for Item
Characteristics Curve (ICC), Item Information Curve (IIC), Test
Information Function (TIF), Latent Ability Curve of the examinees, and
Uni-dimensionality Plot of the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mfrow = c(2, 3))

plot(mod_1pl, type=c(&amp;quot;ICC&amp;quot;), 
     legend = TRUE, cx = &amp;quot;bottomright&amp;quot;, lwd = 2, 
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;1PL Item Characteristics Curve&amp;quot;)

plot(mod_1pl,type=c(&amp;quot;IIC&amp;quot;),
     legend = TRUE, cx = &amp;quot;topright&amp;quot;, lwd = 2,
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;1PL Item Information Curve&amp;quot;)

plot(mod_1pl,type=c(&amp;quot;IIC&amp;quot;),items=c(0), lwd = 2,
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;Test Information Function&amp;quot;)

plot(0:1, 0:1, type = &amp;quot;n&amp;quot;, ann = FALSE, axes = FALSE)
info1 &amp;lt;- information(mod_1pl, c(-4, 0)) #Item information on the negative area of theta (-4 to 0)
info2 &amp;lt;- information(mod_1pl, c(0, 4)) #Item information on the positive area of theta (0 to 4)
text(0.5, 0.5, labels = paste(&amp;quot;Total Information:&amp;quot;, round(info1$InfoTotal, 3),
                               &amp;quot;\n\nInformation in (-4, 0):&amp;quot;, round(info1$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info1$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;),
                               &amp;quot;\n\nInformation in (0, 4):&amp;quot;, round(info2$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info2$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;)))

theta.rasch&amp;lt;-ltm::factor.scores(mod_1pl)
summary(theta.rasch$score.dat$z1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-1.9104 -0.9594 -0.4660 -0.6867 -0.4660  0.5930 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(theta.rasch, main = &amp;quot;Latent Ability of the Examinee&amp;quot;)

unitest_1pl &amp;lt;- unidimTest(mod_1pl,LSAT)
plot(unitest_1pl, type = &amp;quot;b&amp;quot;, pch = 1:2, main = &amp;quot;Modified Parallel Analysis Plot&amp;quot;)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Real Data&amp;quot;, &amp;quot;Average Simulated Data&amp;quot;), lty = 1, 
    pch = 1:2, col = 1:2, bty = &amp;quot;n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file38c4da22ad3_files/figure-html/unnamed-chunk-11-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The first plot is ICC of out 1PL model. ICC shows the
relationship between examinee ability (θ) and the probability of
examinees answering an item correctly based on their ability. On ICC,
item discrimination is represented by the steepness of the curve, and
item difficulty is represented by the position where the probability of
getting the item correct is 0.5.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Try comparing item 1 and item 3. For item 1, the point where the
probability of getting the item correctly is 0.5 matches with the
ability point -4 on the X-axis. However, for item 3, the point where the
probability of getting the item correctly is 0.5 matches with the
ability point 0 on the X-axis. In other words, you need more ability to
get item 3 correct than item 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The second plot is IIC. IIC shows how much “information” about
the latent trait ability an item can provide. Item information curves
peak at the point of difficulty value, where the item has the highest
discrimination and the probability of answering the item correctly is
0.5. To put it in plain language, a very difficult item will provide
very little information about persons with low ability (because the item
is already too hard), and very easy items will provide little
information about persons with high ability levels (because it is too
easy).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The third plot is TIF of the whole test. This is simply the sum
of the individual IICs above. The curves shows how much information this
test offers in terms of ability level of examinees. Ideally, we want a
test which provides fairly good coverage of a wide range of latent
ability levels. Otherwise, the test is only good at identifying a
limited range of examinees. The current TIF shows that more information
is yielded around examinees with -2 ability level. The test could use
more items for people with high ability (more difficult item is needed).
In particular, the amount of Test Information for ability levels in the
interval (-4 - 0) is almost 60%, and the item that seems to distinguish
between respondents with higher ability levels is item 3 as it is the
most difficult item.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Latent Ability Curve shows the distribution of examinee’s
latent ability level. The plot shows that most examinees are located
around 0 to 1 ability level.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;unitest_1pl&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Unidimensionality Check using Modified Parallel Analysis

Call:
rasch(data = LSAT, constraint = NULL)

Matrix of tertachoric correlations
       Item 1 Item 2 Item 3 Item 4 Item 5
Item 1 1.0000 0.1703 0.2275 0.1072 0.0665
Item 2 0.1703 1.0000 0.1891 0.1111 0.1724
Item 3 0.2275 0.1891 1.0000 0.1867 0.1055
Item 4 0.1072 0.1111 0.1867 1.0000 0.2009
Item 5 0.0665 0.1724 0.1055 0.2009 1.0000

Alternative hypothesis: the second eigenvalue of the observed data is substantially larger 
            than the second eigenvalue of data under the assumed IRT model

Second eigenvalue in the observed data: 0.2254
Average of second eigenvalues in Monte Carlo samples: 0.2474
Monte Carlo samples: 100
p-value: 0.6535&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The last plot is a test for unidimensionality of the test with
modified parallel analysis &lt;a
href="https://psycnet.apa.org/record/1983-31736-001"&gt;(Drasgpw &amp;amp;
Lissak, 1983&lt;/a&gt;). The output above shows that the result is
non-significant, meaning that the 1PL model fits the data well and we
are actually measuring a single trait here. The data is a law school
test, so it should be measuring contents about law, not maths or
English. The unidimensionality analysis shows that the test is measuring
what it is intended to measure.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;summary(mod_1pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = NULL)

Model Summary:
   log.Lik      AIC      BIC
 -2466.938 4945.875 4975.322

Coefficients:
                value std.err   z.vals
Dffclt.Item 1 -3.6153  0.3266 -11.0680
Dffclt.Item 2 -1.3224  0.1422  -9.3009
Dffclt.Item 3 -0.3176  0.0977  -3.2518
Dffclt.Item 4 -1.7301  0.1691 -10.2290
Dffclt.Item 5 -2.7802  0.2510 -11.0743
Dscrmn         0.7551  0.0694  10.8757

Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 2.9e-05 
quasi-Newton: BFGS &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;coef(mod_1pl, prob = TRUE, order = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           Dffclt    Dscrmn P(x=1|z=0)
Item 1 -3.6152665 0.7551347  0.9387746
Item 5 -2.7801716 0.7551347  0.8908453
Item 4 -1.7300903 0.7551347  0.7869187
Item 2 -1.3224208 0.7551347  0.7307844
Item 3 -0.3176306 0.7551347  0.5596777&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The output above suggests that the discrimination parameter of our
unconstrained model is different from 1, meaning that our constrained
and unconstrained Rasch models are different. The difference can be
tested with a likelihood ratio test using &lt;code&gt;anova().&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;anova(mod_rasch, mod_1pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 Likelihood Ratio Table
              AIC     BIC  log.Lik   LRT df p.value
mod_rasch 4956.11 4980.65 -2473.05                 
mod_1pl   4945.88 4975.32 -2466.94 12.23  1  &amp;lt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;By comparing model summary of the constrained and unconstrained
version, the latter is more suitable for the LSAT data due to its
smaller Akaike’s Information Criterion (AIC) and Bayesian Information
Criterion (BIC) values. AIC and BIC are measures of model performance
that account for model complexity. AIC is a measure that determines
which model fits the data better. The lower the score, the better fit
the model is. Similarly for BIC, the score measures complexity of the
model. BIC penalizes the model more for its complexity, meaning that
more complex models will have a worse (larger) score and will, in turn,
be less likely to be selected.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can double check the result by testing the unconstrained model
with the three-way margins, which yields a problematic fit with the
constrained model.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_1pl, type = &amp;quot;three-way&amp;quot;, nprint = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = NULL)

Fit on the Three-Way Margins

Response: (0,0,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      3      5   6  9.40      1.23  
2      3      4      5  30 25.85      0.67  

Response: (1,0,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      2      4      5  28 22.75      1.21  
2      2      3      4  81 74.44      0.58  

Response: (0,1,0)
  Item i Item j Item k Obs  Exp (O-E)^2/E  
1      1      2      5   3 7.58      2.76  
2      1      3      4   5 9.21      1.92  

Response: (1,1,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      2      4      5  51 57.49      0.73  
2      3      4      5  48 42.75      0.64  

Response: (0,0,1)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      1      3      5  41  33.07      1.90  
2      2      3      4 108 101.28      0.45  

Response: (1,0,1)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      2      3      4 210 218.91      0.36  
2      1      2      4 190 185.56      0.11  

Response: (0,1,1)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      3      5  23 28.38      1.02  
2      1      4      5  46 42.51      0.29  

Response: (1,1,1)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      1      2      4 520 526.36      0.08  
2      1      2      3 398 393.30      0.06  &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The new three-way margins suggests a good fit with the unconstrained
Rasch model. Finally, we investigate two more possible extensions of the
unconstrained Rasch model, the two-parameter logistic (2PL) model that
assumes a different discrimination parameter per item, and Rasch model
that incorporates a guessing parameter (3PL).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="parameter-logistics-model-2pl"&gt;2 Parameter Logistics Model
(2PL)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The 2PL model has the same equation as the 1PL model, but unlike
1PL, 2PL allows item discrimination and item difficulty to vary across
items instead of fixing it to a constant value.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The 2PL model can also be fitted with &lt;code&gt;ltm()&lt;/code&gt;.The
formula of &lt;code&gt;ltm()&lt;/code&gt; is two-sided, where its left is either a
data frame or a matrix, and its right allows only &lt;code&gt;z1&lt;/code&gt; and/or
&lt;code&gt;z2&lt;/code&gt;. Latent variables with &lt;code&gt;z2&lt;/code&gt; serves in the
case of interaction.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_2pl &amp;lt;- ltm(LSAT ~ z1)
summary(mod_2pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
ltm(formula = LSAT ~ z1)

Model Summary:
   log.Lik      AIC      BIC
 -2466.653 4953.307 5002.384

Coefficients:
                value std.err  z.vals
Dffclt.Item 1 -3.3597  0.8669 -3.8754
Dffclt.Item 2 -1.3696  0.3073 -4.4565
Dffclt.Item 3 -0.2799  0.0997 -2.8083
Dffclt.Item 4 -1.8659  0.4341 -4.2982
Dffclt.Item 5 -3.1236  0.8700 -3.5904
Dscrmn.Item 1  0.8254  0.2581  3.1983
Dscrmn.Item 2  0.7229  0.1867  3.8721
Dscrmn.Item 3  0.8905  0.2326  3.8281
Dscrmn.Item 4  0.6886  0.1852  3.7186
Dscrmn.Item 5  0.6575  0.2100  3.1306

Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 0.024 
quasi-Newton: BFGS &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The output above shows that the model estimated both item difficulty
and item discrimination. Next, we can try comparing our 1PL model with
the newly fitted 2PL model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#compare
anova(mod_1pl, mod_2pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 Likelihood Ratio Table
            AIC     BIC  log.Lik  LRT df p.value
mod_1pl 4945.88 4975.32 -2466.94                
mod_2pl 4953.31 5002.38 -2466.65 0.57  4   0.967&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The comparison suggested no significant difference between both
models. Next, we ca nrequest for ICC, IIC, TIF, Latent Ability
Distribution, and Unidimensionality Plot like we did with the 1PL
model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mfrow = c(2, 3))

plot(mod_2pl, type=c(&amp;quot;ICC&amp;quot;), 
     legend = TRUE, cx = &amp;quot;bottomright&amp;quot;, lwd = 2, 
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;2PL Item Characteristics Curve&amp;quot;)

plot(mod_2pl,type=c(&amp;quot;IIC&amp;quot;),
     legend = TRUE, cx = &amp;quot;topright&amp;quot;, lwd = 2,
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;2PL Item Information Curve&amp;quot;)

plot(mod_2pl,type=c(&amp;quot;IIC&amp;quot;),items=c(0), lwd = 2,
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;Test Information Function&amp;quot;)

plot(0:1, 0:1, type = &amp;quot;n&amp;quot;, ann = FALSE, axes = FALSE)
info1_2pl &amp;lt;- information(mod_2pl, c(-4, 0)) #Item information on the negative area of theta (-4 to 0)
info2_2pl &amp;lt;- information(mod_2pl, c(0, 4)) #Item information on the positive area of theta (0 to 4)
text(0.5, 0.5, labels = paste(&amp;quot;Total Information:&amp;quot;, round(info1_2pl$InfoTotal, 3),
                               &amp;quot;\n\nInformation in (-4, 0):&amp;quot;, round(info1_2pl$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info1_2pl$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;),
                               &amp;quot;\n\nInformation in (0, 4):&amp;quot;, round(info2_2pl$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info2_2pl$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;)))

theta.2pl&amp;lt;-ltm::factor.scores(mod_2pl)
summary(theta.2pl$score.dat$z1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-1.8953 -1.0026 -0.5397 -0.6629 -0.3572  0.6064 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(theta.2pl, main = &amp;quot;Latent ability scores of the participants 2PL&amp;quot;)

unitest_2pl &amp;lt;- unidimTest(mod_2pl,LSAT)
plot(unitest_2pl, type = &amp;quot;b&amp;quot;, pch = 1:2)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Real Data&amp;quot;, &amp;quot;Average Simulated Data&amp;quot;), lty = 1, 
    pch = 1:2, col = 1:2, bty = &amp;quot;n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file38c4da22ad3_files/figure-html/unnamed-chunk-18-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ICC and IIC of the two models are different, meaning that when we
allow item discrimination to vary, characteristics and yielded
information of each item also changed accordingly.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;unitest_2pl&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Unidimensionality Check using Modified Parallel Analysis

Call:
ltm(formula = LSAT ~ z1)

Matrix of tertachoric correlations
       Item 1 Item 2 Item 3 Item 4 Item 5
Item 1 1.0000 0.1703 0.2275 0.1072 0.0665
Item 2 0.1703 1.0000 0.1891 0.1111 0.1724
Item 3 0.2275 0.1891 1.0000 0.1867 0.1055
Item 4 0.1072 0.1111 0.1867 1.0000 0.2009
Item 5 0.0665 0.1724 0.1055 0.2009 1.0000

Alternative hypothesis: the second eigenvalue of the observed data is substantially larger 
            than the second eigenvalue of data under the assumed IRT model

Second eigenvalue in the observed data: 0.2254
Average of second eigenvalues in Monte Carlo samples: 0.2642
Monte Carlo samples: 100
p-value: 0.7327&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The above results of unidimensionality testing also suggests that
the test measures only one construct. When considering two models
together, it might be more preferable for us to choose the 1PL model as
there is no difference between both 1PL and 2PL; however, by nature, 1PL
model is more simple and easier to explain comparing to 2PL. Let us try
fitting the data to a 3PL model just in case.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="parameter-logistics-model-3pl"&gt;3 Parameter Logistics Model
(3PL)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The 3PL model is very similar to the 2PL model; however, the model
includes an additional parameter: lower asymptote (also known as the
guessing parameter). Under this model, individuals with zero ability
have a nonzero chance of correctly answering any item just by guessing
randomly. A 3PL model can be fitted with &lt;code&gt;tpm()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_3pl &amp;lt;- tpm(LSAT, type = &amp;quot;latent.trait&amp;quot;, max.guessing = 1)

summary(mod_3pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
tpm(data = LSAT, type = &amp;quot;latent.trait&amp;quot;, max.guessing = 1)

Model Summary:
  log.Lik      AIC      BIC
 -2466.66 4963.319 5036.935

Coefficients:
                value std.err  z.vals
Gussng.Item 1  0.0374  0.8650  0.0432
Gussng.Item 2  0.0777  2.5282  0.0307
Gussng.Item 3  0.0118  0.2815  0.0419
Gussng.Item 4  0.0353  0.5769  0.0612
Gussng.Item 5  0.0532  1.5596  0.0341
Dffclt.Item 1 -3.2965  1.7788 -1.8532
Dffclt.Item 2 -1.1451  7.5166 -0.1523
Dffclt.Item 3 -0.2490  0.7527 -0.3308
Dffclt.Item 4 -1.7658  1.6162 -1.0925
Dffclt.Item 5 -2.9902  4.0606 -0.7364
Dscrmn.Item 1  0.8286  0.2877  2.8797
Dscrmn.Item 2  0.7604  1.3774  0.5520
Dscrmn.Item 3  0.9016  0.4190  2.1516
Dscrmn.Item 4  0.7007  0.2574  2.7219
Dscrmn.Item 5  0.6658  0.3282  2.0284

Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Optimizer: optim (BFGS)
Convergence: 0 
max(|grad|): 0.028 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The model summary above suggested that all three item parameters of
item discrimination (&lt;em&gt;a&lt;/em&gt;), item difficulty (&lt;em&gt;b&lt;/em&gt;), and item
guessing (&lt;em&gt;c&lt;/em&gt;) are allowed to vary. Like what we did, we can try
requesting for ICC, IIC, TIF, latent ability distribution, and
unidimensionality plot.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mfrow = c(2, 3))

plot(mod_3pl, type=c(&amp;quot;ICC&amp;quot;), 
     legend = TRUE, cx = &amp;quot;bottomright&amp;quot;, lwd = 2, 
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;3PL Item Characteristics Curve&amp;quot;)

plot(mod_3pl,type=c(&amp;quot;IIC&amp;quot;),
     legend = TRUE, cx = &amp;quot;topright&amp;quot;, lwd = 2,
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;3PL Item Information Curve&amp;quot;)

plot(mod_3pl,type=c(&amp;quot;IIC&amp;quot;),items=c(0))

plot(0:1, 0:1, type = &amp;quot;n&amp;quot;, ann = FALSE, axes = FALSE)
info1_3pl &amp;lt;- information(mod_3pl, c(-4, 0)) #Item information on the negative area of theta (-4 to 0)
info2_3pl &amp;lt;- information(mod_3pl, c(0, 4)) #Item information on the positive area of theta (0 to 4)
text(0.5, 0.5, labels = paste(&amp;quot;Total Information:&amp;quot;, round(info1_3pl$InfoTotal, 3),
                               &amp;quot;\n\nInformation in (-4, 0):&amp;quot;, round(info1_3pl$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info1_3pl$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;),
                               &amp;quot;\n\nInformation in (0, 4):&amp;quot;, round(info2_3pl$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info2_3pl$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;)))

theta.3pl&amp;lt;-ltm::factor.scores(mod_3pl)
summary(theta.3pl$score.dat$z1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-1.8706 -0.9992 -0.5368 -0.6584 -0.3590  0.6116 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(theta.3pl, main = &amp;quot;Latent ability scores of the participants 3PL&amp;quot;)

unitest_3pl &amp;lt;- unidimTest(mod_3pl,LSAT)
plot(unitest_3pl, type = &amp;quot;b&amp;quot;, pch = 1:2)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Real Data&amp;quot;, &amp;quot;Average Simulated Data&amp;quot;), lty = 1, 
    pch = 1:2, col = 1:2, bty = &amp;quot;n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file38c4da22ad3_files/figure-html/unnamed-chunk-21-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;unitest_3pl&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Unidimensionality Check using Modified Parallel Analysis

Call:
tpm(data = LSAT, type = &amp;quot;latent.trait&amp;quot;, max.guessing = 1)

Matrix of tertachoric correlations
       Item 1 Item 2 Item 3 Item 4 Item 5
Item 1 1.0000 0.1703 0.2275 0.1072 0.0665
Item 2 0.1703 1.0000 0.1891 0.1111 0.1724
Item 3 0.2275 0.1891 1.0000 0.1867 0.1055
Item 4 0.1072 0.1111 0.1867 1.0000 0.2009
Item 5 0.0665 0.1724 0.1055 0.2009 1.0000

Alternative hypothesis: the second eigenvalue of the observed data is substantially larger 
            than the second eigenvalue of data under the assumed IRT model

Second eigenvalue in the observed data: 0.2254
Average of second eigenvalues in Monte Carlo samples: 0.2459
Monte Carlo samples: 100
p-value: 0.5545&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The unidimensionality testing above suggested non-significant
result, meaning that the test measures one construct as intended.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;anova(mod_1pl, mod_3pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 Likelihood Ratio Table
            AIC     BIC  log.Lik  LRT df p.value
mod_1pl 4945.88 4975.32 -2466.94                
mod_3pl 4963.32 5036.94 -2466.66 0.56  9       1&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The comparison between the 1PL and the 3PL model also suggests no
statistical differences. The 3PL model also has larger AIC and BIC;
therefore, it is more preferable for us to use 1PL model with this data.
Finally, we can request for ability estimates for all response pattern
with the &lt;code&gt;factor.scores()&lt;/code&gt; function.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;factor.scores(mod_1pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = NULL)

Scoring Method: Empirical Bayes

Factor-Scores for observed response patterns:
   Item 1 Item 2 Item 3 Item 4 Item 5 Obs     Exp     z1 se.z1
1       0      0      0      0      0   3   2.364 -1.910 0.790
2       0      0      0      0      1   6   5.468 -1.439 0.793
3       0      0      0      1      0   2   2.474 -1.439 0.793
4       0      0      0      1      1  11   8.249 -0.959 0.801
5       0      0      1      0      0   1   0.852 -1.439 0.793
6       0      0      1      0      1   1   2.839 -0.959 0.801
7       0      0      1      1      0   3   1.285 -0.959 0.801
8       0      0      1      1      1   4   6.222 -0.466 0.816
9       0      1      0      0      0   1   1.819 -1.439 0.793
10      0      1      0      0      1   8   6.063 -0.959 0.801
11      0      1      0      1      1  16  13.288 -0.466 0.816
12      0      1      1      0      1   3   4.574 -0.466 0.816
13      0      1      1      1      0   2   2.070 -0.466 0.816
14      0      1      1      1      1  15  14.749  0.049 0.836
15      1      0      0      0      0  10  10.273 -1.439 0.793
16      1      0      0      0      1  29  34.249 -0.959 0.801
17      1      0      0      1      0  14  15.498 -0.959 0.801
18      1      0      0      1      1  81  75.060 -0.466 0.816
19      1      0      1      0      0   3   5.334 -0.959 0.801
20      1      0      1      0      1  28  25.834 -0.466 0.816
21      1      0      1      1      0  15  11.690 -0.466 0.816
22      1      0      1      1      1  80  83.310  0.049 0.836
23      1      1      0      0      0  16  11.391 -0.959 0.801
24      1      1      0      0      1  56  55.171 -0.466 0.816
25      1      1      0      1      0  21  24.965 -0.466 0.816
26      1      1      0      1      1 173 177.918  0.049 0.836
27      1      1      1      0      0  11   8.592 -0.466 0.816
28      1      1      1      0      1  61  61.235  0.049 0.836
29      1      1      1      1      0  28  27.709  0.049 0.836
30      1      1      1      1      1 298 295.767  0.593 0.862&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;By default, &lt;code&gt;factor.scores()&lt;/code&gt; produces ability estimates
for the observed response patterns (every combination available); if
ability estimates are required for non observed or specific response
patterns, these could be specified using the &lt;code&gt;resp.patterns&lt;/code&gt;
argument.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;factor.scores(mod_1pl, resp.patterns = rbind(c(1,1,1,1,1), c(0,0,0,0,0)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = NULL)

Scoring Method: Empirical Bayes

Factor-Scores for specified response patterns:
  Item 1 Item 2 Item 3 Item 4 Item 5 Obs     Exp     z1 se.z1
1      1      1      1      1      1 298 295.767  0.593 0.862
2      0      0      0      0      0   3   2.364 -1.910 0.790&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The specified response patterns above are for examinees who got
all items correctly and incorrectly respectively. The results suggested
that the examinee who got all item correctly has ability level of 0.50
and the examinee who got all item incorrectly has ability level of
-1.91.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The model we discussed so far, Rasch, 1PL,2PL, 3PL are all
suitable for dichotomous test items (True/False), but what if item
responses have more than 2 categories like in a survey (i.e., 1 =
Strongly disagree 2 = Disagree 3 = Agree 4 = Strongly Agree)? This is
when we use polytomous IRT models.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="polytomous-item"&gt;Polytomous Item&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The data we consider here comes from the Environment section of the
1990 British Social Attitudes Survey, N = 291, 6 items, 3 ordinal
response options. The data can be loaded with
&lt;code&gt;data(Environment)&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;data(Environment)
descript(Environment)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Descriptive statistics for the &amp;#39;Environment&amp;#39; data-set

Sample:
 6 items and 291 sample units; 0 missing values

Proportions for each level of response:
             very concerned slightly concerned not very concerned
LeadPetrol           0.6151             0.3265             0.0584
RiverSea             0.8007             0.1753             0.0241
RadioWaste           0.7457             0.1924             0.0619
AirPollution         0.6495             0.3196             0.0309
Chemicals            0.7491             0.1924             0.0584
Nuclear              0.5155             0.3265             0.1581


Frequencies of total scores:
      6  7  8  9 10 11 12 13 14 15 16 17 18
Freq 96 51 37 27 26 18 13  7  6  6  1  1  2


Cronbach&amp;#39;s alpha:
                        value
All Items              0.8215
Excluding LeadPetrol   0.8218
Excluding RiverSea     0.7990
Excluding RadioWaste   0.7767
Excluding AirPollution 0.7751
Excluding Chemicals    0.7790
Excluding Nuclear      0.8058


Pairwise Associations:
   Item i Item j p.value
1       1      2   0.001
2       1      3   0.001
3       1      4   0.001
4       1      5   0.001
5       1      6   0.001
6       2      3   0.001
7       2      4   0.001
8       2      5   0.001
9       2      6   0.001
10      3      4   0.001&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;From the descriptive output, the first response, “&lt;em&gt;very
concerned&lt;/em&gt;”, has the highest frequency. The p-values for the
pairwise associations indicate significant associations between all
items. An alternative method to explore the degree of association
between pairs of items can be done with &lt;code&gt;rcor.test()&lt;/code&gt; for
non-parametric correlation coefficients.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;rcor.test(Environment, method = &amp;quot;kendall&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
             LeadPetrol RiverSea RadioWaste AirPollution Chemicals
LeadPetrol    *****      0.385    0.260      0.457        0.305   
RiverSea     &amp;lt;0.001      *****    0.399      0.548        0.403   
RadioWaste   &amp;lt;0.001     &amp;lt;0.001    *****      0.506        0.623   
AirPollution &amp;lt;0.001     &amp;lt;0.001   &amp;lt;0.001      *****        0.504   
Chemicals    &amp;lt;0.001     &amp;lt;0.001   &amp;lt;0.001     &amp;lt;0.001        *****   
Nuclear      &amp;lt;0.001     &amp;lt;0.001   &amp;lt;0.001     &amp;lt;0.001       &amp;lt;0.001   
             Nuclear
LeadPetrol    0.279 
RiverSea      0.320 
RadioWaste    0.484 
AirPollution  0.382 
Chemicals     0.463 
Nuclear       ***** 

upper diagonal part contains correlation coefficient estimates 
lower diagonal part contains corresponding p-values&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;rcor.test()&lt;/code&gt; provides two options for nonparametric
calculation, Kendall’s &lt;em&gt;Tau&lt;/em&gt; and Spearman’s &lt;em&gt;rho&lt;/em&gt; in
&lt;code&gt;method&lt;/code&gt; argument. Initially, we will fit the partial credit
model (PCM), which is a Polytomous version of the Rasch model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="partial-credit-model"&gt;Partial Credit Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;PCM fixes the discrimination parameter of all item as 1 in the
same way as what Rasch’s model does. The threshold (or the parameter
that represents the trait level necessary for an examinee to have 50% to
pick a response category) of PCM is allowed to vary.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For the &lt;code&gt;gpcm()&lt;/code&gt; function, If
&lt;code&gt;constraint = "rasch"&lt;/code&gt;, then the discrimination parameter is
assumed equal for all items and fixed at one. If
&lt;code&gt;constraint = "1PL"&lt;/code&gt;, then the discrimination parameter βi is
assumed equal for all items but is estimated. Here, we are fixing all
discrimination parameter to 1.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_pcm_rasch &amp;lt;- gpcm(Environment, constraint = &amp;quot;rasch&amp;quot;)
summary(mod_pcm_rasch)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
gpcm(data = Environment, constraint = &amp;quot;rasch&amp;quot;)

Model Summary:
   log.Lik      AIC      BIC
 -1147.176 2318.351 2362.431

Coefficients:
$LeadPetrol
        value std.err z.value
Catgr.1 0.680   0.153   4.450
Catgr.2 2.785   0.292   9.541
Dscrmn  1.000      NA      NA

$RiverSea
        value std.err z.value
Catgr.1 1.822   0.180  10.149
Catgr.2 3.385   0.435   7.781
Dscrmn  1.000      NA      NA

$RadioWaste
        value std.err z.value
Catgr.1 1.542   0.174   8.879
Catgr.2 2.328   0.302   7.709
Dscrmn  1.000      NA      NA

$AirPollution
        value std.err z.value
Catgr.1 0.822   0.153   5.363
Catgr.2 3.517   0.376   9.343
Dscrmn  1.000      NA      NA

$Chemicals
        value std.err z.value
Catgr.1 1.555   0.174   8.949
Catgr.2 2.399   0.308   7.788
Dscrmn  1.000      NA      NA

$Nuclear
        value std.err z.value
Catgr.1 0.316   0.156   2.029
Catgr.2 1.498   0.208   7.218
Dscrmn  1.000      NA      NA


Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 0.0079 
optimizer: nlminb &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The model summary provides AIC and BIC of the model. Same as what we
did with our dichotomous data, we can request for parameter estimates of
each item.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;coef(mod_pcm_rasch, prob = TRUE, order = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;             Catgr.1 Catgr.2 Dscrmn
LeadPetrol     0.680   2.785      1
RiverSea       1.822   3.385      1
RadioWaste     1.542   2.328      1
AirPollution   0.822   3.517      1
Chemicals      1.555   2.399      1
Nuclear        0.316   1.498      1&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;In order to check the fit of the model to the data, the argument
&lt;code&gt;GoF.gpcm&lt;/code&gt; and &lt;code&gt;margins()&lt;/code&gt; can be used.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;GoF.gpcm(mod_pcm_rasch, B = 199) # B = Bootstrap sample&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Parametric Bootstrap Approximation to Pearson chi-squared Goodness-of-Fit Measure

Call:
gpcm(data = Environment, constraint = &amp;quot;rasch&amp;quot;)

Tobs: 1001.41 
# data-sets: 200 
p-value: 0.085 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Based on 200 data-points, the non significant p-value suggests an
acceptable fit of the model. The null hypothesis states that the
observed data and the model fit with each other (no differences). We can
move on to the two-way margin analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_pcm_rasch)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
gpcm(data = Environment, constraint = &amp;quot;rasch&amp;quot;)

Fit on the Two-Way Margins

             LeadPetrol RiverSea RadioWaste AirPollution Chemicals
LeadPetrol   &amp;lt;NA&amp;gt;       35.47     3.04      34.43         7.41    
RiverSea     ***        &amp;lt;NA&amp;gt;     20.07      77.50        18.75    
RadioWaste                       &amp;lt;NA&amp;gt;       44.64        68.12    
AirPollution ***        ***      ***        &amp;lt;NA&amp;gt;         33.29    
Chemicals                        ***        ***          &amp;lt;NA&amp;gt;     
Nuclear                          ***                              
             Nuclear
LeadPetrol    4.71  
RiverSea     10.36  
RadioWaste   43.35  
AirPollution 16.56  
Chemicals    27.68  
Nuclear      &amp;lt;NA&amp;gt;   

&amp;#39;***&amp;#39; denotes pairs of items with lack-of-fit&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The upper diagonal part of the output contains the residuals, and
the lower diagonal part indicates the pairs for which the residuals
exceed the threshold value. The two-way margin analysis above suggests
problematic fit of the data with the PCM model, meaning that PCM might
not be suitable for this data. We can try using the next model, the
Graded Response Model (GRM).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="graded-response-model"&gt;Graded Response Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;GRM is the polytomous version of the 2PL model (&lt;a
href="https://www.frontiersin.org/articles/10.3389/feduc.2021.721963/full"&gt;Dai
et al., 2021&lt;/a&gt;). Despite able to constrain the discrimination
parameter, GRM works differently than PCM. PCM estimates separate
category response parameters for each item, while the GRM model further
assumes that the thresholds for category response are also equal across
items (&lt;a
href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4520411/#:~:text=The%20difference%20between%20the%20two,are%20also%20equal%20across%20items."&gt;Nguyen
et al., 2014&lt;/a&gt;). Initially, we can try fitting the constrained version
of Graded Response Model (GRM) that assumes equal &lt;em&gt;a&lt;/em&gt; parameter
across items (similar to Rasch model). The model is fitted by
&lt;code&gt;grm()&lt;/code&gt; as follows&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_grm &amp;lt;- grm(Environment, constrained = TRUE)
summary(mod_grm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment, constrained = TRUE)

Model Summary:
   log.Lik      AIC      BIC
 -1106.193 2238.386 2286.139

Coefficients:
$LeadPetrol
        value
Extrmt1 0.395
Extrmt2 1.988
Dscrmn  2.218

$RiverSea
        value
Extrmt1 1.060
Extrmt2 2.560
Dscrmn  2.218

$RadioWaste
        value
Extrmt1 0.832
Extrmt2 1.997
Dscrmn  2.218

$AirPollution
        value
Extrmt1 0.483
Extrmt2 2.448
Dscrmn  2.218

$Chemicals
        value
Extrmt1 0.855
Extrmt2 2.048
Dscrmn  2.218

$Nuclear
        value
Extrmt1 0.062
Extrmt2 1.266
Dscrmn  2.218


Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 0.0049 
quasi-Newton: BFGS &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;If standard errors for the parameter estimates are required, you can
add the argument &lt;code&gt;Hessian = T&lt;/code&gt; to the function
&lt;code&gt;grm()&lt;/code&gt;. Similarly to our dichotomous case, the fit of the
model can be checked using &lt;code&gt;margins()&lt;/code&gt; for two-way
margins.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_grm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment, constrained = TRUE)

Fit on the Two-Way Margins

             LeadPetrol RiverSea RadioWaste AirPollution Chemicals
LeadPetrol   -          10.03     9.98       5.19         7.85    
RiverSea                -         5.06      17.12         2.56    
RadioWaste                       -           6.78        20.60    
AirPollution                                -             4.49    
Chemicals                                                -        
Nuclear                                                           
             Nuclear
LeadPetrol   16.93  
RiverSea      7.14  
RadioWaste   12.09  
AirPollution  4.57  
Chemicals     3.85  
Nuclear      -      &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The output above looks good as there is no indication of poor fit.
Next, we will try with the three-way margins.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_grm, type = &amp;quot;three&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment, constrained = TRUE)

Fit on the Three-Way Margins

   Item i Item j Item k (O-E)^2/E  
1       1      2      3     28.52  
2       1      2      4     34.26  
3       1      2      5     29.91  
4       1      2      6     42.74  
5       1      3      4     33.03  
6       1      3      5     66.72  
7       1      3      6     65.31  
8       1      4      5     25.48  
9       1      4      6     34.46  
10      1      5      6     39.49  
11      2      3      4     29.63  
12      2      3      5     37.74  
13      2      3      6     32.50  
14      2      4      5     27.08  
15      2      4      6     36.77  
16      2      5      6     19.49  
17      3      4      5     38.99  
18      3      4      6     26.91  
19      3      5      6     39.62  
20      4      5      6     22.25  &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Both the two- and three-way residuals show a good fit of the
constrained model to the data, but checking the fit of the model in the
margins does not correspond to an overall goodness-of-fit test. As a
result, we will fit the unconstrained version of the GRM as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_grm_unconstrained &amp;lt;- grm(Environment) #unconstrained GRM

summary(mod_grm_unconstrained)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment)

Model Summary:
   log.Lik      AIC      BIC
 -1090.404 2216.807 2282.927

Coefficients:
$LeadPetrol
        value
Extrmt1 0.487
Extrmt2 2.584
Dscrmn  1.378

$RiverSea
        value
Extrmt1 1.058
Extrmt2 2.499
Dscrmn  2.341

$RadioWaste
        value
Extrmt1 0.779
Extrmt2 1.793
Dscrmn  3.123

$AirPollution
        value
Extrmt1 0.457
Extrmt2 2.157
Dscrmn  3.283

$Chemicals
        value
Extrmt1 0.809
Extrmt2 1.868
Dscrmn  2.947

$Nuclear
        value
Extrmt1 0.073
Extrmt2 1.427
Dscrmn  1.761


Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 0.003 
quasi-Newton: BFGS &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can use a likelihood ratio test to check if the unconstrained
version GRM is better than its constrained one.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;anova(mod_grm, mod_grm_unconstrained)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 Likelihood Ratio Table
                          AIC     BIC  log.Lik   LRT df p.value
mod_grm               2238.39 2286.14 -1106.19                 
mod_grm_unconstrained 2216.81 2282.93 -1090.40 31.58  5  &amp;lt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The likelihood ratio test indicates that the unconstrained GRM is
preferable for the Environment data. We can plot the Item Characteristic
Curve (ICC) of all 6 items, Item Information Curve (IIC), and Test
Information Curve (TIC) below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mar=c(2,6,2,2), mfrow = c(3, 3))

plot(mod_grm_unconstrained, lwd = 2, cex = 0.6, legend = TRUE, cx = &amp;quot;left&amp;quot;, 
     xlab = &amp;quot;Latent Trait&amp;quot;, cex.main = 0.7, cex.lab = 0.7, cex.axis = 1)

##############################################################

plot(mod_grm_unconstrained, type = &amp;quot;IIC&amp;quot;, lwd = 2, cex = 0.6, legend = TRUE, cx = &amp;quot;topleft&amp;quot;,
     xlab = &amp;quot;Latent Trait&amp;quot;, cex.main = 0.7, cex.lab = 0.7, cex.axis = 1)

plot(mod_grm_unconstrained, type = &amp;quot;IIC&amp;quot;, items = 0, lwd = 2, xlab = &amp;quot;Latent Trait&amp;quot;,cex.main = 0.7, cex.lab = 0.7, cex.axis = 1)

info3 &amp;lt;- information(mod_grm_unconstrained, c(-4, 0))
info4 &amp;lt;- information(mod_grm_unconstrained, c(0, 4))

text(-1.9, 8, labels = paste(&amp;quot;Information in (-4, 0):&amp;quot;,
                             paste(round(100 * info3$PropRange, 1), &amp;quot;%&amp;quot;, sep = &amp;quot;&amp;quot;),
                             &amp;quot;\n\nInformation in (0, 4):&amp;quot;,
                             paste(round(100 * info4$PropRange, 1), &amp;quot;%&amp;quot;, sep = &amp;quot;&amp;quot;)), cex = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file38c4da22ad3_files/figure-html/unnamed-chunk-37-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;From the Item Characteristic Curve, we observe that there is low
probability of endorsing the the first option, “very concerned”, for
relatively high latent trait levels, which means that the questions
asked are not considered as major environmental issues by the
respondent.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Test Information Curve also tells us that the test provides
89% of the total information for high latent trait levels.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally, the Item Information Curve indicates that items in
LeadPetrol and Nuclear provide little information in the whole latent
trait continuum. We can check this in detail using
&lt;code&gt;information()&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;information(mod_grm_unconstrained, c(-4, 4))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment)

Total Information = 26.97
Information in (-4, 4) = 26.7 (98.97%)
Based on all the items&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;For item 1 and item 6&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;information(mod_grm_unconstrained, c(-4, 4), items = c(1, 6)) #for item 1 and 6&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment)

Total Information = 5.36
Information in (-4, 4) = 5.17 (96.38%)
Based on items 1, 6&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We observe that item 1 and 6 provide only the 5.36% of the total
information (from the total of 26.97); Thus, they could probably be
excluded from a similar future study. Finally, a useful comparison is to
plot the ICC of each response option separately.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mar=c(2,6,2,2), mfrow = c(2, 2))

plot(mod_grm_unconstrained, category = 1, lwd = 2, cex = 0.7, legend = TRUE, cx = -4.5,
     cy = 0.85, xlab = &amp;quot;Latent Trait&amp;quot;, cex.main = 0.7, cex.lab = 0.7,
     cex.axis = 1)

for (ctg in 2:3) 
  {
  plot(mod_grm_unconstrained, category = ctg, lwd = 2, cex = 0.5, annot = FALSE,
      xlab = &amp;quot;Latent Trait&amp;quot;, cex.main = 0.7, cex.lab = 0.7,
      cex.axis = 1)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file38c4da22ad3_files/figure-html/unnamed-chunk-40-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;From the plot, the response option for RadioWaste and Chemicals have
nearly identical characteristic curves for all categories, indicating
that these two items are probably regarded to have the same effect on
the environment.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;So far, we have discussed what IRT is, including its model for
dichotomous and polytomous test items. There are several models for us
to choose from, and each model has its parameter we can adjust to suit
our needs as well depending on how complex we want our model to be.
There is no right or wrong answer model selection. For example, if we
want the model to be as simple as possible with a dichotomous test (say,
a math test), we could go for Rasch model. If we want our model to be
more realistic, we may want to use 2PL or 3PL, which comes with expenses
such as the need for more sample size or more difficulty to fit the
model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, given how useful IRT is in analyzing test items with
several unique parameters, it doesn’t mean that we need to disregard the
concept of CTT in our practice. Both theories have its own contribution
and usefulness. For example, CTT is more practical to implement in the
classroom setting where there is small amount of students and there is
no need to investigate items at a deeper level. For example, if we want
to use a classroom assessment for formative purposes (i.e., a practice
quiz to help students prepare for the final exam), using CTT might be
sufficient. If we want to develop a large-scale test to measure students
at a national level, maybe IRT might be more appropriate to improve the
test with a more realistic model.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a
href="https://d1wqtxts1xzle7.cloudfront.net/47596077/ITEMS_Module_16-with-cover-page-v2.pdf?Expires=1652597267&amp;amp;Signature=dRiumBzQzxIiFeQvRCtF3cuBjpeaui-mWZXaskGDaQYlwenB7ZZVx0dBUxSnl47nyQnveBh0ZBHdY3IbzNnPHZqAhAOdIN~QIQVCfSQ11d0ZV3mef~kmo8nEzXj459sakTPeZCeTwZSVxklbbtIE52mzRfFItBw4ZF70kJMT9S3FC9YaI~lDDXN5YUbb4VL7ZgCH-lsBIthNbTSDUXxqGuwgeHgxKfotwPdlQRr5iLEEwlLkea2Fr1zow4uLrNg38yK31MdNg55m73x9iwJZA2s~6wlj9IfsWINVJlwPoIOjT5Zm0bIuyIfzGbUKRk~3AiXLjThGUjIi0wlc1Y6flQ__&amp;amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"&gt;Hambleton
and Jones (1993&lt;/a&gt;) did a really great job in comparing the difference
between CTT and IRT in their work. I have presented the table below for
your information.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;The Comparison between CTT and IRT Models (&lt;a
href="https://d1wqtxts1xzle7.cloudfront.net/47596077/ITEMS_Module_16-with-cover-page-v2.pdf?Expires=1652597267&amp;amp;Signature=dRiumBzQzxIiFeQvRCtF3cuBjpeaui-mWZXaskGDaQYlwenB7ZZVx0dBUxSnl47nyQnveBh0ZBHdY3IbzNnPHZqAhAOdIN~QIQVCfSQ11d0ZV3mef~kmo8nEzXj459sakTPeZCeTwZSVxklbbtIE52mzRfFItBw4ZF70kJMT9S3FC9YaI~lDDXN5YUbb4VL7ZgCH-lsBIthNbTSDUXxqGuwgeHgxKfotwPdlQRr5iLEEwlLkea2Fr1zow4uLrNg38yK31MdNg55m73x9iwJZA2s~6wlj9IfsWINVJlwPoIOjT5Zm0bIuyIfzGbUKRk~3AiXLjThGUjIi0wlc1Y6flQ__&amp;amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"&gt;Hambleton
&amp;amp; Jones, 1993&lt;/a&gt;, p.43)&lt;/caption&gt;
&lt;colgroup&gt;
&lt;col width="24%" /&gt;
&lt;col width="34%" /&gt;
&lt;col width="41%" /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th&gt;Area&lt;/th&gt;
&lt;th&gt;Classical Test Theory&lt;/th&gt;
&lt;th&gt;Item Response Theory&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;Model&lt;/td&gt;
&lt;td&gt;Linear&lt;/td&gt;
&lt;td&gt;Non-linear&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;Level&lt;/td&gt;
&lt;td&gt;Test&lt;/td&gt;
&lt;td&gt;Item&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;Assumptions&lt;/td&gt;
&lt;td&gt;Weak (i.e., easy to fit with test data)&lt;/td&gt;
&lt;td&gt;Strong (i.e., more difficult to meet with test data)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;Item-Ability Relationship&lt;/td&gt;
&lt;td&gt;Not Specified&lt;/td&gt;
&lt;td&gt;Item Characteristics Function&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;Examinee Ability&lt;/td&gt;
&lt;td&gt;Represented by test scores or estimated true scores&lt;/td&gt;
&lt;td&gt;Represented by latent ability (Theta/θ)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;Invariance of item and person statistics&lt;/td&gt;
&lt;td&gt;Unavailable / item and person parameters are sample dependent&lt;/td&gt;
&lt;td&gt;Item and person parameters are sample dependent if the model fits
the data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;Item Statistics&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;em&gt;p&lt;/em&gt; = item difficulty&lt;/p&gt;
&lt;p&gt;&lt;em&gt;r&lt;/em&gt; = item discrimination&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;Item discrimination (&lt;em&gt;a&lt;/em&gt;), Item difficulty (&lt;em&gt;b&lt;/em&gt;),
guessing parameter (&lt;em&gt;c&lt;/em&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;Sample Size (for item parameter estimation)&lt;/td&gt;
&lt;td&gt;200-500&lt;/td&gt;
&lt;td&gt;More than 500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Similar to CTT, IRT can be used to develop tests, scales,
surveys, or other measurement tools. The model can analyze item-level
data of both dichotomous (i.e., exams with true/false) and polytomous
(i.e., surveys with no right/wrong answers) tests to provide information
on sensitivity of measurement across a range of latent trait. Knowing
information like item difficulty, item discrimination, and item guessing
is useful when building tests as we can examine which item is a good
item and which item is a not-so-useful one. For example, a test item
that is easy to guess might not be appropriate because everyone can do
it, so it doesn’t really measure anything.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IRT also allows us to put ability levels and difficulties of
items into the same scale to match the trait levels of a target
population. If we want a test to measure someone with “just enough”
knowledge to pass (say, a driver license exam), we can build a test to
measure people with low to medium knowledge. I mean, a taxi driver or a
racing driver know how to drive a car, and that is enough. However, if
we want to develop a test to select the best-of-the-best candidates for
scholarship selection, we might want to build a difficult test to
separate low-to-mid tier students to high performance students. Anyway,
that is all for this post. Thank you so much for reading this! Have a
good day!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>2530d599a3222722a8076a89ee971d4a</distill:md5>
      <category>R</category>
      <category>Psychometric</category>
      <guid>https://taridwong.github.io/posts/2022-05-15-irt</guid>
      <pubDate>Sun, 15 May 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-05-15-irt/IIC.png" medium="image" type="image/png" width="656" height="551"/>
    </item>
    <item>
      <title>Making Sense of Machine Learning with Explanable Artificial Intelligence</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-28-xai</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In my &lt;a
href="https://taridwong.github.io/posts/2022-04-09-ensemble/"&gt;previous
post&lt;/a&gt; on ensemble machine learning models, I mentioned that one major
drawback in the artificial intelligence (AI) field is &lt;a
href="https://www.technologyreview.com/2017/04/11/5113/the-dark-secret-at-the-heart-of-ai/"&gt;the
black box problem&lt;/a&gt;, which hampers interpretability of the results
from complex algorithms such as Random Forest or Extreme Gradient
Boosting. Not knowing how the algorithm works behind the prediction
could reduce applicability of the method itself as the audience can’t
fully comprehend the result and therefore unable to use it to inform
their decisions; this problem could therefore damage trust from the
stakeholders (users, policy makers, general audience) to the field as
well &lt;a href="https://doi.org/10.1175/BAMS-D-18-0195.1"&gt;(McGovern et
al., 2019)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On the developer’s side, fully understanding the machine learning
models through the explanable approach (aka the white-box approach)
allows developers to identify potential problems such as &lt;a
href="https://www.kaggle.com/code/alexisbcook/data-leakage/tutorial"&gt;data
leakage&lt;/a&gt; in the algorithm and fix (or debug) it with relative ease
(&lt;a
href="https://ieeexplore.ieee.org/abstract/document/8882211"&gt;Loyola-Gonzalez,
2019)&lt;/a&gt;. Further, knowing which variable affects the prediction the
most can inform feature engineering to reduce model complexity and
direct future data collection as well by focusing on collecting the
variables that matter &lt;a
href="https://www.kaggle.com/code/dansbecker/use-cases-for-model-insights"&gt;(Becker
&amp;amp; Cook, 2021)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On stakeholder’s side, it is important to emphasize model
explanability especially in industries such as healthcare, finances, and
military to foster trust between the people inside and outside of the
field that could lead to the extent that the result is used to inform
decisions made by humans such as financial credit approval &lt;a
href="https://www.kaggle.com/code/dansbecker/use-cases-for-model-insights"&gt;(Becker
&amp;amp; Cook, 2021&lt;/a&gt;; &lt;a
href="https://ieeexplore.ieee.org/abstract/document/8882211"&gt;Loyola-Gonzalez,
2019)&lt;/a&gt;. Clearly Understanding how, where, and why the model also
benefits the model itself as users are able to identify potential
problems in its performance and provide the develoeprs with their
feedback &lt;a
href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=9401991"&gt;(Velez
et al., 2021)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The above examples knowing how to extract human-understandable
insights from a complex machine learning model is important, especially
in social science data where the theoretical part is as important as the
methodological and the practical part. For that reason, I will be
applying the methods of Explanable Artificial Intelligence (XAI) to
extract interpretable insights from a classification model that predicts
students’ grade repetition. We will begin by setting up the environment
as usual.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

from imblearn.combine import SMOTEENN

from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score

import warnings
warnings.filterwarnings(&amp;quot;ignore&amp;quot;)

RANDOM_STATE = 123&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;I will be using the same data set as my &lt;a
href="https://taridwong.github.io/posts/2022-02-27-statlearning/"&gt;previous
post about statistical learning&lt;/a&gt;, namely the Programme for
International Student Assessment (PISA) 2018 (&lt;a
href="https://www.oecd.org/education/pisa-2018-results-volume-i-5f07c754-en.htm"&gt;OECD,
2019&lt;/a&gt;). However, the set of variables that I am examining will be
different as PISA contains several school-related variables that can be
shifted as the researcher sees fit. For this post, I will predict
students’ class repetition from 25 predictors (or features as called in
the field of machine learning) such as students’ socio-economic status,
history of bullying involvement, and their learning motivation. The data
is collected from Thai student in 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;df = pd.read_csv(&amp;quot;PISA_TH.csv&amp;quot;)

X = df.drop(&amp;#39;REPEAT&amp;#39;, axis=1)
y = df[&amp;#39;REPEAT&amp;#39;]

df.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   REPEAT    ESCS  DAYSKIP  ...  Invest_effort  WEALTH  Home_resource
0       0 -0.7914        1  ...              6  0.0721        -1.4469
1       0  0.8188        1  ...              8 -0.3429         1.1793
2       0  0.4509        1  ...             10  0.3031         1.1793
3       0  0.7086        1  ...             10 -0.5893        -0.1357
4       0  0.8361        1  ...             10  0.5406         1.1793

[5 rows x 25 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="addressing-sample-imbalance"&gt;Addressing Sample Imbalance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The problem is that our targeted variable is imbalance; that is,
the number of students who repeated a class is smaller than the number
of students who did not. This situation makes sense in the real-world
data as normal samples are usually more prevalent than the abnormal
ones, but it is undesirable in the machine learning scenario as the
model could recognize minority samples as unimportant and therefore
disregard them as noises &lt;a
href="https://arxiv.org/pdf/1106.1813.pdf"&gt;(Chawla et al., 2022)&lt;/a&gt;. As
a result, the model could give misleadingly optimistic performance on
classification datasets as it classifies only students who did not
repeat a class.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;See the t-Distributed Stochastic Neighbor Embedding (tSNE) plot
below for the visualization. There isn’t much samples of repeaters in
contrary to non-repeater students. Plus, the pattern is not prominent
enough as the blut dots (repeaters) stay very close to the red dots
(non-repeaters). This could make the pattern difficult to be learned by
the machine due to its ambiguity. One way we can mitigate this problem
is to perform data augmentation via oversampling and undersampling,
which synthesizes more minority samples and deletes or merges majority
samples to improve performance of the machine (&lt;a
href="https://ieeexplore.ieee.org/abstract/document/9034624?casa_token=P33Jkz0x1zEAAAAA:Xtz22PhKDSZ_ktb6X7w-Le7PHkxHwfzzRzvrL3qJcJIDwmyaAMizIr1lUBSK5Lpz1qyk4Ls"&gt;Budhiman
et al., 2019&lt;/a&gt;; &lt;a
href="https://ieeexplore.ieee.org/abstract/document/7797091"&gt;Wong et
al;., 2016&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;Counter(y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Counter({0: 8044, 1: 589})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;tsne = TSNE(n_components=2, random_state=RANDOM_STATE)

TSNE_result = tsne.fit_transform(X)

plt.figure(figsize=(12,8))
sns.scatterplot(TSNE_result[:,0], TSNE_result[:,1], hue=y, legend=&amp;#39;full&amp;#39;, palette=&amp;quot;hls&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file38c45259d0d_files/figure-html/unnamed-chunk-5-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To balance the data, I will use both oversampling and
undersampling. Normal oversampling methods duplicates minority samples
for more sample size; however, this approach does not add any more
information to the model (more of the same, basically). Instead, we can
&lt;em&gt;synthesize&lt;/em&gt; minority samples by creating samples that are
&lt;em&gt;similar&lt;/em&gt; to the existing minority samples; this technique is
named as &lt;strong&gt;Synthetic Minority Oversampling TEchnique
(SMOTE)&lt;/strong&gt; &lt;a
href="https://www.wiley.com/en-us/Imbalanced+Learning%3A+Foundations%2C+Algorithms%2C+and+Applications-p-9781118074626"&gt;(He
and Ma, 2013)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Also, we can further enhance the effectiveness of SMOTE by adding
undersampling into the process &lt;a
href="https://arxiv.org/pdf/1106.1813.pdf"&gt;(Chawla et al., 2022)&lt;/a&gt;.
Instead of randomly delete our majority samples, we will use the
&lt;strong&gt;Edited Nearest Neighbor (ENN)&lt;/strong&gt; method, which deletes
data points based on their neighbors to make the difference between
majority and minority samples &lt;a
href="https://www.researchgate.net/profile/Duke-T-J-Ludera/publication/348663430_Credit_Card_Fraud_Detection_by_Combining_Synthetic_Minority_Oversampling_and_Edited_Nearest_Neighbours/links/6009f844a6fdccdcb86fc68c/Credit-Card-Fraud-Detection-by-Combining-Synthetic-Minority-Oversampling-and-Edited-Nearest-Neighbours.pdf"&gt;(Ludera,
2021)&lt;/a&gt;. The combination of these two techniques is called &lt;a
href="https://imbalanced-learn.org/stable/auto_examples/combine/plot_comparison_combine.html#sphx-glr-auto-examples-combine-plot-comparison-combine-py"&gt;&lt;strong&gt;SMOTEENN&lt;/strong&gt;&lt;/a&gt;
See Figure 1 for the example of ENN from &lt;a
href="https://doi.org/10.1016/j.ins.2009.02.011"&gt;Guan et al.,
(2009)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-xaienn.png" style="width:70.0%" alt="" /&gt;
&lt;p class="caption"&gt;ENN Editing with 1-NN Classifier. No copyright
infringement is intended&lt;/p&gt;
&lt;/div&gt;
&lt;pre class="python"&gt;&lt;code&gt;smote_enn = SMOTEENN(random_state=RANDOM_STATE, sampling_strategy = &amp;#39;minority&amp;#39;, n_jobs=-1)

X_resampled, y_resampled = smote_enn.fit_resample(X, y)

Counter(y_resampled)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Counter({1: 8040, 0: 4794})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;tsne = TSNE(n_components=2, random_state=RANDOM_STATE)

TSNE_result = tsne.fit_transform(X_resampled)

plt.figure(figsize=(12,8))
sns.scatterplot(TSNE_result[:,0], TSNE_result[:,1], hue=y_resampled, legend=&amp;#39;full&amp;#39;, palette=&amp;quot;hls&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file38c45259d0d_files/figure-html/unnamed-chunk-7-3.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The second tSNE plot shows a more noticable pattern between student
repeaters and non-repeaters. The number of repeaters is increased while
the number of non-repeaters is decreased. Next, we can put our augmented
data into the Random Forest model for prediction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="random-forest-ensemble"&gt;Random Forest Ensemble&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We will be splitting the data set into a training and a testing set
as usual. For a quick recap, Random Forest is a machine learning model
that consists of several unique and uncorrelated decision trees; hence
the word Random in its name. Those trees work together to improve the
predictive accuracy of that dataset than a single decision tree &lt;a
href="https://mitpress.mit.edu/books/introduction-machine-learning-second-edition"&gt;(Kubat,
2017)&lt;/a&gt;. The model will be evaluated with the repeated stratified
10-folds technique to test our model prediction on different sets of
unseen data to ensure its accuracy, especially in the case of imbalanced
data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;CV = RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=RANDOM_STATE)


X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size = 0.30, 
                                                    random_state = RANDOM_STATE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;# random forest model creation
clf_rfc = RandomForestClassifier(random_state=RANDOM_STATE)
clf_rfc.fit(X_train, y_train)

# predictions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;RandomForestClassifier(random_state=123)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;rfc_predict = clf_rfc.predict(X_test)

rfc_cv_score = cross_val_score(clf_rfc, X_resampled, y_resampled, cv=CV, scoring=&amp;#39;roc_auc&amp;#39;)

print(&amp;quot;=== All AUC Scores ===&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;=== All AUC Scores ===&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(rfc_cv_score)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[0.98907675 0.99357898 0.99228597 0.99276793 0.99409399 0.99378369
 0.9931644  0.9901627  0.98967714 0.99287747 0.99384328 0.99252177
 0.99452348 0.99187137 0.99040419 0.99201929 0.9896265  0.99140778
 0.99115331 0.99457436]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;#39;\n&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;=== Mean AUC Score ===&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;=== Mean AUC Score ===&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;Mean AUC Score - RandForest: &amp;quot;, rfc_cv_score.mean())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean AUC Score - RandForest:  0.9921707177188173&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;#define metrics for normal RF
from sklearn import metrics

y_pred_proba_rf = clf_rfc.predict_proba(X_test)[::,1]
fpr_rf, tpr_rf, _ = metrics.roc_curve(y_test,  y_pred_proba_rf)

auc_rf = metrics.roc_auc_score(y_test, y_pred_proba_rf)
plt.plot(fpr_rf,tpr_rf, label=&amp;quot;AUC for Random Forest Classifier = &amp;quot;+str(auc_rf.round(3)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;lt;matplotlib.lines.Line2D object at 0x0000013A712CD6A0&amp;gt;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.legend(loc=&amp;quot;lower right&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.legend.Legend object at 0x0000013A712CD640&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;True Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;True Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.xlabel(&amp;#39;False Positive Rate&amp;#39;)
           &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;False Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Receiver-Operator Curve (ROC)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Receiver-Operator Curve (ROC)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file38c45259d0d_files/figure-html/unnamed-chunk-10-5.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The results will be evaluated with the Receiver Operator
Characteristic (ROC) curve, which shows the diagnostic ability of binary
classifiers. One approach to use ROC is to evaluate its Area Under Curve
(AUC), which measures of the ability of a classifier to distinguish
between classes and is used as a summary of the ROC curve. The higher
the AUC, the better the performance of the model at distinguishing
between the positive and negative classes. The mean of 20 rounds of
testing (randomly splitting the data into 10 stratified parts, repeated
it for 2 times) looks good is around 0.99, meaning that there is a 99%
chance that the model is able to correctly predict which student is a
reapeater and which is not based on the data used to train the machine.
Now we know that the model works well with our data, let us move on to
interpreting it with XAI techniques.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="explaining-ai"&gt;Explaining AI&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;XAI is a set of methods that allows a machine learning model and its
results understandable to human in terms of how it works in terms of
prediction, including the impace of variables to the prediction results
&lt;a
href="https://link.springer.com/book/10.1007/978-3-030-68640-6"&gt;(Gianfagna
&amp;amp; Di Cecco, 2021)&lt;/a&gt;. The XAI methods that we will extract insights
are permutation importance, partial dependence plot, and Shapley
Additive explanations (SHAP) values.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="permutation-importance"&gt;Permutation Importance&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;One of the most basic questions we might ask of a model is: What
features have the biggest impact on predictions? This quention could be
answered through the examination of &lt;strong&gt;feature importance&lt;/strong&gt;.
There are multiple ways to measure feature importance. One way is to
extract the feature importance plot from the model itself as
demonstrated below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Create a pd.Series of features importances
importances_rf = pd.Series(clf_rfc.feature_importances_, index = X_resampled.columns)

# Sort importances_rf
sorted_importance_rf = importances_rf.sort_values()

#Horizontal bar plot
sorted_importance_rf.plot(kind=&amp;#39;barh&amp;#39;, color=&amp;#39;lightgreen&amp;#39;); 
plt.xlabel(&amp;#39;Feature Importance Score&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;Feature Importance Score&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;Features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;Features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Visualizing Important Features&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Visualizing Important Features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file38c45259d0d_files/figure-html/unnamed-chunk-11-7.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Another way, which we will focus on in this post, is to use the
permutation importance score from the area of XAI. Permutation
importance is calculated by asking the following question: “If I
randomly shuffle a single column of the validation data, leaving the
target and all other columns in place, how would that affect the
accuracy of predictions in that now-shuffled data?”. Randomly
re-ordering a single column should cause less accurate predictions,
since the resulting data no longer corresponds to anything observed in
the real world. Model accuracy especially suffers if we shuffle a column
that the model relied on heavily for predictions. In our case, if we
mess with the “BEINGBULLIED” variable, the model would be severely
affected by the reduced prediction accuracy. The same would happen to
the variable “Parent_emosup”, “Positive_feel” and so forth as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;import eli5
from eli5.sklearn import PermutationImportance

FEATURES = X_test.columns.tolist()

perm = PermutationImportance(clf_rfc, random_state=RANDOM_STATE).fit(X_test, y_test)
eli5.show_weights(perm, feature_names = FEATURES, top = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;IPython.core.display.HTML object&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-xaiper-imp.png" style="width:40.0%" alt="" /&gt;
&lt;p class="caption"&gt;Permutation Importance&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The permutation importance results are consistent with the
feature importance score we extracted from the model. The values towards
the top are the most important features, and those towards the bottom
matter least. The first number in each row shows how much model
performance decreased with a random shuffling (in this case, using
“accuracy” as the performance metric). Like most things in data science,
there is some randomness to the exact performance change from a
shuffling a column. We measure the amount of randomness in our
permutation importance calculation by repeating the process with
multiple shuffles. The number after the ± measures how performance
varied from one-reshuffling to the next.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In our example, the most important feature was “BEINGBULLIED”,
which is the index of exposure to bullying. The index was constructed
from questions that ask if students have experienced bullying in the
past 12 months from statements such as “Other students left me out of
things on purpose”; “Other students made fun of me”; “I was threatened
by other students”. Positive values on this scale indicate that the
student was more exposed to bullying at school than the average student
in OECD countries; negative values on this scale indicate that the
student was less exposed to bullying at school than the average student
across OECD countries. This result is consistent with the literature
that students’ grade repetition is associated with the likelihood of
being bullied (&lt;a
href="https://journals.plos.org/Plosmedicine/article?id=10.1371/journal.pmed.1003846"&gt;Lian
et al., 2021&lt;/a&gt;; &lt;a
href="https://www.tandfonline.com/doi/full/10.1080/21683603.2019.1699215?casa_token=OB1MKY8CNuMAAAAA%3A5gwLS94ZgeACsQtZhKmiDLGtJUCu_qUMTtNyy_ftGl14WekcRoUErjezdZcOvI1s6bJEg_HYFIo"&gt;Ozada
Nazim &amp;amp; Duyan, 2019&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="partial-dependence-plots"&gt;Partial Dependence Plots&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;While feature importance shows what variables most affect
predictions, partial dependence plots show how a feature affects
predictions. For our case, partial dependence plots can be used to
answer questions such as “Controlling for all variables, what impact
does the index of exposure to bullying have on the prediction of grade
repetition?”. The interpretation of partial dependence plot is somewhat
similar to the interpretation of linear or logistic regression. On this
plot, The y axis is interpreted as change in the prediction from what it
would be predicted at the baseline or leftmost value. A blue shaded area
indicates level of confidence.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The plot below indicates that being subjected to bullying (as
reflected by having positive value of the variable) increases the
likelihood of students to repeat a grade. Positive values in this index
indicate that the student is more exposed to bullying at school than the
average student in OECD countries. Negative values in this index
indicate that the student is less exposed to bullying at school than the
average student in OECD countries; therefore, having zero does not mean
students did not experience any form of bullying, but rather
experiencing bullying to some degree (i.e., being bullied a bit).
However, the predicting power does not change much after 0, meaning that
the amount of exposure to bullying does not matter in predicting
students’ grade repetition.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;from pdpbox import pdp

pdp_bullied = pdp.pdp_isolate(model=clf_rfc, dataset=X_test, model_features=FEATURES, feature=&amp;#39;BEINGBULLIED&amp;#39;)

pdp.pdp_plot(pdp_bullied, &amp;#39;BEINGBULLIED&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(&amp;lt;Figure size 1500x950 with 2 Axes&amp;gt;, {&amp;#39;title_ax&amp;#39;: &amp;lt;AxesSubplot:&amp;gt;, &amp;#39;pdp_ax&amp;#39;: &amp;lt;AxesSubplot:xlabel=&amp;#39;BEINGBULLIED&amp;#39;&amp;gt;})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file38c45259d0d_files/figure-html/unnamed-chunk-13-9.png" width="1440" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Partial Dependence Plots can also be used to examine interactions
between variables as well. The graph below shows predictions for any
combination of students’ exposure to bullying and the amount of
emotional support from parents. The prediction power is highest when
students score 0 in the index of exposure to bullying (i.e., being
bullied a bit) and having scores on the index of parents’ emotional
support between -1.7 to +0.5. Positive values on this scale mean that
students perceived greater levels of emotional support from their
parents than did the average student across OECD countries while
negative value means otherwise. Having higher exposure to bullying
reduces prediction power of the model as indicated by the changing color
from yellow to green, and when the score in the index of emotional
support reaches 1, the score of the exposure to bullying index becomes
less matter as the prediction power reduces.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;features_to_plot = [&amp;#39;BEINGBULLIED&amp;#39;, &amp;#39;Parent_emosup&amp;#39;]

inter1  =  pdp.pdp_interact(model=clf_rfc, dataset=X_test, model_features=FEATURES, features=features_to_plot)

pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type=&amp;#39;contour&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(&amp;lt;Figure size 750x950 with 3 Axes&amp;gt;, {&amp;#39;title_ax&amp;#39;: &amp;lt;AxesSubplot:&amp;gt;, &amp;#39;pdp_inter_ax&amp;#39;: &amp;lt;AxesSubplot:xlabel=&amp;#39;BEINGBULLIED&amp;#39;, ylabel=&amp;#39;Parent_emosup&amp;#39;&amp;gt;})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file38c45259d0d_files/figure-html/unnamed-chunk-14-11.png" width="720" data-distill-preview=1 /&gt;&lt;/p&gt;
&lt;h3 id="shap-values"&gt;SHAP Values&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Finally, SHAP value allows us to interpret the prediction at a
fine-grained level to the components of individual predictions to show
the impact of each feature. For our case, CHAP value can be used to
answer questions like “On what basis did the model predict that student
A is likely to repeat a grade?”. The plot is quite straightforward to
interpret. The red part shows what increases the likelihood of repeating
a grade, and the blue part shows what decreases the likelihood of
repeating a grade.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On the plot below, the prediction is at the base alue of 0.60,
meaning that it is the average of the model output. For this particular
student, their likelihood to repeat a grade is increased by being
exposed to bullying (BEINGBULLIED), having mediocre emotional support
from parents (Parent_emosup), and having poor overall social standing as
indicated by -0.9 the variable the index of socio-economic, social and
cultural status (ESCS). However, the likelihood is decreased by their
educational resources at home (Home_resource), having cooperative class
(Stu_coop), their parents’ occupational status (Parent_occupation), and
having low record of class skipping (CLASSKIP).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-xaishap_10th_java.png" style="width:85.0%" alt="" /&gt;
&lt;p class="caption"&gt;SHAP Value for a prediction&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id="shap-summary-plot"&gt;SHAP Summary Plot&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In addition to the breakdown of each individual prediction, we
can also visualize groups of SHAP values with SHAP summary plot and SHAP
dependence contribution plot. SHAP summary plots give us an overview of
feature importance and what is driving the prediction. This plot is made
of many dots. Each dot has three characteristics as follows: a)
horizontal location (the x-axis) that indicates whether the effect of
that value caused a higher or lower prediction; b) vertical location
(the y-axis) that indicates the variable name, in order of importance
from top to bottom. c) Gradient color indicates the original value for
that variable. In booleans (i.e., yes/no variable), it will take two
colors, but in number it can contain the whole spectrum.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For example, the left most point in the ‘Parent_emosup’ row is
red in color, meaning that for that particular student, having greater
levels of emotional support from their parents reduces their likelihood
of repeating a grade by roughly 0.3. Seeing variables have a wide spread
in range can be inferred that permutation importance is high; however,
it is best to use permutation importance to measure which variable is
important to the prediction.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Some features such as &lt;code&gt;Home_resource&lt;/code&gt; (educational
resource at home) have reasonably clear separation between the blue and
pink dots, which implies a straightforward meaning that the increase in
the variable value lower (i.e., more resource) the likelihood of
repeating a grade while the decrease in educational resource impacts the
variable in the other direction (higher chance to repeat a
grade).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, some variables such as &lt;code&gt;Stu_coop&lt;/code&gt; (the degree
of cooperativeness within classrooms) have blue and pink dots jumbled
together, suggesting that the increase in this variable leads to higher
predictions, and other times it leads to a lower prediction. In other
words, both high and low values of the variable can have both positive
and negative effects on the prediction. The most likely explanation for
this “jumbling” of effects is that the variable (in this case
&lt;code&gt;Stu_coop&lt;/code&gt;) has an interaction effect with other variables.
For example, there may be some situations where cooperating with other
students lead to &lt;a
href="https://www.simplypsychology.org/social-loafing.html"&gt;social
loafing&lt;/a&gt; - when stduents contribute less effort when working as a
group, and therefore learns less. This interaction needs further
investigation.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;shap_values_summary = explainer.shap_values(X_test)
shap.summary_plot(shap_values[1], X_test)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-xaishap_summary.png" style="width:45.0%" alt="" /&gt;
&lt;p class="caption"&gt;SHAP Summary Plot&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id="shap-dependence-contribution-plot"&gt;SHAP Dependence Contribution
Plot&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The earlier Partial Dependence Plots to show how a single feature
impacts predictions. This is insightful and relevant for many real-world
use cases. The interpretation is also friendly to non-technical audience
as well. However, there is a lot that we still don’t know; for example,
what is the distribution of effects? Is the effect of having a certain
value pretty constant, or does it vary a lot depending on the values of
other feaures. SHAP dependence contribution plots provide a similar
insight to the partial dependence plot, but they add a lot more detail.
The plot shows scatter dots that explain how the effect a single feature
has on the predictions made by the model. The plot can be read as
follows: a) The x-axis is the value of the feature; b) The y-axis is the
SHAP value for that feature, which represents how much knowing that
feature’s value changes the output of the model prediction; c) The color
corresponds to a second feature that may have an interaction effect with
the feature we are plotting.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The plot below shows the relatively flat trend of the
&lt;code&gt;BEINGBULLIED&lt;/code&gt; feature, meaning that this variable does
impact the prediction regardless of the value; this trend is consistent
with the partial dependence plot shown earlier in the post. However,
there is a sign of interaction as there are points with similar value
that produce different outcome. See the left of the 2D pane, for
example. For some students, being less exposed to bullying gives them
more chance to repeat a grade while some students got less chance. There
might be other features that interact with this variable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;While the primary trend is that being bullied increases the
chance of repeating a grade , there are some variations that can be
explained by the interaction of features as well. For a concrete
explanation, see the right of the 2D pane. Being positioned overthere
means that those students experience a lot of bullying, but their chance
of repeating a grade is relatively lower than those who experience less
bullying. One explanation is that some of those students have positive
feelings for themselves (indicated by the red color), which could make
them more resilient toward being bullied.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;shap.dependence_plot(&amp;#39;BEINGBULLIED&amp;#39;, shap_values_summary[1], X_test, interaction_index=&amp;quot;Positive_feel&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-xaishap_dependence_bulliedXpositivefeel.png" style="width:60.0%"
alt="" /&gt;
&lt;p class="caption"&gt;SHAP Dependence Contribution Plot&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;What we have done so far is making a prediction with a Random Forest
Ensemble model, which has high predictive power at the price of being
challenging to explain due to its complexity (&lt;a
href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2822360/"&gt;Zhang &amp;amp;
Wang, 2009&lt;/a&gt;). XAI tools such as permutation importance, partial
dependence plot, and SHAP values, allow us to understand outputs of the
model at various levels from the overall picture to fine-grained
individual cases. Knowing how predictions are made also allow
establishes venues for future studies as well. XAI results are important
to bridge the knowledge gap between technical (e.g., developers) and
non-technical (e.g., customers, users) audiences, which could build
trust and confidence when putting the AI models into the actual use. XAI
also helps an organization develop a responsible approach to AI
development by avoiding the reliance on results that we do not
understand to inform our decisions.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;However, note that XAI is not perfect. Its results are
context-dependent, meaning that if the context changes, so does the
result (&lt;a
href="https://www.sciencedirect.com/science/article/pii/S0740624X21001027"&gt;de
Brujin et al., 2021&lt;/a&gt;). The prediction and how it happens can only be
used as a factor to be considered along with other lines of evidence
such as expert opinion, counter explanations, and potential
consequences. Regardless, XAI is still a useful too to have in expanding
the knowledge we get from machine learning. Thank you very much for
reading!&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>f40f1e89b04d8b250adbd706645111a3</distill:md5>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-28-xai</guid>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-28-xai/xai_files/figure-html5/unnamed-chunk-14-11.png" medium="image" type="image/png" width="1440" height="1824"/>
    </item>
    <item>
      <title>Addressing Data Imbalance with Semi-Supervised Learning</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-28-semisupervised</link>
      <description>For this post, I will use semi-supervised learning approach to perform a classification task with a highly imbalance data.  

(7 min read)</description>
      <category>Python</category>
      <category>Unsupervised Machine Learning</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-28-semisupervised</guid>
      <pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-28-semisupervised/semi-ml.png" medium="image" type="image/png" width="900" height="450"/>
    </item>
    <item>
      <title>Examining Customer Cluster with Unsupervised Machine Learning</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-18-unsupervisedml</link>
      <description>In this post, I will be using two unsupervised learning techniques with a data set, namely K-means clustering and Hierarchical clustering, to determine groups of customers from their age, income, and spending behavior data.  

(8 min read)</description>
      <category>Python</category>
      <category>Unsupervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-18-unsupervisedml</guid>
      <pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-18-unsupervisedml/unsupervisedml_files/figure-html5/unnamed-chunk-14-19.png" medium="image" type="image/png" width="2880" height="1152"/>
    </item>
    <item>
      <title>Combining Multiple Machine Learning Models with the Ensemble Methods</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-09-ensemble</link>
      <description>This entry explores different ways to combine supervised machine learning models to maximize their predictive capability.  

(13 min read)</description>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-09-ensemble</guid>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-09-ensemble/robot.png" medium="image" type="image/png" width="626" height="528"/>
    </item>
    <item>
      <title>Examining PISA 2018 Data Set with Statistical Learning Approach</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-02-27-statlearning</link>
      <description>In this post, I will be developing two statistical learning models, namely Linear Regression and Polynomial Regression, and apply it to Thai student data from the Programme for International Student Assessment (PISA) 2018 data set to examine the impact of classroom competition and cooperation to students' academic performance.  

(14 min read)</description>
      <category>R</category>
      <category>Statistics</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-02-27-statlearning</guid>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-02-27-statlearning/statlearn.jpeg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Classical Test Theory in R</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-01-15-ctt</link>
      <description>For this post, I will be analyzing characteristics of test items based on the framework of Classical Test Theory (CTT).

(13 min read)</description>
      <category>R</category>
      <category>Psychometric</category>
      <guid>https://taridwong.github.io/posts/2022-01-15-ctt</guid>
      <pubDate>Sat, 15 Jan 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-01-15-ctt/ctt_files/figure-html5/unnamed-chunk-28-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Examining the Big 5 personality Dataset with factor analysis</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-01-01-efacfa</link>
      <description>For this entry, I will be examining the Big 5 personality Inventory data set with Exploratory Data Analysis to identify potential structures of personality trait and verify them with Confirmatory Factor Analysis.

(8 min read)</description>
      <category>R</category>
      <category>Statistics</category>
      <guid>https://taridwong.github.io/posts/2022-01-01-efacfa</guid>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-01-01-efacfa/corrmatrix.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Measuring Text Similarity with Movie plot data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-29-movie-similarity</link>
      <description>For this post, I will be analyzing textual data of movie plots to determine their similarity with TF-IDF and Clustering.

(7 min read)</description>
      <category>Python</category>
      <category>Natural Language Processing</category>
      <category>Unsupervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-29-movie-similarity</guid>
      <pubDate>Wed, 29 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-29-movie-similarity/movie-similarity_files/figure-html5/unnamed-chunk-11-1.png" medium="image" type="image/png" width="484" height="483"/>
    </item>
    <item>
      <title>Missing Data Analysis</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-27-missingdata</link>
      <description>For this post, I will examine missing data in a large-scale dataset and discuss about numerous ways we can clean them as a part of data preparation.

(10 min read)</description>
      <category>R</category>
      <category>Data Visualization</category>
      <category>Statistics</category>
      <guid>https://taridwong.github.io/posts/2021-12-27-missingdata</guid>
      <pubDate>Mon, 27 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-27-missingdata/missingpic.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Applying Machine Learning to Audio Data: Visualization, Classification, and Recommendation</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data</link>
      <description>For this entry, I am trying my hands on audio data to extract its features for exploratory data analysis (EDA), using machine learning algorithms to perform music classification, and finally build up on that result to develop a recommendation system for music of similar characteristics.

(13 min read)</description>
      <category>Python</category>
      <category>Data Visualization</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data</guid>
      <pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data/applying-machine-learning-to-audio-data_files/figure-html5/unnamed-chunk-13-11.png" medium="image" type="image/png" width="3072" height="1152"/>
    </item>
    <item>
      <title>Interactive plots for Suicide Data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data</link>
      <description>For this entry, will be visualizing suicide data from 1958 to 2015 with interactive plots to communicate insights to non-technical audience.  

(14 min read)</description>
      <category>R</category>
      <category>Data Visualization</category>
      <category>Quantitative research</category>
      <guid>https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data</guid>
      <pubDate>Thu, 09 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data/preview.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Image Recognition with Artificial Neural Networks</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks</link>
      <description>In this entry, we will be developing a deep learning algorithm - a sub-field of machine learning inspired by the structure of human brain (neural networks) - to classify images of single digit number (0-9).  

(9 min read)</description>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <category>Deep Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks</guid>
      <pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks/deepnn.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Anomaly Detection with New York City taxi data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data</link>
      <description>In this entry, I will be conducting anomaly detection to identify points of anomaly in the taxi passengers data in New York City from July 2014 to January 2015 at half-hourly intervals.
 
 (4 min read)</description>
      <category>Unsupervised Machine Learning</category>
      <category>R</category>
      <guid>https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data</guid>
      <pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data/anomaly-detection-with-new-york-city-taxi-data_files/figure-html5/unnamed-chunk-9-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Crime mapping in San Francisco with police data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data</link>
      <description>In this entry, we will explore San Francisco crime data from 2016 to 2018 to understand the relationship between civilian-reported incidents of crime and police-reported incidents of crime. Along the way we will use table intersection methods to subset our data, aggregation methods to calculate important statistics, and simple visualizations to understand crime trends.  

(7 min read)</description>
      <category>Data Visualization</category>
      <category>R</category>
      <category>Quantitative research</category>
      <guid>https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data</guid>
      <pubDate>Tue, 30 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data/crime-mapping-in-san-francisco-with-police-data_files/figure-html5/unnamed-chunk-6-1.png" medium="image" type="image/png" width="1920" height="1920"/>
    </item>
    <item>
      <title>Exploring COVID-19 data from twitter with topic modeling</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds</link>
      <description>


&lt;h2 id="covid-19-situation-in-alberta-canada"&gt;COVID-19 situation in
Alberta, Canada&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The province of Alberta, Canada, has suffered from the COVID-19
pandemic like all other places. Alberta has gone through cycles of
reopening and returning to provincial lock down since April 2020. The
province, however, has lifted almost all restrictions and enacted its
reopening plan on the recent &lt;a
href="https://calgary.ctvnews.ca/alberta-moves-to-stage-3-of-reopening-plan-on-canada-day-1.5475913"&gt;Canada
day&lt;/a&gt; when 70% of Alberta population has received at least one dose of
&lt;a
href="https://www.canada.ca/en/public-health/services/diseases/coronavirus-disease-covid-19/vaccines.html"&gt;approved
COVID-19 vaccination&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The navigation of the province through this pandemic was led by
the Alberta’s Chief Medical Officer of Health, &lt;a
href="https://www.alberta.ca/office-of-the-chief-medical-officer-of-health.aspx"&gt;Dr. Deena
Hinshaw&lt;/a&gt;. Dr.Hinshaw usually held public health briefings almost
every day during wave 1 to wave 3 of the pandemic, but her communication
channel has changed in wave 4 as less public health briefing was held
and more tweets were posted on the &lt;a
href="https://twitter.com/CMOH_Alberta"&gt;her account&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For that, I believe we could use &lt;a
href="https://en.wikipedia.org/wiki/Natural_language_processing"&gt;Natural
Language Processing (NLP)&lt;/a&gt; techniques to extract themes and
characteristics from Dr.Hinshaw’s tweet to examine the essence of public
health messages since the provincial reopening date, specifically from
July 1st to October 31st, 2021.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="text-mining-and-word-cloud-fundamentals"&gt;Text mining and word
cloud fundamentals&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For this post, we will use text mining and word clouds to
initially explore characteristics of the data set. Text mining is an
exploratory method for textual data under Natural Language Processing
(NLP), a branch of Artificial Intelligence concerning the understanding
of words and spoken texts. NLP is also a type of unsupervised machine
learning approach to discover hidden structures in the data to inform
decisions made by experts of the subject matter.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Word cloud is also a popular way to to communicate findings from
textual data in a visually engaging way. The more frequent a word appear
in the data set (or corpus) the bigger that word will be in the
cloud.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will use Python to perform this analysis on R platform with
&lt;code&gt;reticulate::repl_python()&lt;/code&gt;. First of all, we will be
importing necessary modules and twitter data set that we mined from
Dr. Hinshaw’s account with &lt;code&gt;pd.read_csv&lt;/code&gt;. There are 538
tweets in total, and we can print out examples of the tweets via
&lt;code&gt;tweets_df.Text.head(5)&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;#Import necessary modules

import numpy as np #for numpy array
import pandas as pd #for data reading and processing
import matplotlib.pyplot as plt #for plotting
import re #for Regex text cleaning
from wordcloud import WordCloud, STOPWORDS #for word clouds
from nltk.stem import WordNetLemmatizer #to reduce text to base form
from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation as LDA #for topic modeling
import warnings

warnings.filterwarnings(&amp;quot;ignore&amp;quot;) #suppress the warning that Python kindly gave me

tweets_df = pd.read_csv(&amp;quot;text-query-tweets.csv&amp;quot;)

tweets_df.shape

# Print out the first rows of papers&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(538, 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(tweets_df.Text.head(5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    We all have the ability to take small actions ...
1    As we head into Halloween weekend, I encourage...
2    Sadly, 9 new deaths related to COVID-19 were a...
3    Over the past 24 hours, we ID’d 603 new cases ...
4    Here is a summary of the latest #COVID19AB num...
Name: Text, dtype: object&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="lets-clean-the-text-first"&gt;Let’s clean the text first&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;After we imported our data into the system, we have to clean our
data to get rid of textual elements that we do not need such as
punctuation, numbers, as well as convert all words to lower case.
Painful as it may be, this has to be done. It took me days (not that
much, but I felt it that way) to clean all of this and make sure that no
junk is left behind (well, there could be. Do let me know if you find
any).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The phrase “Garbage in, garbage out” is really applicable here in
data work context. If you let any junk (corrupted data) in, the most you
will get is processed junk. After we cleaned the text, let us print them
out again to see what they look like. All numbers are gone. All texts
are in lowercase. All URLs and punctuation is gone. Good
riddance!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ATTN nerds: Note that in the code below, we will pass the
original &lt;code&gt;Text&lt;/code&gt; column in &lt;code&gt;tweets_df&lt;/code&gt; to the
&lt;code&gt;re.sub&lt;/code&gt; function only once. For the second cleaning function
onward, we will pass &lt;code&gt;tweets_df['Text_processed']&lt;/code&gt; instead to
stack our text cleaning results on the same column. Yes, I wrote this to
remind myself because I struggled on it for hours (half an hour,
actually).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;
#remove all numbers from the text with list comprehension
tweets_df[&amp;#39;Text_processed&amp;#39;] = tweets_df[&amp;#39;Text&amp;#39;].map(lambda x: re.sub(r&amp;#39;[0-9]+&amp;#39;, &amp;#39;&amp;#39;, x))

# Remove punctuation
tweets_df[&amp;#39;Text_processed&amp;#39;] = tweets_df[&amp;#39;Text_processed&amp;#39;].map(lambda x: re.sub(r&amp;#39;[^\w\s\,\.!?]&amp;#39;, &amp;#39;&amp;#39;, x))

# Convert the tweets to lowercase
tweets_df[&amp;#39;Text_processed&amp;#39;] = tweets_df[&amp;#39;Text_processed&amp;#39;].map(lambda x: x.lower())

#Clean out URLs
tweets_df[&amp;#39;Text_processed&amp;#39;] = tweets_df[&amp;#39;Text_processed&amp;#39;].map(lambda x: re.sub(r&amp;quot;http\S+&amp;quot;, &amp;quot;&amp;quot;, x))

# Print the processed titles of the first rows 
print(tweets_df[&amp;#39;Text_processed&amp;#39;].head())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    we all have the ability to take small actions ...
1    as we head into halloween weekend, i encourage...
2    sadly,  new deaths related to covid were also ...
3    over the past  hours, we idd  new cases amp co...
4    here is a summary of the latest covidab number...
Name: Text_processed, dtype: object&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="so-this-is-whats-happening-over-time"&gt;So this is what’s
happening over time&lt;/h2&gt;
&lt;pre class="python"&gt;&lt;code&gt;
#Change datetime format to datetime
tweets_df[&amp;#39;Datetime&amp;#39;] = pd.to_datetime(tweets_df[&amp;#39;Datetime&amp;#39;])

#Extract month from datetime
tweets_df[&amp;#39;Month&amp;#39;] = tweets_df[&amp;#39;Datetime&amp;#39;].dt.month

# Group the papers by year
groups = tweets_df.groupby(&amp;#39;Month&amp;#39;)

# Determine the size of each group
counts = groups.size()

# Visualize the counts as a bar plot

# Vertical lines
plt.axvline(x = 7.0, color = &amp;#39;forestgreen&amp;#39;, label = &amp;#39;The reopening date&amp;#39;, linestyle=&amp;#39;--&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.lines.Line2D object at 0x0000013A868AB4C0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.axvline(x = 8.0, color = &amp;#39;firebrick&amp;#39;, label = &amp;#39;Wave 4 started&amp;#39;, linestyle=&amp;#39;--&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.lines.Line2D object at 0x0000013A868AB9D0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.legend(bbox_to_anchor = (1.0, 1), loc = &amp;#39;upper right&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.legend.Legend object at 0x0000013A868AB490&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Tweet count across months&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Tweet count across months&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;quot;Tweet count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;Tweet count&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.xlabel(&amp;quot;Month&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;Month&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;counts.plot()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;AxesSubplot:title={&amp;#39;center&amp;#39;:&amp;#39;Tweet count across months&amp;#39;}, xlabel=&amp;#39;Month&amp;#39;, ylabel=&amp;#39;Tweet count&amp;#39;&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file38c42d7e4cb1_files/figure-html/unnamed-chunk-5-1.png" width="720" style="display: block; margin: auto;" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The line plot above represents tweet counts across months after the
provincial reopening date. The x-axis indicates months and the y-axis
indicates the number of twitter post of Dr. Hinshaw. The number of tweet
dropped slightly from July to August as cases decreased, but wave 4 of
the pandemic started in August as cases were on the rise again. We can
see that the number of cases aligns with the number of tweets posted on
Dr. Hinshaw’s account.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="lets-see-the-big-picture-with-word-cloud"&gt;Let’s see the big
picture with word cloud&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Now that we know the frequency of tweets over months, we can plot a
word cloud from our processed text to see the big picture of twitter
data. There are 114,362 words in total after combining all 538 tweets
together. The word cloud below suggests that “covid” was mentioned the
most during the past four months, following by “vaccine”, “new cases”,
and “unvaccinated”.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;
text_all = &amp;quot; &amp;quot;.join(tweet for tweet in tweets_df.Text_processed)
print (&amp;quot;There are {} words in the combination of all tweets&amp;quot;.format(len(text_all)))

#lemmatize all words&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;There are 114362 words in the combination of all tweets&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;lemmatizer = WordNetLemmatizer()
text_all = &amp;quot;&amp;quot;.join([lemmatizer.lemmatize(i) for i in text_all])

# Create Stopword list:
stopwords_cloud = set(STOPWORDS)
stopwords_cloud.update([&amp;quot;https://&amp;quot;, &amp;quot;(/)&amp;quot;, &amp;quot;Online:&amp;quot;, 
                        &amp;quot;Twitter:&amp;quot;, &amp;quot;Join&amp;quot;, &amp;quot;us&amp;quot;, &amp;quot;virtually&amp;quot;,
                        &amp;quot;pm&amp;quot;, &amp;quot;:&amp;quot;, &amp;quot;https&amp;quot;, &amp;quot;t&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;co&amp;quot;, &amp;quot;amp&amp;quot;, &amp;quot;will&amp;quot;])
                      
#Generate a word cloud image
wordcloud_tweet = WordCloud(stopwords=stopwords_cloud, background_color=&amp;quot;white&amp;quot;,random_state=7).generate(text_all)

#Display the generated image:
#the matplotlib way:
plt.figure(figsize=[10,10])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1000x1000 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.imshow(wordcloud_tweet, interpolation=&amp;#39;bilinear&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage object at 0x0000013A868DA5E0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.axis(&amp;quot;off&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(-0.5, 399.5, 199.5, -0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file38c42d7e4cb1_files/figure-html/unnamed-chunk-6-3.png" width="960" style="display: block; margin: auto;" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The thing is, word cloud can only provide a rough visual
presentation for the characteristics of our textual data. We would need
to dive a little bit deeper to graphs and numbers to examine what is
truly going on. Let us visualize them all on a bar plot.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2
id="common-word-bar-plot-and-text-preprocessing-for-topic-modeling"&gt;Common
word bar plot and text preprocessing for topic modeling&lt;/h2&gt;
&lt;pre class="python"&gt;&lt;code&gt;
# Helper function to count common words

def plot_10_most_common_words(count_data, tfidf_vectorizer):
    import matplotlib.pyplot as plt
    words = tfidf_vectorizer.get_feature_names()
    total_counts = np.zeros(len(words))
    for t in count_data:
        total_counts+=t.toarray()[0]
    
    count_dict = (zip(words, total_counts))
    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]
    words = [w[0] for w in count_dict]
    counts = [w[1] for w in count_dict]
    x_pos = np.arange(len(words)) 

    plt.bar(x_pos, counts,align=&amp;#39;center&amp;#39;)
    plt.xticks(x_pos, words, rotation=90) 
    plt.xlabel(&amp;#39;words&amp;#39;)
    plt.ylabel(&amp;#39;counts&amp;#39;)
    plt.title(&amp;#39;10 most common words&amp;#39;)
    plt.show()

#Make your own list of stop words
my_additional_stop_words = (&amp;quot;https://&amp;quot;, &amp;quot;(/)&amp;quot;, &amp;quot;➡Online:&amp;quot;, 
                        &amp;quot;➡Twitter:&amp;quot;, &amp;quot;Join&amp;quot;, &amp;quot;us&amp;quot;, &amp;quot;virtually&amp;quot;,
                        &amp;quot;pm&amp;quot;, &amp;quot;:&amp;quot;, &amp;quot;https&amp;quot;, &amp;quot;t&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;co&amp;quot;, &amp;quot;amp&amp;quot;, &amp;quot;today&amp;quot;, &amp;quot;new&amp;quot;, &amp;quot;covid&amp;quot;,
                        &amp;quot;covidab&amp;quot;, &amp;quot;hours&amp;quot;, &amp;quot;completed&amp;quot;)
                        
stop_words_lda = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)    

# Initialize the count vectorizer with the English stop words
tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words_lda)

# Fit and transform the processed titles
count_data = tfidf_vectorizer.fit_transform(tweets_df[&amp;#39;Text_processed&amp;#39;])

# Visualise the 10 most common words
plot_10_most_common_words(count_data, tfidf_vectorizer)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file38c42d7e4cb1_files/figure-html/unnamed-chunk-7-5.png" width="960" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The bar plot above gave us a more detailed information of which
word occurs more frequently than the others based on the term
frequency–inverse document frequency (TFIDF) statistics. TFIDF gives
each word a weight that reflects its importance to a document. For
TFIDF, words that occur too frequent like “the” provides little meaning
while words rarely occur doesn’t tell us much as well. We are taking
about the COVID-19 pandemic here, so it is obvious that “vaccinated” is
going to be mentioned the most in Dr. Hinshaw’s tweet. “Cases” and
“unvaccinated” seem to be reasonable to be mentioned as the second- and
third most important words as the government of Alberta has been putting
more effort in identifying more cases in the province and encourage
unvaccinated individuals to get their vaccine.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will also create a &lt;code&gt;tfidf_vectorizer&lt;/code&gt; model with
our own list of stopwords (or words that have little meaning such as
“is, am, are”) to prepare our data for Latent Dirichlet Allocation (LDA)
topic modeling.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2
id="finally-lets-see-potential-topics-from-dr.-hinshaws-tweet"&gt;Finally,
let’s see potential topics from Dr. Hinshaw’s tweet&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Latent Dirichlet Allocation is a powerful natural language
processing technique that discovers hidden patterns in topic from
unstructured textual data with statistical models &lt;a
href="https://link.springer.com/content/pdf/10.1007/s11042-018-6894-4.pdf"&gt;(Jelodar
et al., 2019)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Here, we can use LDA to discover potential topics among the sea
of tweets posted by Dr. Hinshaw to find out what she talked about since
the provincial reopening and wave 4 of the pandemic. I have specified
the model to extract 8 topics from the data, with 5 words per topics.
Note that these numbers are arbitrary chosen.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If we extracted too few topics, we might not be able to capture
the whole picture of the data. On the other hand, extracting too much
topics could just give us more of the same overlapping themes. We need
to find the middle ground.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;
# Helper function to print out the topics
def print_topics(model, tfidf_vectorizer, n_top_words):
    words = tfidf_vectorizer.get_feature_names()
    for topic_idx, topic in enumerate(model.components_):
        print(&amp;quot;\nTopic #%d:&amp;quot; % topic_idx)
        print(&amp;quot; &amp;quot;.join([words[i]
                        for i in topic.argsort()[:-n_top_words - 1:-1]]))
                        
#How many topic and words per topic we want to see
number_topics = 8
number_words = 5 
                      
# Create and fit the LDA model
lda = LDA(n_components=number_topics, random_state = 1)
lda.fit(count_data)

# Print the topics found by the LDA model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;LatentDirichletAllocation(n_components=8, random_state=1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print_topics(lda, tfidf_vectorizer, number_words)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Topic #0:
twitter online join video transcript

Topic #1:
possible information protection protect soon

Topic #2:
vaccines protect vaccine dose book

Topic #3:
cases tests partially unvaccinated idd

Topic #4:
reported deaths sadly condolences alberta

Topic #5:
oct age steps important dr

Topic #6:
matter pandemic report continue health

Topic #7:
ahs participating prevent book available&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="wrapping-up-here.-what-can-we-conclude"&gt;Wrapping up here. What
can we conclude?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The topics we discovered above can be inferred as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Topic 0: An invitation for the general population to join a live
update video on Twitter.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Topic 1: The availability of possible information on COVID-19
protection&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Topic 2: Encouragement to book for a vaccination for more
protection.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Topic 3: The proportion of unvaxxed vs vaxxed vs partiallyvaxxed
patients.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Topic 4: Covid-related death.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The insights that we gained could also be further supported by
opinion from public health experts as they could provide information at
a greater depth into their field.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;From what we have discussed so far, we can see that with the
right tool, LDA for our case, we could take advantage of the vast
availability of textual data that revolves around us in our everyday
lives and use that information to deepen our understanding of social
phenomena. We could explore how students opinion changed from pre- to
post-COVID era, or we could use this technique to media transcription of
social events such as political protests, election speech, or even
product review in the marketing field. Thank you for your
reading!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>150e32d23689517567eba0ddc47378ec</distill:md5>
      <category>Python</category>
      <category>Natural Language Processing</category>
      <category>Unsupervised Machine Learning</category>
      <category>COVID-19</category>
      <guid>https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds</guid>
      <pubDate>Tue, 23 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds/word-cloud-pv.png" medium="image" type="image/png" width="1920" height="1920"/>
    </item>
    <item>
      <title>Finding a home among the paradigm push-back with Dialectical Pluralism</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-07-introducing-dialectical-pluralism</link>
      <description>This entry discusses the reconciliation of quantitative and qualitative worldviews amidst the paradigm wars with pluralistic stance.

(2 min read)</description>
      <category>Mixed methods research</category>
      <guid>https://taridwong.github.io/posts/2021-11-07-introducing-dialectical-pluralism</guid>
      <pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://www.relevantinsights.com/wp-content/uploads/2020/05/Qualitative-Quantitative-Research-For-New-Product-Development.png" medium="image" type="image/png"/>
    </item>
  </channel>
</rss>
