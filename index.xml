<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>Tarid Wongvorachan</title>
    <link>https://taridwong.github.io/</link>
    <atom:link href="https://taridwong.github.io/index.xml" rel="self" type="application/rss+xml"/>
    <description>Welcome to my data science blog!
</description>
    <generator>Distill</generator>
    <lastBuildDate>Sun, 30 Apr 2023 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Examining Big 5 Personality Inventory Data with Network Psychometrics</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2023-04-30-networkpsych</link>
      <description>


&lt;h2 id="saying-hi"&gt;Saying Hi&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Hi, everyone. We are heading into spring here in Edmonton. There
is no class to teach, so I have some more free time to work on projects
that I have on hands. I took an advance psychometric course this past
winter 2023 semester and learned a lot of useful techniques. One of them
is Network psychometrics.&lt;br /&gt;
&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Network psychometrics is a relatively novel approach to
psychometrics research that examines relationships between observed
variables (assessment items) without relying on the assumption of latent
variables (&lt;a
href="https://www.nature.com/articles/s43586-021-00055-w"&gt;Borsboom et
al., 2021&lt;/a&gt;). Items that are related to each other may appear closer
while items that are less relevant may be positioned further on a
network graph. This approach offers an alternative evidence to validity
of the interpretations and uses of a test in addition to the traditional
factor analysis method (&lt;a
href="https://link.springer.com/10.3758/s13428-020-01500-6"&gt;Christensen
&amp;amp; Golino, 2021&lt;/a&gt;).&lt;br /&gt;
&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In this post, I used response data to the Big 5 personality
inventory from the &lt;a
href="http://openpsychometrics.org/_rawdata/"&gt;open-source psychometrics
project&lt;/a&gt;. Items from the test were put together from the
international personality item pool. The data set has N = 19,719
responses to 50 Likert-scale items asking of respondents’ agreement to
the presented statements such as “I don’t talk a lot”, “I shirk my
duties”, or “I am quick to understand things”. Responses were coded as 1
to, where 1=Disagree, 3=Neutral, 5=Agree. Five personality traits of
extraversion, neuroticism, agreeableness, conscientiousness, and
openness to experience are reported as results.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will start by loading packages that we will use.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(&amp;quot;dplyr&amp;quot;)
# for network modeling 
library(&amp;quot;qgraph&amp;quot;)
library(&amp;quot;psychonetrics&amp;quot;)
library(&amp;quot;bootnet&amp;quot;)  
library(&amp;quot;EGAnet&amp;quot;)  &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Next, we will extract response data from the whole data set, as well
as group items into their respective dimensions.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;df_response &amp;lt;- df[, 8:57]

groups &amp;lt;- list(Extraversion = 1:10,
               Neuroticism = 11:20,
               Agreeableness = 21:30,
               Conscientiousness = 31:40,
               Openness = 41:50)

obsvars &amp;lt;- colnames(df_response)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="network-psychometrics"&gt;Network Psychometrics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To construct a psychometrics network, we will use the
&lt;code&gt;estimateNetwork&lt;/code&gt; function from &lt;code&gt;bootnet&lt;/code&gt; package.
Relationships between items will be determined by polychoric
correlations, and the network graph was estimated with the graphical
least absolute shrinkage and selection operator (Glasso) estimator.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;network_big5 &amp;lt;- bootnet::estimateNetwork(
  data = df_response,
  # Alternatively, &amp;quot;cov&amp;quot; for covariances, &amp;quot;cor&amp;quot; for correlations 
  corMethod = &amp;quot;cor_auto&amp;quot;, # for polychoric and polyserial correlations
  # Alternatively, &amp;quot;ggmModSelect&amp;quot; for an unregularized GGM using glasso
  default = &amp;quot;EBICglasso&amp;quot;, # for estimating GGM with gLASSO and EBIC
  tuning = 0.5 # EBIC tuning parameter; set to zero for BIC model selection. If you make it large, you should justify it.
)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Below is th initial network of the Big 5 personality data set that
we used. Items of the same dimensions are grouped together, meaning that
they measure similar construct to each other. Note that edges (or
linkages) between items can be adjusted. We can remove (or prune)
statistically insignificant edges and add edges that improve model
fit.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(network_big5, 
     layout = &amp;quot;spring&amp;quot;, 
     palette = &amp;quot;colorblind&amp;quot;, 
     groups = groups, # to label each group
     font = 2,
     label.cex = 1
     ) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3da070c04f0c_files/figure-html/unnamed-chunk-6-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We will use the &lt;code&gt;prune&lt;/code&gt; function from
&lt;code&gt;psychonetrics&lt;/code&gt; package to prune edges that do not achieve
statistical significance at alpha 0.05 (step down method). Then, we will
add more edges until model fit of the data set based on Bayesian
Information Criterion (BIC) does not improve anymore based on alpha =
0.05 with the &lt;code&gt;stepup&lt;/code&gt; function (step up method).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;network_big5_optimized &amp;lt;- psychonetrics::ggm(df_response, 
                                     vars = obsvars) %&amp;gt;%
  psychonetrics::runmodel() %&amp;gt;%
  psychonetrics::prune(adjust = &amp;quot;fdr&amp;quot;, alpha = 0.05) %&amp;gt;%
  # To automatically add edges at  alpha=0.05 until BIC is no longer be improved
  psychonetrics::stepup(criterion = &amp;quot;bic&amp;quot;, alpha = 0.05) %&amp;gt;%
  psychonetrics::modelsearch()&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can check fit indices of the optimized network model and its
network graph with the code below. The network is a lot more complex
with more edges added to the graph. The items still stay with their
peers in their respective domain, meaning that structure of the test
still holds after the optimization.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Look at the model fit
network_big5_optimized %&amp;gt;% psychonetrics::fit()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           Measure       Value
              logl -1373902.53
 unrestricted.logl -1373377.80
     baseline.logl -1561979.97
              nvar          50
              nobs        1325
              npar         747
                df         578
         objective       47.45
             chisq     1049.45
            pvalue         ~ 0
    baseline.chisq   377204.34
       baseline.df        1225
   baseline.pvalue         ~ 0
               nfi         1.0
              pnfi        0.47
               tli         1.0
              nnfi         1.0
               rfi        0.99
               ifi         1.0
               rni         1.0
               cfi         1.0
             rmsea      0.0064
    rmsea.ci.lower      0.0058
    rmsea.ci.upper      0.0070
      rmsea.pvalue           1
            aic.ll  2749299.06
           aic.ll2  2749357.96
             aic.x     -106.55
            aic.x2     2543.45
               bic  2755192.39
              bic2  2752818.46
           ebic.25  2758114.67
            ebic.5  2761036.96
           ebic.75  2763374.78
             ebic1  2766881.52&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Obtain the network plot
net_optimized &amp;lt;- psychonetrics::getmatrix(network_big5_optimized, &amp;quot;omega&amp;quot;)

qgraph::qgraph(net_optimized, 
               layout = &amp;quot;spring&amp;quot;, 
               theme = &amp;quot;colorblind&amp;quot;,
               labels = obsvars,
               groups = groups,
               negDashed = T, # should negative edges be dashed?
               font = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3da070c04f0c_files/figure-html/unnamed-chunk-8-1.png" width="672" /&gt;&lt;/p&gt;
&lt;h2
id="exploratory-graph-analysis-for-dimensional-stability"&gt;Exploratory
Graph Analysis for Dimensional Stability&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Aside from investigating the network structure, we can also use
the network approach to examine dimensional stability of the test with
the exploratory graph analysis (EGA) method. EGA tests dimensional
stability of a test by examining its structure across several resampling
iterations. In other words, EGA checks if structure of the test is
similar across different response patterns.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will use the &lt;code&gt;bootEGA&lt;/code&gt; function from the
&lt;code&gt;EGAnet&lt;/code&gt; package to perform EGA with the big 5 personality
inventory data. Usually 500 resampling iterations is recommended, but we
will stick to 100 to make it computationally feasible.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;bootEGA_big5 &amp;lt;- EGAnet::bootEGA(
  # we could also provide the cor matrix but then
  # n (i.e., number of rows) must also be specified
  data = df_response, 
  cor = &amp;quot;cor_auto&amp;quot;,
  uni.method = &amp;quot;louvain&amp;quot;,
  iter = 100, # Number of replica samples to generate
  # resampling&amp;quot; for n random subsamples of the original data
  # parametric&amp;quot; for n synthetic samples from multivariate normal dist.
  type = &amp;quot;parametric&amp;quot;, 
  # EGA Uses standard exploratory graph analysis
  # EGA.fit Uses total entropy fit index (tefi) to determine best fit of EGA
  # hierEGA Uses hierarchical exploratory graph analysis
  EGA.type = &amp;quot;EGA&amp;quot;, 
  model = &amp;quot;glasso&amp;quot;, 
  algorithm = &amp;quot;walktrap&amp;quot;, # or &amp;quot;louvain&amp;quot; (better for unidimensional structures)
  # use &amp;quot;highest_modularity&amp;quot;, &amp;quot;most_common&amp;quot;, or &amp;quot;lowest_tefi&amp;quot;
  consensus.method = &amp;quot;highest_modularity&amp;quot;, 
  typicalStructure = TRUE, # typical network of partial correlations
  plot.typicalStructure = TRUE, # returns a plot of the typical network
  ncores = 4, # Number of cores to use in computing results
  progress = FALSE ,
  summary.table = TRUE
)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The network below shows that structure of our data set holds despite
being tested on different data sets across 100 resampling iterations.
This means that the 5 dimensions of our test is quite stable.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2023-04-30-networkpsychega.png" alt="" /&gt;
&lt;p class="caption"&gt;Exploratory Graph Analysis Result&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;We can also request for written results of EGA. All items are loaded
onto their respective dimensions. For example, all 10 extraversion items
are loaded onto the 5th dimension. The same applies to the remaining 40
items as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# View the number of communities
bootEGA_big5$EGA&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Number of communities: 5 

 E1  E2  E3  E4  E5  E6  E7  E8  E9 E10  N1  N2  N3  N4  N5  N6  N7 
  5   5   5   5   5   5   5   5   5   5   3   3   3   3   3   3   3 
 N8  N9 N10  A1  A2  A3  A4  A5  A6  A7  A8  A9 A10  C1  C2  C3  C4 
  3   3   3   2   2   2   2   2   2   2   2   2   2   4   4   4   4 
 C5  C6  C7  C8  C9 C10  O1  O2  O3  O4  O5  O6  O7  O8  O9 O10 
  4   4   4   4   4   4   1   1   1   1   1   1   1   1   1   1 

Methods:
                                                         
Correlations =          auto (from qgraph)               
Model =                 glasso                           
Algorithm =             walktrap                         
Unidimensional Method = louvain with consensus clustering&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;bootEGA_big5$typicalGraph$typical.dim.variables&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    items dimension
O1     O1         1
O2     O2         1
O3     O3         1
O4     O4         1
O5     O5         1
O6     O6         1
O7     O7         1
O8     O8         1
O9     O9         1
O10   O10         1
A1     A1         2
A2     A2         2
A3     A3         2
A4     A4         2
A5     A5         2
A6     A6         2
A7     A7         2
A8     A8         2
A9     A9         2
A10   A10         2
N1     N1         3
N2     N2         3
N3     N3         3
N4     N4         3
N5     N5         3
N6     N6         3
N7     N7         3
N8     N8         3
N9     N9         3
N10   N10         3
C1     C1         4
C2     C2         4
C3     C3         4
C4     C4         4
C5     C5         4
C6     C6         4
C7     C7         4
C8     C8         4
C9     C9         4
C10   C10         4
E1     E1         5
E2     E2         5
E3     E3         5
E4     E4         5
E5     E5         5
E6     E6         5
E7     E7         5
E8     E8         5
E9     E9         5
E10   E10         5&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can also check if all items are stable across 100 iterations. The
empirical EGA Communities plot below indicates that all items and
dimensions have value = 1 across all iterations, meaning that the items
are loaded into the same test community (aka measuring the same
construct) across different response patterns.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Dimension (i.e., structural) stability results
dim_big5 &amp;lt;- EGAnet::dimensionStability(bootEGA_big5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3da070c04f0c_files/figure-html/unnamed-chunk-11-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;dim_big5$dimension.stability&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$structural.consistency
1 2 3 4 5 
1 1 1 1 1 

$average.item.stability
1 2 3 4 5 
1 1 1 1 1 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Item stability results
dim_big5$item.stability&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$membership
$membership$empirical
 E1  E2  E3  E4  E5  E6  E7  E8  E9 E10  N1  N2  N3  N4  N5  N6  N7 
  5   5   5   5   5   5   5   5   5   5   3   3   3   3   3   3   3 
 N8  N9 N10  A1  A2  A3  A4  A5  A6  A7  A8  A9 A10  C1  C2  C3  C4 
  3   3   3   2   2   2   2   2   2   2   2   2   2   4   4   4   4 
 C5  C6  C7  C8  C9 C10  O1  O2  O3  O4  O5  O6  O7  O8  O9 O10 
  4   4   4   4   4   4   1   1   1   1   1   1   1   1   1   1 

$membership$unique
[1] 5 3 2 4 1

$membership$bootstrap
    [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
E1     5    5    5    5    5    5    5    5    5     5     5     5
E2     5    5    5    5    5    5    5    5    5     5     5     5
E3     5    5    5    5    5    5    5    5    5     5     5     5
E4     5    5    5    5    5    5    5    5    5     5     5     5
E5     5    5    5    5    5    5    5    5    5     5     5     5
E6     5    5    5    5    5    5    5    5    5     5     5     5
E7     5    5    5    5    5    5    5    5    5     5     5     5
E8     5    5    5    5    5    5    5    5    5     5     5     5
E9     5    5    5    5    5    5    5    5    5     5     5     5
E10    5    5    5    5    5    5    5    5    5     5     5     5
N1     3    3    3    3    3    3    3    3    3     3     3     3
N2     3    3    3    3    3    3    3    3    3     3     3     3
N3     3    3    3    3    3    3    3    3    3     3     3     3
N4     3    3    3    3    3    3    3    3    3     3     3     3
N5     3    3    3    3    3    3    3    3    3     3     3     3
N6     3    3    3    3    3    3    3    3    3     3     3     3
N7     3    3    3    3    3    3    3    3    3     3     3     3
N8     3    3    3    3    3    3    3    3    3     3     3     3
N9     3    3    3    3    3    3    3    3    3     3     3     3
N10    3    3    3    3    3    3    3    3    3     3     3     3
A1     2    2    2    2    2    2    2    2    2     2     2     2
A2     2    2    2    2    2    2    2    2    2     2     2     2
A3     2    2    2    2    2    2    2    2    2     2     2     2
A4     2    2    2    2    2    2    2    2    2     2     2     2
A5     2    2    2    2    2    2    2    2    2     2     2     2
A6     2    2    2    2    2    2    2    2    2     2     2     2
A7     2    2    2    2    2    2    2    2    2     2     2     2
A8     2    2    2    2    2    2    2    2    2     2     2     2
A9     2    2    2    2    2    2    2    2    2     2     2     2
A10    2    2    2    2    2    2    2    2    2     2     2     2
C1     4    4    4    4    4    4    4    4    4     4     4     4
C2     4    4    4    4    4    4    4    4    4     4     4     4
C3     4    4    4    4    4    4    4    4    4     4     4     4
C4     4    4    4    4    4    4    4    4    4     4     4     4
C5     4    4    4    4    4    4    4    4    4     4     4     4
C6     4    4    4    4    4    4    4    4    4     4     4     4
C7     4    4    4    4    4    4    4    4    4     4     4     4
C8     4    4    4    4    4    4    4    4    4     4     4     4
C9     4    4    4    4    4    4    4    4    4     4     4     4
C10    4    4    4    4    4    4    4    4    4     4     4     4
O1     1    1    1    1    1    1    1    1    1     1     1     1
O2     1    1    1    1    1    1    1    1    1     1     1     1
O3     1    1    1    1    1    1    1    1    1     1     1     1
O4     1    1    1    1    1    1    1    1    1     1     1     1
O5     1    1    1    1    1    1    1    1    1     1     1     1
O6     1    1    1    1    1    1    1    1    1     1     1     1
O7     1    1    1    1    1    1    1    1    1     1     1     1
O8     1    1    1    1    1    1    1    1    1     1     1     1
O9     1    1    1    1    1    1    1    1    1     1     1     1
O10    1    1    1    1    1    1    1    1    1     1     1     1
    [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23]
E1      5     5     5     5     5     5     5     5     5     5     5
E2      5     5     5     5     5     5     5     5     5     5     5
E3      5     5     5     5     5     5     5     5     5     5     5
E4      5     5     5     5     5     5     5     5     5     5     5
E5      5     5     5     5     5     5     5     5     5     5     5
E6      5     5     5     5     5     5     5     5     5     5     5
E7      5     5     5     5     5     5     5     5     5     5     5
E8      5     5     5     5     5     5     5     5     5     5     5
E9      5     5     5     5     5     5     5     5     5     5     5
E10     5     5     5     5     5     5     5     5     5     5     5
N1      3     3     3     3     3     3     3     3     3     3     3
N2      3     3     3     3     3     3     3     3     3     3     3
N3      3     3     3     3     3     3     3     3     3     3     3
N4      3     3     3     3     3     3     3     3     3     3     3
N5      3     3     3     3     3     3     3     3     3     3     3
N6      3     3     3     3     3     3     3     3     3     3     3
N7      3     3     3     3     3     3     3     3     3     3     3
N8      3     3     3     3     3     3     3     3     3     3     3
N9      3     3     3     3     3     3     3     3     3     3     3
N10     3     3     3     3     3     3     3     3     3     3     3
A1      2     2     2     2     2     2     2     2     2     2     2
A2      2     2     2     2     2     2     2     2     2     2     2
A3      2     2     2     2     2     2     2     2     2     2     2
A4      2     2     2     2     2     2     2     2     2     2     2
A5      2     2     2     2     2     2     2     2     2     2     2
A6      2     2     2     2     2     2     2     2     2     2     2
A7      2     2     2     2     2     2     2     2     2     2     2
A8      2     2     2     2     2     2     2     2     2     2     2
A9      2     2     2     2     2     2     2     2     2     2     2
A10     2     2     2     2     2     2     2     2     2     2     2
C1      4     4     4     4     4     4     4     4     4     4     4
C2      4     4     4     4     4     4     4     4     4     4     4
C3      4     4     4     4     4     4     4     4     4     4     4
C4      4     4     4     4     4     4     4     4     4     4     4
C5      4     4     4     4     4     4     4     4     4     4     4
C6      4     4     4     4     4     4     4     4     4     4     4
C7      4     4     4     4     4     4     4     4     4     4     4
C8      4     4     4     4     4     4     4     4     4     4     4
C9      4     4     4     4     4     4     4     4     4     4     4
C10     4     4     4     4     4     4     4     4     4     4     4
O1      1     1     1     1     1     1     1     1     1     1     1
O2      1     1     1     1     1     1     1     1     1     1     1
O3      1     1     1     1     1     1     1     1     1     1     1
O4      1     1     1     1     1     1     1     1     1     1     1
O5      1     1     1     1     1     1     1     1     1     1     1
O6      1     1     1     1     1     1     1     1     1     1     1
O7      1     1     1     1     1     1     1     1     1     1     1
O8      1     1     1     1     1     1     1     1     1     1     1
O9      1     1     1     1     1     1     1     1     1     1     1
O10     1     1     1     1     1     1     1     1     1     1     1
    [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34]
E1      5     5     5     5     5     5     5     5     5     5     5
E2      5     5     5     5     5     5     5     5     5     5     5
E3      5     5     5     5     5     5     5     5     5     5     5
E4      5     5     5     5     5     5     5     5     5     5     5
E5      5     5     5     5     5     5     5     5     5     5     5
E6      5     5     5     5     5     5     5     5     5     5     5
E7      5     5     5     5     5     5     5     5     5     5     5
E8      5     5     5     5     5     5     5     5     5     5     5
E9      5     5     5     5     5     5     5     5     5     5     5
E10     5     5     5     5     5     5     5     5     5     5     5
N1      3     3     3     3     3     3     3     3     3     3     3
N2      3     3     3     3     3     3     3     3     3     3     3
N3      3     3     3     3     3     3     3     3     3     3     3
N4      3     3     3     3     3     3     3     3     3     3     3
N5      3     3     3     3     3     3     3     3     3     3     3
N6      3     3     3     3     3     3     3     3     3     3     3
N7      3     3     3     3     3     3     3     3     3     3     3
N8      3     3     3     3     3     3     3     3     3     3     3
N9      3     3     3     3     3     3     3     3     3     3     3
N10     3     3     3     3     3     3     3     3     3     3     3
A1      2     2     2     2     2     2     2     2     2     2     2
A2      2     2     2     2     2     2     2     2     2     2     2
A3      2     2     2     2     2     2     2     2     2     2     2
A4      2     2     2     2     2     2     2     2     2     2     2
A5      2     2     2     2     2     2     2     2     2     2     2
A6      2     2     2     2     2     2     2     2     2     2     2
A7      2     2     2     2     2     2     2     2     2     2     2
A8      2     2     2     2     2     2     2     2     2     2     2
A9      2     2     2     2     2     2     2     2     2     2     2
A10     2     2     2     2     2     2     2     2     2     2     2
C1      4     4     4     4     4     4     4     4     4     4     4
C2      4     4     4     4     4     4     4     4     4     4     4
C3      4     4     4     4     4     4     4     4     4     4     4
C4      4     4     4     4     4     4     4     4     4     4     4
C5      4     4     4     4     4     4     4     4     4     4     4
C6      4     4     4     4     4     4     4     4     4     4     4
C7      4     4     4     4     4     4     4     4     4     4     4
C8      4     4     4     4     4     4     4     4     4     4     4
C9      4     4     4     4     4     4     4     4     4     4     4
C10     4     4     4     4     4     4     4     4     4     4     4
O1      1     1     1     1     1     1     1     1     1     1     1
O2      1     1     1     1     1     1     1     1     1     1     1
O3      1     1     1     1     1     1     1     1     1     1     1
O4      1     1     1     1     1     1     1     1     1     1     1
O5      1     1     1     1     1     1     1     1     1     1     1
O6      1     1     1     1     1     1     1     1     1     1     1
O7      1     1     1     1     1     1     1     1     1     1     1
O8      1     1     1     1     1     1     1     1     1     1     1
O9      1     1     1     1     1     1     1     1     1     1     1
O10     1     1     1     1     1     1     1     1     1     1     1
    [,35] [,36] [,37] [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45]
E1      5     5     5     5     5     5     5     5     5     5     5
E2      5     5     5     5     5     5     5     5     5     5     5
E3      5     5     5     5     5     5     5     5     5     5     5
E4      5     5     5     5     5     5     5     5     5     5     5
E5      5     5     5     5     5     5     5     5     5     5     5
E6      5     5     5     5     5     5     5     5     5     5     5
E7      5     5     5     5     5     5     5     5     5     5     5
E8      5     5     5     5     5     5     5     5     5     5     5
E9      5     5     5     5     5     5     5     5     5     5     5
E10     5     5     5     5     5     5     5     5     5     5     5
N1      3     3     3     3     3     3     3     3     3     3     3
N2      3     3     3     3     3     3     3     3     3     3     3
N3      3     3     3     3     3     3     3     3     3     3     3
N4      3     3     3     3     3     3     3     3     3     3     3
N5      3     3     3     3     3     3     3     3     3     3     3
N6      3     3     3     3     3     3     3     3     3     3     3
N7      3     3     3     3     3     3     3     3     3     3     3
N8      3     3     3     3     3     3     3     3     3     3     3
N9      3     3     3     3     3     3     3     3     3     3     3
N10     3     3     3     3     3     3     3     3     3     3     3
A1      2     2     2     2     2     2     2     2     2     2     2
A2      2     2     2     2     2     2     2     2     2     2     2
A3      2     2     2     2     2     2     2     2     2     2     2
A4      2     2     2     2     2     2     2     2     2     2     2
A5      2     2     2     2     2     2     2     2     2     2     2
A6      2     2     2     2     2     2     2     2     2     2     2
A7      2     2     2     2     2     2     2     2     2     2     2
A8      2     2     2     2     2     2     2     2     2     2     2
A9      2     2     2     2     2     2     2     2     2     2     2
A10     2     2     2     2     2     2     2     2     2     2     2
C1      4     4     4     4     4     4     4     4     4     4     4
C2      4     4     4     4     4     4     4     4     4     4     4
C3      4     4     4     4     4     4     4     4     4     4     4
C4      4     4     4     4     4     4     4     4     4     4     4
C5      4     4     4     4     4     4     4     4     4     4     4
C6      4     4     4     4     4     4     4     4     4     4     4
C7      4     4     4     4     4     4     4     4     4     4     4
C8      4     4     4     4     4     4     4     4     4     4     4
C9      4     4     4     4     4     4     4     4     4     4     4
C10     4     4     4     4     4     4     4     4     4     4     4
O1      1     1     1     1     1     1     1     1     1     1     1
O2      1     1     1     1     1     1     1     1     1     1     1
O3      1     1     1     1     1     1     1     1     1     1     1
O4      1     1     1     1     1     1     1     1     1     1     1
O5      1     1     1     1     1     1     1     1     1     1     1
O6      1     1     1     1     1     1     1     1     1     1     1
O7      1     1     1     1     1     1     1     1     1     1     1
O8      1     1     1     1     1     1     1     1     1     1     1
O9      1     1     1     1     1     1     1     1     1     1     1
O10     1     1     1     1     1     1     1     1     1     1     1
    [,46] [,47] [,48] [,49] [,50] [,51] [,52] [,53] [,54] [,55] [,56]
E1      5     5     5     5     5     5     5     5     5     5     5
E2      5     5     5     5     5     5     5     5     5     5     5
E3      5     5     5     5     5     5     5     5     5     5     5
E4      5     5     5     5     5     5     5     5     5     5     5
E5      5     5     5     5     5     5     5     5     5     5     5
E6      5     5     5     5     5     5     5     5     5     5     5
E7      5     5     5     5     5     5     5     5     5     5     5
E8      5     5     5     5     5     5     5     5     5     5     5
E9      5     5     5     5     5     5     5     5     5     5     5
E10     5     5     5     5     5     5     5     5     5     5     5
N1      3     3     3     3     3     3     3     3     3     3     3
N2      3     3     3     3     3     3     3     3     3     3     3
N3      3     3     3     3     3     3     3     3     3     3     3
N4      3     3     3     3     3     3     3     3     3     3     3
N5      3     3     3     3     3     3     3     3     3     3     3
N6      3     3     3     3     3     3     3     3     3     3     3
N7      3     3     3     3     3     3     3     3     3     3     3
N8      3     3     3     3     3     3     3     3     3     3     3
N9      3     3     3     3     3     3     3     3     3     3     3
N10     3     3     3     3     3     3     3     3     3     3     3
A1      2     2     2     2     2     2     2     2     2     2     2
A2      2     2     2     2     2     2     2     2     2     2     2
A3      2     2     2     2     2     2     2     2     2     2     2
A4      2     2     2     2     2     2     2     2     2     2     2
A5      2     2     2     2     2     2     2     2     2     2     2
A6      2     2     2     2     2     2     2     2     2     2     2
A7      2     2     2     2     2     2     2     2     2     2     2
A8      2     2     2     2     2     2     2     2     2     2     2
A9      2     2     2     2     2     2     2     2     2     2     2
A10     2     2     2     2     2     2     2     2     2     2     2
C1      4     4     4     4     4     4     4     4     4     4     4
C2      4     4     4     4     4     4     4     4     4     4     4
C3      4     4     4     4     4     4     4     4     4     4     4
C4      4     4     4     4     4     4     4     4     4     4     4
C5      4     4     4     4     4     4     4     4     4     4     4
C6      4     4     4     4     4     4     4     4     4     4     4
C7      4     4     4     4     4     4     4     4     4     4     4
C8      4     4     4     4     4     4     4     4     4     4     4
C9      4     4     4     4     4     4     4     4     4     4     4
C10     4     4     4     4     4     4     4     4     4     4     4
O1      1     1     1     1     1     1     1     1     1     1     1
O2      1     1     1     1     1     1     1     1     1     1     1
O3      1     1     1     1     1     1     1     1     1     1     1
O4      1     1     1     1     1     1     1     1     1     1     1
O5      1     1     1     1     1     1     1     1     1     1     1
O6      1     1     1     1     1     1     1     1     1     1     1
O7      1     1     1     1     1     1     1     1     1     1     1
O8      1     1     1     1     1     1     1     1     1     1     1
O9      1     1     1     1     1     1     1     1     1     1     1
O10     1     1     1     1     1     1     1     1     1     1     1
    [,57] [,58] [,59] [,60] [,61] [,62] [,63] [,64] [,65] [,66] [,67]
E1      5     5     5     5     5     5     5     5     5     5     5
E2      5     5     5     5     5     5     5     5     5     5     5
E3      5     5     5     5     5     5     5     5     5     5     5
E4      5     5     5     5     5     5     5     5     5     5     5
E5      5     5     5     5     5     5     5     5     5     5     5
E6      5     5     5     5     5     5     5     5     5     5     5
E7      5     5     5     5     5     5     5     5     5     5     5
E8      5     5     5     5     5     5     5     5     5     5     5
E9      5     5     5     5     5     5     5     5     5     5     5
E10     5     5     5     5     5     5     5     5     5     5     5
N1      3     3     3     3     3     3     3     3     3     3     3
N2      3     3     3     3     3     3     3     3     3     3     3
N3      3     3     3     3     3     3     3     3     3     3     3
N4      3     3     3     3     3     3     3     3     3     3     3
N5      3     3     3     3     3     3     3     3     3     3     3
N6      3     3     3     3     3     3     3     3     3     3     3
N7      3     3     3     3     3     3     3     3     3     3     3
N8      3     3     3     3     3     3     3     3     3     3     3
N9      3     3     3     3     3     3     3     3     3     3     3
N10     3     3     3     3     3     3     3     3     3     3     3
A1      2     2     2     2     2     2     2     2     2     2     2
A2      2     2     2     2     2     2     2     2     2     2     2
A3      2     2     2     2     2     2     2     2     2     2     2
A4      2     2     2     2     2     2     2     2     2     2     2
A5      2     2     2     2     2     2     2     2     2     2     2
A6      2     2     2     2     2     2     2     2     2     2     2
A7      2     2     2     2     2     2     2     2     2     2     2
A8      2     2     2     2     2     2     2     2     2     2     2
A9      2     2     2     2     2     2     2     2     2     2     2
A10     2     2     2     2     2     2     2     2     2     2     2
C1      4     4     4     4     4     4     4     4     4     4     4
C2      4     4     4     4     4     4     4     4     4     4     4
C3      4     4     4     4     4     4     4     4     4     4     4
C4      4     4     4     4     4     4     4     4     4     4     4
C5      4     4     4     4     4     4     4     4     4     4     4
C6      4     4     4     4     4     4     4     4     4     4     4
C7      4     4     4     4     4     4     4     4     4     4     4
C8      4     4     4     4     4     4     4     4     4     4     4
C9      4     4     4     4     4     4     4     4     4     4     4
C10     4     4     4     4     4     4     4     4     4     4     4
O1      1     1     1     1     1     1     1     1     1     1     1
O2      1     1     1     1     1     1     1     1     1     1     1
O3      1     1     1     1     1     1     1     1     1     1     1
O4      1     1     1     1     1     1     1     1     1     1     1
O5      1     1     1     1     1     1     1     1     1     1     1
O6      1     1     1     1     1     1     1     1     1     1     1
O7      1     1     1     1     1     1     1     1     1     1     1
O8      1     1     1     1     1     1     1     1     1     1     1
O9      1     1     1     1     1     1     1     1     1     1     1
O10     1     1     1     1     1     1     1     1     1     1     1
    [,68] [,69] [,70] [,71] [,72] [,73] [,74] [,75] [,76] [,77] [,78]
E1      5     5     5     5     5     5     5     5     5     5     5
E2      5     5     5     5     5     5     5     5     5     5     5
E3      5     5     5     5     5     5     5     5     5     5     5
E4      5     5     5     5     5     5     5     5     5     5     5
E5      5     5     5     5     5     5     5     5     5     5     5
E6      5     5     5     5     5     5     5     5     5     5     5
E7      5     5     5     5     5     5     5     5     5     5     5
E8      5     5     5     5     5     5     5     5     5     5     5
E9      5     5     5     5     5     5     5     5     5     5     5
E10     5     5     5     5     5     5     5     5     5     5     5
N1      3     3     3     3     3     3     3     3     3     3     3
N2      3     3     3     3     3     3     3     3     3     3     3
N3      3     3     3     3     3     3     3     3     3     3     3
N4      3     3     3     3     3     3     3     3     3     3     3
N5      3     3     3     3     3     3     3     3     3     3     3
N6      3     3     3     3     3     3     3     3     3     3     3
N7      3     3     3     3     3     3     3     3     3     3     3
N8      3     3     3     3     3     3     3     3     3     3     3
N9      3     3     3     3     3     3     3     3     3     3     3
N10     3     3     3     3     3     3     3     3     3     3     3
A1      2     2     2     2     2     2     2     2     2     2     2
A2      2     2     2     2     2     2     2     2     2     2     2
A3      2     2     2     2     2     2     2     2     2     2     2
A4      2     2     2     2     2     2     2     2     2     2     2
A5      2     2     2     2     2     2     2     2     2     2     2
A6      2     2     2     2     2     2     2     2     2     2     2
A7      2     2     2     2     2     2     2     2     2     2     2
A8      2     2     2     2     2     2     2     2     2     2     2
A9      2     2     2     2     2     2     2     2     2     2     2
A10     2     2     2     2     2     2     2     2     2     2     2
C1      4     4     4     4     4     4     4     4     4     4     4
C2      4     4     4     4     4     4     4     4     4     4     4
C3      4     4     4     4     4     4     4     4     4     4     4
C4      4     4     4     4     4     4     4     4     4     4     4
C5      4     4     4     4     4     4     4     4     4     4     4
C6      4     4     4     4     4     4     4     4     4     4     4
C7      4     4     4     4     4     4     4     4     4     4     4
C8      4     4     4     4     4     4     4     4     4     4     4
C9      4     4     4     4     4     4     4     4     4     4     4
C10     4     4     4     4     4     4     4     4     4     4     4
O1      1     1     1     1     1     1     1     1     1     1     1
O2      1     1     1     1     1     1     1     1     1     1     1
O3      1     1     1     1     1     1     1     1     1     1     1
O4      1     1     1     1     1     1     1     1     1     1     1
O5      1     1     1     1     1     1     1     1     1     1     1
O6      1     1     1     1     1     1     1     1     1     1     1
O7      1     1     1     1     1     1     1     1     1     1     1
O8      1     1     1     1     1     1     1     1     1     1     1
O9      1     1     1     1     1     1     1     1     1     1     1
O10     1     1     1     1     1     1     1     1     1     1     1
    [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86] [,87] [,88] [,89]
E1      5     5     5     5     5     5     5     5     5     5     5
E2      5     5     5     5     5     5     5     5     5     5     5
E3      5     5     5     5     5     5     5     5     5     5     5
E4      5     5     5     5     5     5     5     5     5     5     5
E5      5     5     5     5     5     5     5     5     5     5     5
E6      5     5     5     5     5     5     5     5     5     5     5
E7      5     5     5     5     5     5     5     5     5     5     5
E8      5     5     5     5     5     5     5     5     5     5     5
E9      5     5     5     5     5     5     5     5     5     5     5
E10     5     5     5     5     5     5     5     5     5     5     5
N1      3     3     3     3     3     3     3     3     3     3     3
N2      3     3     3     3     3     3     3     3     3     3     3
N3      3     3     3     3     3     3     3     3     3     3     3
N4      3     3     3     3     3     3     3     3     3     3     3
N5      3     3     3     3     3     3     3     3     3     3     3
N6      3     3     3     3     3     3     3     3     3     3     3
N7      3     3     3     3     3     3     3     3     3     3     3
N8      3     3     3     3     3     3     3     3     3     3     3
N9      3     3     3     3     3     3     3     3     3     3     3
N10     3     3     3     3     3     3     3     3     3     3     3
A1      2     2     2     2     2     2     2     2     2     2     2
A2      2     2     2     2     2     2     2     2     2     2     2
A3      2     2     2     2     2     2     2     2     2     2     2
A4      2     2     2     2     2     2     2     2     2     2     2
A5      2     2     2     2     2     2     2     2     2     2     2
A6      2     2     2     2     2     2     2     2     2     2     2
A7      2     2     2     2     2     2     2     2     2     2     2
A8      2     2     2     2     2     2     2     2     2     2     2
A9      2     2     2     2     2     2     2     2     2     2     2
A10     2     2     2     2     2     2     2     2     2     2     2
C1      4     4     4     4     4     4     4     4     4     4     4
C2      4     4     4     4     4     4     4     4     4     4     4
C3      4     4     4     4     4     4     4     4     4     4     4
C4      4     4     4     4     4     4     4     4     4     4     4
C5      4     4     4     4     4     4     4     4     4     4     4
C6      4     4     4     4     4     4     4     4     4     4     4
C7      4     4     4     4     4     4     4     4     4     4     4
C8      4     4     4     4     4     4     4     4     4     4     4
C9      4     4     4     4     4     4     4     4     4     4     4
C10     4     4     4     4     4     4     4     4     4     4     4
O1      1     1     1     1     1     1     1     1     1     1     1
O2      1     1     1     1     1     1     1     1     1     1     1
O3      1     1     1     1     1     1     1     1     1     1     1
O4      1     1     1     1     1     1     1     1     1     1     1
O5      1     1     1     1     1     1     1     1     1     1     1
O6      1     1     1     1     1     1     1     1     1     1     1
O7      1     1     1     1     1     1     1     1     1     1     1
O8      1     1     1     1     1     1     1     1     1     1     1
O9      1     1     1     1     1     1     1     1     1     1     1
O10     1     1     1     1     1     1     1     1     1     1     1
    [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98] [,99]
E1      5     5     5     5     5     5     5     5     5     5
E2      5     5     5     5     5     5     5     5     5     5
E3      5     5     5     5     5     5     5     5     5     5
E4      5     5     5     5     5     5     5     5     5     5
E5      5     5     5     5     5     5     5     5     5     5
E6      5     5     5     5     5     5     5     5     5     5
E7      5     5     5     5     5     5     5     5     5     5
E8      5     5     5     5     5     5     5     5     5     5
E9      5     5     5     5     5     5     5     5     5     5
E10     5     5     5     5     5     5     5     5     5     5
N1      3     3     3     3     3     3     3     3     3     3
N2      3     3     3     3     3     3     3     3     3     3
N3      3     3     3     3     3     3     3     3     3     3
N4      3     3     3     3     3     3     3     3     3     3
N5      3     3     3     3     3     3     3     3     3     3
N6      3     3     3     3     3     3     3     3     3     3
N7      3     3     3     3     3     3     3     3     3     3
N8      3     3     3     3     3     3     3     3     3     3
N9      3     3     3     3     3     3     3     3     3     3
N10     3     3     3     3     3     3     3     3     3     3
A1      2     2     2     2     2     2     2     2     2     2
A2      2     2     2     2     2     2     2     2     2     2
A3      2     2     2     2     2     2     2     2     2     2
A4      2     2     2     2     2     2     2     2     2     2
A5      2     2     2     2     2     2     2     2     2     2
A6      2     2     2     2     2     2     2     2     2     2
A7      2     2     2     2     2     2     2     2     2     2
A8      2     2     2     2     2     2     2     2     2     2
A9      2     2     2     2     2     2     2     2     2     2
A10     2     2     2     2     2     2     2     2     2     2
C1      4     4     4     4     4     4     4     4     4     4
C2      4     4     4     4     4     4     4     4     4     4
C3      4     4     4     4     4     4     4     4     4     4
C4      4     4     4     4     4     4     4     4     4     4
C5      4     4     4     4     4     4     4     4     4     4
C6      4     4     4     4     4     4     4     4     4     4
C7      4     4     4     4     4     4     4     4     4     4
C8      4     4     4     4     4     4     4     4     4     4
C9      4     4     4     4     4     4     4     4     4     4
C10     4     4     4     4     4     4     4     4     4     4
O1      1     1     1     1     1     1     1     1     1     1
O2      1     1     1     1     1     1     1     1     1     1
O3      1     1     1     1     1     1     1     1     1     1
O4      1     1     1     1     1     1     1     1     1     1
O5      1     1     1     1     1     1     1     1     1     1
O6      1     1     1     1     1     1     1     1     1     1
O7      1     1     1     1     1     1     1     1     1     1
O8      1     1     1     1     1     1     1     1     1     1
O9      1     1     1     1     1     1     1     1     1     1
O10     1     1     1     1     1     1     1     1     1     1
    [,100]
E1       5
E2       5
E3       5
E4       5
E5       5
E6       5
E7       5
E8       5
E9       5
E10      5
N1       3
N2       3
N3       3
N4       3
N5       3
N6       3
N7       3
N8       3
N9       3
N10      3
A1       2
A2       2
A3       2
A4       2
A5       2
A6       2
A7       2
A8       2
A9       2
A10      2
C1       4
C2       4
C3       4
C4       4
C5       4
C6       4
C7       4
C8       4
C9       4
C10      4
O1       1
O2       1
O3       1
O4       1
O5       1
O6       1
O7       1
O8       1
O9       1
O10      1


$item.stability
$item.stability$empirical.dimensions
 O1  O2  O3  O4  O5  O6  O7  O8  O9 O10  A1  A2  A3  A4  A5  A6  A7 
  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 
 A8  A9 A10  N1  N2  N3  N4  N5  N6  N7  N8  N9 N10  C1  C2  C3  C4 
  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 
 C5  C6  C7  C8  C9 C10  E1  E2  E3  E4  E5  E6  E7  E8  E9 E10 
  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 

$item.stability$all.dimensions
    1 2 3 4 5
O1  1 0 0 0 0
O2  1 0 0 0 0
O3  1 0 0 0 0
O4  1 0 0 0 0
O5  1 0 0 0 0
O6  1 0 0 0 0
O7  1 0 0 0 0
O8  1 0 0 0 0
O9  1 0 0 0 0
O10 1 0 0 0 0
A1  0 1 0 0 0
A2  0 1 0 0 0
A3  0 1 0 0 0
A4  0 1 0 0 0
A5  0 1 0 0 0
A6  0 1 0 0 0
A7  0 1 0 0 0
A8  0 1 0 0 0
A9  0 1 0 0 0
A10 0 1 0 0 0
N1  0 0 1 0 0
N2  0 0 1 0 0
N3  0 0 1 0 0
N4  0 0 1 0 0
N5  0 0 1 0 0
N6  0 0 1 0 0
N7  0 0 1 0 0
N8  0 0 1 0 0
N9  0 0 1 0 0
N10 0 0 1 0 0
C1  0 0 0 1 0
C2  0 0 0 1 0
C3  0 0 0 1 0
C4  0 0 0 1 0
C5  0 0 0 1 0
C6  0 0 0 1 0
C7  0 0 0 1 0
C8  0 0 0 1 0
C9  0 0 0 1 0
C10 0 0 0 1 0
E1  0 0 0 0 1
E2  0 0 0 0 1
E3  0 0 0 0 1
E4  0 0 0 0 1
E5  0 0 0 0 1
E6  0 0 0 0 1
E7  0 0 0 0 1
E8  0 0 0 0 1
E9  0 0 0 0 1
E10 0 0 0 0 1


$plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3da070c04f0c_files/figure-html/unnamed-chunk-11-2.png" width="672" /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
$mean.loadings
           1        2        3        4        5
O1   0.30720 -0.00221 -0.00160  0.00456 -0.01138
O2  -0.29093  0.00492  0.04399  0.00098  0.00649
O3   0.25147  0.00162  0.00959 -0.02048 -0.00347
O4  -0.23413 -0.01246 -0.00031  0.00609  0.00063
O5   0.25141  0.01206 -0.00408  0.04092  0.02301
O6  -0.26856 -0.00680  0.00000  0.00000  0.01373
O7   0.18369  0.00716 -0.01910  0.08451  0.00000
O8   0.24276  0.03165  0.00532 -0.01625 -0.00529
O9   0.10796  0.03400  0.04442  0.02276 -0.00433
O10  0.34513  0.00150  0.00000  0.00780  0.02958
A1  -0.01025  0.14071  0.00551 -0.01950  0.01227
A2   0.00160  0.19575  0.00000 -0.01028  0.07268
A3   0.02445  0.11329  0.06184 -0.06368  0.03116
A4   0.00985  0.40845  0.01414  0.00000  0.00002
A5  -0.00415 -0.28542 -0.00007 -0.00001  0.00073
A6  -0.01330  0.22036  0.04996  0.00000  0.00014
A7  -0.00937 -0.30437  0.00907 -0.00492 -0.00137
A8   0.00252  0.22963 -0.00030  0.00958  0.00943
A9   0.02002  0.31385  0.00795  0.00829  0.00000
A10  0.02088  0.12357 -0.01261  0.02884  0.09563
N1  -0.01954  0.00033  0.31789 -0.00424 -0.00012
N2   0.00568  0.00702 -0.18227 -0.00387  0.02775
N3   0.03103  0.03924  0.24779  0.04157  0.01181
N4  -0.00130  0.00480 -0.12695  0.01135  0.00284
N5  -0.02750  0.00220  0.15908 -0.02181  0.00023
N6  -0.01337  0.03106  0.32793 -0.01479  0.00000
N7   0.00457  0.00432  0.27570 -0.01549  0.00000
N8   0.00001  0.00201  0.34925 -0.03390  0.00000
N9  -0.00086 -0.07244  0.25831  0.00091  0.00575
N10  0.02728 -0.00005  0.25664 -0.03733 -0.05148
C1   0.02873  0.00000 -0.00310  0.27463  0.00220
C2   0.01296  0.01085 -0.00389 -0.22476 -0.00018
C3   0.06676  0.00850  0.01771  0.16283  0.00213
C4   0.00748  0.03473  0.08806 -0.24955 -0.00499
C5  -0.00905  0.01259 -0.01097  0.28608  0.00661
C6   0.00438  0.00000  0.00755 -0.31143  0.00000
C7   0.00435  0.00000  0.01701  0.23044  0.00832
C8   0.00000 -0.00894  0.03057 -0.19143  0.00135
C9  -0.00687  0.00684  0.00180  0.28896  0.00155
C10  0.05659  0.02013  0.00000  0.20565  0.00000
E1   0.00301  0.03257 -0.00035  0.00000  0.23667
E2   0.00000 -0.01167 -0.00103  0.00013  0.26156
E3  -0.00591  0.09845 -0.07217  0.01457  0.21283
E4   0.01271 -0.00009  0.01909 -0.00082 -0.30472
E5   0.00311  0.06126  0.00000  0.00122  0.30649
E6  -0.05808 -0.01897  0.00038 -0.00121  0.20654
E7   0.00000  0.03499  0.00000  0.00000  0.31879
E8   0.00067  0.00474  0.00000  0.01106  0.21917
E9   0.02462  0.01562  0.00000 -0.00006  0.24571
E10  0.01618  0.00024  0.02189  0.00000  0.17401

attr(,&amp;quot;class&amp;quot;)
[1] &amp;quot;itemStability&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="measurement-invariance-with-network-model"&gt;Measurement
Invariance with Network Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We can also test whether structure of the test is the same or
similar across subgroup of populations such as age groups, gender, and
race with the measurement invariance analysis. This technique allows us
to check whether our test is consistent across different
populations.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We begin by setting up our model parameter first. We will create
a matrix called ‘Lambda’ and load all 50 items into their respective
dimensions.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#paste(1:10, collapse = &amp;quot;, &amp;quot;)

Lambda &amp;lt;- matrix(0, 50, 5)
Lambda[c(1,2,3,4,5,6,7,8,9,10)] &amp;lt;- 1 # first factor (E)
Lambda[c(11,12,13,14,15,16,17,18,19,20), 2] &amp;lt;- 1 # second factor (N)
Lambda[c(21,22,23,24,25,26,27,28,29,30), 3] &amp;lt;- 1 # second factor (A)
Lambda[c(31,32,33,34,35,36,37,38,39,40), 4] &amp;lt;- 1 # second factor (C)
Lambda[c(41,42,43,44,45,46,47,48,49,50), 5] &amp;lt;- 1 # second factor (O)

latents &amp;lt;- c(&amp;quot;E&amp;quot;, &amp;quot;N&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;O&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Then, we will recode the gender variable into factor, as well as
removing missing data (coded 0) and gender ‘other’ (coded 3) because
there is not enough sample size (less than 100).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#recode gender

df_mi &amp;lt;- subset(df, gender != 3 &amp;amp; gender != 0)

df_mi$gender &amp;lt;-as.factor  (df_mi$gender)

levels(df_mi$gender) &amp;lt;- c(&amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We will then establish our network models based on subgroups with
the &lt;code&gt;lvm&lt;/code&gt; function in &lt;code&gt;psychonetrics&lt;/code&gt;
package.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Configural model with free residuals across groups
mod_configural &amp;lt;- psychonetrics::lvm(data = df_mi, # data
                                     lambda = Lambda, # factor structure
                                     vars = obsvars, # items to be analyzed
                                     latents = latents, # factor names
                                     identification = &amp;quot;variance&amp;quot;, # Fix variance to 1
                                     groups =  &amp;quot;gender&amp;quot;, # group variable
                                     # if &amp;quot;full&amp;quot;, it frees the residual variance-covariance matrix
                                     # Default: &amp;quot;empty&amp;quot; --&amp;gt; fixing them to zero
                                     omega_epsilon = &amp;quot;full&amp;quot;,
                                     residual = &amp;quot;ggm&amp;quot;) %&amp;gt;%
  psychonetrics::runmodel()&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Then, we can visualize the network models based on gender male and
female. We can see that the structure of both models are similar, but
edges between items may be different. For example, an edge between item
A7 to A9 in female subgroup is more prominent than male subgroup. This
may indicate differences how the two items work between respondents of
different gender.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Get the residual/error matrix
net_male &amp;lt;- mod_configural %&amp;gt;% 
  psychonetrics::getmatrix(&amp;quot;omega_epsilon&amp;quot;) %&amp;gt;%
  .$&amp;quot;male&amp;quot;

net_female &amp;lt;- mod_configural %&amp;gt;% 
  psychonetrics::getmatrix(&amp;quot;omega_epsilon&amp;quot;) %&amp;gt;%
  .$&amp;quot;female&amp;quot;

# Obtain an average/joint layout over several graphs
Layout &amp;lt;- qgraph::averageLayout(net_male, net_female)

# Plot both networks together
layout(t(1:2)) # 1 row and 2 columns layout

qgraph::qgraph(net_male, 
               layout = Layout, 
               theme = &amp;quot;colorblind&amp;quot;, 
               title = &amp;quot;Male&amp;quot;, 
               labels = obsvars)

qgraph::qgraph(net_female, 
               layout = Layout, 
               theme = &amp;quot;colorblind&amp;quot;, 
               title = &amp;quot;Female&amp;quot;,
               labels = obsvars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3da070c04f0c_files/figure-html/unnamed-chunk-15-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Reset the layout to 1:1 again
layout(t(1:1)) &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can further test the invariance of our model across levels like
weak invariance (equal factor loadings), strong invariance (equal factor
loadings and intercepts), and strict invariance (equal factor loadings,
intercepts, and even residuals) across all subgroups. See &lt;a
href="http://journals.sagepub.com/doi/10.1177/01466216231151700"&gt;Finch
et al. (2023)&lt;/a&gt; and &lt;a
href="https://towardsdatascience.com/testing-for-measurement-invariance-in-r-b44cace10148"&gt;Bulut
(2020)&lt;/a&gt; for more information.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="concluding-remark"&gt;Concluding Remark&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To support validity of a test, we need as much evidence as possible
to argue that our test, its structure, and its functions actually hold
across different conditions. For this reason, network psychometric is a
useful approach to examine validity evidence of a test and ensure its
continuing interpretations and uses. Thank you so much for reading
this!&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>565bc3a4bd2e3b6d978b3cb391e60aca</distill:md5>
      <category>R</category>
      <category>Psychometric</category>
      <guid>https://taridwong.github.io/posts/2023-04-30-networkpsych</guid>
      <pubDate>Sun, 30 Apr 2023 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2023-04-30-networkpsych/ega.png" medium="image" type="image/png" width="700" height="432"/>
    </item>
    <item>
      <title>Examining State of the Field with Bibliometric Analysis</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2023-03-20-bibanalysis</link>
      <description>


&lt;h2 id="introduction-import-and-convert-the-data"&gt;Introduction, import,
and convert the data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Hi Everyone. It’s been a busy semester for me. I taught a class
(it’s over). I am interning at Alberta Education. I am also taking a
doctoral-level coursework. I am also doing research regularly as always.
But I am managing, I guess. I found an interesting analysis technique
from one of my reading club that I think would be beneficial to you.
It’s called bibliometric analysis.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Some key reading that inspired me to try my hands on this
techniques are &lt;a
href="https://www.sciencedirect.com/science/article/pii/S0148296321003155"&gt;Donthu
et al. (2021)&lt;/a&gt;, &lt;a
href="https://revista.profesionaldelainformacion.com/index.php/EPI/article/view/epi.2020.ene.03"&gt;Moral-Muñoz
et al. (2020)&lt;/a&gt;, and &lt;a
href="https://doi.org/10.3390/educsci12030209"&gt;Sudakova et
al. (2022)&lt;/a&gt;. This post will revolve around the three papers I
mentioned, as well as some &lt;a
href="https://bibliometrix.org/documents/bibliometrix_Report.html"&gt;online
tutorial&lt;/a&gt; of the package. I will be performing a bibliometric
analysis with bibliographic records of the learning analytics field in
R. The package I will mainly use is the &lt;code&gt;bibliometrix&lt;/code&gt;
package (&lt;a
href="https://www.sciencedirect.com/science/article/pii/S1751157717300500"&gt;Aria
&amp;amp; Cuccurullo, 2017&lt;/a&gt;) to perform the analysis, with some
additional helper packages such as &lt;code&gt;ggplot2&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Bibliometric analysis is an exploratory method that analyzes
large volumes of bibliometric records to examine trends of scientific
publications, journal performance, collaboration pattern, and
relationships between topics in the field (Donthu et al., 2021).
Researchers can use this technique to gain a broad perspective of the
field to identify a research gap that they can study, as well as
determine its relative importance to other topics. Bibliometric records
can be extracted from any scientific databases such as Web of science,
SCOPUS, Digital science dimensions, and PubMed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For the data set in this post, I extracted a set of bibliometric
records from Web of Science because it provides more information on
meta-data (e.g., keyword plus, cited references) and less missing data
than Scopus. However, note that Scopus has more publication data for the
arts and humanities field.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I used the search syntax of: &lt;strong&gt;Learning&lt;/strong&gt;
&lt;strong&gt;analytic&lt;/strong&gt;* (All fields) AND &lt;strong&gt;Education&lt;/strong&gt;*
(All fields) NOT &lt;strong&gt;Medical&lt;/strong&gt; (All fields), with filter for
review article or proceeding Paper. Then, I extracted the first 500
entries from the database based on their relevance to the search term.
Note that Web of Science only allows for the maximum of 500 entries to
be exported if I requested for full bibliographic record. &lt;a
href="https://www.webofscience.com/wos/woscc/summary/d38a9cd2-86d2-43fa-aa1e-03365f7ec574-78c4a9cf/relevance/1"&gt;Here
is the link&lt;/a&gt; to the search result.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;First, we will set our working directory and load the packages as
usual.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I exported the bibliographic file in as a plain text document. I
will use the &lt;code&gt;convert2df&lt;/code&gt; function to import the data set
into R environment. We have to specify source and format of data set for
the package to correctly process the data&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#using plain text file. 
file &amp;lt;- c(&amp;quot;la.txt&amp;quot;)

M &amp;lt;- convert2df(file, dbsource = &amp;quot;wos&amp;quot;, format = &amp;quot;plaintext&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Converting your wos collection into a bibliographic dataframe

Done!


Generating affiliation field tag AU_UN from C1:  Done!&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can use the &lt;code&gt;biblioAnalysis&lt;/code&gt; function to perform
descriptive analysis of the data set. We can check for information such
as time span of the literature, number of journals, document types and
so on. Some of the more interesting results are most relevant sources
(i.e., journals or conferences), and top manuscripts per citation. We
can identify popular topics and papers from these results.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#Summary
results &amp;lt;- biblioAnalysis(M, sep = &amp;quot;;&amp;quot;)
S &amp;lt;- summary(object = results, k = 10, pause = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;

MAIN INFORMATION ABOUT DATA

 Timespan                              2007 : 2023 
 Sources (Journals, Books, etc)        238 
 Documents                             500 
 Annual Growth Rate %                  11.85 
 Document Average Age                  4.52 
 Average citations per doc             9.354 
 Average citations per year per doc    1.638 
 References                            11674 
 
DOCUMENT TYPES                     
 article                         162 
 article; book chapter           3 
 article; early access           19 
 article; proceedings paper      1 
 proceedings paper               300 
 review                          11 
 review; book chapter            1 
 review; early access            3 
 
DOCUMENT CONTENTS
 Keywords Plus (ID)                    321 
 Author&amp;#39;s Keywords (DE)                1206 
 
AUTHORS
 Authors                               1274 
 Author Appearances                    1722 
 Authors of single-authored docs       47 
 
AUTHORS COLLABORATION
 Single-authored docs                  49 
 Documents per Author                  0.392 
 Co-Authors per Doc                    3.44 
 International co-authorships %        29.8 
 

Annual Scientific Production

 Year    Articles
    2007        1
    2011        1
    2012        4
    2013       15
    2014       14
    2015       17
    2016       41
    2017       70
    2018       76
    2019       62
    2020       53
    2021       66
    2022       52
    2023        6

Annual Percentage Growth Rate 11.85 


Most Productive Authors

       Authors        Articles Authors        Articles Fractionalized
1  OGATA H                  19   OGATA H                         5.33
2  GASEVIC D                16   GASEVIC D                       3.87
3  RIENTIES B               12   RIENTIES B                      3.68
4  YAMADA M                 11   NGUYEN Q                        2.98
5  NGUYEN Q                 10   YAMADA M                        2.85
6  DRACHSLER H               9   IFENTHALER D                    2.57
7  KINSHUK                   8   GIANNAKOS M                     2.33
8  PARDO A                   8   TEMPELAAR D                     2.25
9  FERNANDEZ-MANJON B        7   KINSHUK                         2.16
10 FREIRE M                  7   DRACHSLER H                     2.15


Top manuscripts per citations

                                                                                                                                       Paper         
1  GRELLER W, 2012, EDUC TECHNOL SOC                                                                                                                 
2  SHUM SB, 2012, EDUC TECHNOL SOC                                                                                                                   
3  LU OHT, 2018, EDUC TECHNOL SOC                                                                                                                    
4  JIVET I, 2018, PROCEEDINGS OF THE 8TH INTERNATIONAL CONFERENCE ON LEARNING ANALYTICS &amp;amp; KNOWLEDGE (LAK&amp;#39;18): TOWARDS USER-CENTRED LEARNING ANALYTICS
5  TABUENCA B, 2015, COMPUT EDUC                                                                                                                     
6  LEITNER P, 2017, STUD SYST DECIS CONT                                                                                                             
7  WILLIAMS R, 2011, INT REV RES OPEN DIS                                                                                                            
8  TSAI YS, 2017, SEVENTH INTERNATIONAL LEARNING ANALYTICS &amp;amp; KNOWLEDGE CONFERENCE (LAK&amp;#39;17)                                                           
9  JIVET I, 2017, LECT NOTES COMPUT SC                                                                                                               
10 BODILY R, 2017, SEVENTH INTERNATIONAL LEARNING ANALYTICS &amp;amp; KNOWLEDGE CONFERENCE (LAK&amp;#39;17)                                                          
                             DOI  TC TCperYear   NTC
1  NA                            374     31.17  2.29
2  NA                            260     21.67  1.59
3  NA                            120     20.00 10.39
4  10.1145/3170358.3170421       118     19.67 10.21
5  10.1016/j.compedu.2015.08.004 109     12.11  7.92
6  10.1007/978-3-319-52977-6_1   102     14.57  8.71
7  10.19173/irrodl.v12i3.883      83      6.38  1.00
8  10.1145/3027385.3027400        75     10.71  6.40
9  10.1007/978-3-319-66610-5_7    71     10.14  6.06
10 10.1145/3027385.3027403        71     10.14  6.06


Corresponding Author&amp;#39;s Countries

          Country Articles   Freq SCP MCP MCP_Ratio
1  CHINA                52 0.1055  38  14     0.269
2  USA                  48 0.0974  34  14     0.292
3  JAPAN                36 0.0730  30   6     0.167
4  AUSTRALIA            34 0.0690  18  16     0.471
5  SPAIN                28 0.0568  25   3     0.107
6  UNITED KINGDOM       26 0.0527  18   8     0.308
7  GERMANY              23 0.0467  17   6     0.261
8  NETHERLANDS          20 0.0406   9  11     0.550
9  NORWAY               19 0.0385  11   8     0.421
10 CANADA               14 0.0284   7   7     0.500


SCP: Single Country Publications

MCP: Multiple Country Publications


Total Citations per Country

     Country      Total Citations Average Article Citations
1  NETHERLANDS                853                    42.650
2  UNITED KINGDOM             679                    26.115
3  AUSTRALIA                  516                    15.176
4  CHINA                      498                     9.577
5  USA                        319                     6.646
6  JAPAN                      243                     6.750
7  SPAIN                      207                     7.393
8  NORWAY                     174                     9.158
9  AUSTRIA                    147                    18.375
10 GERMANY                    109                     4.739


Most Relevant Sources

                                                                         Sources        Articles
1  SEVENTH INTERNATIONAL LEARNING ANALYTICS &amp;amp; KNOWLEDGE CONFERENCE (LAK&amp;#39;17)                   18
2  INTERACTIVE LEARNING ENVIRONMENTS                                                          16
3  EDUCATIONAL TECHNOLOGY &amp;amp; SOCIETY                                                           15
4  9TH INTERNATIONAL CONFERENCE ON EDUCATION AND NEW LEARNING TECHNOLOGIES (EDULEARN17)       12
5  JOURNAL OF LEARNING ANALYTICS                                                              11
6  IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES                                                  9
7  EDULEARN18: 10TH INTERNATIONAL CONFERENCE ON EDUCATION AND NEW LEARNING TECHNOLOGIES        8
8  ETR&amp;amp;D-EDUCATIONAL TECHNOLOGY RESEARCH AND DEVELOPMENT                                       8
9  IEEE 21ST INTERNATIONAL CONFERENCE ON ADVANCED LEARNING TECHNOLOGIES (ICALT 2021)           8
10 JOURNAL OF COMPUTER ASSISTED LEARNING                                                       8


Most Relevant Keywords

   Author Keywords (DE)      Articles Keywords-Plus (ID)     Articles
1    LEARNING ANALYTICS           382            PERFORMANCE       41
2    HIGHER EDUCATION              52            MODEL             31
3    EDUCATIONAL DATA MINING       34            DESIGN            28
4    LEARNING DESIGN               33            FRAMEWORK         25
5    E-LEARNING                    28            EDUCATION         24
6    SELF-REGULATED LEARNING       28            ONLINE            22
7    BLENDED LEARNING              25            STUDENTS          21
8    MACHINE LEARNING              24            ANALYTICS         17
9    ONLINE LEARNING               23            IMPACT            16
10   EDUCATION                     17            ACHIEVEMENT       14&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can check for most cited references with the code below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;CR &amp;lt;- citations(M, field = &amp;quot;article&amp;quot;, sep = &amp;quot;;&amp;quot;)
cbind(CR$Cited[1:20])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                                                                                                    [,1]
LONG PHIL, 2011, EDUCAUSE REVIEW, V46, P31                                                            58
GASEVIC D, 2015, TECHTRENDS, V59, P64, DOI 10.1007/S11528-014-0822-X                                  57
GRELLER W, 2012, EDUC TECHNOL SOC, V15, P42                                                           56
FERGUSON R, 2012, INT J TECHNOL ENHANC, V4, P304, DOI 10.1504/IJTEL.2012.051816                       54
SIEMENS G, 2013, AM BEHAV SCI, V57, P1380, DOI 10.1177/0002764213498851                               42
ARNOLD K.E., 2012, P 2 INT C LEARN AN K, DOI DOI 10.1145/2330601.2330666, 10.1145/2330601.2330666     40
GASEVIC D, 2016, INTERNET HIGH EDUC, V28, P68, DOI 10.1016/J.IHEDUC.2015.10.002                       40
CHATTI MA, 2012, INT J TECHNOL ENHANC, V4, P318, DOI 10.1504/IJTEL.2012.051815                        39
LOCKYER L, 2013, AM BEHAV SCI, V57, P1439, DOI 10.1177/0002764213479367                               39
TEMPELAAR DT, 2015, COMPUT HUM BEHAV, V47, P157, DOI 10.1016/J.CHB.2014.05.038                        37
CLOW D., 2012, P 2 INT C LEARNING A, P134, DOI 10.1145/2330601.2330636, DOI 10.1145/2330601.2330636   31
SLADE S, 2013, AM BEHAV SCI, V57, P1510, DOI 10.1177/0002764213479366                                 29
PAPAMITSIOU Z, 2014, EDUC TECHNOL SOC, V17, P49                                                       28
SCHWENDIMANN BA, 2017, IEEE T LEARN TECHNOL, V10, P30, DOI 10.1109/TLT.2016.2599522                   27
SIEMENS G, 2012, P 2 INT C LEARNING A, DOI 10.1145/2330601.2330661, DOI 10.1145/2330601.2330661       27
VIBERG O, 2018, COMPUT HUM BEHAV, V89, P98, DOI 10.1016/J.CHB.2018.07.027                             27
MACFADYEN LP, 2010, COMPUT EDUC, V54, P588, DOI 10.1016/J.COMPEDU.2009.09.008                         26
AGUDO-PEREGRINA AF, 2014, COMPUT HUM BEHAV, V31, P542, DOI 10.1016/J.CHB.2013.05.031                  24
SHUM SB, 2012, EDUC TECHNOL SOC, V15, P3                                                              24
WISE A. F., 2014, P 4 INT C LEARNING A, P203                                                          23&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can check for authors’ dominance ranking to see which author
published the most, and the way that they work (single-authored,
multi-authored).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#Authors’ Dominance ranking
DF &amp;lt;- dominance(results, k = 10)
DF&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                Author Dominance Factor Tot Articles Single-Authored
1             KHALIL M       0.66666667            6               0
2              TSAI YS       0.57142857            7               0
3         IFENTHALER D       0.42857143            7               0
4             NGUYEN Q       0.30000000           10               0
5           RIENTIES B       0.16666667           12               0
6             DAWSON S       0.16666667            6               0
7              EBNER M       0.16666667            6               0
8  RODRIGUEZ-TRIANA MJ       0.16666667            6               0
9             YAMADA M       0.09090909           11               0
10           GASEVIC D       0.06250000           16               0
   Multi-Authored First-Authored Rank by Articles Rank by DF
1               6              4                7          1
2               7              4                5          2
3               7              3                5          3
4              10              3                4          4
5              12              2                2          5
6               6              1                7          5
7               6              1                7          5
8               6              1                7          5
9              11              1                3          9
10             16              1                1         10&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Finally, we can plot the output to make it understandable at a
glance with the regular &lt;code&gt;plot&lt;/code&gt; function.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(x=results, k=10, pause=FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3da016543ce3_files/figure-html/unnamed-chunk-6-1.png" width="672" /&gt;&lt;img src="file3da016543ce3_files/figure-html/unnamed-chunk-6-2.png" width="672" /&gt;&lt;img src="file3da016543ce3_files/figure-html/unnamed-chunk-6-3.png" width="672" /&gt;&lt;img src="file3da016543ce3_files/figure-html/unnamed-chunk-6-4.png" width="672" /&gt;&lt;img src="file3da016543ce3_files/figure-html/unnamed-chunk-6-5.png" width="672" /&gt;&lt;/p&gt;
&lt;h2
id="examining-relationships-between-authors-with-co-citation-analysis"&gt;Examining
relationships between authors with co-citation Analysis&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We can check how the papers in our data set cited each other with
co-citation analysis. Results will be displayed as co-citation network
below. We can see that there are two main clusters, indicating that
papers within a cluster cited each other the most.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;NetMatrix &amp;lt;- biblioNetwork(M, analysis = &amp;quot;co-citation&amp;quot;, network = &amp;quot;references&amp;quot;, sep = &amp;quot;;&amp;quot;)

net=networkPlot(NetMatrix, weighted=NULL, n = 50, 
                Title = &amp;quot;Co-Citation Network&amp;quot;, type = &amp;quot;fruchterman&amp;quot;, 
                size=4, size.cex=TRUE, remove.multiple=FALSE, labelsize=1, label.n=10, label.cex=F, edgesize = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3da016543ce3_files/figure-html/unnamed-chunk-7-1.png" width="960" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can also perform co-citation analysis at the source level (i.e.,
journals and conferences) to examine how sources in our data set cited
each other.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;Source=metaTagExtraction(M,&amp;quot;CR_SO&amp;quot;,sep=&amp;quot;;&amp;quot;)

NetMatrix &amp;lt;- biblioNetwork(Source, analysis = &amp;quot;co-citation&amp;quot;, network = &amp;quot;sources&amp;quot;, sep = &amp;quot;;&amp;quot;)

net=networkPlot(NetMatrix, n = 50, Title = &amp;quot;Co-Citation Network-Journal&amp;quot;, type = &amp;quot;auto&amp;quot;, size.cex=TRUE, size=3, remove.multiple=FALSE, labelsize=1,edgesize = 10, edges.min=5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3da016543ce3_files/figure-html/unnamed-chunk-8-1.png" width="960" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can also examine top 5 keywords used by publication authors in
our data set with the code below. We can see popular topics based on
keyword with this result.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#Author keyword network

A &amp;lt;- cocMatrix(M, Field = &amp;quot;DE&amp;quot;, sep = &amp;quot;;&amp;quot;)
sort(Matrix::colSums(A), decreasing = TRUE)[1:5]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;     LEARNING ANALYTICS        HIGHER EDUCATION 
                    382                      52 
EDUCATIONAL DATA MINING         LEARNING DESIGN 
                     33                      33 
             E-LEARNING 
                     28 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can also examine author coupling cluster to see whose work is
related to whose. Once we know the author, we can refer back to the list
of our bibliographic records to examine their work.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;res &amp;lt;- couplingMap(M, analysis = &amp;quot;authors&amp;quot;, field = &amp;quot;CR&amp;quot;, n = 250, impact.measure=&amp;quot;local&amp;quot;,
minfreq = 3, size = 0.5, repel = TRUE)

plot(res$map)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3da016543ce3_files/figure-html/unnamed-chunk-10-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Finally, we can use a three fields plot to see the relationship
between authors (AU), keywords that they used (DE), and
journals/conferences that they submitted (SO).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;threeFieldsPlot(M, fields = c(&amp;quot;AU&amp;quot;, &amp;quot;DE&amp;quot;, &amp;quot;SO&amp;quot;), n = c(20, 20, 20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-03-20-bibanalysisthreefields.png" /&gt;&lt;/p&gt;
&lt;h2 id="examining-conceptual-structure-with-co-word-analysis"&gt;Examining
conceptual structure with co-word analysis&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Co-word analysis is a useful tool to examine conceptual structure of
the field. That is, we can see relationship between topics within the
field of interest. The keyword co-occurrence network plot below shows
the how frequent each keyword is presented in our bibliographic data
from the size of its circle. The bigger they are, the more frequent they
are used. Position of keywords on the plot also indicates which keywords
are usually presented together.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#Co-occurrences network

# keywords
NetMatrix &amp;lt;- biblioNetwork(M, analysis = &amp;quot;co-occurrences&amp;quot;, network = &amp;quot;keywords&amp;quot;, sep = &amp;quot;;&amp;quot;)

# Plot the network
net = networkPlot(NetMatrix, normalize=&amp;quot;association&amp;quot;, weighted=T, n = 30, 
                  Title = &amp;quot;Keyword Co-occurrences&amp;quot;, type = &amp;quot;fruchterman&amp;quot;, 
                  size=TRUE, edgesize = 5, labelsize=0.7, remove.multiple=FALSE, label.cex=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3da016543ce3_files/figure-html/unnamed-chunk-12-1.png" width="960" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can also perform correspondence analysis to see relationship
between topics with multiple correspondence analysis (MCA) or
hierarchical cluster plot as well. The factorial maps also show papers
with the highest contribution within the keyword clusters in the
output.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Conceptual Structure using keywords (method=&amp;quot;MCA&amp;quot;)
CS &amp;lt;- conceptualStructure(M, field=&amp;quot;ID&amp;quot;, method=&amp;quot;MCA&amp;quot;, minDegree=4, clust=5, stemming=FALSE, labelsize=15, documents=20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3da016543ce3_files/figure-html/unnamed-chunk-13-1.png" width="1440" /&gt;&lt;img src="file3da016543ce3_files/figure-html/unnamed-chunk-13-2.png" width="1440" /&gt;&lt;img src="file3da016543ce3_files/figure-html/unnamed-chunk-13-3.png" width="1440" /&gt;&lt;img src="file3da016543ce3_files/figure-html/unnamed-chunk-13-4.png" width="1440" /&gt;&lt;/p&gt;
&lt;h2
id="examining-keyword-usage-across-time-with-historiograph"&gt;Examining
keyword usage across time with historiograph&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Most analyses we performed earlier did not use time point as a
variable. We can plot a historiograph to see the evolution of keyword
usage across time, as well as their relative popularity to each other.
The plot below indicates that “learning analytics” is the post popular
as it gained more interest since 2007 based on our data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(reshape2)
library(ggplot2)

kword &amp;lt;- KeywordGrowth(M, Tag = &amp;quot;DE&amp;quot;, sep = &amp;quot;;&amp;quot;, top = 15, cdf = TRUE)

DF = melt(kword, id=&amp;#39;Year&amp;#39;)

# Timeline keywords ggplot
ggplot(DF,aes(x=Year,y=value, group=variable, shape=variable, colour=variable))+
  geom_point()+geom_line()+ 
  scale_shape_manual(values = 1:15)+
  labs(color=&amp;quot;Author Keywords&amp;quot;)+
  scale_x_continuous(breaks = seq(min(DF$Year), max(DF$Year), by = 5))+
  scale_y_continuous(breaks = seq(0, max(DF$value), by=10))+
  guides(color=guide_legend(title = &amp;quot;Author Keywords&amp;quot;), shape=FALSE)+
  labs(y=&amp;quot;Count&amp;quot;, variable=&amp;quot;Author Keywords&amp;quot;, title = &amp;quot;Author&amp;#39;s Keywords Usage Evolution Over Time&amp;quot;)+
  theme(text = element_text(size = 10))+
  facet_grid(variable ~ .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3da016543ce3_files/figure-html/unnamed-chunk-14-1.png" width="960" /&gt;&lt;/p&gt;
&lt;h2 id="examining-trends-of-the-field-with-thematic-map"&gt;Examining
trends of the field with thematic map&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Aside from examining frequency of keyword usage, we can use
thematic map to examine trend of the topics within the field. The map
below places topics within four panes (&lt;a
href="https://doi.org/10.1016/j.procs.2018.10.278"&gt;Cobo et al.,
2018&lt;/a&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Upper right pane indicates motor theme that is important to the
field as it is constantly used, hence the name motor;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Upper left pane indicates niche theme that is very specialized to
certain groups of research;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lower left pane indicates emerging/declining theme that is still
weakly developed, meaning that it could still growing or starting to
disappear;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lower right pane indicates basic theme that is important to the
field but not well developed as motor theme, meaning that topics in this
pane are usually studied for general knowledge of the field.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;Map=thematicMap(M, field = &amp;quot;ID&amp;quot;, n = 250, minfreq = 4,
  stemming = FALSE, size = 0.7, n.labels=5, repel = TRUE)
plot(Map$map)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3da016543ce3_files/figure-html/unnamed-chunk-15-1.png" width="1440" data-distill-preview=1 /&gt;&lt;/p&gt;
&lt;h2
id="examining-collaboration-with-social-structure-analysis"&gt;Examining
collaboration with social structure analysis&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We can examine collaboration pattern between authors from our data
set with collaboration analysis. The author collaboration network below
shows who collaborated with whom at the author level.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;NetMatrix &amp;lt;- biblioNetwork(M, analysis = &amp;quot;collaboration&amp;quot;,  network = &amp;quot;authors&amp;quot;, sep = &amp;quot;;&amp;quot;)

net=networkPlot(NetMatrix,  n = 50, Title = &amp;quot;Author collaboration&amp;quot;,type = &amp;quot;auto&amp;quot;, size=5,size.cex=T,edgesize = 5,labelsize=1, community.repulsion = 0.1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3da016543ce3_files/figure-html/unnamed-chunk-16-1.png" width="960" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can also plot an institution collaboration network to examine
collaboration pattern between institutions as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;NetMatrix &amp;lt;- biblioNetwork(M, analysis = &amp;quot;collaboration&amp;quot;,  network = &amp;quot;universities&amp;quot;, sep = &amp;quot;;&amp;quot;)
net=networkPlot(NetMatrix,  n = 50, Title = &amp;quot;Institution collaboration&amp;quot;,type = &amp;quot;auto&amp;quot;, size=4,size.cex=F,edgesize = 3,labelsize=1, community.repulsion = 0.05)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3da016543ce3_files/figure-html/unnamed-chunk-17-1.png" width="960" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lastly, we can examine collaboration pattern at international level
with country collaboration network. This analysis shows countries that
are productive within the field (indicated by size of the circle), as
well as countries that they collaborate with as indicated by linkages in
the plot.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;country &amp;lt;- metaTagExtraction(M, Field = &amp;quot;AU_CO&amp;quot;, sep = &amp;quot;;&amp;quot;)
NetMatrix &amp;lt;- biblioNetwork(country, analysis = &amp;quot;collaboration&amp;quot;,  network = &amp;quot;countries&amp;quot;, sep = &amp;quot;;&amp;quot;)

net=networkPlot(NetMatrix,  n = dim(NetMatrix)[1], Title = &amp;quot;Country collaboration&amp;quot;,type = &amp;quot;circle&amp;quot;, size=10,size.cex=T,edgesize = 1,labelsize=0.6, cluster=&amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3da016543ce3_files/figure-html/unnamed-chunk-18-1.png" width="960" /&gt;&lt;/p&gt;
&lt;h2 id="concluding-remarks"&gt;Concluding remarks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;What I like about bibliometric analysis is that it examines the
field at the meta level, meaning that it is a research that is built on
other research. Without the work of other researchers out there, this
analysis would be impossible.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This analysis method is a useful tool to gain a bird-eye view on
state of the field that we are interested in. Results can be used inform
early career researchers or students in their topic selection.
Basically, it could be helpful for us to know which topic is popular
(and therefore has a lot of papers we could read) or which topic is
declining.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;By identifying authors that publish a lot in the field, we can
decide whether we want to follow their work to keep up with research
trend, or deviate our work off their path so that we can study something
new to the field. Anyway, thank you so much for reading this! I hope you
like it :)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>0fabd9ac6e025bacc0d7313c8c5a9710</distill:md5>
      <category>R</category>
      <category>Data Visualization</category>
      <guid>https://taridwong.github.io/posts/2023-03-20-bibanalysis</guid>
      <pubDate>Wed, 22 Mar 2023 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2023-03-20-bibanalysis/threefields.png" medium="image" type="image/png" width="2444" height="1474"/>
    </item>
    <item>
      <title>Data Exploration with ggstatsplot</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-12-31-ggstat</link>
      <description>In this post, I will be performing and visualizing data exploration techniques such as Pearson's correlation test, Chi-square Goodness of Fit test, Chi-square Test of Independence, One-sample t-test, and Paired-sample t-test.

(8 min read)</description>
      <category>R</category>
      <category>Statistics</category>
      <guid>https://taridwong.github.io/posts/2022-12-31-ggstat</guid>
      <pubDate>Sat, 31 Dec 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-12-31-ggstat/ggstat_files/figure-html5/unnamed-chunk-13-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Test Shortening with Genetic Algorithm and Ant Colony Optimization</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-08-14-ga-aco</link>
      <description>In this post, I will use Genetic Algorithm and Ant Colony Optimization Algorithm to automatically shorten the length of a test.

 (8 min read)</description>
      <category>R</category>
      <category>Psychometric</category>
      <guid>https://taridwong.github.io/posts/2022-08-14-ga-aco</guid>
      <pubDate>Sun, 14 Aug 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-08-14-ga-aco/ga-aco_files/figure-html5/unnamed-chunk-11-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Leveraging a Large-Scale Educational Data Set with Educational Data Mining</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-08-06-edm</link>
      <description>In this post, I will be predicting students' high school dropout rate through a large-scale educational data set.

 (10 min read)</description>
      <category>R</category>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-08-06-edm</guid>
      <pubDate>Sat, 06 Aug 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-08-06-edm/edm_files/figure-html5/unnamed-chunk-24-11.png" medium="image" type="image/png" width="2304" height="1536"/>
    </item>
    <item>
      <title>Item Response Theory</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-05-15-irt</link>
      <description>In this post, I will be examining characteristics of test items based on the Item Response Theory framework.

 (18 min read)</description>
      <category>R</category>
      <category>Psychometric</category>
      <guid>https://taridwong.github.io/posts/2022-05-15-irt</guid>
      <pubDate>Sun, 15 May 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-05-15-irt/IIC.png" medium="image" type="image/png" width="656" height="551"/>
    </item>
    <item>
      <title>Making Sense of Machine Learning with Explanable Artificial Intelligence</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-28-xai</link>
      <description>I will be applying the methods of Explanable Artificial Intelligence (XAI) to extract interpretable insights from a classification model that predicts students' grade repetition.

(14 min read)</description>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-28-xai</guid>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-28-xai/xai_files/figure-html5/unnamed-chunk-14-11.png" medium="image" type="image/png" width="1440" height="1824"/>
    </item>
    <item>
      <title>Addressing Data Imbalance with Semi-Supervised Learning</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-28-semisupervised</link>
      <description>For this post, I will use semi-supervised learning approach to perform a classification task with a highly imbalance data.  

(7 min read)</description>
      <category>Python</category>
      <category>Unsupervised Machine Learning</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-28-semisupervised</guid>
      <pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-28-semisupervised/semi-ml.png" medium="image" type="image/png" width="900" height="450"/>
    </item>
    <item>
      <title>Examining Customer Cluster with Unsupervised Machine Learning</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-18-unsupervisedml</link>
      <description>In this post, I will be using two unsupervised learning techniques with a data set, namely K-means clustering and Hierarchical clustering, to determine groups of customers from their age, income, and spending behavior data.  

(8 min read)</description>
      <category>Python</category>
      <category>Unsupervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-18-unsupervisedml</guid>
      <pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-18-unsupervisedml/unsupervisedml_files/figure-html5/unnamed-chunk-14-19.png" medium="image" type="image/png" width="2880" height="1152"/>
    </item>
    <item>
      <title>Combining Multiple Machine Learning Models with the Ensemble Methods</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-09-ensemble</link>
      <description>This entry explores different ways to combine supervised machine learning models to maximize their predictive capability.  

(13 min read)</description>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-09-ensemble</guid>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-09-ensemble/robot.png" medium="image" type="image/png" width="626" height="528"/>
    </item>
    <item>
      <title>Examining PISA 2018 Data Set with Statistical Learning Approach</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-02-27-statlearning</link>
      <description>In this post, I will be developing two statistical learning models, namely Linear Regression and Polynomial Regression, and apply it to Thai student data from the Programme for International Student Assessment (PISA) 2018 data set to examine the impact of classroom competition and cooperation to students' academic performance.  

(14 min read)</description>
      <category>R</category>
      <category>Statistics</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-02-27-statlearning</guid>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-02-27-statlearning/statlearn.jpeg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Classical Test Theory in R</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-01-15-ctt</link>
      <description>For this post, I will be analyzing characteristics of test items based on the framework of Classical Test Theory (CTT).

(13 min read)</description>
      <category>R</category>
      <category>Psychometric</category>
      <guid>https://taridwong.github.io/posts/2022-01-15-ctt</guid>
      <pubDate>Sat, 15 Jan 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-01-15-ctt/ctt_files/figure-html5/unnamed-chunk-28-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Examining the Big 5 personality Dataset with factor analysis</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-01-01-efacfa</link>
      <description>For this entry, I will be examining the Big 5 personality Inventory data set with Exploratory Data Analysis to identify potential structures of personality trait and verify them with Confirmatory Factor Analysis.

(8 min read)</description>
      <category>R</category>
      <category>Statistics</category>
      <guid>https://taridwong.github.io/posts/2022-01-01-efacfa</guid>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-01-01-efacfa/corrmatrix.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Measuring Text Similarity with Movie plot data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-29-movie-similarity</link>
      <description>For this post, I will be analyzing textual data of movie plots to determine their similarity with TF-IDF and Clustering.

(7 min read)</description>
      <category>Python</category>
      <category>Natural Language Processing</category>
      <category>Unsupervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-29-movie-similarity</guid>
      <pubDate>Wed, 29 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-29-movie-similarity/movie-similarity_files/figure-html5/unnamed-chunk-11-1.png" medium="image" type="image/png" width="484" height="483"/>
    </item>
    <item>
      <title>Missing Data Analysis</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-27-missingdata</link>
      <description>For this post, I will examine missing data in a large-scale dataset and discuss about numerous ways we can clean them as a part of data preparation.

(10 min read)</description>
      <category>R</category>
      <category>Data Visualization</category>
      <category>Statistics</category>
      <guid>https://taridwong.github.io/posts/2021-12-27-missingdata</guid>
      <pubDate>Mon, 27 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-27-missingdata/missingpic.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Applying Machine Learning to Audio Data: Visualization, Classification, and Recommendation</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data</link>
      <description>For this entry, I am trying my hands on audio data to extract its features for exploratory data analysis (EDA), using machine learning algorithms to perform music classification, and finally build up on that result to develop a recommendation system for music of similar characteristics.

(13 min read)</description>
      <category>Python</category>
      <category>Data Visualization</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data</guid>
      <pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data/applying-machine-learning-to-audio-data_files/figure-html5/unnamed-chunk-13-11.png" medium="image" type="image/png" width="3072" height="1152"/>
    </item>
    <item>
      <title>Interactive plots for Suicide Data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data</link>
      <description>For this entry, will be visualizing suicide data from 1958 to 2015 with interactive plots to communicate insights to non-technical audience.  

(14 min read)</description>
      <category>R</category>
      <category>Data Visualization</category>
      <category>Quantitative research</category>
      <guid>https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data</guid>
      <pubDate>Thu, 09 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data/preview.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Image Recognition with Artificial Neural Networks</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks</link>
      <description>In this entry, we will be developing a deep learning algorithm - a sub-field of machine learning inspired by the structure of human brain (neural networks) - to classify images of single digit number (0-9).  

(9 min read)</description>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <category>Deep Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks</guid>
      <pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks/deepnn.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Anomaly Detection with New York City taxi data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data</link>
      <description>In this entry, I will be conducting anomaly detection to identify points of anomaly in the taxi passengers data in New York City from July 2014 to January 2015 at half-hourly intervals.
 
 (4 min read)</description>
      <category>Unsupervised Machine Learning</category>
      <category>R</category>
      <guid>https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data</guid>
      <pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data/anomaly-detection-with-new-york-city-taxi-data_files/figure-html5/unnamed-chunk-9-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Crime mapping in San Francisco with police data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data</link>
      <description>In this entry, we will explore San Francisco crime data from 2016 to 2018 to understand the relationship between civilian-reported incidents of crime and police-reported incidents of crime. Along the way we will use table intersection methods to subset our data, aggregation methods to calculate important statistics, and simple visualizations to understand crime trends.  

(7 min read)</description>
      <category>Data Visualization</category>
      <category>R</category>
      <category>Quantitative research</category>
      <guid>https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data</guid>
      <pubDate>Tue, 30 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data/crime-mapping-in-san-francisco-with-police-data_files/figure-html5/unnamed-chunk-6-1.png" medium="image" type="image/png" width="1920" height="1920"/>
    </item>
    <item>
      <title>Exploring COVID-19 data from twitter with topic modeling</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds</link>
      <description>This entry focuses on the exploration of twitter data from Alberta's Chief Medical Officer of Health via word cloud and topic modeling to gain insights in characteristics of public health messaging during the COVID-19 pandemic.  

(7 min read)</description>
      <category>Python</category>
      <category>Natural Language Processing</category>
      <category>Unsupervised Machine Learning</category>
      <category>COVID-19</category>
      <guid>https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds</guid>
      <pubDate>Tue, 23 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds/word-cloud-pv.png" medium="image" type="image/png" width="1920" height="1920"/>
    </item>
    <item>
      <title>Finding a home among the paradigm push-back with Dialectical Pluralism</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-07-introducing-dialectical-pluralism</link>
      <description>This entry discusses the reconciliation of quantitative and qualitative worldviews amidst the paradigm wars with pluralistic stance.

(2 min read)</description>
      <category>Mixed methods research</category>
      <guid>https://taridwong.github.io/posts/2021-11-07-introducing-dialectical-pluralism</guid>
      <pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://www.relevantinsights.com/wp-content/uploads/2020/05/Qualitative-Quantitative-Research-For-New-Product-Development.png" medium="image" type="image/png"/>
    </item>
  </channel>
</rss>
