<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>Tarid Wongvorachan</title>
    <link>https://taridwong.github.io/</link>
    <atom:link href="https://taridwong.github.io/index.xml" rel="self" type="application/rss+xml"/>
    <description>Welcome to my data science blog!
</description>
    <generator>Distill</generator>
    <lastBuildDate>Sun, 17 Nov 2024 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Evaluating Synthetic Data with A Psychometric Approach</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2024-11-15-synth</link>
      <description>


&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(synthpop)
library(tidyverse)
library(CTT)
library(mirt)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Hi Everyone. It’s been a while since my last blog post. I have
some time while finishing up my semester to try out new research methods
and document what I learned. As a researcher or a data scientist, I
often have urges to try my hands on several datasets and, if possible,
share my findings to the public.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, handling sensitive datasets, particularly in
psychological research, can pose challenges related to confidentiality
and ethics. Psychological studies frequently involve surveys that ask
participants about their personal experiences, thoughts, and feelings.
These responses can include sensitive topics like trauma, substance
abuse, or mental health issues. Researchers must handle this data with
care to maintain participant confidentiality and trust.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Synthetic data offers a practical alternative, allowing
researchers to analyze or teach using synthetic datasets while
preserving the statistical properties of the original data. This blog
demonstrates synthesizing a dataset of examinees’ responses to a 12-item
psychological scale using R’s synthpop package and evaluating the
psychometric properties of the synthetic data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="preparing-the-dataset"&gt;Preparing the Dataset&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The dataset (&lt;code&gt;data_org&lt;/code&gt;) contains real examinee
responses to a 12-item scale measuring a psychological construct. Each
question is responded using a seven-point Likert scale, with 7 means
“definitely true” to 1 means “mostly false” to examinees’ agreement to
statements presented to them.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Before synthesizing the data, missing data is removed using
&lt;code&gt;na.omit()&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;data_org &amp;lt;- read.csv(&amp;quot;df.csv&amp;quot;, header = TRUE)
data_org &amp;lt;- na.omit(data_org)&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id="generating-synthetic-data"&gt;Generating Synthetic Data&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The syn function from the &lt;code&gt;synthpop&lt;/code&gt; package
synthesizes data using the CART (Classification and Regression Trees)
method. The documenttation of the package can be viewed &lt;a
href="https://synthpop.org.uk/about-synthpop.html"&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This step generates a synthetic dataset (&lt;code&gt;data_synth&lt;/code&gt;)
designed to mirror the statistical properties of
&lt;code&gt;data_org&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;synth.obj &amp;lt;- syn(data_org, method = &amp;quot;cart&amp;quot;, seed = 12345)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;data_synth &amp;lt;- synth.obj$syn&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id="comparing-original-and-synthetic-data"&gt;Comparing Original and
Synthetic Data&lt;/h1&gt;
&lt;h2 id="visualizing-differences"&gt;Visualizing Differences&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;compare&lt;/code&gt; function visually compares the
distributions of selected variables between the original and synthetic
datasets:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mycols &amp;lt;- c(&amp;quot;darkmagenta&amp;quot;, &amp;quot;turquoise&amp;quot;)
object &amp;lt;- compare(synth.obj, data_org, nrow = 3, ncol = 4, cols = mycols)
object$plots&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34105387243_files/figure-html/unnamed-chunk-7-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Overall, the comparison between the synthetic and original datasets
shows a high degree of similarity. The bar charts for each question (Q1
to Q12) indicate that the synthetic data closely mirrors the original
data in terms of the distribution of responses.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="utility-assessment"&gt;Utility Assessment&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;utility.gen&lt;/code&gt; function quantifies the similarity
between datasets using utility measures.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;util_gen &amp;lt;- utility.gen(synth.obj, data_org)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Ideally, we would want the propensity score standardized
mean-squared error (S_pMSE) to be closer to 1.0, meaning that the
synthetic data has perfect semblance to the original dataset. However,
we can use other methods to assess quality of the synthetic data as well
&lt;a href="https://blogs.ed.ac.uk/graab/workshop/idpln/"&gt;Raab &amp;amp; Nowok,
2022&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;util_gen&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Utility score calculated by method: cart

Call:
utility.gen.synds(object = synth.obj, data = data_org)

Null utilities simulated from a permutation test with 50 replications.

Selected utility measures
    pMSE   S_pMSE 
0.063602 1.691113 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Since creating a perfect, 100% replica of a dataset is inherently
challenging, I validated the synthetic data using conventional
statistical analyses and psychometric evaluation methods. This approach
ensures that the synthetic dataset maintains its integrity and produces
results aligned with the dataset’s intended purpose as a measurement
tool.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="statistical-testing"&gt;Statistical Testing&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To assess whether the synthetic data aligns with the original
dataset at the item level, I conducted independent sample t-tests for
each of the 12 items in the scale. This statistical test evaluates
whether there are significant differences in the mean responses between
the original (&lt;code&gt;data_org&lt;/code&gt;) and synthetic
(&lt;code&gt;data_synth&lt;/code&gt;) datasets.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;First, I initialized a data frame, &lt;code&gt;t_test_results&lt;/code&gt;,
to store the outcomes of the t-tests. This data frame included columns
for the item number (&lt;code&gt;Item&lt;/code&gt;), the calculated t-value
(&lt;code&gt;T_Statistic&lt;/code&gt;), the corresponding p-value
(&lt;code&gt;P_Value&lt;/code&gt;), and a logical indicator
(&lt;code&gt;Significant&lt;/code&gt;) that flagged whether the difference was
statistically significant at the 5% level (p &amp;lt; 0.05). This setup
provided a structured way to systematically record and interpret the
results.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Next, I used a loop to iterate through all 12 items in the
dataset. For each item, the responses from the original and synthetic
datasets were extracted. An independent sample t-test was performed to
compare their means, and the results were stored in the
&lt;code&gt;t_test_results&lt;/code&gt; data frame. Specifically, the t-statistic
and p-value were recorded, and the &lt;code&gt;Significant&lt;/code&gt; column was
updated to indicate whether the observed differences were statistically
significant. This approach ensured a consistent and transparent
validation process across all items.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally, the completed &lt;code&gt;t_test_results&lt;/code&gt; data frame
offered a comprehensive summary of the t-test results for all items.
Items with significant differences (p &amp;lt; 0.05) were flagged,
highlighting variables where the synthetic data deviated notably from
the original. Conversely, non-significant results confirmed that the
synthetic data closely replicated the original for those items. These
findings provided valuable insights into the synthetic dataset’s
accuracy, identifying potential areas for refinement and reinforcing its
suitability for statistical and psychometric applications.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Initialize an empty data frame to store the t-test results
t_test_results &amp;lt;- data.frame(
  Item = 1:12,
  T_Statistic = numeric(12),
  P_Value = numeric(12),
  Significant = logical(12)
)

# Perform independent sample t-test for each item
for (i in 1:12) {
  # Extract responses for the current item from both original and synthesized datasets
  org_responses &amp;lt;- data_org[, i]
  synth_responses &amp;lt;- data_synth[, i]
  
  # Perform the independent t-test
  t_test &amp;lt;- t.test(org_responses, synth_responses)
  
  # Store the results in the data frame
  t_test_results$T_Statistic[i] &amp;lt;- t_test$statistic
  t_test_results$P_Value[i] &amp;lt;- t_test$p.value
  
  # Check if the p-value is less than 0.05 (significant at the 5% level)
  t_test_results$Significant[i] &amp;lt;- t_test$p.value &amp;lt; 0.05
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# View the t-test results
print(t_test_results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Item T_Statistic   P_Value Significant
1     1  1.24725294 0.2125793       FALSE
2     2  0.37685436 0.7063567       FALSE
3     3  0.03819008 0.9695433       FALSE
4     4  1.04233137 0.2974951       FALSE
5     5  0.75686676 0.4492967       FALSE
6     6  0.24844901 0.8038351       FALSE
7     7 -0.63765775 0.5238332       FALSE
8     8  0.08628668 0.9312547       FALSE
9     9  0.56532140 0.5719741       FALSE
10   10  0.74877731 0.4541563       FALSE
11   11  0.46332527 0.6432256       FALSE
12   12  0.65988433 0.5094704       FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="visualizing-t-test-results"&gt;Visualizing T-Test Results&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To visually interpret the results of the independent sample t-tests,
I created a bar plot using the &lt;code&gt;ggplot2&lt;/code&gt; package. This
visualization illustrates the t-statistic for each item, making it
easier to identify which items showed significant differences between
the original and synthetic datasets. The plot serves as a clear and
intuitive representation of the t-test results, providing stakeholders
with a quick overview of the dataset’s alignment.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;ggplot(t_test_results, aes(x = factor(Item), y = T_Statistic, fill = Significant)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, position = &amp;quot;dodge&amp;quot;, width = 0.7) +
  labs(title = &amp;quot;Independent Sample T-Test: Item-wise Comparison at p &amp;lt; .05&amp;quot;, 
       x = &amp;quot;Item&amp;quot;, y = &amp;quot;T-Statistic&amp;quot;) +
  scale_fill_manual(values = c(&amp;quot;TRUE&amp;quot; = &amp;quot;turquoise&amp;quot;, &amp;quot;FALSE&amp;quot; = &amp;quot;darkmagenta&amp;quot;)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  geom_text(aes(label = ifelse(Significant, &amp;quot;*&amp;quot;, &amp;quot;&amp;quot;)), 
            vjust = -0.5, color = &amp;quot;black&amp;quot;, size = 5) +
  annotate(&amp;quot;text&amp;quot;, x = 2, y = max(t_test_results$T_Statistic) + 2, 
           label = &amp;quot;Negative t-statistic: Mean of original &amp;lt; Synthesized\nPositive t-statistic: Mean of original &amp;gt; Synthesized&amp;quot;, 
           size = 4, hjust = 0, color = &amp;quot;black&amp;quot;, fontface = &amp;quot;italic&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34105387243_files/figure-html/unnamed-chunk-12-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The T-statistics for each item range from approximately -1 to 2.
This indicates the degree of difference between the means of the
original and synthetic datasets for each item.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;None of the comparisons are significant, as indicated by the
legend (all labeled as “FALSE”). This means that the differences between
the original and synthetic datasets are not statistically significant at
the &lt;em&gt;p&lt;/em&gt; &amp;lt; .05 level.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="psychometric-properties-comparison"&gt;Psychometric Properties
Comparison&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To evaluate and compare the psychometric properties of the
original and synthetic datasets, I conducted item analyses using the
&lt;code&gt;itemAnalysis&lt;/code&gt; function from the &lt;code&gt;CTT&lt;/code&gt; package.
This step aimed to assess whether the synthetic dataset preserved key
measurement characteristics, such as item means and discrimination
indices, ensuring it aligns with the original dataset’s intended purpose
as a measurement instrument.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For the original dataset (&lt;code&gt;data_org&lt;/code&gt;), the
&lt;code&gt;itemAnalysis&lt;/code&gt; function was applied to generate a detailed
report of psychometric properties. This analysis included calculations
of item means, item-total correlations (discrimination indices), and
flagged items based on pre-specified thresholds for performance issues.
The same analysis was then conducted on the synthetic dataset
(&lt;code&gt;data_synth&lt;/code&gt;) using identical parameters.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;By comparing the results from both datasets, I could evaluate
whether the synthetic dataset accurately replicated the original
dataset’s psychometric properties. Specifically, alignment in item means
would suggest that the synthetic data preserves the central tendency of
responses, while similarity in discrimination indices would indicate
that the synthetic data retains the original dataset’s ability to
differentiate between examinees effectively. These analyses are
essential for validating the synthetic dataset’s utility for
psychometric research and educational applications.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Perform item analysis on original data
org_item_analysis &amp;lt;- itemAnalysis(data_org, itemReport=TRUE, NA.Delete=TRUE, pBisFlag = T,  bisFlag = T, flagStyle = c(&amp;quot;X&amp;quot;,&amp;quot;&amp;quot;))

# Perform item analysis on synthesized data
synth_item_analysis &amp;lt;- itemAnalysis(data_synth, itemReport=TRUE, NA.Delete=TRUE, pBisFlag = T,  bisFlag = T, flagStyle = c(&amp;quot;X&amp;quot;,&amp;quot;&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="visualizing-item-level-properties"&gt;Visualizing Item-Level
Properties&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To visually compare the psychometric properties of the original
and synthetic datasets, I created side-by-side bar charts for two key
metrics: item means and item discrimination indices. These metrics are
critical for understanding the central tendency of item responses and
the items’ ability to differentiate between respondents based on their
overall performance.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;First, I extracted the item means and discrimination indices from
the results of the &lt;code&gt;itemAnalysis&lt;/code&gt; for both datasets. This
involved organizing the data into a data frame that paired each item
with its corresponding mean and discrimination index values for both the
original and synthetic datasets. This structure enabled a direct
comparison of the two datasets’ properties.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To prepare the data for visualization, I reshaped the data frame
into a “long” format using the &lt;code&gt;pivot_longer&lt;/code&gt; function from
the &lt;code&gt;tidyverse&lt;/code&gt; package. For item means, I created a data
frame where each item was associated with its mean score from both
datasets, labeled as either “Original_Mean” or “Synthesized_Mean.”
Similarly, for item discrimination indices, I created another data frame
with labels “Original_Discrimination” and “Synthesized_Discrimination.”
This long-format structure is ideal for creating grouped bar charts in
&lt;code&gt;ggplot2&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This step laid the groundwork for creating side-by-side bar
charts that clearly depict how closely the synthetic data aligns with
the original data. By focusing on item means and discrimination indices,
these visualizations provide critical insights into whether the
synthetic dataset faithfully preserves the original data’s psychometric
characteristics, ensuring it remains a valid representation for
educational or research purposes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#Plot Side-by-Side Bar Charts for Item Means and Discrimination

# Extract item mean and discrimination for original and synthetic data
item_comparison &amp;lt;- data.frame(
  Item = seq_along(org_item_analysis$itemReport$itemMean),
  Original_Mean = org_item_analysis$itemReport$itemMean,
  Synthesized_Mean = synth_item_analysis$itemReport$itemMean,
  Original_Discrimination = org_item_analysis$itemReport$pBis,
  Synthesized_Discrimination = synth_item_analysis$itemReport$pBis
)

# Reshape the data for ggplot
item_means_long &amp;lt;- item_comparison %&amp;gt;%
  select(Item, Original_Mean, Synthesized_Mean) %&amp;gt;%
  tidyr::pivot_longer(cols = c(Original_Mean, Synthesized_Mean), names_to = &amp;quot;Dataset&amp;quot;, values_to = &amp;quot;Mean&amp;quot;)

item_discrimination_long &amp;lt;- item_comparison %&amp;gt;%
  select(Item, Original_Discrimination, Synthesized_Discrimination) %&amp;gt;%
  tidyr::pivot_longer(cols = c(Original_Discrimination, Synthesized_Discrimination), names_to = &amp;quot;Dataset&amp;quot;, values_to = &amp;quot;Discrimination&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;To visualize the comparison of item means between the original and
synthetic datasets, I used a grouped bar chart created with the ggplot2
package. This chart highlights how closely the synthetic dataset
replicates the central tendency of responses in the original
dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Plot item means
ggplot(item_means_long, aes(x = as.factor(Item), y = Mean, fill = Dataset)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, position = &amp;quot;dodge&amp;quot;) +
  labs(title = &amp;quot;Item Means Comparison&amp;quot;, x = &amp;quot;Item&amp;quot;, y = &amp;quot;Mean Score&amp;quot;) +
  theme_minimal() +
  scale_fill_manual(values = c(&amp;quot;Original_Mean&amp;quot; = &amp;quot;darkmagenta&amp;quot;, &amp;quot;Synthesized_Mean&amp;quot; = &amp;quot;turquoise&amp;quot;)) + 
  theme(plot.title = element_text(hjust = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34105387243_files/figure-html/unnamed-chunk-15-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For most items, the mean scores of the original and synthesized
datasets are quite similar. This indicates that the synthetic data
closely replicates the central tendencies of the original data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To assess the ability of items to differentiate between
respondents based on their trait levels, I created a grouped bar chart
to compare item discrimination (point biserial correlation - pBis)
indices from the original and synthetic datasets. A solid red horizontal
line is added at a discrimination index of 0.2 using
&lt;code&gt;geom_hline(yintercept = 0.2, color = "red", linetype = "solid")&lt;/code&gt;.
This line marks a commonly accepted threshold for minimal acceptable
discrimination, offering a benchmark against which the indices can be
evaluated.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Plot item discrimination
ggplot(item_discrimination_long, aes(x = as.factor(Item), y = Discrimination, fill = Dataset)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, position = &amp;quot;dodge&amp;quot;) +
  labs(title = &amp;quot;Item Discrimination Comparison \n The red line represents minimal acceptable value&amp;quot;, x = &amp;quot;Item&amp;quot;, y = &amp;quot;Discrimination Index&amp;quot;) +
  theme_minimal() +
  scale_fill_manual(values = c(&amp;quot;Original_Discrimination&amp;quot; = &amp;quot;darkmagenta&amp;quot;, &amp;quot;Synthesized_Discrimination&amp;quot; = &amp;quot;turquoise&amp;quot;)) + 
  geom_hline(yintercept = 0.2, color = &amp;quot;red&amp;quot;, linetype = &amp;quot;solid&amp;quot;) + 
  theme(plot.title = element_text(hjust = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34105387243_files/figure-html/unnamed-chunk-16-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For most items, the discrimination indices of the original and
synthesized datasets are quite similar, with both datasets meeting or
exceeding the minimal acceptable value of 0.2 for most items. The chart
demonstrates that the synthetic dataset closely mirrors the original
dataset in terms of item discrimination, maintaining the ability to
differentiate between high and low performers. This suggests that the
synthetic data retains the psychometric properties of the original data,
making it a somewhat reliable substitute for analysis purposes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="reliability-analysis"&gt;Reliability Analysis&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To compare the reliability of the original and synthetic
datasets, I constructed a bar chart that visualizes the coefficient
alpha values for each dataset. Coefficient alpha is a measure of
internal consistency, indicating the extent to which items in a scale
measure the same construct. This comparison helps assess whether the
synthetic data faithfully reproduces the reliability of the original
data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The input data for this visualization is encapsulated in the
alpha_comparison data frame, which contains two rows: one for the
original dataset and one for the synthesized dataset. Each row includes
the respective alpha value, calculated during the item analysis
process.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Reliability Comparison Data Frame
alpha_comparison &amp;lt;- data.frame(
  Dataset = c(&amp;quot;Original&amp;quot;, &amp;quot;Synthesized&amp;quot;),
  Alpha = c(org_item_analysis$alpha, synth_item_analysis$alpha)
)

# Bar Plot with Values Displayed Above Bars
ggplot(alpha_comparison, aes(x = Dataset, y = Alpha, fill = Dataset)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, width = 0.5) +
  geom_text(aes(label = round(Alpha, 3)), vjust = -0.3, color = &amp;quot;black&amp;quot;, size = 5) + # Display values above bars
  labs(title = &amp;quot;Reliability (Coefficient Alpha) Comparison&amp;quot;,
       y = &amp;quot;Alpha Reliability Coefficient&amp;quot;) +
  theme_minimal() +
  scale_fill_manual(values = c(&amp;quot;Original&amp;quot; = &amp;quot;darkmagenta&amp;quot;, &amp;quot;Synthesized&amp;quot; = &amp;quot;turquoise&amp;quot;)) +
  theme(plot.title = element_text(hjust = 0.5)) # Center the title&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34105387243_files/figure-html/unnamed-chunk-17-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The chart indicates that the synthetic dataset maintains a somewhat
high level of internal consistency (higher than .70), comparable to the
original dataset. This suggests that the synthetic data is a reliable
representation of the original data in terms of internal
consistency.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="item-response-theory-test-information-function"&gt;Item Response
Theory Test Information Function&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To evaluate the psychometric properties of the original and
synthesized datasets under the framework of Item Response Theory (IRT),
I implemented Graded Response Models (GRM) using the &lt;code&gt;mirt&lt;/code&gt;
package in R. GRMs are particularly well-suited for analyzing polytomous
item responses, as they estimate parameters that describe the
relationship between latent traits (e.g., ability) and the probability
of item responses across multiple categories.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The GRM was fitted separately to the original and synthesized
datasets using the &lt;code&gt;mirt&lt;/code&gt; function. The syntax specifies
&lt;code&gt;model = 1&lt;/code&gt;, indicating a unidimensional model, and
&lt;code&gt;itemtype = "graded"&lt;/code&gt;, which defines the GRM as the chosen
IRT model. The outputs are stored in &lt;code&gt;model_org&lt;/code&gt; and
&lt;code&gt;model_synth&lt;/code&gt; for the original and synthesized data,
respectively.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Fit a Graded Response Model (GRM) for original data
model_org &amp;lt;- mirt(data_org, model = 1, itemtype = &amp;quot;graded&amp;quot;)

# Fit a Graded Response Model (GRM) for synthesized data
model_synth &amp;lt;- mirt(data_synth, model = 1, itemtype = &amp;quot;graded&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="estimated-irt-model-of-the-original-dataset"&gt;Estimated IRT Model
of the Original Dataset&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;After fitting the models, I examined their structures and key
parameters. The object &lt;code&gt;model_org&lt;/code&gt; provides an overview of
the original dataset’s IRT model, including the number of dimensions and
items. A detailed summary using &lt;code&gt;summary(model_org)&lt;/code&gt; displays
item parameters such as discrimination and difficulty thresholds.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;model_org&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
mirt(data = data_org, model = 1, itemtype = &amp;quot;graded&amp;quot;)

Full-information item factor analysis with 1 factor(s).
Converged within 1e-04 tolerance after 31 EM iterations.
mirt version: 1.43 
M-step optimizer: BFGS 
EM acceleration: Ramsay 
Number of rectangular quadrature: 61
Latent density type: Gaussian 

Log-likelihood = -9210.903
Estimated parameters: 84 
AIC = 18589.81
BIC = 18949.52; SABIC = 18682.87
G2 (1e+10) = 11699.78, p = 1
RMSEA = 0, CFI = NaN, TLI = NaN&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;summary(model_org)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          Length            Class             Mode 
               1 SingleGroupClass               S4 &lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="estimated-irt-model-of-the-synthetic-dataset"&gt;EStimated IRT
Model of the Synthetic Dataset&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Similarly, &lt;code&gt;model_synth&lt;/code&gt; and its summary provide
corresponding information for the synthesized dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;model_synth&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
mirt(data = data_synth, model = 1, itemtype = &amp;quot;graded&amp;quot;)

Full-information item factor analysis with 1 factor(s).
Converged within 1e-04 tolerance after 22 EM iterations.
mirt version: 1.43 
M-step optimizer: BFGS 
EM acceleration: Ramsay 
Number of rectangular quadrature: 61
Latent density type: Gaussian 

Log-likelihood = -9220.621
Estimated parameters: 83 
AIC = 18607.24
BIC = 18962.67; SABIC = 18699.2
G2 (1e+10) = 11721.99, p = 1
RMSEA = 0, CFI = NaN, TLI = NaN&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;summary(model_synth)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          Length            Class             Mode 
               1 SingleGroupClass               S4 &lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="test-information-function-comparison"&gt;Test Information Function
Comparison&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To further compare the psychometric properties of the original
and synthesized datasets, I evaluated the Test Information Function
(TIF), a crucial aspect in IRT models. The TIF indicates the amount of
information provided by the test at various levels of the latent trait
(theta), which reflects the precision with which a test can measure a
person’s ability at different points on the ability scale.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Comparing the TIFs for the original and synthesized datasets
allows us to assess if the synthetic data preserves the measurement
capabilities of the original test, ensuring that the synthesized items
are psychometrically valid across the latent trait continuum.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;By plotting and comparing the TIFs, we can also identify regions
where the synthesized data may overestimate or underestimate test
information, providing valuable insights into its alignment with the
original dataset’s performance.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Define a range of theta values (latent trait values) to calculate test information
theta_values &amp;lt;- seq(-3, 3, by = 0.1)  # Adjust the range and step size as needed

# Extract the Test Information Function (TIF) for original data
TIF_org &amp;lt;- testinfo(model_org, Theta = theta_values)

# Extract the Test Information Function (TIF) for synthesized data
TIF_synth &amp;lt;- testinfo(model_synth, Theta = theta_values)


# Plot Test Information Function for original and synthesized data
plot(theta_values, TIF_org, type = &amp;quot;l&amp;quot;, col = &amp;quot;darkmagenta&amp;quot;, lwd = 2, 
     xlim = c(-3, 3), ylim = c(0, max(TIF_org, TIF_synth)), 
     xlab = &amp;quot;Theta&amp;quot;, ylab = &amp;quot;Test Information&amp;quot;, 
     main = &amp;quot;Test Information Function (TIF) Comparison&amp;quot;)

# Add the TIF for synthesized data to the plot
lines(theta_values, TIF_synth, col = &amp;quot;turquoise&amp;quot;, lwd = 2)

# Add a legend
legend(&amp;quot;topright&amp;quot;, legend = c(&amp;quot;Original Data&amp;quot;, &amp;quot;Synthesized Data&amp;quot;), 
       col = c(&amp;quot;darkmagenta&amp;quot;, &amp;quot;turquoise&amp;quot;), lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34105387243_files/figure-html/unnamed-chunk-23-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The purple line represents the original data, while the cyan line
represents the synthesized data. The graph shows that the synthesized
data generally provides higher test information than the original data,
especially in the theta range from -3 to 2. This means that both dataset
provide comparable measurement properties in examinees whose latent
trait at high level (i.e., &amp;gt; 2).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, given that the distribution of item responses in two
datasets are not statistically different as indicated by t-test results,
we could infer that the different in test information between the two
datasets are also somewhat comparable.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="concluding-remark-and-practical-implications"&gt;Concluding Remark
and Practical Implications&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In conclusion, this analysis demonstrates how synthetic data,
when generated thoughtfully and validated against original data, can
serve as a valuable alternative for research purposes. By comparing
metrics such as item means, discrimination, reliability, and the test
information function (TIF), we were able to ensure that the synthesized
dataset retains critical psychometric properties of the original
dataset, providing robust support for further analysis.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For practical implications, the ability to generate synthetic
data that closely mirrors the statistical properties of real-world data
is helpful. This can be particularly beneficial in situations where
access to original datasets is limited due to privacy concerns or
logistical constraints. For instance, when working with sensitive data,
synthetic datasets can serve as a useful substitute for testing
algorithms, conducting simulations, or performing validation studies,
all without compromising individual privacy.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Moreover, understanding how synthetic data behaves compared to
its original counterpart allows researchers and data scientists to
confidently use synthesized data for a variety of applications, such as
model development, training machine learning algorithms, or performing
sensitivity analyses. The ability to validate the quality of synthetic
data ensures that any decisions or inferences drawn from its use are
based on reliable, comparable data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;From a methodological perspective, this analysis also provides a
valuable framework for evaluating synthetic data across different
contexts. By applying various psychometric methods like item analysis
and IRT, or using traditional means like t-test, researchers can
identify areas of strength and potential improvements in synthetic data,
tailoring their data generation processes to meet specific research
needs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Suppose a researcher is studying a new psychological assessment
tool. By conducting item analysis on both the original and synthesized
datasets, they could determine whether the synthesized data maintains
the same level of item difficulty and discrimination as the original
data.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;If a researcher is using synthetic data to simulate different groups
(e.g., a treatment group vs. a control group), performing t-tests can
help ensure that the mean differences between groups in the synthetic
dataset align with those in the original data.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, it’s essential to remember that synthetic data is not a
one-size-fits-all solution. Its effectiveness depends on the quality of
the generative models and the methods used to validate the data. It’s
always crucial to perform rigorous checks, as we did in this example, to
ensure that the synthetic data aligns closely with the original
dataset’s characteristics. If the synthetic data does not match with the
original data, researchers could adjust the data generation model to
better reflect the variability of responses in real-world data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In conclusion, the thorough validation of synthesized datasets
against real-world data offers reassurance that synthetic data can serve
as a trustworthy tool in many research and practical applications. This
not only enhances the credibility of synthetic data as an alternative to
original datasets but also supports its increasing adoption across
disciplines where data privacy, availability, or ethical concerns may
otherwise limit the use of actual datasets.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Thank you for following along—hopefully, this post has given you
some ideas about how you might use synthetic data in your own
work!&lt;br /&gt;
&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>76705c73e8af2fb05d59413bfebb9e3a</distill:md5>
      <category>R</category>
      <category>Psychometric</category>
      <guid>https://taridwong.github.io/posts/2024-11-15-synth</guid>
      <pubDate>Sun, 17 Nov 2024 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2024-11-15-synth/synth_files/figure-html5/unnamed-chunk-7-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Exploring Nature's 2023 postdoc survey with predictive modeling and NLP</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2023-12-10-naturepostdoc</link>
      <description>


&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Hi Everyone. I have got some time during the winter break to
write another post. I came across Nature’s postdoc 2023 survey, which is
a series of articles published by Nature to examine the situation of
postdoctoral researchers such as their &lt;a
href="https://www.nature.com/articles/d41586-023-03163-7"&gt;post-pandemic
well being&lt;/a&gt;, their &lt;a
href="https://www.nature.com/articles/d41586-023-03235-8"&gt;use of
artificial intelligence chatbot&lt;/a&gt; such as ChatGPT, and their &lt;a
href="https://www.nature.com/articles/d41586-023-03296-9"&gt;career
outlook&lt;/a&gt;. The general impression I got is that postdoc researchers
are pressured to secure fundings and stable jobs, despite their high
educational level. &lt;a
href="https://www.nature.com/articles/d41586-023-02848-3"&gt;Some of them
event went on strike&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Aside from postdoc, I also came across some articles discussing
the prospect of early career researchers who earned their Ph.D and got
into a faculty position, &lt;a
href="10.1080/02680939.2023.2288339"&gt;'Academics without publications are
just like imperial concubines without sons': the 'new times' of Chinese
higher education&lt;/a&gt;. The paper highlights how the publish or perish
culture drives researchers to produce as much research works as possible
to earn job security as a tenure professor; This status quo reflects the
state of perpetual competition (dubbed as “academic hunger games”) that
threatens mental health and work-life balance or early career
researchers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As discussed in van Dalen (2020)‘s paper titled “&lt;a
href="https://doi.org/10.1007/s11192-020-03786-x"&gt;How the
publish-or-perish principle divides a science: the case of
economists&lt;/a&gt;”, while the publish or perish culture promotes academics’
rank and position, its serious drawbacks include the excessive number of
papers that are hardly read, makes researchers turn their back on
national issues as they focus on publishing, and increases the
probability of unethical behavior like fraud or data
manipulation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It is interesting how Nature &lt;a
href="https://figshare.com/articles/dataset/Nature_Post-Doctoral_Survey_2023/24236875"&gt;made
available their dataset from their 2023 postdoc survey&lt;/a&gt;. Taking what
I learned from Nature’s postdoc 2023 survey and articles around the
publish or perish culture, I think it could be interesting to explore
factors that influence postdoctoral researchers’ perspective to career
in scientific research with predictive modeling and natural language
processing. The data set as 89 variables, including both numerical and
textual data. I divided the dataset into a numerical dataset for
predictive machine learning model, and a textual dataset for the
follow-up natural language processing analysis. The data was cleaned in
R, but I will be using Python in this post.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;import pandas as pd
import numpy as np
import sweetviz
import seaborn as sns
import matplotlib.pyplot as plt

#Read the data set
df = pd.read_csv(&amp;quot;postdoc_processed.csv&amp;quot;, encoding = &amp;quot;ISO-8859-1&amp;quot;) #For prediction task

df_text = df[[&amp;#39;HOWEMPLOYED&amp;#39;, &amp;#39;EMPLOYSTATUS&amp;#39;, &amp;#39;WHYMOVE&amp;#39;, &amp;#39;WHYEXPCT&amp;#39;,
                     &amp;#39;WHYINCREASE&amp;#39;, &amp;#39;WHYDECREASE&amp;#39;, &amp;#39;WHYSECONDJOB&amp;#39;, &amp;#39;DISCRIMCAT&amp;#39;,
                     &amp;#39;DISCRIMPER&amp;#39;, &amp;#39;DISCRIMOBSCAT&amp;#39;, &amp;#39;CHALLENGE&amp;#39;, &amp;#39;LACKEDSKILLS&amp;#39;,
                     &amp;#39;REFLECT&amp;#39;, &amp;#39;CONTINENT&amp;#39;, &amp;#39;COUNTRY&amp;#39;, &amp;#39;AGE&amp;#39;,
                     &amp;#39;GENDER&amp;#39;, &amp;#39;RACE&amp;#39;]]
                     
df_reflect = df_text.dropna(subset=[&amp;#39;REFLECT&amp;#39;]) #For NLP&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;As always, we need to import packages and dataset into the
environment.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="data-exploration"&gt;Data exploration&lt;/h1&gt;
&lt;pre class="python"&gt;&lt;code&gt;#Read the data set
df = pd.read_csv(&amp;quot;postdoc_prediction_cleaned.csv&amp;quot;, encoding = &amp;quot;ISO-8859-1&amp;quot;)

# Use the analyze function to perform EDA
analyze_df = sweetviz.analyze(df, target_feat = &amp;quot;REGRET&amp;quot;)

# Render and show the results of EDA
analyze_df.show_html(&amp;quot;analyze.html&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-12-10-naturepostdocsweetviz.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I used the &lt;a
href="https://pypi.org/project/sweetviz/"&gt;&lt;code&gt;sweetviz&lt;/code&gt;&lt;/a&gt;
package in Python to generate a report on exploratory data analysis as
seen above. The targeted variable is named &lt;code&gt;REGRET&lt;/code&gt;, which is
respondents’ answer to a question of “Would you recommend to your
younger self that you pursue a career in scientific research?”. The
answer are either 1 (Yes) or 0 (No). As seen from the data exploration
report, both yes and no have similar proportion.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="predicting-regret-in-postdoc"&gt;Predicting regret in postdoc&lt;/h1&gt;
&lt;h2 id="feature-selection"&gt;Feature selection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Given that we have 66 variables (65 predictors and 1 targeted
variable), it could be useful to perform feature selection to filter out
variables that may not be useful for the predictive analysis. We will be
using recursive feature elimination with cross validation to fit a model
and remove weakest predictors until the optimal performance score is
reached. I am setting the &lt;code&gt;RFECV&lt;/code&gt; with a default Random
forest classification model, with minimum feature number == 20, and
using “roc_auc” as the performance metric.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;X = df.drop(&amp;#39;REGRET&amp;#39;, axis=1)
y = df[&amp;#39;REGRET&amp;#39;]

# Perform train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the RandomForestClassifier
rf_classifier = RandomForestClassifier(random_state=123)

# Initialize the RFECV
rfecv_model = RFECV(estimator=rf_classifier, 
                    step=1, cv=5, 
                    scoring=&amp;#39;roc_auc&amp;#39;, 
                    min_features_to_select=20)

rfecv = rfecv_model.fit(X_train, y_train)

print(&amp;#39;Optimal number of features :&amp;#39;, rfecv.n_features_)
print(&amp;#39;Best features by RFECV with random forest:&amp;#39;, X_train.columns[rfecv.support_])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-12-10-naturepostdocRFECV.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As shown above, the algorithm selected 63 features to be used as
predictors. Next, we will tune the model for optimal prediction
result.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;X_train_reduced = X_train[[&amp;#39;POSTDOCNATIVE&amp;#39;, &amp;#39;POSTDOCMOVE&amp;#39;, &amp;#39;FIRSTPOSTDOC&amp;#39;, &amp;#39;PREVPOSTDOC&amp;#39;,
       &amp;#39;POSTDOCEXPCT&amp;#39;, &amp;#39;CAREERSECTOR&amp;#39;, &amp;#39;SALARY&amp;#39;, &amp;#39;SALARYCHANGE&amp;#39;, &amp;#39;PAIDVACAY&amp;#39;,
       &amp;#39;PAIDSICK&amp;#39;, &amp;#39;INSURANCE&amp;#39;, &amp;#39;PENSION&amp;#39;, &amp;#39;PARENTLEAVE&amp;#39;, &amp;#39;CHILDCARE&amp;#39;,
       &amp;#39;BECOMEPARENT&amp;#39;, &amp;#39;DEBT&amp;#39;, &amp;#39;SAVING&amp;#39;, &amp;#39;SAVINGPROBLEM&amp;#39;, &amp;#39;WKHOURS&amp;#39;,
       &amp;#39;OVERTIME&amp;#39;, &amp;#39;OVERNIGHT&amp;#39;, &amp;#39;WEEKENDWK&amp;#39;, &amp;#39;POSTDOCVACANCY&amp;#39;, &amp;#39;SATCHANGE&amp;#39;,
       &amp;#39;SATSALARY&amp;#39;, &amp;#39;SATWELLNESS&amp;#39;, &amp;#39;SATFUNDING&amp;#39;, &amp;#39;SATTIME&amp;#39;, &amp;#39;SATCAREER&amp;#39;,
       &amp;#39;SATTRAINING&amp;#39;, &amp;#39;SATJOBSCURITY&amp;#39;, &amp;#39;SATOPT&amp;#39;, &amp;#39;SATSUPERVISE&amp;#39;, &amp;#39;SATORG&amp;#39;,
       &amp;#39;SATDECISION&amp;#39;, &amp;#39;SATWLB&amp;#39;, &amp;#39;SATWKHOURS&amp;#39;, &amp;#39;SATINTEREST&amp;#39;, &amp;#39;SATACCOM&amp;#39;,
       &amp;#39;SATRELATION&amp;#39;, &amp;#39;SATINDE&amp;#39;, &amp;#39;SATRECOG&amp;#39;, &amp;#39;SATSAFETY&amp;#39;, &amp;#39;SATDIVERSE&amp;#39;,
       &amp;#39;SUPERVISETIME&amp;#39;, &amp;#39;DISCRIMEXP&amp;#39;, &amp;#39;DISCRIMOBS&amp;#39;, &amp;#39;NEEDMHHELP&amp;#39;,
       &amp;#39;AGREEMHSUPP&amp;#39;, &amp;#39;AGREEDIRECTSUPP&amp;#39;, &amp;#39;AGREEWKSUPP&amp;#39;, &amp;#39;AGREEWLB&amp;#39;,
       &amp;#39;AGREELNGWK&amp;#39;, &amp;#39;MHLEAVESCI&amp;#39;, &amp;#39;BLIVGENDEREQ&amp;#39;, &amp;#39;BLIVETHEQ&amp;#39;, &amp;#39;BLIVSAFETY&amp;#39;,
       &amp;#39;BLIVDIGNITY&amp;#39;, &amp;#39;JOBPRSPCT&amp;#39;, &amp;#39;PRSPCTCHNGE&amp;#39;, &amp;#39;POSTDOCLEAVE&amp;#39;, &amp;#39;MINORITY&amp;#39;,
       &amp;#39;LTHEALTHPROB&amp;#39;]]

X_test_reduced = X_test[[&amp;#39;POSTDOCNATIVE&amp;#39;, &amp;#39;POSTDOCMOVE&amp;#39;, &amp;#39;FIRSTPOSTDOC&amp;#39;, &amp;#39;PREVPOSTDOC&amp;#39;,
       &amp;#39;POSTDOCEXPCT&amp;#39;, &amp;#39;CAREERSECTOR&amp;#39;, &amp;#39;SALARY&amp;#39;, &amp;#39;SALARYCHANGE&amp;#39;, &amp;#39;PAIDVACAY&amp;#39;,
       &amp;#39;PAIDSICK&amp;#39;, &amp;#39;INSURANCE&amp;#39;, &amp;#39;PENSION&amp;#39;, &amp;#39;PARENTLEAVE&amp;#39;, &amp;#39;CHILDCARE&amp;#39;,
       &amp;#39;BECOMEPARENT&amp;#39;, &amp;#39;DEBT&amp;#39;, &amp;#39;SAVING&amp;#39;, &amp;#39;SAVINGPROBLEM&amp;#39;, &amp;#39;WKHOURS&amp;#39;,
       &amp;#39;OVERTIME&amp;#39;, &amp;#39;OVERNIGHT&amp;#39;, &amp;#39;WEEKENDWK&amp;#39;, &amp;#39;POSTDOCVACANCY&amp;#39;, &amp;#39;SATCHANGE&amp;#39;,
       &amp;#39;SATSALARY&amp;#39;, &amp;#39;SATWELLNESS&amp;#39;, &amp;#39;SATFUNDING&amp;#39;, &amp;#39;SATTIME&amp;#39;, &amp;#39;SATCAREER&amp;#39;,
       &amp;#39;SATTRAINING&amp;#39;, &amp;#39;SATJOBSCURITY&amp;#39;, &amp;#39;SATOPT&amp;#39;, &amp;#39;SATSUPERVISE&amp;#39;, &amp;#39;SATORG&amp;#39;,
       &amp;#39;SATDECISION&amp;#39;, &amp;#39;SATWLB&amp;#39;, &amp;#39;SATWKHOURS&amp;#39;, &amp;#39;SATINTEREST&amp;#39;, &amp;#39;SATACCOM&amp;#39;,
       &amp;#39;SATRELATION&amp;#39;, &amp;#39;SATINDE&amp;#39;, &amp;#39;SATRECOG&amp;#39;, &amp;#39;SATSAFETY&amp;#39;, &amp;#39;SATDIVERSE&amp;#39;,
       &amp;#39;SUPERVISETIME&amp;#39;, &amp;#39;DISCRIMEXP&amp;#39;, &amp;#39;DISCRIMOBS&amp;#39;, &amp;#39;NEEDMHHELP&amp;#39;,
       &amp;#39;AGREEMHSUPP&amp;#39;, &amp;#39;AGREEDIRECTSUPP&amp;#39;, &amp;#39;AGREEWKSUPP&amp;#39;, &amp;#39;AGREEWLB&amp;#39;,
       &amp;#39;AGREELNGWK&amp;#39;, &amp;#39;MHLEAVESCI&amp;#39;, &amp;#39;BLIVGENDEREQ&amp;#39;, &amp;#39;BLIVETHEQ&amp;#39;, &amp;#39;BLIVSAFETY&amp;#39;,
       &amp;#39;BLIVDIGNITY&amp;#39;, &amp;#39;JOBPRSPCT&amp;#39;, &amp;#39;PRSPCTCHNGE&amp;#39;, &amp;#39;POSTDOCLEAVE&amp;#39;, &amp;#39;MINORITY&amp;#39;,
       &amp;#39;LTHEALTHPROB&amp;#39;]]&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="hyperparameter-tuning"&gt;Hyperparameter Tuning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;I tuned the model with &lt;code&gt;RandomizedSearchCV&lt;/code&gt; to randomly
search the search space of hyperparameter value for the combinnation of
hyperparameter that yields the best result. I am tuning n_estimators,
max_depth, min_samples_leaf, and min_samples_split with five folds
cross-validation, using accuracy as the performance metric.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Define the hyperparameter grid to search
param_grid = {
    &amp;#39;n_estimators&amp;#39;: [50, 100, 200],
    &amp;#39;max_depth&amp;#39;: [None, 10, 20, 30],
    &amp;#39;min_samples_split&amp;#39;: [2, 5, 10],
    &amp;#39;min_samples_leaf&amp;#39;: [1, 2, 4]
}

# Initialize RandomizedSearchCV with the classifier, hyperparameter grid, and cross-validation
random_search = RandomizedSearchCV(
    rf_classifier,
    param_distributions=param_grid,
    n_iter=10,  # Adjust the number of iterations as needed
    scoring=&amp;#39;accuracy&amp;#39;,
    cv=5,
    random_state=42
)

# Fit the randomized search on the selected features
random_search.fit(X_train_reduced, y_train)

# Get the best hyperparameters
best_params = random_search.best_params_
print(&amp;quot;Best Hyperparameters:&amp;quot;, best_params)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-12-10-naturepostdocBestparam.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As shown above, the best hyperparameter values are: n_estimators =
100, min_samples_split = 5, min_samples_leaf = 2, max_depth = 20. Let’s
plug the number in and evaluate the model performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="model-evaluation"&gt;Model evaluation&lt;/h2&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Initialize the RandomForestClassifier
rf_classifier_tuned = RandomForestClassifier(n_estimators = 100, min_samples_split = 5, min_samples_leaf = 2, max_depth = 20, random_state=123)

rf_classifier_tuned.fit(X_train_reduced, y_train)
rfc_tuned_predict = rf_classifier_tuned.predict(X_test_reduced)
rfc_tuned_cv_score = cross_val_score(rf_classifier_tuned, X_test_reduced, y_test, cv=5, scoring=&amp;#39;roc_auc&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn import metrics

#%% Evaluate the tuned RF

print(&amp;quot;=== Confusion Matrix ===&amp;quot;)
print(confusion_matrix(y_test, rfc_tuned_predict))
print(&amp;#39;\n&amp;#39;)

print(&amp;quot;=== Classification Report ===&amp;quot;)
print(classification_report(y_test, rfc_tuned_predict))
print(&amp;#39;\n&amp;#39;)

print(&amp;quot;=== All AUC Scores ===&amp;quot;)
print(rfc_tuned_cv_score)
print(&amp;#39;\n&amp;#39;)

print(&amp;quot;=== Mean AUC Score ===&amp;quot;)
print(&amp;quot;Mean AUC Score - Random Forest: &amp;quot;, rfc_tuned_cv_score.mean())


#Accuracy of the tuned RF: test data
print(&amp;quot;accuracy score of the test set for tuned RF&amp;quot;, rf_classifier_tuned.score(X_test_reduced, y_test))

#Root mean square error
mse_rfc_tuned = mean_squared_error(y_test, rfc_tuned_predict)
rmse_rfc_tuned = sqrt(mse_rfc_tuned)


print(&amp;#39;RMSE of tuned RF = &amp;#39;, rmse_rfc_tuned)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-12-10-naturepostdocModel_eval.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Based on five-fold cross validation, the mean AUC score is 0.74,
accuracy of the test dataset = 0.70, and RMSE = 0.54. Not ideal, but not
too bad. Let’s generate an ROC curve based on the prediction
result.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;#define metrics for tuned RFC
y_pred_proba_tuned_rf = rf_classifier_tuned.predict_proba(X_test_reduced)[::,1]
fpr_tuned_rf, tpr_tuned_rf, _ = metrics.roc_curve(y_test,  y_pred_proba_tuned_rf)

auc_tuned_rf = metrics.roc_auc_score(y_test, y_pred_proba_tuned_rf)

#create ROC curve
plt.plot(fpr_tuned_rf, tpr_tuned_rf, label=&amp;quot;Tuned AUC for RF=&amp;quot;+str(auc_tuned_rf.round(3)))

plt.legend(loc=&amp;quot;lower right&amp;quot;)

plt.ylabel(&amp;#39;True Positive Rate&amp;#39;)
plt.xlabel(&amp;#39;False Positive Rate&amp;#39;)

# displaying the title
plt.title(&amp;quot;ROC of Tuned RF&amp;quot;)

plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-12-10-naturepostdocroc_auc.png" /&gt;&lt;/p&gt;
&lt;h2 id="explaining-the-classification-results-with-xai"&gt;Explaining the
classification results with XAI&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;It is one thing to develop a classification model, but it is another
thing to try to explain it. Let’s produce feature importance plot of the
model to examine for influential predictors.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;#%% Feature importance report of the tuned RF

# Create a pd.Series of features importances
importances_rf = pd.Series(rf_classifier_tuned.feature_importances_, index = X_test_reduced.columns)

# Sort importances_rf
sorted_importance_rf = importances_rf.sort_values()

#Horizontal bar plot
plt.figure(figsize=(8, 20))

sorted_importance_rf.plot(kind=&amp;#39;barh&amp;#39;, color=&amp;#39;lightgreen&amp;#39;);
plt.xlabel(&amp;#39;Feature Importance Score&amp;#39;)
plt.ylabel(&amp;#39;Features&amp;#39;)
plt.title(&amp;quot;Visualizing Important Features&amp;quot;)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-12-10-naturepostdocfeature_importance.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;From the plot above, the four most influential predictors are as
follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;MHLEAVESCI&lt;/strong&gt;: Have you considered leaving science
because of depression, anxiety, or other mental health concerns related
to your work? (0 = No 1 = Yes).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;JOBPRSPCT&lt;/strong&gt;: How do you feel about your future job
prospects? (1 = Extremely negative 2 = Somewhat negative 3 = Neither
positive nor negative&lt;br /&gt;
4 = Somewhat positive 5 = Extremely positive)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;SATCAREER&lt;/strong&gt;: Thinking about your current postdoc,
how satisfied are you with the following? -Career advancement
opportunities. (1 = extremely dissatisfied 4 = neither satisfied nor
dissatisfied 7 = extremely satisfied).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;SATCHANGE&lt;/strong&gt;: In the past year would you say your
level of satisfaction has… (1 = Significantly worsened 2 = Worsened a
little 3 = Stayed the same 4 = Improved a little 5 = Significantly
improved).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Let’s try explaining it further with &lt;a
href="https://dalex.drwhy.ai/"&gt;DALEX&lt;/a&gt;: moDel Agnostic Language for
Exploration and eXplanation (&lt;a
href="http://jmlr.org/papers/v22/20-1473.html"&gt;Baniecki et al.,
2021&lt;/a&gt;). We will use partial dependence plot (PDP), breakdown plot,
and SHapley Additive exPlanations (SHAP) plot to explain the
prediction.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;import dalex as dx

#we created an explainer with dalex package
exp = dx.Explainer(rf_classifier_tuned, X_test_reduced, y_test)

# PDP profile for surface and construction.year variable
RF_mprofile = exp.model_profile(variables = [&amp;quot;MHLEAVESCI&amp;quot;, &amp;quot;JOBPRSPCT&amp;quot;, &amp;quot;SATCAREER&amp;quot;, &amp;quot;SATCHANGE&amp;quot;] , type = &amp;quot;partial&amp;quot;)

# comparison for random forest and linear regression model
RF_mprofile.plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-12-10-naturepostdocpartial_dep_plot.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a
href="https://medium.com/responsibleml/basic-xai-with-dalex-part-3-partial-dependence-profile-caf8b2ad1c9d"&gt;The
PDP plot&lt;/a&gt; above shows that MHLEAVESCI has negative influence to the
prediction, meaning that individuals who respond 1 (considered leaving
science because of mental health concerns) are more likely to have the
negative outcome (not recommend their younger self to pursue a career in
scientific research).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The other three predictors show positive influence to the
prediction, meaning that the more positive the individual feels about
their job prospect, their career advancement opportunities, and their
overall level of satisfaction in the past year, the more likely for them
to have the positive outcome (recommend their younger self to pursue a
career in scientific research).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can further explore the prediction in instances of positive
and negative cases. Positive case means post doctoral researchers who
answered 1 in their targeted variable (recommend their younger self to
pursue a career in scientific research) while the negative case answered
the targeted variable otherwise. We will use &lt;a
href="https://medium.com/responsibleml/basic-xai-with-dalex-part-4-break-down-method-2cd4de43abdd"&gt;breakdown
plot&lt;/a&gt; and &lt;a
href="https://medium.com/responsibleml/basic-xai-with-dalex-part-5-shapley-values-85ceb4b58c99"&gt;SHAP
plot&lt;/a&gt; to explore the prediction of each individual case.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3
id="negative-case-not-recommend-younger-self-for-research-career"&gt;Negative
case: Not recommend younger self for research career&lt;/h3&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Break Down for apartment observation
rf_pparts = exp.predict_parts(new_observation = X_test_reduced.iloc[202], type = &amp;quot;break_down&amp;quot;)

# plot Break Down
rf_pparts.plot()&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Case No: 1176. This person regrets going into postdoc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-12-10-naturepostdocnegative_breakdown.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can see that factors such as SATSOBSECURITY (Thinking about your
current postdoc, how satisfied are you with the following? -Job
security) and POSTDOCEXPCT (How does being a postdoc meet your original
expectations?) pushed the prediction to the positive side. However,
other predictors were dragging the prediction down. Let’s explore it
further with the SHAP plot.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Break Down for apartment observation
rf_pparts_shap = exp.predict_parts(new_observation = X_test_reduced.iloc[202], type = &amp;quot;shap&amp;quot;)

# plot Break Down
rf_pparts_shap.plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-12-10-naturepostdocnegative_shap.png" /&gt;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;# lime explanation
rf_pparts_lime = exp.predict_surrogate(new_observation = X_test_reduced.iloc[202])

# plot LIME
rf_pparts_lime.show_in_notebook()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-12-10-naturepostdocnegative_lime.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This person considered leaving science because of mental health
concerns (MHLEAVESCI == 1), as well as considering the overall level of
satisfaction in the past year as significantly worsened. Other factors
such as feeling about job prospect (JOBPRSPCT) and career satisfaction
(SATCAREER) also contributed negatively to the prediction outcome as
well.&lt;/li&gt;
&lt;li&gt;To investigate further into the case, there is a question that asked
the respondent to reflect on themselves about their postdoctoral
researcher career (With the benefit of hindsight, what one thing do you
know now which you wish you’d known about before starting work as a when
you started postdoc?). This is their response:
&lt;ul&gt;
&lt;li&gt;“I wish I’d known that I will, in any case, need to basically invent
my own career path. Creativity, entrepreneurship, research, writing.
That I’m not satisfied being an employee and that academic jobs
basically don’t exist. I might have advised myself to make a different,
more self-directed plan”&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Now that we have learned about a negative case. Let’s explore a
positive case.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3
id="positive-case-still-recommend-younger-self-for-research-career"&gt;Positive
case: Still recommend younger self for research career&lt;/h3&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Break Down for apartment observation
rf_pparts = exp.predict_parts(new_observation = X_test_reduced.iloc[6], type = &amp;quot;break_down&amp;quot;)

# plot Break Down
rf_pparts.plot()&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Case No: 219. This person does not regret going into postdoc&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-12-10-naturepostdocpositive_breakdown.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can see that factors such as MHLEAVESCI (considered leaving
science because of mental health concerns) and SATCAREER (Satisfaction
about career advancement) pushed the prediction to the positive side
despite having some other predictors dragging the outcome variable
down.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Break Down for apartment observation
rf_pparts_shap = exp.predict_parts(new_observation = X_test_reduced.iloc[6], type = &amp;quot;shap&amp;quot;)

# plot Break Down
rf_pparts_shap.plot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-12-10-naturepostdocpositive_shap.png" /&gt;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;# lime explanation
rf_pparts_lime = exp.predict_surrogate(new_observation = X_test_reduced.iloc[6])

# plot LIME
rf_pparts_lime.show_in_notebook()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-12-10-naturepostdocpositive_lime.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The SHAP plot reveals that variables that negatively influence the
prediction are SATSALARY (satisfaction with one’s salary/compensation)
and JOBPRSPCT (feeling about future job prospect). However, there is a
lot of predictors that push the prediction to the positive side. Some of
them are satisfaction on work-life-balance (SATWLB) and satisfaction on
one’s total work hour (SATWKHOURS). This is their reflection on their
postdoc career:
&lt;ul&gt;
&lt;li&gt;“Amount of work will not correspond to new findings - so you need to
learn to choose which ideas to pursue.”&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The reflection on this case seems to be more neutral compared to
that of the negative case. If we explore further into this variable, we
may be able to learn something. We will use NLP to explore themes and
topics of respondents’ reflection on their postdoctoral researcher
career.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="examining-written-response-with-nlp"&gt;Examining written response
with NLP&lt;/h1&gt;
&lt;h2 id="exploring-textual-data"&gt;Exploring textual data&lt;/h2&gt;
&lt;pre class="python"&gt;&lt;code&gt;df_text[&amp;#39;REFLECT&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-12-10-naturepostdocdf_text_reflect.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We already extracted the variable earlier in the post. Not everybody
wrote a reflection. The missing response is coded as &lt;em&gt;NaN&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="wordcloud"&gt;Wordcloud&lt;/h2&gt;
&lt;pre class="python"&gt;&lt;code&gt;from wordcloud import WordCloud, STOPWORDS

# Custom stopwords
custom_stopwords = [&amp;quot;post doc&amp;quot;, &amp;quot;doc&amp;quot;, &amp;quot;nan&amp;quot;, &amp;quot;don&amp;quot;, &amp;quot;phd&amp;quot;, &amp;quot;postdoc&amp;quot;]

# Concatenate all text data in the &amp;#39;REFLECT&amp;#39; column
text_data = &amp;#39; &amp;#39;.join(df_reflect[&amp;#39;REFLECT&amp;#39;])

# Remove English stopwords and custom stopwords
stopwords = set(STOPWORDS)
stopwords.update(custom_stopwords)

# Create a WordCloud object with specified stopwords
wordcloud = WordCloud(width=800, height=400, background_color=&amp;#39;white&amp;#39;, stopwords=stopwords).generate(text_data)

# Display the word cloud using matplotlib
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation=&amp;#39;bilinear&amp;#39;)
plt.axis(&amp;#39;off&amp;#39;)  # Turn off axis numbers and ticks
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-12-10-naturepostdoccloud_overall.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Here is the frequency-based visualization of word cloud. Some of the
prominent words are “Research”, “work”, “time”. Let’s explore it further
with Term Frequency - Inverse Document Frequency (TF-IDF), which
measures word importance based on its relative frequency.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="common-word"&gt;Common word&lt;/h2&gt;
&lt;pre class="python"&gt;&lt;code&gt;def plot_10_most_common_words(count_data, tfidf_vectorizer):
    import matplotlib.pyplot as plt
    words = tfidf_vectorizer.get_feature_names()
    total_counts = np.zeros(len(words))
    for t in count_data:
        total_counts += t.toarray()[0]
    
    count_dict = (zip(words, total_counts))
    count_dict = sorted(count_dict, key=lambda x: x[1], reverse=True)[0:10]
    words = [w[0] for w in count_dict]
    counts = [w[1] for w in count_dict]
    x_pos = np.arange(len(words)) 

    plt.bar(x_pos, counts, align=&amp;#39;center&amp;#39;)
    plt.xticks(x_pos, words, rotation=90) 
    plt.xlabel(&amp;#39;Words&amp;#39;)
    plt.ylabel(&amp;#39;Counts&amp;#39;)
    plt.title(&amp;#39;10 Most Common Words&amp;#39;)
    plt.show()
    
# Custom stopwords, including your own stopwords
custom_stopwords = set([&amp;quot;post doc&amp;quot;, &amp;quot;doc&amp;quot;, &amp;quot;nan&amp;quot;, &amp;quot;don&amp;quot;, &amp;quot;phd&amp;quot;, &amp;quot;postdoc&amp;quot;])

# Combine standard English stopwords with custom stopwords
stopwords = set(text.ENGLISH_STOP_WORDS).union(custom_stopwords)

# Handle NaN values by filling them with an empty string
df_text[&amp;#39;REFLECT&amp;#39;] = df_text[&amp;#39;REFLECT&amp;#39;].fillna(&amp;#39;&amp;#39;)

# Initialize the count vectorizer with the English stop words
tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords)

# Fit and transform the processed titles
count_data = tfidf_vectorizer.fit_transform(df_text[&amp;#39;REFLECT&amp;#39;])

# Visualize the 10 most common words
plot_10_most_common_words(count_data, tfidf_vectorizer)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-12-10-naturepostdoc10_most_common_words.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;These are the 10 most common words from the respondents’ reflection.
Postdoc research position is a job, so it is not surprising to see words
such as “work”, “academic”, and “lab”. I want to see what the topic will
be like if I divided the dataset by positive and negative cases. Next, I
will use Latent Dirichlet allocation (LDA) topic modeling to extract
themes from respondents’ reflection.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="topic-modeling"&gt;Topic modeling&lt;/h2&gt;
&lt;h3
id="negative-case-not-recommend-younger-self-for-research-career-1"&gt;Negative
case: Not recommend younger self for research career&lt;/h3&gt;
&lt;pre class="python"&gt;&lt;code&gt;from sklearn.feature_extraction import text
import pyLDAvis
import pyLDAvis.sklearn
pyLDAvis.enable_notebook()

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation

custom_stopwords = [&amp;quot;post doc&amp;quot;, &amp;quot;doc&amp;quot;, &amp;quot;nan&amp;quot;, &amp;quot;don&amp;quot;, &amp;quot;phd&amp;quot;, &amp;quot;postdoc&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Custom stopwords, including your own stopwords
custom_stopwords = set([&amp;quot;post doc&amp;quot;, &amp;quot;doc&amp;quot;, &amp;quot;nan&amp;quot;, &amp;quot;don&amp;quot;, &amp;quot;phd&amp;quot;])

# Combine standard English stopwords with custom stopwords
stopwords = set(text.ENGLISH_STOP_WORDS).union(custom_stopwords)

tf_vectorizer = CountVectorizer(strip_accents=&amp;#39;unicode&amp;#39;,
                                stop_words=stopwords,
                                lowercase=True,
                                token_pattern=r&amp;#39;\b[a-zA-Z]{3,}\b&amp;#39;,
                                ngram_range=(1, 2),
                                max_df=0.5,
                                min_df=10)

dtm_tf = tf_vectorizer.fit_transform(df_text[&amp;#39;REFLECT&amp;#39;].values.astype(&amp;#39;U&amp;#39;))

tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())
dtm_tfidf = tfidf_vectorizer.fit_transform(df_text[&amp;#39;REFLECT&amp;#39;].values.astype(&amp;#39;U&amp;#39;))

# for TF DTM
lda_tf = LatentDirichletAllocation(n_components=5, random_state=0)
lda_tf.fit(dtm_tf)

# for TFIDF DTM
lda_tfidf = LatentDirichletAllocation(n_components=5, random_state=0)
lda_tfidf.fit(dtm_tfidf)

pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-12-10-naturepostdoclda_regret0.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;From visual inspection,there are two topics that seem to be close.
Other topics seem distinct on their own. Let’s extract the word from the
LDA results.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;for i,topic in enumerate(lda_tf.components_):
    print(f&amp;#39;Top 10 words for topic #{i}:&amp;#39;)
    print([tf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])
    print(&amp;#39;\n&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-12-10-naturepostdoclda_regret0_words.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Most topics are about work in academia and research, with topic 0
and 2 that are different. Topic 0 seems to be about concerns for
resources such as time and funding. Topic 2 seems to be about financial
matter such as cost of living and salary. Some excerpts from
respondents’ reflection are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;“Mostly, I would advise my past self to choose a different PI and
lab for my postdoc. Beyond that, I wish had known about the cost of
doing a postdoc in terms of lost income potential relative to other
careers, and the reality of how few academic jobs there are in my
field.”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“High degree of competitiveness, low salaries, strenuous hours,
publish or perish, excessive stress, no psychological support, salary
relationship with respect to the degree of studies/specialization with
respect to other jobs”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“Wish I was aware of lack of support system for Postdocs. Beyond
publishing our work, there is no other way of measuring ones caliber in
science. It also depends of how well known the PI is. I also wish I had
a better insight into how low the salary and other benefits are. Above
all I wish I had known the difficulty and lack of available positions in
academia to be pursued.”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“I wish that they were teaching us more that working in academia
means never having job security. I wish that I knew that there is not
enough jobs and that only way to land a permanent job in academia is to
become a professor.”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“Science is not valued as it should be. Job prospects and
opportunities have gotten worse. And most importantly the salary as post
doc doesn’t help you make the ends meet at all. Post doc and academia
only work for those who have lots of savings and strong financial
ability to be underpaid for years of post doc until they build up their
resume and publish sufficient amount of papers without having to think
about financial problems.”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“There is a wealth of research possible in industry, that pays
better, is organized better, and allows you to do MORE research, less
beauracracy and teaching, working with real professionals, is goal
oriented, not article oriented, and allows you to maintain a work life
balance, have time for a partner, family, a house, and
retirement.”&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3
id="positive-case-still-recommend-younger-self-for-research-career-1"&gt;Positive
case: Still recommend younger self for research career&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-12-10-naturepostdoclda_regret1.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Visual inspection of reflections from positive respondent cases show
similar results to those from negative cases. There are five topics, but
two of them (3 and 1) seem to considerably overlap.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-12-10-naturepostdoclda_regret1_words.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Wordings of the topics seem to be similar to the negative case
results as well. However, topic 4 is different from results from
negative cases as it discusses about balancing the task and
responsibility (e.g., work, project, writing). This is not shown from
the negative case dataset. Some excerpts from respondents’ reflection
are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;“Personal enthusiasm and expectations for academic research will
decline rapidly at a certain time, and good habits need to be developed
to resist the frustration brought about by the decline in enthusiasm and
physical strength; to exercise writing skills; to balance family and
research, and to Research can always help yourself, but family doesn’t
necessarily”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“Although I would still pursue a career in science, I would
definitely not waste the years I have spent in my postdoc and rather
head straight for industry job after my PhD. I am already miles behind
in terms of salary and growth compared to my peers who went straight to
industry after their degrees.”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“Having a better saving culture, work/family time balance, and
having participated more in financing processes or acquiring
scholarships, etc.”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“Everyone is going to be working just as hard as you, the best
you can do is to do good science, follow your interests, and pursue
every potential opportunity for funding and development.”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“Publishing in high impact journals is not as awarding as having
a great life-work balance. Reach out and accept help and initiate
collaborations.”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“You will have way less time for bench research because you will
be writing fellowships/grants, spending time for committee service, and
mentoring compared to when in grad school.”&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="concluding-remark"&gt;Concluding remark&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In concluding, the data analysis methods discussed in this post,
while not novel (as I’ve previously posted about them), reveal an
intriguing aspect—how their combination can offer insights into a
subject. The findings consistently align with existing literature,
emphasizing the pivotal role of mental health in individual resilience
in academia.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Navigating a postdoctoral research role naturally prompts
considerations of job prospects, salary, and work-life balance, all
crucial factors influencing one’s professional satisfaction. In
academia, where competition is constant, elements such as maintaining
work-life balance, coping with the pressure to publish, and confronting
job insecurity can significantly impact mental well-being.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This analysis underscores the importance of prioritizing personal
well-being over professional pursuits when faced with such choices.
Although academia boasts merits, uncertainties regarding financial
stability, job security, and work-life balance still exist. Choosing a
career in academia demands thoughtful consideration of these
complexities. Anyway, thank you so much for reading!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>67bf1b7a6e09037f24ce8fb3d4fdd4ab</distill:md5>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <category>Unsupervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2023-12-10-naturepostdoc</guid>
      <pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2023-12-10-naturepostdoc/Thumbnail cloud.png" medium="image" type="image/png" width="790" height="405"/>
    </item>
    <item>
      <title>Examining Differences among Psychological Networks with Network Tree</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2023-12-08-networktree</link>
      <description>


&lt;h1 id="setting-up-and-introducing-the-dataset"&gt;Setting up and
introducing the dataset&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Hi Everyone. It’s been awhile since my last blog post. Today I want
to write about a network psychometrics analysis method named “&lt;a
href="https://www.zeileis.org/news/networktree100/"&gt;Networktree&lt;/a&gt;”.
This method is a way of visualizing differences between psychological
networks between population with different characteristics.&lt;/li&gt;
&lt;li&gt;Have you ever wondered how to measure the connections between
different items in a survey? For example, how does a question asking
about sadness relate to a question that asks about whether you have
laughed today? One way to find out is to use psychometric networks,
which are graphical models that show the strength and direction of the
associations between variables.&lt;/li&gt;
&lt;li&gt;But what if these associations are not the same for everyone? What
if some factors, such as age, gender, or education, make a difference in
how variables are related? This is where network trees come in. Network
tree is a new method that can help you discover subgroups in your data
that have different structure of psychological networks. By splitting
your data based on covariates, such as demographic or clinical
variables, network trees can reveal how the structure of the network
changes across different groups. This can help you identify sources of
heterogeneity and tailor your interventions accordingly.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#read a csv file
df &amp;lt;- read.csv(&amp;quot;riasec.csv&amp;quot;, header = TRUE)

library(networktree)
library(bootnet)
library(tidyverse)

df_subsetted &amp;lt;- df[, c(1:48, 78, 81, 84)]

# save(df_subsetted, cor_RIASEC,
#      tree1, tree2, tree3, 
#      compare_tree, subtract_tree, 
#      net_engnat1, net_engnat2,
#      file = &amp;quot;riasec_tree.RData&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;In this post, we will explore a dataset that measures different
kinds of vocational interest. This dataset is based on the RIASEC model,
which was created by &lt;a
href="https://doi.org/10.1016/j.jvb.2007.12.002"&gt;Liao, Armstrong, and
Rounds (2008)&lt;/a&gt; to provide an alternative to the commercialized
Holland RIASEC test for public research and development. RIASEC stands
for realistic, investigative, artistic, social, enterprising and
conventional. These are six categories of vocational interest that can
help people find careers that match their personality and preferences.
For example, someone who scores high on the realistic type might enjoy
working with tools, machines or animals, while someone who scores high
on the artistic type might enjoy creative activities such as writing,
painting or music.&lt;/li&gt;
&lt;li&gt;Before we explore how the data set is organized, we should take a
look at its content. This will help us understand what kind of
information it contains and how it can be useful for our analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;head(df_subsetted)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  R1 R2 R3 R4 R5 R6 R7 R8 I1 I2 I3 I4 I5 I6 I7 I8 A1 A2 A3 A4 A5 A6
1  3  4  3  1  1  4  1  3  5  5  4  3  4  5  4  3  5  4  1  2  4  5
2  1  1  2  4  1  2  2  1  5  5  5  4  4  4  4  4  2  1  4  2  1  3
3  2  1  1  1  1  1  1  1  4  1  1  1  1  1  1  1  1  1  1  2  1  1
4  3  1  1  2  2  2  2  2  4  1  2  4  3  2  3  2  5  2  4  5  1  4
5  4  1  1  2  1  1  1  2  5  5  5  3  5  5  5  3  3  5  5  5  5  5
6  3  5  1  3  1  5  3  4  4  5  4  4  4  3  3  5  5  5  4  5  5  5
  A7 A8 S1 S2 S3 S4 S5 S6 S7 S8 E1 E2 E3 E4 E5 E6 E7 E8 C1 C2 C3 C4
1  2  4  3  5  5  4  5  5  5  5  2  1  4  1  2  2  1  3  1  3  1  1
2  4  2  2  3  4  3  4  2  3  1  1  1  1  1  1  1  1  3  1  1  2  1
3  3  1  3  1  5  3  5  5  4  4  1  3  3  5  1  4  4  3  1  3  2  2
4  4  2  4  2  3  3  2  1  3  2  5  4  3  2  3  3  2  3  3  2  3  3
5  1  5  5  4  4  4  5  5  5  5  2  3  2  3  2  4  2  2  4  2  2  4
6  3  5  3  5  5  4  5  4  4  4  3  1  1  1  2  1  1  3  3  2  1  1
  C5 C6 C7 C8 education engnat religion
1  1  3  1  1         2      1        7
2  1  2  1  1         2      1        7
3  1  2  4  1         2      1        7
4  2  2  2  2         1      2        0
5  5  5  2  2         3      2        4
6  3  3  1  3         3      2        2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;var_name &amp;lt;- colnames(df_subsetted)

cor_RIASEC &amp;lt;- cor(df_subsetted[,var_name])

riasec_dim &amp;lt;- list(R = 1:8,
                I = 9:16,
                A = 17:24,
                S = 25:32,
                E = 33:40,
                C = 41:48)

q1 &amp;lt;- qgraph::qgraph(cor_RIASEC, layout = &amp;quot;spring&amp;quot;, labels = var_name, groups = riasec_dim,
                     color=c(&amp;quot;#d73027&amp;quot;, &amp;quot;#fc8d59&amp;quot;, &amp;quot;#fee090&amp;quot;, &amp;quot;#eddfea&amp;quot;, &amp;quot;#91bfdb&amp;quot;, &amp;quot;#7FFFD4&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;div class="float"&gt;
&lt;img src="https://taridwong.github.io//posts/2023-12-08-networktreeriasec_network.png"
alt="Network structure of the RIASEC dataset" /&gt;
&lt;div class="figcaption"&gt;Network structure of the RIASEC dataset&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;This is the network structure of the RIASEC dataset. It has six
dimensions, represented by six clusters of items. There are also three
categorical indicators of native English status (engnat), religion, and
educational status.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;qgraph::centralityPlot(q1, include =&amp;quot;all&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="float"&gt;
&lt;img src="https://taridwong.github.io//posts/2023-12-08-networktreeriasec_centrality_overall.png"
alt="Centrality indices of the overall RIASEC network structure" /&gt;
&lt;div class="figcaption"&gt;Centrality indices of the overall RIASEC network
structure&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;These are centrality indices of the overall RIASEC structure. They
are numerical values that describe how important or influential a node
is in a network. There are different ways to calculate centrality
indices: strength, closeness, betweenness, and expected influence (&lt;a
href="10.1037/abn0000446"&gt;Bringmann et al., 2019&lt;/a&gt;; &lt;a
href="10.1037/abn0000181"&gt;Robinaugh et al., 2016&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Strength is the sum of the weights of the edges connected to a node.
It indicates how much a node is directly connected to other nodes in the
network. For example, if a node has a strength value of 10, it means
that it has 10 units of connection with other nodes. A high strength
value can imply that a node has a dominant or prominent role in the
network.&lt;/li&gt;
&lt;li&gt;Closeness is the inverse of the average distance from a node to all
other nodes in the network. It measures how easily a node can reach
other nodes through the shortest paths. For example, if a node has a
closeness value of 0.5, it means that the average distance from it to
any other node is 2 steps. A high closeness value can indicate that a
node has a central or accessible position in the network.&lt;/li&gt;
&lt;li&gt;Betweenness is the number of shortest paths between any two nodes in
the network that pass through a given node. It reflects how often a node
acts as a bridge or a mediator between other nodes. For example, if a
node has a betweenness value of 20, it means that 20 pairs of nodes
depend on it to communicate with each other. A high betweenness value
can suggest that a node has a strategic or influential role in the
network.&lt;/li&gt;
&lt;li&gt;Expected influence is the product of the strength and the closeness
of a node. It represents how much impact a node can have on other nodes
in the network. For example, if a node has an expected influence value
of 5, it means that it can reach 5 units of connection with other nodes
in one step. A high expected influence value can imply that a node has a
powerful or persuasive role in the network.&lt;/li&gt;
&lt;li&gt;By using these centrality indices, we can analyze the RIASEC
structure and see which interest types are more central or peripheral in
the network. This can help us understand how different interest types
relate to each other and how they influence vocational choices and
outcomes. For example, we can compare the centrality indices of
Realistic and Artistic nodes and see how they differ in their network
positions and roles.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="introducing-networktree"&gt;Introducing Networktree&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;The network tree method is developed by &lt;a
href="https://doi.org/10.1007/s11336-020-09731-4"&gt;Jones et
al. (2020)&lt;/a&gt;, using recursive partitioning to reveal significant
differences among psychological network between subgroups of the
population such as religion, educational status, and race.&lt;/li&gt;
&lt;li&gt;Results from this method can be used to identify relationships
between items (or nodes in network term) are different in psychological
networks among respondents of diverse characteristics. This way, we can
know which part of the assessment can be improved to make it stable
across subgroups.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="network-tree-between-educational-level"&gt;Network tree between
educational level&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;tree2 &amp;lt;- networktree(R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + I1 + I2 + I3 + I4 + I5 + 
    I6 + I7 + I8 + A1 + A2 + A3 + A4 + A5 + A6 + A7 + A8 + S1 + 
    S2 + S3 + S4 + S5 + S6 + S7 + S8 + E1 + E2 + E3 + E4 + E5 + 
    E6 + E7 + E8 + C1 + C2 + C3 + C4 + C5 + C6 + C7 + C8 ~ education,
                         data = df_subsetted)

plot(tree2, labels = var_name, layout = &amp;quot;spring&amp;quot;, groups = riasec_dim,
                     color=c(&amp;quot;#d73027&amp;quot;, &amp;quot;#fc8d59&amp;quot;, &amp;quot;#fee090&amp;quot;, &amp;quot;#eddfea&amp;quot;, &amp;quot;#91bfdb&amp;quot;, &amp;quot;#7FFFD4&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;div class="float"&gt;
&lt;img src="https://taridwong.github.io//posts/2023-12-08-networktreetree2.png" alt="Network tree X educational level" /&gt;
&lt;div class="figcaption"&gt;Network tree X educational level&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;We can start by investigating network of the dataset as divided by
educational level. 1=Less than high school, 2=High school, 3=University
degree, 4=Graduate degree. It seems high school serves as the higher
level split (i.e., value &amp;lt;= 2 or &amp;gt; 2). Then, the network branches
further by Less than high school status and University degree
(undergraduate) status.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="network-tree-between-native-english-status"&gt;Network tree between
native English status&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;tree3 &amp;lt;- networktree(R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + I1 + I2 + I3 + I4 + I5 + 
    I6 + I7 + I8 + A1 + A2 + A3 + A4 + A5 + A6 + A7 + A8 + S1 + 
    S2 + S3 + S4 + S5 + S6 + S7 + S8 + E1 + E2 + E3 + E4 + E5 + 
    E6 + E7 + E8 + C1 + C2 + C3 + C4 + C5 + C6 + C7 + C8 ~ engnat,
                         data = df_subsetted)

plot(tree3, labels = var_name, layout = &amp;quot;spring&amp;quot;, groups = riasec_dim,
                     color=c(&amp;quot;#d73027&amp;quot;, &amp;quot;#fc8d59&amp;quot;, &amp;quot;#fee090&amp;quot;, &amp;quot;#eddfea&amp;quot;, &amp;quot;#91bfdb&amp;quot;, &amp;quot;#7FFFD4&amp;quot;))

print(tree3)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="float"&gt;
&lt;img src="https://taridwong.github.io//posts/2023-12-08-networktreetree3.png" alt="Network tree X native English status" /&gt;
&lt;div class="figcaption"&gt;Network tree X native English status&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;We can split the network by using a binary variable such as native
English status, which indicates whether English is the first language of
the participants (“Is English your native language?”, 1=Yes, 2=No). This
variable can help us examine how language proficiency affects the
network structure and dynamics.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2
id="network-tree-between-educational-level-and-native-english-status"&gt;Network
tree between educational level and native English status&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;tree1 &amp;lt;- networktree(R1 + R2 + R3 + R4 + R5 + R6 + R7 + R8 + I1 + I2 + I3 + I4 + I5 + 
    I6 + I7 + I8 + A1 + A2 + A3 + A4 + A5 + A6 + A7 + A8 + S1 + 
    S2 + S3 + S4 + S5 + S6 + S7 + S8 + E1 + E2 + E3 + E4 + E5 + 
    E6 + E7 + E8 + C1 + C2 + C3 + C4 + C5 + C6 + C7 + C8 ~ education + engnat,
                         data = df_subsetted)

plot(tree1, labels = var_name, layout = &amp;quot;spring&amp;quot;, groups = riasec_dim,
                     color=c(&amp;quot;#d73027&amp;quot;, &amp;quot;#fc8d59&amp;quot;, &amp;quot;#fee090&amp;quot;, &amp;quot;#eddfea&amp;quot;, &amp;quot;#91bfdb&amp;quot;, &amp;quot;#7FFFD4&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;div class="float"&gt;
&lt;img src="https://taridwong.github.io//posts/2023-12-08-networktreetree1.png"
alt="Network tree X educational level and native English status" /&gt;
&lt;div class="figcaption"&gt;Network tree X educational level and native
English status&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Another way to partition the network is to use multiple grouping
variables simultaneously. For example, we can split the network
according to the educational level and the native English status of the
nodes. This allows us to see how these two factors affect the network
structure and dynamics.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="understanding-the-difference-between-trees"&gt;Understanding the
difference between trees&lt;/h1&gt;
&lt;pre class="r"&gt;&lt;code&gt;compare_tree &amp;lt;- comparetree(tree_full, id1 = 2, id2 = 3, highlights = 4, plot = TRUE, plot.type = &amp;quot;compare&amp;quot;,
                            groups = riasec_dim, color=c(&amp;quot;#d73027&amp;quot;, &amp;quot;#fc8d59&amp;quot;, &amp;quot;#fee090&amp;quot;, &amp;quot;#eddfea&amp;quot;, &amp;quot;#91bfdb&amp;quot;, &amp;quot;#7FFFD4&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;compare_tree$highlights&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  node1 node2          id1        id2 (id1 - id2)
1    R2    R8  0.708318736 0.58935851   0.1189602
2    A1    A2  0.560195173 0.44242742   0.1177678
3    E2    E6  0.591883279 0.47636222   0.1155211
4    R1    S5 -0.005596371 0.09861601  -0.1042124&lt;/code&gt;&lt;/pre&gt;
&lt;div class="float"&gt;
&lt;img src="https://taridwong.github.io//posts/2023-12-08-networktreecomparetree3_additive.png" alt="Compare tree3 additive" /&gt;
&lt;div class="figcaption"&gt;Compare tree3 additive&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;We can use &lt;code&gt;compare_tree&lt;/code&gt; function to compare network
within a tree. The &lt;code&gt;compare_tree$highlights&lt;/code&gt; code displays
information of nodes with significant different between the two
trees.&lt;/li&gt;
&lt;li&gt;One way to analyze the network structure within a tree is to use the
&lt;code&gt;compare_tree&lt;/code&gt; function. This function takes a network tree
as an input and returns network structures within that tree as an
output. The &lt;code&gt;compare_tree$highlights&lt;/code&gt; code can be used to
display the information of nodes with significant different between the
two networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;subtract_tree &amp;lt;- comparetree(tree1, highlights = 4, plot = TRUE, plot.type = &amp;quot;subtract&amp;quot;, groups = riasec_dim, color=c(&amp;quot;#d73027&amp;quot;, &amp;quot;#fc8d59&amp;quot;, &amp;quot;#fee090&amp;quot;, &amp;quot;#eddfea&amp;quot;, &amp;quot;#91bfdb&amp;quot;, &amp;quot;#7FFFD4&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;subtract_tree$highlights&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  node1 node2          id1        id2 (id1 - id2)
1    R2    R8  0.708318736 0.58935851   0.1189602
2    A1    A2  0.560195173 0.44242742   0.1177678
3    E2    E6  0.591883279 0.47636222   0.1155211
4    R1    S5 -0.005596371 0.09861601  -0.1042124&lt;/code&gt;&lt;/pre&gt;
&lt;div class="float"&gt;
&lt;img src="https://taridwong.github.io//posts/2023-12-08-networktreecomparetree3_subtract.png" alt="Compare tree3 substract" /&gt;
&lt;div class="figcaption"&gt;Compare tree3 substract&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Another option for plotting the network is to set plot.type =
“subtract”, which will highlight the edges that indicate differences
between nodes in red. This can help us visually identify which node is
an outlier or dissimilar from the rest, especially for a network with a
small number of nodes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="compare-centrality-indices-between-two-subgroups"&gt;Compare
centrality indices between two subgroups&lt;/h1&gt;
&lt;pre class="r"&gt;&lt;code&gt;engnat1 &amp;lt;- df_subsetted %&amp;gt;% filter(engnat == 1)
engnat2 &amp;lt;- df_subsetted %&amp;gt;% filter(engnat == 2)

net_engnat1 &amp;lt;- bootnet::estimateNetwork(engnat1[, 1:48], default = &amp;quot;ggmModSelect&amp;quot;, verbose = FALSE)
net_engnat2 &amp;lt;- bootnet::estimateNetwork(engnat2[, 1:48], default = &amp;quot;ggmModSelect&amp;quot;, verbose = FALSE)

qgraph::centralityPlot(net_engnat1, include =&amp;quot;all&amp;quot;)
qgraph::centralityPlot(net_engnat2, include =&amp;quot;all&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="float"&gt;
&lt;img src="https://taridwong.github.io//posts/2023-12-08-networktreecentrality_engnat1.png" alt="Centrality engnat1" /&gt;
&lt;div class="figcaption"&gt;Centrality engnat1&lt;/div&gt;
&lt;/div&gt;
&lt;div class="float"&gt;
&lt;img src="https://taridwong.github.io//posts/2023-12-08-networktreecentrality_engnat2.png" alt="Centrality engnat2" /&gt;
&lt;div class="figcaption"&gt;Centrality engnat2&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;To investigate further, we can subset the dataset by characteristics
of the respondent and find differences between networks from their
centrality indices. I am going to need you to squint your eyes here.
Node R2, A1, A2 has different strengths (as examined visually) depending
on whether the respondents are native English speakers or not.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="concluding-remark"&gt;Concluding remark&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Thank you for reading this far! Network tree is a relatively novel
data analysis method that could be useful in the development of
assessments in addition to the existing psychometrics analyses such as
differential item functioning or factor analysis. I enjoy experimenting
with new ways of analyzing data, and I will share any results that I
find on this platform. I appreciate your attention!&lt;br /&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>e0a88954858ff4571319a5f64d097beb</distill:md5>
      <category>R</category>
      <category>Psychometric</category>
      <guid>https://taridwong.github.io/posts/2023-12-08-networktree</guid>
      <pubDate>Fri, 08 Dec 2023 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2023-12-08-networktree/tree3.png" medium="image" type="image/png" width="864" height="610"/>
    </item>
    <item>
      <title>Examining Big 5 Personality Inventory Data with Network Psychometrics</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2023-04-30-networkpsych</link>
      <description>


&lt;h2 id="saying-hi"&gt;Saying Hi&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Hi, everyone. We are heading into spring here in Edmonton. There is
no class to teach, so I have some more free time to work on projects
that I have on hands. I took an advance psychometric course this past
winter 2023 semester and learned a lot of useful techniques. One of them
is Network psychometrics.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Network psychometrics is a relatively novel approach to
psychometrics research that examines relationships between observed
variables (assessment items) without relying on the assumption of latent
variables (&lt;a
href="https://www.nature.com/articles/s43586-021-00055-w"&gt;Borsboom et
al., 2021&lt;/a&gt;). Items that are related to each other may appear closer
while items that are less relevant may be positioned further on a
network graph. This approach offers an alternative evidence to validity
of the interpretations and uses of a test in addition to the traditional
factor analysis method (&lt;a
href="https://link.springer.com/10.3758/s13428-020-01500-6"&gt;Christensen
&amp;amp; Golino, 2021&lt;/a&gt;).&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;In this post, I used response data to the Big 5 personality
inventory from the &lt;a
href="http://openpsychometrics.org/_rawdata/"&gt;open-source psychometrics
project&lt;/a&gt;. Items from the test were put together from the
international personality item pool. The data set has N = 19,719
responses to 50 Likert-scale items asking of respondents’ agreement to
the presented statements such as “I don’t talk a lot”, “I shirk my
duties”, or “I am quick to understand things”. Responses were coded as 1
to, where 1=Disagree, 3=Neutral, 5=Agree. Five personality traits of
extraversion, neuroticism, agreeableness, conscientiousness, and
openness to experience are reported as results.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;setwd(&amp;quot;D:/Program/Private_project/R_private/Network&amp;quot;)

load(&amp;quot;r_obj_big5.RData&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We will start by loading packages that we will use.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(&amp;quot;dplyr&amp;quot;)
# for network modeling 
library(&amp;quot;qgraph&amp;quot;)
library(&amp;quot;psychonetrics&amp;quot;)
library(&amp;quot;bootnet&amp;quot;)  
library(&amp;quot;EGAnet&amp;quot;)  &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Next, we will extract response data from the whole data set, as well
as group items into their respective dimensions.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;df_response &amp;lt;- df[, 8:57]

groups &amp;lt;- list(Extraversion = 1:10,
               Neuroticism = 11:20,
               Agreeableness = 21:30,
               Conscientiousness = 31:40,
               Openness = 41:50)

obsvars &amp;lt;- colnames(df_response)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="network-psychometrics"&gt;Network Psychometrics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To construct a psychometrics network, we will use the
&lt;code&gt;estimateNetwork&lt;/code&gt; function from &lt;code&gt;bootnet&lt;/code&gt; package.
Relationships between items will be determined by polychoric
correlations, and the network graph was estimated with the graphical
least absolute shrinkage and selection operator (Glasso) estimator.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;network_big5 &amp;lt;- bootnet::estimateNetwork(
  data = df_response,
  # Alternatively, &amp;quot;cov&amp;quot; for covariances, &amp;quot;cor&amp;quot; for correlations 
  corMethod = &amp;quot;cor_auto&amp;quot;, # for polychoric and polyserial correlations
  # Alternatively, &amp;quot;ggmModSelect&amp;quot; for an unregularized GGM using glasso
  default = &amp;quot;EBICglasso&amp;quot;, # for estimating GGM with gLASSO and EBIC
  tuning = 0.5 # EBIC tuning parameter; set to zero for BIC model selection. If you make it large, you should justify it.
)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Below is th initial network of the Big 5 personality data set that
we used. Items of the same dimensions are grouped together, meaning that
they measure similar construct to each other. Note that edges (or
linkages) between items can be adjusted. We can remove (or prune)
statistically insignificant edges and add edges that improve model
fit.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(network_big5, 
     layout = &amp;quot;spring&amp;quot;, 
     palette = &amp;quot;colorblind&amp;quot;, 
     groups = groups, # to label each group
     font = 2,
     label.cex = 1
     ) &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We will use the &lt;code&gt;prune&lt;/code&gt; function from
&lt;code&gt;psychonetrics&lt;/code&gt; package to prune edges that do not achieve
statistical significance at alpha 0.05 (step down method). Then, we will
add more edges until model fit of the data set based on Bayesian
Information Criterion (BIC) does not improve anymore based on alpha =
0.05 with the &lt;code&gt;stepup&lt;/code&gt; function (step up method).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;network_big5_optimized &amp;lt;- psychonetrics::ggm(df_response, 
                                     vars = obsvars) %&amp;gt;%
  psychonetrics::runmodel() %&amp;gt;%
  psychonetrics::prune(adjust = &amp;quot;fdr&amp;quot;, alpha = 0.05) %&amp;gt;%
  # To automatically add edges at  alpha=0.05 until BIC is no longer be improved
  psychonetrics::stepup(criterion = &amp;quot;bic&amp;quot;, alpha = 0.05) %&amp;gt;%
  psychonetrics::modelsearch()&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can check fit indices of the optimized network model and its
network graph with the code below. The network is a lot more complex
with more edges added to the graph. The items still stay with their
peers in their respective domain, meaning that structure of the test
still holds after the optimization.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Look at the model fit
network_big5_optimized %&amp;gt;% psychonetrics::fit()

# Obtain the network plot
net_optimized &amp;lt;- psychonetrics::getmatrix(network_big5_optimized, &amp;quot;omega&amp;quot;)

qgraph::qgraph(net_optimized, 
               layout = &amp;quot;spring&amp;quot;, 
               theme = &amp;quot;colorblind&amp;quot;,
               labels = obsvars,
               groups = groups,
               negDashed = T, # should negative edges be dashed?
               font = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;h2
id="exploratory-graph-analysis-for-dimensional-stability"&gt;Exploratory
Graph Analysis for Dimensional Stability&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Aside from investigating the network structure, we can also use
the network approach to examine dimensional stability of the test with
the exploratory graph analysis (EGA) method. EGA tests dimensional
stability of a test by examining its structure across several resampling
iterations. In other words, EGA checks if structure of the test is
similar across different response patterns.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will use the &lt;code&gt;bootEGA&lt;/code&gt; function from the
&lt;code&gt;EGAnet&lt;/code&gt; package to perform EGA with the big 5 personality
inventory data. Usually 500 resampling iterations is recommended, but we
will stick to 100 to make it computationally feasible.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;bootEGA_big5 &amp;lt;- EGAnet::bootEGA(
  # we could also provide the cor matrix but then
  # n (i.e., number of rows) must also be specified
  data = df_response, 
  cor = &amp;quot;cor_auto&amp;quot;,
  uni.method = &amp;quot;louvain&amp;quot;,
  iter = 100, # Number of replica samples to generate
  # resampling&amp;quot; for n random subsamples of the original data
  # parametric&amp;quot; for n synthetic samples from multivariate normal dist.
  type = &amp;quot;parametric&amp;quot;, 
  # EGA Uses standard exploratory graph analysis
  # EGA.fit Uses total entropy fit index (tefi) to determine best fit of EGA
  # hierEGA Uses hierarchical exploratory graph analysis
  EGA.type = &amp;quot;EGA&amp;quot;, 
  model = &amp;quot;glasso&amp;quot;, 
  algorithm = &amp;quot;walktrap&amp;quot;, # or &amp;quot;louvain&amp;quot; (better for unidimensional structures)
  # use &amp;quot;highest_modularity&amp;quot;, &amp;quot;most_common&amp;quot;, or &amp;quot;lowest_tefi&amp;quot;
  consensus.method = &amp;quot;highest_modularity&amp;quot;, 
  typicalStructure = TRUE, # typical network of partial correlations
  plot.typicalStructure = TRUE, # returns a plot of the typical network
  ncores = 4, # Number of cores to use in computing results
  progress = FALSE ,
  summary.table = TRUE
)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The network below shows that structure of our data set holds despite
being tested on different data sets across 100 resampling iterations.
This means that the 5 dimensions of our test is quite stable.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="float"&gt;
&lt;img src="https://taridwong.github.io//posts/2023-04-30-networkpsychega.png" alt="Exploratory Graph Analysis Result" /&gt;
&lt;div class="figcaption"&gt;Exploratory Graph Analysis Result&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;We can also request for written results of EGA. All items are loaded
onto their respective dimensions. For example, all 10 extraversion items
are loaded onto the 5th dimension. The same applies to the remaining 40
items as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# View the number of communities
bootEGA_big5$EGA
bootEGA_big5$typicalGraph$typical.dim.variables&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can also check if all items are stable across 100 iterations. The
empirical EGA Communities plot below indicates that all items and
dimensions have value = 1 across all iterations, meaning that the items
are loaded into the same test community (aka measuring the same
construct) across different response patterns.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Dimension (i.e., structural) stability results
dim_big5 &amp;lt;- EGAnet::dimensionStability(bootEGA_big5)
dim_big5$dimension.stability

# Item stability results
dim_big5$item.stability&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="measurement-invariance-with-network-model"&gt;Measurement
Invariance with Network Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We can also test whether structure of the test is the same or
similar across subgroup of populations such as age groups, gender, and
race with the measurement invariance analysis. This technique allows us
to check whether our test is consistent across different
populations.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We begin by setting up our model parameter first. We will create
a matrix called ‘Lambda’ and load all 50 items into their respective
dimensions.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#paste(1:10, collapse = &amp;quot;, &amp;quot;)

Lambda &amp;lt;- matrix(0, 50, 5)
Lambda[c(1,2,3,4,5,6,7,8,9,10)] &amp;lt;- 1 # first factor (E)
Lambda[c(11,12,13,14,15,16,17,18,19,20), 2] &amp;lt;- 1 # second factor (N)
Lambda[c(21,22,23,24,25,26,27,28,29,30), 3] &amp;lt;- 1 # second factor (A)
Lambda[c(31,32,33,34,35,36,37,38,39,40), 4] &amp;lt;- 1 # second factor (C)
Lambda[c(41,42,43,44,45,46,47,48,49,50), 5] &amp;lt;- 1 # second factor (O)

latents &amp;lt;- c(&amp;quot;E&amp;quot;, &amp;quot;N&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;O&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Then, we will recode the gender variable into factor, as well as
removing missing data (coded 0) and gender ‘other’ (coded 3) because
there is not enough sample size (less than 100).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#recode gender

df_mi &amp;lt;- subset(df, gender != 3 &amp;amp; gender != 0)

df_mi$gender &amp;lt;-as.factor  (df_mi$gender)

levels(df_mi$gender) &amp;lt;- c(&amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We will then establish our network models based on subgroups with
the &lt;code&gt;lvm&lt;/code&gt; function in &lt;code&gt;psychonetrics&lt;/code&gt;
package.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Configural model with free residuals across groups
mod_configural &amp;lt;- psychonetrics::lvm(data = df_mi, # data
                                     lambda = Lambda, # factor structure
                                     vars = obsvars, # items to be analyzed
                                     latents = latents, # factor names
                                     identification = &amp;quot;variance&amp;quot;, # Fix variance to 1
                                     groups =  &amp;quot;gender&amp;quot;, # group variable
                                     # if &amp;quot;full&amp;quot;, it frees the residual variance-covariance matrix
                                     # Default: &amp;quot;empty&amp;quot; --&amp;gt; fixing them to zero
                                     omega_epsilon = &amp;quot;full&amp;quot;,
                                     residual = &amp;quot;ggm&amp;quot;) %&amp;gt;%
  psychonetrics::runmodel()&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Then, we can visualize the network models based on gender male and
female. We can see that the structure of both models are similar, but
edges between items may be different. For example, an edge between item
A7 to A9 in female subgroup is more prominent than male subgroup. This
may indicate differences how the two items work between respondents of
different gender.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Get the residual/error matrix
net_male &amp;lt;- mod_configural %&amp;gt;% 
  psychonetrics::getmatrix(&amp;quot;omega_epsilon&amp;quot;) %&amp;gt;%
  .$&amp;quot;male&amp;quot;

net_female &amp;lt;- mod_configural %&amp;gt;% 
  psychonetrics::getmatrix(&amp;quot;omega_epsilon&amp;quot;) %&amp;gt;%
  .$&amp;quot;female&amp;quot;

# Obtain an average/joint layout over several graphs
Layout &amp;lt;- qgraph::averageLayout(net_male, net_female)

# Plot both networks together
layout(t(1:2)) # 1 row and 2 columns layout

qgraph::qgraph(net_male, 
               layout = Layout, 
               theme = &amp;quot;colorblind&amp;quot;, 
               title = &amp;quot;Male&amp;quot;, 
               labels = obsvars)

qgraph::qgraph(net_female, 
               layout = Layout, 
               theme = &amp;quot;colorblind&amp;quot;, 
               title = &amp;quot;Female&amp;quot;,
               labels = obsvars)


# Reset the layout to 1:1 again
layout(t(1:1)) &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can further test the invariance of our model across levels like
weak invariance (equal factor loadings), strong invariance (equal factor
loadings and intercepts), and strict invariance (equal factor loadings,
intercepts, and even residuals) across all subgroups. See &lt;a
href="http://journals.sagepub.com/doi/10.1177/01466216231151700"&gt;Finch
et al. (2023)&lt;/a&gt; and &lt;a
href="https://towardsdatascience.com/testing-for-measurement-invariance-in-r-b44cace10148"&gt;Bulut
(2020)&lt;/a&gt; for more information.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="concluding-remark"&gt;Concluding Remark&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To support validity of a test, we need as much evidence as possible
to argue that our test, its structure, and its functions actually hold
across different conditions. For this reason, network psychometric is a
useful approach to examine validity evidence of a test and ensure its
continuing interpretations and uses. Thank you so much for reading
this!&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>565bc3a4bd2e3b6d978b3cb391e60aca</distill:md5>
      <category>R</category>
      <category>Psychometric</category>
      <guid>https://taridwong.github.io/posts/2023-04-30-networkpsych</guid>
      <pubDate>Sun, 30 Apr 2023 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2023-04-30-networkpsych/ega.png" medium="image" type="image/png" width="700" height="432"/>
    </item>
    <item>
      <title>Examining State of the Field with Bibliometric Analysis</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2023-03-20-bibanalysis</link>
      <description>


&lt;h2 id="introduction-import-and-convert-the-data"&gt;Introduction, import,
and convert the data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Hi Everyone. It’s been a busy semester for me. I taught a class
(it’s over). I am interning at Alberta Education. I am also taking a
doctoral-level coursework. I am also doing research regularly as always.
But I am managing, I guess. I found an interesting analysis technique
from one of my reading club that I think would be beneficial to you.
It’s called bibliometric analysis.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Some key reading that inspired me to try my hands on this
techniques are &lt;a
href="https://www.sciencedirect.com/science/article/pii/S0148296321003155"&gt;Donthu
et al. (2021)&lt;/a&gt;, &lt;a
href="https://revista.profesionaldelainformacion.com/index.php/EPI/article/view/epi.2020.ene.03"&gt;Moral-Muñoz
et al. (2020)&lt;/a&gt;, and &lt;a
href="https://doi.org/10.3390/educsci12030209"&gt;Sudakova et
al. (2022)&lt;/a&gt;. This post will revolve around the three papers I
mentioned, as well as some &lt;a
href="https://bibliometrix.org/documents/bibliometrix_Report.html"&gt;online
tutorial&lt;/a&gt; of the package. I will be performing a bibliometric
analysis with bibliographic records of the learning analytics field in
R. The package I will mainly use is the &lt;code&gt;bibliometrix&lt;/code&gt;
package (&lt;a
href="https://www.sciencedirect.com/science/article/pii/S1751157717300500"&gt;Aria
&amp;amp; Cuccurullo, 2017&lt;/a&gt;) to perform the analysis, with some
additional helper packages such as &lt;code&gt;ggplot2&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Bibliometric analysis is an exploratory method that analyzes
large volumes of bibliometric records to examine trends of scientific
publications, journal performance, collaboration pattern, and
relationships between topics in the field (Donthu et al., 2021).
Researchers can use this technique to gain a broad perspective of the
field to identify a research gap that they can study, as well as
determine its relative importance to other topics. Bibliometric records
can be extracted from any scientific databases such as Web of science,
SCOPUS, Digital science dimensions, and PubMed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For the data set in this post, I extracted a set of bibliometric
records from Web of Science because it provides more information on
meta-data (e.g., keyword plus, cited references) and less missing data
than Scopus. However, note that Scopus has more publication data for the
arts and humanities field.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I used the search syntax of: &lt;strong&gt;Learning&lt;/strong&gt;
&lt;strong&gt;analytic&lt;/strong&gt;* (All fields) AND &lt;strong&gt;Education&lt;/strong&gt;*
(All fields) NOT &lt;strong&gt;Medical&lt;/strong&gt; (All fields), with filter for
review article or proceeding Paper. Then, I extracted the first 500
entries from the database based on their relevance to the search term.
Note that Web of Science only allows for the maximum of 500 entries to
be exported if I requested for full bibliographic record. &lt;a
href="https://www.webofscience.com/wos/woscc/summary/d38a9cd2-86d2-43fa-aa1e-03365f7ec574-78c4a9cf/relevance/1"&gt;Here
is the link&lt;/a&gt; to the search result.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;First, we will set our working directory and load the packages as
usual.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I exported the bibliographic file in as a plain text document. I
will use the &lt;code&gt;convert2df&lt;/code&gt; function to import the data set
into R environment. We have to specify source and format of data set for
the package to correctly process the data&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#using plain text file. 
file &amp;lt;- c(&amp;quot;la.txt&amp;quot;)

M &amp;lt;- convert2df(file, dbsource = &amp;quot;wos&amp;quot;, format = &amp;quot;plaintext&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Converting your wos collection into a bibliographic dataframe

Done!


Generating affiliation field tag AU_UN from C1:  Done!&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can use the &lt;code&gt;biblioAnalysis&lt;/code&gt; function to perform
descriptive analysis of the data set. We can check for information such
as time span of the literature, number of journals, document types and
so on. Some of the more interesting results are most relevant sources
(i.e., journals or conferences), and top manuscripts per citation. We
can identify popular topics and papers from these results.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#Summary
results &amp;lt;- biblioAnalysis(M, sep = &amp;quot;;&amp;quot;)
S &amp;lt;- summary(object = results, k = 10, pause = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;

MAIN INFORMATION ABOUT DATA

 Timespan                              2007 : 2023 
 Sources (Journals, Books, etc)        238 
 Documents                             500 
 Annual Growth Rate %                  11.85 
 Document Average Age                  5.52 
 Average citations per doc             9.354 
 Average citations per year per doc    1.367 
 References                            11674 
 
DOCUMENT TYPES                     
 article                         162 
 article; book chapter           3 
 article; early access           19 
 article; proceedings paper      1 
 proceedings paper               300 
 review                          11 
 review; book chapter            1 
 review; early access            3 
 
DOCUMENT CONTENTS
 Keywords Plus (ID)                    321 
 Author&amp;#39;s Keywords (DE)                1206 
 
AUTHORS
 Authors                               1274 
 Author Appearances                    1722 
 Authors of single-authored docs       47 
 
AUTHORS COLLABORATION
 Single-authored docs                  49 
 Documents per Author                  0.392 
 Co-Authors per Doc                    3.44 
 International co-authorships %        29.8 
 

Annual Scientific Production

 Year    Articles
    2007        1
    2011        1
    2012        4
    2013       15
    2014       14
    2015       17
    2016       41
    2017       70
    2018       76
    2019       62
    2020       53
    2021       66
    2022       52
    2023        6

Annual Percentage Growth Rate 11.85 


Most Productive Authors

       Authors        Articles Authors        Articles Fractionalized
1  OGATA H                  19   OGATA H                         5.33
2  GASEVIC D                16   GASEVIC D                       3.87
3  RIENTIES B               12   RIENTIES B                      3.68
4  YAMADA M                 11   NGUYEN Q                        2.98
5  NGUYEN Q                 10   YAMADA M                        2.85
6  DRACHSLER H               9   IFENTHALER D                    2.57
7  KINSHUK                   8   GIANNAKOS M                     2.33
8  PARDO A                   8   TEMPELAAR D                     2.25
9  FERNANDEZ-MANJON B        7   KINSHUK                         2.16
10 FREIRE M                  7   DRACHSLER H                     2.15


Top manuscripts per citations

                                                                                                                                       Paper         
1  GRELLER W, 2012, EDUC TECHNOL SOC                                                                                                                 
2  SHUM SB, 2012, EDUC TECHNOL SOC                                                                                                                   
3  LU OHT, 2018, EDUC TECHNOL SOC                                                                                                                    
4  JIVET I, 2018, PROCEEDINGS OF THE 8TH INTERNATIONAL CONFERENCE ON LEARNING ANALYTICS &amp;amp; KNOWLEDGE (LAK&amp;#39;18): TOWARDS USER-CENTRED LEARNING ANALYTICS
5  TABUENCA B, 2015, COMPUT EDUC                                                                                                                     
6  LEITNER P, 2017, STUD SYST DECIS CONT                                                                                                             
7  WILLIAMS R, 2011, INT REV RES OPEN DIS                                                                                                            
8  TSAI YS, 2017, SEVENTH INTERNATIONAL LEARNING ANALYTICS &amp;amp; KNOWLEDGE CONFERENCE (LAK&amp;#39;17)                                                           
9  JIVET I, 2017, LECT NOTES COMPUT SC                                                                                                               
10 BODILY R, 2017, SEVENTH INTERNATIONAL LEARNING ANALYTICS &amp;amp; KNOWLEDGE CONFERENCE (LAK&amp;#39;17)                                                          
                             DOI  TC TCperYear   NTC
1  NA                            374     28.77  2.29
2  NA                            260     20.00  1.59
3  NA                            120     17.14 10.39
4  10.1145/3170358.3170421       118     16.86 10.21
5  10.1016/j.compedu.2015.08.004 109     10.90  7.92
6  10.1007/978-3-319-52977-6_1   102     12.75  8.71
7  10.19173/irrodl.v12i3.883      83      5.93  1.00
8  10.1145/3027385.3027400        75      9.38  6.40
9  10.1007/978-3-319-66610-5_7    71      8.88  6.06
10 10.1145/3027385.3027403        71      8.88  6.06


Corresponding Author&amp;#39;s Countries

          Country Articles   Freq SCP MCP MCP_Ratio
1  CHINA                52 0.1055  38  14     0.269
2  USA                  48 0.0974  34  14     0.292
3  JAPAN                36 0.0730  30   6     0.167
4  AUSTRALIA            34 0.0690  18  16     0.471
5  SPAIN                28 0.0568  25   3     0.107
6  UNITED KINGDOM       26 0.0527  18   8     0.308
7  GERMANY              23 0.0467  17   6     0.261
8  NETHERLANDS          20 0.0406   9  11     0.550
9  NORWAY               19 0.0385  11   8     0.421
10 CANADA               14 0.0284   7   7     0.500


SCP: Single Country Publications

MCP: Multiple Country Publications


Total Citations per Country

     Country      Total Citations Average Article Citations
1  NETHERLANDS                853                    42.650
2  UNITED KINGDOM             679                    26.115
3  AUSTRALIA                  516                    15.176
4  CHINA                      498                     9.577
5  USA                        319                     6.646
6  JAPAN                      243                     6.750
7  SPAIN                      207                     7.393
8  NORWAY                     174                     9.158
9  AUSTRIA                    147                    18.375
10 GERMANY                    109                     4.739


Most Relevant Sources

                                                                         Sources        Articles
1  SEVENTH INTERNATIONAL LEARNING ANALYTICS &amp;amp; KNOWLEDGE CONFERENCE (LAK&amp;#39;17)                   18
2  INTERACTIVE LEARNING ENVIRONMENTS                                                          16
3  EDUCATIONAL TECHNOLOGY &amp;amp; SOCIETY                                                           15
4  9TH INTERNATIONAL CONFERENCE ON EDUCATION AND NEW LEARNING TECHNOLOGIES (EDULEARN17)       12
5  JOURNAL OF LEARNING ANALYTICS                                                              11
6  IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES                                                  9
7  EDULEARN18: 10TH INTERNATIONAL CONFERENCE ON EDUCATION AND NEW LEARNING TECHNOLOGIES        8
8  ETR&amp;amp;D-EDUCATIONAL TECHNOLOGY RESEARCH AND DEVELOPMENT                                       8
9  IEEE 21ST INTERNATIONAL CONFERENCE ON ADVANCED LEARNING TECHNOLOGIES (ICALT 2021)           8
10 JOURNAL OF COMPUTER ASSISTED LEARNING                                                       8


Most Relevant Keywords

   Author Keywords (DE)      Articles Keywords-Plus (ID)     Articles
1    LEARNING ANALYTICS           382            PERFORMANCE       41
2    HIGHER EDUCATION              52            MODEL             31
3    EDUCATIONAL DATA MINING       34            DESIGN            28
4    LEARNING DESIGN               33            FRAMEWORK         25
5    E-LEARNING                    28            EDUCATION         24
6    SELF-REGULATED LEARNING       28            ONLINE            22
7    BLENDED LEARNING              25            STUDENTS          21
8    MACHINE LEARNING              24            ANALYTICS         17
9    ONLINE LEARNING               23            IMPACT            16
10   EDUCATION                     17            ACHIEVEMENT       14&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can check for most cited references with the code below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;CR &amp;lt;- citations(M, field = &amp;quot;article&amp;quot;, sep = &amp;quot;;&amp;quot;)
cbind(CR$Cited[1:20])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                                                                                                    [,1]
LONG PHIL, 2011, EDUCAUSE REVIEW, V46, P31                                                            58
GASEVIC D, 2015, TECHTRENDS, V59, P64, DOI 10.1007/S11528-014-0822-X                                  57
GRELLER W, 2012, EDUC TECHNOL SOC, V15, P42                                                           56
FERGUSON R, 2012, INT J TECHNOL ENHANC, V4, P304, DOI 10.1504/IJTEL.2012.051816                       54
SIEMENS G, 2013, AM BEHAV SCI, V57, P1380, DOI 10.1177/0002764213498851                               42
ARNOLD K.E., 2012, P 2 INT C LEARN AN K, DOI DOI 10.1145/2330601.2330666, 10.1145/2330601.2330666     40
GASEVIC D, 2016, INTERNET HIGH EDUC, V28, P68, DOI 10.1016/J.IHEDUC.2015.10.002                       40
CHATTI MA, 2012, INT J TECHNOL ENHANC, V4, P318, DOI 10.1504/IJTEL.2012.051815                        39
LOCKYER L, 2013, AM BEHAV SCI, V57, P1439, DOI 10.1177/0002764213479367                               39
TEMPELAAR DT, 2015, COMPUT HUM BEHAV, V47, P157, DOI 10.1016/J.CHB.2014.05.038                        37
CLOW D., 2012, P 2 INT C LEARNING A, P134, DOI 10.1145/2330601.2330636, DOI 10.1145/2330601.2330636   31
SLADE S, 2013, AM BEHAV SCI, V57, P1510, DOI 10.1177/0002764213479366                                 29
PAPAMITSIOU Z, 2014, EDUC TECHNOL SOC, V17, P49                                                       28
SCHWENDIMANN BA, 2017, IEEE T LEARN TECHNOL, V10, P30, DOI 10.1109/TLT.2016.2599522                   27
SIEMENS G, 2012, P 2 INT C LEARNING A, DOI 10.1145/2330601.2330661, DOI 10.1145/2330601.2330661       27
VIBERG O, 2018, COMPUT HUM BEHAV, V89, P98, DOI 10.1016/J.CHB.2018.07.027                             27
MACFADYEN LP, 2010, COMPUT EDUC, V54, P588, DOI 10.1016/J.COMPEDU.2009.09.008                         26
AGUDO-PEREGRINA AF, 2014, COMPUT HUM BEHAV, V31, P542, DOI 10.1016/J.CHB.2013.05.031                  24
SHUM SB, 2012, EDUC TECHNOL SOC, V15, P3                                                              24
WISE A. F., 2014, P 4 INT C LEARNING A, P203                                                          23&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can check for authors’ dominance ranking to see which author
published the most, and the way that they work (single-authored,
multi-authored).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#Authors’ Dominance ranking
DF &amp;lt;- dominance(results, k = 10)
DF&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                Author Dominance Factor Tot Articles Single-Authored
1             KHALIL M       0.66666667            6               0
2              TSAI YS       0.57142857            7               0
3         IFENTHALER D       0.42857143            7               0
4             NGUYEN Q       0.30000000           10               0
5           RIENTIES B       0.16666667           12               0
6             DAWSON S       0.16666667            6               0
7              EBNER M       0.16666667            6               0
8  RODRIGUEZ-TRIANA MJ       0.16666667            6               0
9             YAMADA M       0.09090909           11               0
10           GASEVIC D       0.06250000           16               0
   Multi-Authored First-Authored Rank by Articles Rank by DF
1               6              4                7          1
2               7              4                5          2
3               7              3                5          3
4              10              3                4          4
5              12              2                2          5
6               6              1                7          5
7               6              1                7          5
8               6              1                7          5
9              11              1                3          9
10             16              1                1         10&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Finally, we can plot the output to make it understandable at a
glance with the regular &lt;code&gt;plot&lt;/code&gt; function.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(x=results, k=10, pause=FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34106c2d70c4_files/figure-html/unnamed-chunk-6-1.png" width="672" /&gt;&lt;img src="file34106c2d70c4_files/figure-html/unnamed-chunk-6-2.png" width="672" /&gt;&lt;img src="file34106c2d70c4_files/figure-html/unnamed-chunk-6-3.png" width="672" /&gt;&lt;img src="file34106c2d70c4_files/figure-html/unnamed-chunk-6-4.png" width="672" /&gt;&lt;img src="file34106c2d70c4_files/figure-html/unnamed-chunk-6-5.png" width="672" /&gt;&lt;/p&gt;
&lt;h2
id="examining-relationships-between-authors-with-co-citation-analysis"&gt;Examining
relationships between authors with co-citation Analysis&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We can check how the papers in our data set cited each other with
co-citation analysis. Results will be displayed as co-citation network
below. We can see that there are two main clusters, indicating that
papers within a cluster cited each other the most.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;NetMatrix &amp;lt;- biblioNetwork(M, analysis = &amp;quot;co-citation&amp;quot;, network = &amp;quot;references&amp;quot;, sep = &amp;quot;;&amp;quot;)

net=networkPlot(NetMatrix, weighted=NULL, n = 50, 
                Title = &amp;quot;Co-Citation Network&amp;quot;, type = &amp;quot;fruchterman&amp;quot;, 
                size=4, size.cex=TRUE, remove.multiple=FALSE, labelsize=1, label.n=10, label.cex=F, edgesize = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34106c2d70c4_files/figure-html/unnamed-chunk-7-1.png" width="960" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can also perform co-citation analysis at the source level (i.e.,
journals and conferences) to examine how sources in our data set cited
each other.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;Source=metaTagExtraction(M,&amp;quot;CR_SO&amp;quot;,sep=&amp;quot;;&amp;quot;)

NetMatrix &amp;lt;- biblioNetwork(Source, analysis = &amp;quot;co-citation&amp;quot;, network = &amp;quot;sources&amp;quot;, sep = &amp;quot;;&amp;quot;)

net=networkPlot(NetMatrix, n = 50, Title = &amp;quot;Co-Citation Network-Journal&amp;quot;, type = &amp;quot;auto&amp;quot;, size.cex=TRUE, size=3, remove.multiple=FALSE, labelsize=1,edgesize = 10, edges.min=5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34106c2d70c4_files/figure-html/unnamed-chunk-8-1.png" width="960" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can also examine top 5 keywords used by publication authors in
our data set with the code below. We can see popular topics based on
keyword with this result.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#Author keyword network

A &amp;lt;- cocMatrix(M, Field = &amp;quot;DE&amp;quot;, sep = &amp;quot;;&amp;quot;)
sort(Matrix::colSums(A), decreasing = TRUE)[1:5]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;     LEARNING ANALYTICS        HIGHER EDUCATION 
                    382                      52 
EDUCATIONAL DATA MINING         LEARNING DESIGN 
                     33                      33 
             E-LEARNING 
                     28 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can also examine author coupling cluster to see whose work is
related to whose. Once we know the author, we can refer back to the list
of our bibliographic records to examine their work.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;res &amp;lt;- couplingMap(M, analysis = &amp;quot;authors&amp;quot;, field = &amp;quot;CR&amp;quot;, n = 250, impact.measure=&amp;quot;local&amp;quot;,
minfreq = 3, size = 0.5, repel = TRUE)

plot(res$map)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34106c2d70c4_files/figure-html/unnamed-chunk-10-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Finally, we can use a three fields plot to see the relationship
between authors (AU), keywords that they used (DE), and
journals/conferences that they submitted (SO).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;threeFieldsPlot(M, fields = c(&amp;quot;AU&amp;quot;, &amp;quot;DE&amp;quot;, &amp;quot;SO&amp;quot;), n = c(20, 20, 20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2023-03-20-bibanalysisthreefields.png" /&gt;&lt;/p&gt;
&lt;h2 id="examining-conceptual-structure-with-co-word-analysis"&gt;Examining
conceptual structure with co-word analysis&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Co-word analysis is a useful tool to examine conceptual structure of
the field. That is, we can see relationship between topics within the
field of interest. The keyword co-occurrence network plot below shows
the how frequent each keyword is presented in our bibliographic data
from the size of its circle. The bigger they are, the more frequent they
are used. Position of keywords on the plot also indicates which keywords
are usually presented together.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#Co-occurrences network

# keywords
NetMatrix &amp;lt;- biblioNetwork(M, analysis = &amp;quot;co-occurrences&amp;quot;, network = &amp;quot;keywords&amp;quot;, sep = &amp;quot;;&amp;quot;)

# Plot the network
net = networkPlot(NetMatrix, normalize=&amp;quot;association&amp;quot;, weighted=T, n = 30, 
                  Title = &amp;quot;Keyword Co-occurrences&amp;quot;, type = &amp;quot;fruchterman&amp;quot;, 
                  size=TRUE, edgesize = 5, labelsize=0.7, remove.multiple=FALSE, label.cex=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34106c2d70c4_files/figure-html/unnamed-chunk-12-1.png" width="960" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can also perform correspondence analysis to see relationship
between topics with multiple correspondence analysis (MCA) or
hierarchical cluster plot as well. The factorial maps also show papers
with the highest contribution within the keyword clusters in the
output.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Conceptual Structure using keywords (method=&amp;quot;MCA&amp;quot;)
CS &amp;lt;- conceptualStructure(M, field=&amp;quot;ID&amp;quot;, method=&amp;quot;MCA&amp;quot;, minDegree=4, clust=5, stemming=FALSE, labelsize=15, documents=20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34106c2d70c4_files/figure-html/unnamed-chunk-13-1.png" width="1440" /&gt;&lt;img src="file34106c2d70c4_files/figure-html/unnamed-chunk-13-2.png" width="1440" /&gt;&lt;img src="file34106c2d70c4_files/figure-html/unnamed-chunk-13-3.png" width="1440" /&gt;&lt;img src="file34106c2d70c4_files/figure-html/unnamed-chunk-13-4.png" width="1440" /&gt;&lt;/p&gt;
&lt;h2
id="examining-keyword-usage-across-time-with-historiograph"&gt;Examining
keyword usage across time with historiograph&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Most analyses we performed earlier did not use time point as a
variable. We can plot a historiograph to see the evolution of keyword
usage across time, as well as their relative popularity to each other.
The plot below indicates that “learning analytics” is the post popular
as it gained more interest since 2007 based on our data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(reshape2)
library(ggplot2)

kword &amp;lt;- KeywordGrowth(M, Tag = &amp;quot;DE&amp;quot;, sep = &amp;quot;;&amp;quot;, top = 15, cdf = TRUE)

DF = melt(kword, id=&amp;#39;Year&amp;#39;)

# Timeline keywords ggplot
ggplot(DF,aes(x=Year,y=value, group=variable, shape=variable, colour=variable))+
  geom_point()+geom_line()+ 
  scale_shape_manual(values = 1:15)+
  labs(color=&amp;quot;Author Keywords&amp;quot;)+
  scale_x_continuous(breaks = seq(min(DF$Year), max(DF$Year), by = 5))+
  scale_y_continuous(breaks = seq(0, max(DF$value), by=10))+
  guides(color=guide_legend(title = &amp;quot;Author Keywords&amp;quot;), shape=FALSE)+
  labs(y=&amp;quot;Count&amp;quot;, variable=&amp;quot;Author Keywords&amp;quot;, title = &amp;quot;Author&amp;#39;s Keywords Usage Evolution Over Time&amp;quot;)+
  theme(text = element_text(size = 10))+
  facet_grid(variable ~ .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34106c2d70c4_files/figure-html/unnamed-chunk-14-1.png" width="960" /&gt;&lt;/p&gt;
&lt;h2 id="examining-trends-of-the-field-with-thematic-map"&gt;Examining
trends of the field with thematic map&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Aside from examining frequency of keyword usage, we can use
thematic map to examine trend of the topics within the field. The map
below places topics within four panes (&lt;a
href="https://doi.org/10.1016/j.procs.2018.10.278"&gt;Cobo et al.,
2018&lt;/a&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Upper right pane indicates motor theme that is important to the
field as it is constantly used, hence the name motor;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Upper left pane indicates niche theme that is very specialized to
certain groups of research;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lower left pane indicates emerging/declining theme that is still
weakly developed, meaning that it could still growing or starting to
disappear;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lower right pane indicates basic theme that is important to the
field but not well developed as motor theme, meaning that topics in this
pane are usually studied for general knowledge of the field.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;Map=thematicMap(M, field = &amp;quot;ID&amp;quot;, n = 250, minfreq = 4,
  stemming = FALSE, size = 0.7, n.labels=5, repel = TRUE)
plot(Map$map)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34106c2d70c4_files/figure-html/unnamed-chunk-15-1.png" width="1440" data-distill-preview=1 /&gt;&lt;/p&gt;
&lt;h2
id="examining-collaboration-with-social-structure-analysis"&gt;Examining
collaboration with social structure analysis&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We can examine collaboration pattern between authors from our data
set with collaboration analysis. The author collaboration network below
shows who collaborated with whom at the author level.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;NetMatrix &amp;lt;- biblioNetwork(M, analysis = &amp;quot;collaboration&amp;quot;,  network = &amp;quot;authors&amp;quot;, sep = &amp;quot;;&amp;quot;)

net=networkPlot(NetMatrix,  n = 50, Title = &amp;quot;Author collaboration&amp;quot;,type = &amp;quot;auto&amp;quot;, size=5,size.cex=T,edgesize = 5,labelsize=1, community.repulsion = 0.1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34106c2d70c4_files/figure-html/unnamed-chunk-16-1.png" width="960" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can also plot an institution collaboration network to examine
collaboration pattern between institutions as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;NetMatrix &amp;lt;- biblioNetwork(M, analysis = &amp;quot;collaboration&amp;quot;,  network = &amp;quot;universities&amp;quot;, sep = &amp;quot;;&amp;quot;)
net=networkPlot(NetMatrix,  n = 50, Title = &amp;quot;Institution collaboration&amp;quot;,type = &amp;quot;auto&amp;quot;, size=4,size.cex=F,edgesize = 3,labelsize=1, community.repulsion = 0.05)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34106c2d70c4_files/figure-html/unnamed-chunk-17-1.png" width="960" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lastly, we can examine collaboration pattern at international level
with country collaboration network. This analysis shows countries that
are productive within the field (indicated by size of the circle), as
well as countries that they collaborate with as indicated by linkages in
the plot.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;country &amp;lt;- metaTagExtraction(M, Field = &amp;quot;AU_CO&amp;quot;, sep = &amp;quot;;&amp;quot;)
NetMatrix &amp;lt;- biblioNetwork(country, analysis = &amp;quot;collaboration&amp;quot;,  network = &amp;quot;countries&amp;quot;, sep = &amp;quot;;&amp;quot;)

net=networkPlot(NetMatrix,  n = dim(NetMatrix)[1], Title = &amp;quot;Country collaboration&amp;quot;,type = &amp;quot;circle&amp;quot;, size=10,size.cex=T,edgesize = 1,labelsize=0.6, cluster=&amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file34106c2d70c4_files/figure-html/unnamed-chunk-18-1.png" width="960" /&gt;&lt;/p&gt;
&lt;h2 id="concluding-remarks"&gt;Concluding remarks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;What I like about bibliometric analysis is that it examines the
field at the meta level, meaning that it is a research that is built on
other research. Without the work of other researchers out there, this
analysis would be impossible.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This analysis method is a useful tool to gain a bird-eye view on
state of the field that we are interested in. Results can be used inform
early career researchers or students in their topic selection.
Basically, it could be helpful for us to know which topic is popular
(and therefore has a lot of papers we could read) or which topic is
declining.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;By identifying authors that publish a lot in the field, we can
decide whether we want to follow their work to keep up with research
trend, or deviate our work off their path so that we can study something
new to the field. Anyway, thank you so much for reading this! I hope you
like it :)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>0fabd9ac6e025bacc0d7313c8c5a9710</distill:md5>
      <category>R</category>
      <category>Data Visualization</category>
      <guid>https://taridwong.github.io/posts/2023-03-20-bibanalysis</guid>
      <pubDate>Wed, 22 Mar 2023 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2023-03-20-bibanalysis/threefields.png" medium="image" type="image/png" width="2444" height="1474"/>
    </item>
    <item>
      <title>Data Exploration with ggstatsplot</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-12-31-ggstat</link>
      <description>In this post, I will be performing and visualizing data exploration techniques such as Pearson's correlation test, Chi-square Goodness of Fit test, Chi-square Test of Independence, One-sample t-test, and Paired-sample t-test.

(8 min read)</description>
      <category>R</category>
      <category>Statistics</category>
      <guid>https://taridwong.github.io/posts/2022-12-31-ggstat</guid>
      <pubDate>Sat, 31 Dec 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-12-31-ggstat/ggstat_files/figure-html5/unnamed-chunk-13-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Test Shortening with Genetic Algorithm and Ant Colony Optimization</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-08-14-ga-aco</link>
      <description>In this post, I will use Genetic Algorithm and Ant Colony Optimization Algorithm to automatically shorten the length of a test.

 (8 min read)</description>
      <category>R</category>
      <category>Psychometric</category>
      <guid>https://taridwong.github.io/posts/2022-08-14-ga-aco</guid>
      <pubDate>Sun, 14 Aug 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-08-14-ga-aco/ga-aco_files/figure-html5/unnamed-chunk-11-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Leveraging a Large-Scale Educational Data Set with Educational Data Mining</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-08-06-edm</link>
      <description>In this post, I will be predicting students' high school dropout rate through a large-scale educational data set.

 (10 min read)</description>
      <category>R</category>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-08-06-edm</guid>
      <pubDate>Sat, 06 Aug 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-08-06-edm/edm_files/figure-html5/unnamed-chunk-24-11.png" medium="image" type="image/png" width="2304" height="1536"/>
    </item>
    <item>
      <title>Item Response Theory</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-05-15-irt</link>
      <description>In this post, I will be examining characteristics of test items based on the Item Response Theory framework.

 (18 min read)</description>
      <category>R</category>
      <category>Psychometric</category>
      <guid>https://taridwong.github.io/posts/2022-05-15-irt</guid>
      <pubDate>Sun, 15 May 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-05-15-irt/IIC.png" medium="image" type="image/png" width="656" height="551"/>
    </item>
    <item>
      <title>Making Sense of Machine Learning with Explanable Artificial Intelligence</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-28-xai</link>
      <description>I will be applying the methods of Explanable Artificial Intelligence (XAI) to extract interpretable insights from a classification model that predicts students' grade repetition.

(14 min read)</description>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-28-xai</guid>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-28-xai/xai_files/figure-html5/unnamed-chunk-14-11.png" medium="image" type="image/png" width="1440" height="1824"/>
    </item>
    <item>
      <title>Addressing Data Imbalance with Semi-Supervised Learning</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-28-semisupervised</link>
      <description>For this post, I will use semi-supervised learning approach to perform a classification task with a highly imbalance data.  

(7 min read)</description>
      <category>Python</category>
      <category>Unsupervised Machine Learning</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-28-semisupervised</guid>
      <pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-28-semisupervised/semi-ml.png" medium="image" type="image/png" width="900" height="450"/>
    </item>
    <item>
      <title>Examining Customer Cluster with Unsupervised Machine Learning</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-18-unsupervisedml</link>
      <description>In this post, I will be using two unsupervised learning techniques with a data set, namely K-means clustering and Hierarchical clustering, to determine groups of customers from their age, income, and spending behavior data.  

(8 min read)</description>
      <category>Python</category>
      <category>Unsupervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-18-unsupervisedml</guid>
      <pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-18-unsupervisedml/unsupervisedml_files/figure-html5/unnamed-chunk-14-19.png" medium="image" type="image/png" width="2880" height="1152"/>
    </item>
    <item>
      <title>Combining Multiple Machine Learning Models with the Ensemble Methods</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-09-ensemble</link>
      <description>This entry explores different ways to combine supervised machine learning models to maximize their predictive capability.  

(13 min read)</description>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-09-ensemble</guid>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-09-ensemble/robot.png" medium="image" type="image/png" width="626" height="528"/>
    </item>
    <item>
      <title>Examining PISA 2018 Data Set with Statistical Learning Approach</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-02-27-statlearning</link>
      <description>In this post, I will be developing two statistical learning models, namely Linear Regression and Polynomial Regression, and apply it to Thai student data from the Programme for International Student Assessment (PISA) 2018 data set to examine the impact of classroom competition and cooperation to students' academic performance.  

(14 min read)</description>
      <category>R</category>
      <category>Statistics</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-02-27-statlearning</guid>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-02-27-statlearning/statlearn.jpeg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Classical Test Theory in R</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-01-15-ctt</link>
      <description>For this post, I will be analyzing characteristics of test items based on the framework of Classical Test Theory (CTT).

(13 min read)</description>
      <category>R</category>
      <category>Psychometric</category>
      <guid>https://taridwong.github.io/posts/2022-01-15-ctt</guid>
      <pubDate>Sat, 15 Jan 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-01-15-ctt/ctt_files/figure-html5/unnamed-chunk-28-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Examining the Big 5 personality Dataset with factor analysis</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-01-01-efacfa</link>
      <description>For this entry, I will be examining the Big 5 personality Inventory data set with Exploratory Data Analysis to identify potential structures of personality trait and verify them with Confirmatory Factor Analysis.

(8 min read)</description>
      <category>R</category>
      <category>Statistics</category>
      <guid>https://taridwong.github.io/posts/2022-01-01-efacfa</guid>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-01-01-efacfa/corrmatrix.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Measuring Text Similarity with Movie plot data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-29-movie-similarity</link>
      <description>For this post, I will be analyzing textual data of movie plots to determine their similarity with TF-IDF and Clustering.

(7 min read)</description>
      <category>Python</category>
      <category>Natural Language Processing</category>
      <category>Unsupervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-29-movie-similarity</guid>
      <pubDate>Wed, 29 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-29-movie-similarity/movie-similarity_files/figure-html5/unnamed-chunk-11-1.png" medium="image" type="image/png" width="484" height="483"/>
    </item>
    <item>
      <title>Missing Data Analysis</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-27-missingdata</link>
      <description>For this post, I will examine missing data in a large-scale dataset and discuss about numerous ways we can clean them as a part of data preparation.

(10 min read)</description>
      <category>R</category>
      <category>Data Visualization</category>
      <category>Statistics</category>
      <guid>https://taridwong.github.io/posts/2021-12-27-missingdata</guid>
      <pubDate>Mon, 27 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-27-missingdata/missingpic.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Applying Machine Learning to Audio Data: Visualization, Classification, and Recommendation</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data</link>
      <description>For this entry, I am trying my hands on audio data to extract its features for exploratory data analysis (EDA), using machine learning algorithms to perform music classification, and finally build up on that result to develop a recommendation system for music of similar characteristics.

(13 min read)</description>
      <category>Python</category>
      <category>Data Visualization</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data</guid>
      <pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data/applying-machine-learning-to-audio-data_files/figure-html5/unnamed-chunk-13-11.png" medium="image" type="image/png" width="3072" height="1152"/>
    </item>
    <item>
      <title>Interactive plots for Suicide Data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data</link>
      <description>For this entry, will be visualizing suicide data from 1958 to 2015 with interactive plots to communicate insights to non-technical audience.  

(14 min read)</description>
      <category>R</category>
      <category>Data Visualization</category>
      <category>Quantitative research</category>
      <guid>https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data</guid>
      <pubDate>Thu, 09 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data/preview.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Image Recognition with Artificial Neural Networks</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks</link>
      <description>In this entry, we will be developing a deep learning algorithm - a sub-field of machine learning inspired by the structure of human brain (neural networks) - to classify images of single digit number (0-9).  

(9 min read)</description>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <category>Deep Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks</guid>
      <pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks/deepnn.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Anomaly Detection with New York City taxi data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data</link>
      <description>In this entry, I will be conducting anomaly detection to identify points of anomaly in the taxi passengers data in New York City from July 2014 to January 2015 at half-hourly intervals.
 
 (4 min read)</description>
      <category>Unsupervised Machine Learning</category>
      <category>R</category>
      <guid>https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data</guid>
      <pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data/anomaly-detection-with-new-york-city-taxi-data_files/figure-html5/unnamed-chunk-9-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Crime mapping in San Francisco with police data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data</link>
      <description>In this entry, we will explore San Francisco crime data from 2016 to 2018 to understand the relationship between civilian-reported incidents of crime and police-reported incidents of crime. Along the way we will use table intersection methods to subset our data, aggregation methods to calculate important statistics, and simple visualizations to understand crime trends.  

(7 min read)</description>
      <category>Data Visualization</category>
      <category>R</category>
      <category>Quantitative research</category>
      <guid>https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data</guid>
      <pubDate>Tue, 30 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data/crime-mapping-in-san-francisco-with-police-data_files/figure-html5/unnamed-chunk-6-1.png" medium="image" type="image/png" width="1920" height="1920"/>
    </item>
    <item>
      <title>Exploring COVID-19 data from twitter with topic modeling</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds</link>
      <description>This entry focuses on the exploration of twitter data from Alberta's Chief Medical Officer of Health via word cloud and topic modeling to gain insights in characteristics of public health messaging during the COVID-19 pandemic.  

(7 min read)</description>
      <category>Python</category>
      <category>Natural Language Processing</category>
      <category>Unsupervised Machine Learning</category>
      <category>COVID-19</category>
      <guid>https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds</guid>
      <pubDate>Tue, 23 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds/word-cloud-pv.png" medium="image" type="image/png" width="1920" height="1920"/>
    </item>
    <item>
      <title>Finding a home among the paradigm push-back with Dialectical Pluralism</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-07-introducing-dialectical-pluralism</link>
      <description>This entry discusses the reconciliation of quantitative and qualitative worldviews amidst the paradigm wars with pluralistic stance.

(2 min read)</description>
      <category>Mixed methods research</category>
      <guid>https://taridwong.github.io/posts/2021-11-07-introducing-dialectical-pluralism</guid>
      <pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://www.relevantinsights.com/wp-content/uploads/2020/05/Qualitative-Quantitative-Research-For-New-Product-Development.png" medium="image" type="image/png"/>
    </item>
  </channel>
</rss>
