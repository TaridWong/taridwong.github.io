<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>Tarid Wongvorachan</title>
    <link>https://taridwong.github.io/</link>
    <atom:link href="https://taridwong.github.io/index.xml" rel="self" type="application/rss+xml"/>
    <description>Blogs about what I learned from my academic lives
</description>
    <generator>Distill</generator>
    <lastBuildDate>Sat, 01 Jan 2022 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Examining the Big 5 personality Dataset with factor analysis</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-01-01-efacfa</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The concept of Big 5 personality traits represents tendency of individuals to possess five personality characteristics of extraversion, agreeableness, openness, conscientiousness, and neuroticism &lt;a href="https://doi.org/10.1002/job.742"&gt;(Neal et al., 2012)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Here, we will examine the data set with Exploratory Data Analysis (EFA) to see if we can identify any other structures aside from the original five groups as indicated by the concept of big 5 personality traits; then, we will verify those structures with Confirmatory Factor Analysis (CFA) to assess and compare their statistical characteristic (i.e., model fit).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will begin by loading essential packages for data preprocessing and statistical modeling as indicated below.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(parameters) #for parameter processing
library(tidyverse) #toolbox for R
library(psych) #for descriptive statistics and the data set
library(ggcorrplot) #for correlation matrix
library(see) #add-on for ggplot2
library(lavaan) #for SEM
library(performance) #assessment of Regression Models Performance
library(semPlot) #to plot path model for CFA
library(dlookr) #missing data diagnosis
library(mice) #missing data imputation&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="dataset"&gt;Dataset&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The data set we use is a built-in data set from &lt;code&gt;psych&lt;/code&gt; package. There are 2800 observations and 25 variables of all 5 personality traits.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will begin by loading in the data set and check for its missing value.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Load the data
data &amp;lt;- psych::bfi[, 1:25]

head(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 E4 E5 N1 N2 N3 N4 N5 O1
61617  2  4  3  4  4  2  3  3  4  4  3  3  3  4  4  3  4  2  2  3  3
61618  2  4  5  2  5  5  4  4  3  4  1  1  6  4  3  3  3  3  5  5  4
61620  5  4  5  4  4  4  5  4  2  5  2  4  4  4  5  4  5  4  2  3  4
61621  4  4  6  5  5  4  4  3  5  5  5  3  4  4  4  2  5  2  4  1  3
61622  2  3  3  4  5  4  4  5  3  2  2  2  5  4  5  2  3  4  4  3  3
61623  6  6  5  6  5  6  6  6  1  3  2  1  6  5  6  3  5  2  2  3  4
      O2 O3 O4 O5
61617  6  3  4  3
61618  2  4  3  3
61620  2  5  5  2
61621  3  4  3  5
61622  3  4  3  3
61623  3  5  6  1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;#diagnose for missing value
dlookr::diagnose(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 25 x 6
   variables types   missing_count missing_percent unique_count
   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;           &amp;lt;int&amp;gt;           &amp;lt;dbl&amp;gt;        &amp;lt;int&amp;gt;
 1 A1        integer            16           0.571            7
 2 A2        integer            27           0.964            7
 3 A3        integer            26           0.929            7
 4 A4        integer            19           0.679            7
 5 A5        integer            16           0.571            7
 6 C1        integer            21           0.75             7
 7 C2        integer            24           0.857            7
 8 C3        integer            20           0.714            7
 9 C4        integer            26           0.929            7
10 C5        integer            16           0.571            7
# ... with 15 more rows, and 1 more variable: unique_rate &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;visdat::vis_miss(data, sort_miss = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b8446d81ab3_files/figure-html/unnamed-chunk-5-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are some degree of missingness in the dataset as indicated by the missingness map. For good measure, we will impute it with the predictive mean matchmaking method by the &lt;code&gt;mice&lt;/code&gt; package.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="data-preprocessing"&gt;Data preprocessing&lt;/h2&gt;
&lt;pre class="r"&gt;&lt;code&gt;#imputation
mice_model &amp;lt;- mice(data, method=&amp;#39;pmm&amp;#39;, seed = 123)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 iter imp variable
  1   1  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  1   2  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  1   3  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  1   4  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  1   5  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  2   1  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  2   2  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  2   3  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  2   4  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  2   5  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  3   1  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  3   2  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  3   3  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  3   4  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  3   5  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  4   1  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  4   2  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  4   3  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  4   4  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  4   5  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  5   1  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  5   2  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  5   3  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  5   4  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5
  5   5  A1  A2  A3  A4  A5  C1  C2  C3  C4  C5  E1  E2  E3  E4  E5  N1  N2  N3  N4  N5  O1  O3  O4  O5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;data_complete &amp;lt;- complete(mice_model)

visdat::vis_miss(data_complete, sort_miss = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b8446d81ab3_files/figure-html/unnamed-chunk-6-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;There is no missing value present in the dataset after the imputation. We can proceed with factor structure checking to assess whether the dataset is appropriate for factor analysis with &lt;code&gt;check_factorstructure()&lt;/code&gt;. Two existing methods are the &lt;em&gt;Bartlett’s Test of Sphericity&lt;/em&gt; and the &lt;em&gt;Kaiser, Meyer, Olkin (KMO)&lt;/em&gt; Measure of Sampling Adequacy (MSA).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The former tests whether a matrix is significantly different from an identity matrix. This statistical test for the presence of correlations among variables, providing the statistical probability that the correlation matrix has significant correlations among at least some of variables. As for factor analysis to work, some relationships between variables are needed, thus, a significant Bartlett’s test of sphericity is required to be significant.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The latter method, ranging from 0 to 1, indicates the degree to which each variable in the dataset is predicted without error by the other variables. A value of 0 indicates that the sum of partial correlations is large relative to the sum correlations, indicating factor analysis is likely to be inappropriate. A KMO value close to 1 indicates that the sum of partial correlations is not large relative to the sum of correlations and so factor analysis should yield distinct and reliable factors.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#check for factor structure
check_factorstructure(data_complete)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Is the data suitable for Factor Analysis?

  - KMO: The Kaiser, Meyer, Olkin (KMO) measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.84).
  - Sphericity: Bartlett&amp;#39;s test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(300) = 20158.27, p &amp;lt; .001).&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The Barlett’s test suggested that there is sufficient significant correlation in the data for factor analysis. Speaking of correlation, let us generate a correlation matrix to check for relationship between variables in the dataset as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;variables.to.use&amp;lt;-c(&amp;quot;A1&amp;quot;, &amp;quot;A2&amp;quot;, &amp;quot;A3&amp;quot;, &amp;quot;A4&amp;quot;, &amp;quot;A5&amp;quot;,
                    &amp;quot;C1&amp;quot;, &amp;quot;C2&amp;quot;, &amp;quot;C3&amp;quot;, &amp;quot;C4&amp;quot;, &amp;quot;C5&amp;quot;,
                    &amp;quot;E1&amp;quot;, &amp;quot;E2&amp;quot;, &amp;quot;E3&amp;quot;, &amp;quot;E4&amp;quot;, &amp;quot;E5&amp;quot;,
                    &amp;quot;N1&amp;quot;, &amp;quot;N2&amp;quot;, &amp;quot;N3&amp;quot;, &amp;quot;N4&amp;quot;, &amp;quot;N5&amp;quot;,
                    &amp;quot;O1&amp;quot;, &amp;quot;O2&amp;quot;, &amp;quot;O3&amp;quot;, &amp;quot;O4&amp;quot;, &amp;quot;O5&amp;quot;)

data.corr&amp;lt;-cor(data_complete[variables.to.use],
                 method = &amp;quot;pearson&amp;quot;,
                 use=&amp;#39;all.obs&amp;#39;)
ggcorrplot(data.corr,
           p.mat=cor_pmat(data_complete[variables.to.use]),
           hc.order=TRUE, 
           type=&amp;#39;lower&amp;#39;,
           color=c(&amp;#39;red3&amp;#39;, &amp;#39;white&amp;#39;, &amp;#39;green3&amp;#39;),
           outline.color = &amp;#39;darkgoldenrod1&amp;#39;, 
           lab=FALSE, #omit the correlation coefficient
           legend.title=&amp;#39;Correlation&amp;#39;,
           pch=4, 
           pch.cex=4, #size of the cross mark for non-significant indicator
           lab_size=6)+ 
  labs(title=&amp;quot;Correlation Matrix&amp;quot;)+
  theme(plot.title=element_text(face=&amp;#39;bold&amp;#39;,size=14,hjust=0.5,colour=&amp;quot;darkred&amp;quot;))+
  theme(legend.position=c(0.10,0.80), legend.box.just = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b8446d81ab3_files/figure-html/unnamed-chunk-8-1.png" width="1440" data-distill-preview=1 /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The green panel indicates positive relationships while the red panel indicates negative relationship. The cross symbol suggests that the relationship between two variables is not statistically significant.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Next, we would need to split the dataset into two to perform EFA and CFA, so that we can make sure to test the model on an unseen set of data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Establish two sets of indices to split the dataset
N &amp;lt;- nrow(data_complete)
indices &amp;lt;- seq(1, N)
indices_EFA &amp;lt;- sample(indices, floor((.5*N)))
indices_CFA &amp;lt;- indices[!(indices %in% indices_EFA)]

# Use those indices to split the dataset into halves for your EFA and CFA
bfi_EFA &amp;lt;- data_complete[indices_EFA, ]
bfi_CFA &amp;lt;- data_complete[indices_CFA, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;check_factorstructure(bfi_EFA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Is the data suitable for Factor Analysis?

  - KMO: The Kaiser, Meyer, Olkin (KMO) measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.84).
  - Sphericity: Bartlett&amp;#39;s test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(300) = 10508.83, p &amp;lt; .001).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;check_factorstructure(bfi_CFA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Is the data suitable for Factor Analysis?

  - KMO: The Kaiser, Meyer, Olkin (KMO) measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.84).
  - Sphericity: Bartlett&amp;#39;s test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(300) = 9929.32, p &amp;lt; .001).&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The two datasets that we splitted are appropriate for factor analysis, so we can proceed with EFA as the first analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="exploratory-factor-analysis"&gt;Exploratory Factor Analysis&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Exploratory Factor Analysis is a statistical technique in social science to explain the variance between several measured variables as a smaller set of latent variables. EFA is often used to consolidate survey data by revealing the groupings (factors) that underly individual questions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An EFA provides information on each item’s relationship to a single factor that is hypothesized to be represented by each of the items. EFA results give you basic information about how well items relate to that hypothesized construct.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The common application of EFA is to investigate relationships between observed variable and latent variables (factor) such as measurement piloting.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="scree-plot"&gt;Scree plot&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To empirically determine the dimensionality of your data, a common strategy is to examine the eigenvalues and scree plot.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The scree plot is a visual representation of eigenvalues that determines potential dimensionality of the dataset. Eigenvalues can be generated from a principal component analysis or a factor analysis, and the &lt;code&gt;scree()&lt;/code&gt; function calculates and plots both by default. Since &lt;code&gt;eigen()&lt;/code&gt; finds eigenvalues via principal components analysis, we will use &lt;code&gt;factors = FALSE&lt;/code&gt; so our scree plot will only display the values corresponding to those results.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Calculate the correlation matrix first
bfi_EFA_cor &amp;lt;- cor(bfi_EFA, use = &amp;quot;pairwise.complete.obs&amp;quot;) 

# Then use that correlation matrix to calculate eigenvalues
eigenvals &amp;lt;- eigen(bfi_EFA_cor)

# Look at the eigenvalues returned
eigenvals$values&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] 5.1260335 2.7283892 2.1703605 1.7962625 1.5810352 1.1067334
 [7] 0.8628232 0.8314676 0.7286709 0.7025545 0.6794529 0.6418044
[13] 0.6236182 0.5865228 0.5568888 0.5537281 0.5017308 0.4883328
[19] 0.4771046 0.4435182 0.4277177 0.3970359 0.3845808 0.3458697
[25] 0.2577638&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Then use the correlation matrix to create the scree plot

scree(bfi_EFA_cor, factors = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b8446d81ab3_files/figure-html/unnamed-chunk-13-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;From the above plot, The point where the slope of the curve is clearly leveling off (the “elbow) indicates that the number of factors that should be retained. For this case, the plot indicated that six factors should be retained.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, the dataset we use is for Big 5 personality traits with 5 factors, so we can investigate both 5 and 6 factors model to compare them both.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="fit-efa-models"&gt;Fit EFA models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We can initially explore the factor structure of 5 groups first as intended by the big 5 personality theory.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;# Fit an EFA
efa &amp;lt;- psych::fa(data, nfactors = 5) %&amp;gt;% 
  model_parameters(sort = TRUE, threshold = &amp;quot;max&amp;quot;)

efa&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Rotated loadings from Factor Analysis (oblimin-rotation)

Variable | MR2  |  MR1  |  MR3  |  MR5  |  MR4  | Complexity | Uniqueness
-------------------------------------------------------------------------
N1       | 0.81 |       |       |       |       |    1.08    |    0.35   
N2       | 0.78 |       |       |       |       |    1.04    |    0.40   
N3       | 0.71 |       |       |       |       |    1.07    |    0.45   
N5       | 0.49 |       |       |       |       |    1.96    |    0.65   
N4       | 0.47 |       |       |       |       |    2.27    |    0.51   
E2       |      | -0.68 |       |       |       |    1.07    |    0.46   
E4       |      | 0.59  |       |       |       |    1.49    |    0.47   
E1       |      | -0.56 |       |       |       |    1.21    |    0.65   
E5       |      | 0.42  |       |       |       |    2.60    |    0.60   
E3       |      | 0.42  |       |       |       |    2.55    |    0.56   
C2       |      |       | 0.67  |       |       |    1.17    |    0.55   
C4       |      |       | -0.61 |       |       |    1.18    |    0.55   
C3       |      |       | 0.57  |       |       |    1.11    |    0.68   
C5       |      |       | -0.55 |       |       |    1.44    |    0.57   
C1       |      |       | 0.55  |       |       |    1.19    |    0.67   
A3       |      |       |       | 0.66  |       |    1.07    |    0.48   
A2       |      |       |       | 0.64  |       |    1.04    |    0.55   
A5       |      |       |       | 0.53  |       |    1.49    |    0.54   
A4       |      |       |       | 0.43  |       |    1.74    |    0.72   
A1       |      |       |       | -0.41 |       |    1.97    |    0.81   
O3       |      |       |       |       | 0.61  |    1.17    |    0.54   
O5       |      |       |       |       | -0.54 |    1.21    |    0.70   
O1       |      |       |       |       | 0.51  |    1.13    |    0.69   
O2       |      |       |       |       | -0.46 |    1.75    |    0.74   
O4       |      |       |       |       | 0.37  |    2.69    |    0.75   

The 5 latent factors (oblimin rotation) accounted for 41.48% of the total variance of the original data (MR2 = 10.28%, MR1 = 8.80%, MR3 = 8.12%, MR5 = 7.94%, MR4 = 6.34%).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;summary(efa)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# (Explained) Variance of Components

Parameter                       |   MR2 |   MR1 |   MR3 |   MR5 |   MR4
-----------------------------------------------------------------------
Eigenvalues                     | 4.493 | 2.249 | 1.505 | 1.188 | 0.934
Variance Explained              | 0.103 | 0.088 | 0.081 | 0.079 | 0.063
Variance Explained (Cumulative) | 0.103 | 0.191 | 0.272 | 0.351 | 0.415
Variance Explained (Proportion) | 0.248 | 0.212 | 0.196 | 0.191 | 0.153&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;As we can see, the 25 items nicely spread on the 5 latent factors as the theory suggests. Based on this model, we can now predict back the scores for each individual for these new variables:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;predict_result &amp;lt;- predict(efa, names = c(&amp;quot;Neuroticism&amp;quot;, &amp;quot;Conscientiousness&amp;quot;, &amp;quot;Extraversion&amp;quot;, &amp;quot;Agreeableness&amp;quot;, &amp;quot;Opennness&amp;quot;))

head(predict_result)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  Neuroticism Conscientiousness Extraversion Agreeableness  Opennness
1 -0.21410935        0.06924675  -1.33208860   -0.85364725 -1.5809244
2  0.15008464        0.48139729  -0.59950262   -0.08478873 -0.1876070
3  0.62827949        0.10964162  -0.04800816   -0.55616873  0.2502735
4 -0.09425827        0.03836489  -1.05089539   -0.10394941 -1.1000032
5 -0.16368420        0.44253657  -0.10519669   -0.71857460 -0.6612203
6  0.18984314        1.08439177   1.40730835    0.39278790  0.6222356&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="how-many-factors-should-we-retain"&gt;How many factors should we retain?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;When running a factor analysis (FA), one often needs to specify how many components (or latent variables) to retain or to extract. This decision is often supported by some statistical indices and procedures aiming at finding the optimal number of factors, e.g., scree plot from (&lt;code&gt;scree()&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Interestingly, a huge amount of methods exist to statistically address this issue. These methods can sometimes contradict with each other in terms of retained factor. As a result, seeking the number that is supported by most methods is a reasonable compromise.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="the-method-agreement-procedure"&gt;The Method Agreement procedure&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The Method Agreement procedure, first implemented in the &lt;code&gt;psycho&lt;/code&gt; package, proposes to rely on the consensus of methods, rather than on one method in particular. This procedure can be used through the &lt;code&gt;n_factors()&lt;/code&gt; by providing a dataframe, and the function will run a large number of routines and return the optimal number of factors based on the higher consensus.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;n_factor &amp;lt;- parameters::n_factors(data_complete)

n_factor&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Method Agreement Procedure:

The choice of 6 dimensions is supported by 4 (17.39%) methods out of 23 (Optimal coordinates, Parallel analysis, Kaiser criterion, SE Scree).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;as.data.frame(n_factor)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   n_Factors              Method              Family
1          1 Acceleration factor               Scree
2          1                 TLI                 Fit
3          1               RMSEA                 Fit
4          3                 CNG                 CNG
5          4                beta Multiple_regression
6          4                  R2            Scree_SE
7          4    VSS complexity 1                 VSS
8          5    VSS complexity 2                 VSS
9          5       Velicer&amp;#39;s MAP        Velicers_MAP
10         6 Optimal coordinates               Scree
11         6   Parallel analysis               Scree
12         6    Kaiser criterion               Scree
13         6            SE Scree            Scree_SE
14         7                   t Multiple_regression
15         7                   p Multiple_regression
16         8                 BIC                 BIC
17         8                 BIC                 Fit
18        12      BIC (adjusted)                 BIC
19        18                CRMS                 Fit
20        22             Bentler             Bentler
21        24            Bartlett             Barlett
22        24            Anderson             Barlett
23        24              Lawley             Barlett&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;For more details, a summary table can be obtained&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;summary(n_factor)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   n_Factors n_Methods
1          1         3
2          3         1
3          4         3
4          5         2
5          6         4
6          7         2
7          8         2
8         12         1
9         18         1
10        22         1
11        24         3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(n_factor) + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b8446d81ab3_files/figure-html/unnamed-chunk-19-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interestingly, most methods also suggest six factors from the model, which is consistent with the &lt;a href="https://en.wikipedia.org/wiki/HEXACO_model_of_personality_structure"&gt;HEXACO model&lt;/a&gt; of personalities that is similar with the Big 5 personality theory.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="confirmatory-factor-analysis"&gt;Confirmatory Factor Analysis&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We’ve seen above that while an EFA with 5 latent variables works great on our dataset, a structure with 6 latent factors may be statistically viable as well. This topic can be statistically tested with a CFA to bridge factor analysis with Structural Equation Modelling (SEM).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, in order to do that cleanly, EFA should be independent from CFA in the sense that the factor structure should be explored on a &lt;strong&gt;training&lt;/strong&gt; set, and then tested (or “confirmed”) on a &lt;strong&gt;test&lt;/strong&gt; set.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="train-test-split"&gt;Train test split&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The data can be easily split into two sets with the data_partition() function, through which we will use 70% of the sample as the training and the rest as the test dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;partitions &amp;lt;- data_partition(bfi_CFA, training_proportion = 0.7, 
                             seed = 999)

training &amp;lt;- partitions$training
test &amp;lt;- partitions$test&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="create-cfa-structures-out-of-efa-models"&gt;Create CFA structures out of EFA models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In the next step, we will run two EFA models on the training set and specifying 5 and 6 latent factors respectively. We will also request for path diagram of the models as well as their density plot of factor score.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;First, we will examine structure of the 5 factors model.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;structure_big5 &amp;lt;- psych::fa(training, nfactors = 5)
fa.diagram(structure_big5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b8446d81ab3_files/figure-html/unnamed-chunk-21-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(density(structure_big5$scores, na.rm = TRUE), 
     main = &amp;quot;Factor Scores&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b8446d81ab3_files/figure-html/unnamed-chunk-22-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Then, for the structure of the 6 factors model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;structure_big6 &amp;lt;- psych::fa(training, nfactors = 6) 
fa.diagram(structure_big6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b8446d81ab3_files/figure-html/unnamed-chunk-23-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(density(structure_big6$scores, na.rm = TRUE), 
     main = &amp;quot;Factor Scores&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b8446d81ab3_files/figure-html/unnamed-chunk-24-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We will then transform both EFA models into &lt;code&gt;lavaan&lt;/code&gt; syntax to perform CFA.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#Converting EFA into a lavaan-ready syntax
cfa_big5 &amp;lt;- efa_to_cfa(structure_big5)

#Investigate how the model looks
cfa_big5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Latent variables
MR2 =~ N1 + N2 + N3 + N4 + N5
MR5 =~ A1 + A2 + A3 + A4 + A5
MR1 =~ E1 + E2 + E3 + E4 + E5
MR3 =~ C1 + C2 + C3 + C4 + C5
MR4 =~ O1 + O2 + O3 + O4 + O5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;cfa_big6 &amp;lt;- efa_to_cfa(structure_big6)
cfa_big6&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# Latent variables
MR2 =~ N1 + N2 + N3 + N4 + N5
MR5 =~ A1 + A2 + A3 + A4 + A5
MR3 =~ C1 + C2 + C3 + C4 + C5
MR1 =~ E1 + E2 + E4 + E5 + O4
MR4 =~ E3 + O1 + O2 + O3 + O5&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="fit-and-compare-models"&gt;Fit and Compare models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Next, we will fit both models with lavaan package before requesting for model fit measure with &lt;code&gt;fitmeasures&lt;/code&gt;. We can also compare both models head-to-head with &lt;code&gt;compare_performance&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;model_big5 &amp;lt;- lavaan::cfa(cfa_big5, data = test)
model_big6 &amp;lt;- lavaan::cfa(cfa_big6, data = test)

fitmeasures(model_big5, fit.measures = &amp;quot;all&amp;quot;, output = &amp;quot;text&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Model Test User Model:

  Test statistic                              1043.136
  Degrees of freedom                               265
  P-value                                        0.000

Model Test Baseline Model:

  Test statistic                              3513.829
  Degrees of freedom                               300
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.758
  Tucker-Lewis Index (TLI)                       0.726
  Bentler-Bonett Non-normed Fit Index (NNFI)     0.726
  Bentler-Bonett Normed Fit Index (NFI)          0.703
  Parsimony Normed Fit Index (PNFI)              0.621
  Bollen&amp;#39;s Relative Fit Index (RFI)              0.664
  Bollen&amp;#39;s Incremental Fit Index (IFI)           0.760
  Relative Noncentrality Index (RNI)             0.758

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)             -17201.035
  Loglikelihood unrestricted model (H1)     -16679.468
                                                      
  Akaike (AIC)                               34522.071
  Bayesian (BIC)                             34764.629
  Sample-size adjusted Bayesian (BIC)        34574.230

Root Mean Square Error of Approximation:

  RMSEA                                          0.084
  90 Percent confidence interval - lower         0.078
  90 Percent confidence interval - upper         0.089
  P-value RMSEA &amp;lt;= 0.05                          0.000

Standardized Root Mean Square Residual:

  RMR                                            0.174
  RMR (No Mean)                                  0.174
  SRMR                                           0.086

Other Fit Indices:

  Hoelter Critical N (CN) alpha = 0.05         123.679
  Hoelter Critical N (CN) alpha = 0.01         130.746
                                                      
  Goodness of Fit Index (GFI)                    0.815
  Adjusted Goodness of Fit Index (AGFI)          0.773
  Parsimony Goodness of Fit Index (PGFI)         0.665
                                                      
  McDonald Fit Index (MFI)                       0.397
                                                      
  Expected Cross-Validation Index (ECVI)         2.763&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;fitmeasures(model_big6, fit.measures = &amp;quot;all&amp;quot;, output = &amp;quot;text&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Model Test User Model:

  Test statistic                              1132.134
  Degrees of freedom                               265
  P-value                                        0.000

Model Test Baseline Model:

  Test statistic                              3513.829
  Degrees of freedom                               300
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.730
  Tucker-Lewis Index (TLI)                       0.695
  Bentler-Bonett Non-normed Fit Index (NNFI)     0.695
  Bentler-Bonett Normed Fit Index (NFI)          0.678
  Parsimony Normed Fit Index (PNFI)              0.599
  Bollen&amp;#39;s Relative Fit Index (RFI)              0.635
  Bollen&amp;#39;s Incremental Fit Index (IFI)           0.733
  Relative Noncentrality Index (RNI)             0.730

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)             -17245.534
  Loglikelihood unrestricted model (H1)     -16679.468
                                                      
  Akaike (AIC)                               34611.069
  Bayesian (BIC)                             34853.627
  Sample-size adjusted Bayesian (BIC)        34663.228

Root Mean Square Error of Approximation:

  RMSEA                                          0.088
  90 Percent confidence interval - lower         0.083
  90 Percent confidence interval - upper         0.093
  P-value RMSEA &amp;lt;= 0.05                          0.000

Standardized Root Mean Square Residual:

  RMR                                            0.181
  RMR (No Mean)                                  0.181
  SRMR                                           0.092

Other Fit Indices:

  Hoelter Critical N (CN) alpha = 0.05         114.036
  Hoelter Critical N (CN) alpha = 0.01         120.546
                                                      
  Goodness of Fit Index (GFI)                    0.802
  Adjusted Goodness of Fit Index (AGFI)          0.757
  Parsimony Goodness of Fit Index (PGFI)         0.654
                                                      
  McDonald Fit Index (MFI)                       0.357
                                                      
  Expected Cross-Validation Index (ECVI)         2.974&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;model_comparison &amp;lt;-performance::compare_performance(model_big5, model_big6)
rmarkdown::paged_table(model_comparison)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable="false"&gt;
&lt;script data-pagedtable-source type="application/json"&gt;
{"columns":[{"label":["Name"],"name":[1],"type":["chr"],"align":["left"]},{"label":["Model"],"name":[2],"type":["chr"],"align":["left"]},{"label":["Chi2"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Chi2_df"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["p_Chi2"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["Baseline"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["Baseline_df"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["p_Baseline"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["GFI"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["AGFI"],"name":[10],"type":["dbl"],"align":["right"]},{"label":["NFI"],"name":[11],"type":["dbl"],"align":["right"]},{"label":["NNFI"],"name":[12],"type":["dbl"],"align":["right"]},{"label":["CFI"],"name":[13],"type":["dbl"],"align":["right"]},{"label":["RMSEA"],"name":[14],"type":["dbl"],"align":["right"]},{"label":["RMSEA_CI_low"],"name":[15],"type":["dbl"],"align":["right"]},{"label":["RMSEA_CI_high"],"name":[16],"type":["dbl"],"align":["right"]},{"label":["p_RMSEA"],"name":[17],"type":["dbl"],"align":["right"]},{"label":["RMR"],"name":[18],"type":["dbl"],"align":["right"]},{"label":["SRMR"],"name":[19],"type":["dbl"],"align":["right"]},{"label":["RFI"],"name":[20],"type":["dbl"],"align":["right"]},{"label":["PNFI"],"name":[21],"type":["dbl"],"align":["right"]},{"label":["IFI"],"name":[22],"type":["dbl"],"align":["right"]},{"label":["RNI"],"name":[23],"type":["dbl"],"align":["right"]},{"label":["Loglikelihood"],"name":[24],"type":["dbl"],"align":["right"]},{"label":["AIC"],"name":[25],"type":["dbl"],"align":["right"]},{"label":["AIC_wt"],"name":[26],"type":["dbl"],"align":["right"]},{"label":["BIC"],"name":[27],"type":["dbl"],"align":["right"]},{"label":["BIC_wt"],"name":[28],"type":["dbl"],"align":["right"]},{"label":["BIC_adjusted"],"name":[29],"type":["dbl"],"align":["right"]}],"data":[{"1":"model_big5","2":"lavaan","3":"1043.136","4":"265","5":"0","6":"3513.829","7":"300","8":"0","9":"0.8152806","10":"0.7734573","11":"0.7031342","12":"0.7259007","13":"0.7578790","14":"0.08351484","15":"0.07822173","16":"0.08887477","17":"1.887379e-15","18":"0.1743771","19":"0.08600019","20":"0.6639255","21":"0.6211019","22":"0.7604874","23":"0.7578790","24":"-17201.04","25":"34522.07","26":"1.000000e+00","27":"34764.63","28":"1.000000e+00","29":"34574.23"},{"1":"model_big6","2":"lavaan","3":"1132.134","4":"265","5":"0","6":"3513.829","7":"300","8":"0","9":"0.8021582","10":"0.7573638","11":"0.6778062","12":"0.6945510","13":"0.7301867","14":"0.08816152","15":"0.08290861","16":"0.09348178","17":"3.719247e-14","18":"0.1809260","19":"0.09247950","20":"0.6352523","21":"0.5987288","22":"0.7330934","23":"0.7301867","24":"-17245.53","25":"34611.07","26":"4.723836e-20","27":"34853.63","28":"4.723836e-20","29":"34663.23"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The model comparison indicated that both model are empirically viable, but we would need a different dataset to re-assess the 6 factor model with appropriate item distribution (i.e., each factor has its own dedicated variables).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Another thing that should be noted is our conclusion should be theoretically permissible; that is, whether we use the big 5 or big 6 models, we should have theories and evidence to support our decision. Otherwise, if we only rely on statistical results, we can use 24 personality factors models because the result said it has the second-most approval rate by method consensus, but that would not make any theoretical sense.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For social science research where we care about the “how”, our decisions should be theoretically-driven, so that we can explain the way our model works in both statistical and theoretical sense.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Anyway, this is all for this post. Thank you so much for reading this as always. The next semester is on around the corner, but I still have some time to write my blog before going back to the regularly-scheduled paper writing. Also, Happy New Year, everyone! Let us do our best in both personal and professional endeavor.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>91399b5a2be494d92b5d7d707afca65d</distill:md5>
      <category>R</category>
      <category>Statistics</category>
      <guid>https://taridwong.github.io/posts/2022-01-01-efacfa</guid>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-01-01-efacfa/efacfa_files/figure-html5/unnamed-chunk-8-1.png" medium="image" type="image/png" width="2880" height="2880"/>
    </item>
    <item>
      <title>Measuring Text Similarity with Movie plot data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-29-movie-similarity</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;One characteristic of textual data in the real world setting is that most of them possess meaning that to convey to their intended audience. The meaning of one message could be similar to another when they are crafted for similar purposes. With the right tool, we can identify such similarities and visualize them to extract insights from textual data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The data set I will use in this post are movie plot summaries available on IMDb and Wikipedia. Here, I will quantify the similarity of movies based on their plot and separate them into groups before plotting them on a dendrogram to represent how closely the movies are related to each other.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As always, we will begin by importing necessary modules and dataset.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Import modules
import numpy as np
import pandas as pd
import nltk

# Set seed for reproducibility
np.random.seed(5)

# Read in IMDb and Wikipedia movie data (both in the same file)
movies_df = pd.read_csv(&amp;quot;movies.csv&amp;quot;)

print(&amp;quot;Number of movies loaded: %s &amp;quot; % (len(movies_df)))

# Display the data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Number of movies loaded: 100 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;movies_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    rank  ...                                          imdb_plot
0      0  ...  In late summer 1945, guests are gathered for t...
1      1  ...  In 1947, Andy Dufresne (Tim Robbins), a banker...
2      2  ...  The relocation of Polish Jews from surrounding...
3      3  ...  The film opens in 1964, where an older and fat...
4      4  ...  In the early years of World War II, December 1...
..   ...  ...                                                ...
95    95  ...  Shortly after moving to Los Angeles with his p...
96    96  ...  L.B. &amp;quot;Jeff&amp;quot; Jeffries (James Stewart) recuperat...
97    97  ...  Sights of Vienna, Austria, flash across the sc...
98    98  ...  At the end of an ordinary work day, advertisin...
99    99  ...                                                NaN

[100 rows x 5 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="combine-wikipedia-and-imdb-plot-summaries"&gt;Combine Wikipedia and IMDb plot summaries&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The dataset we imported currently contains two columns titled &lt;code&gt;wiki_plot&lt;/code&gt; and &lt;code&gt;imdb_plot&lt;/code&gt;. They are the plot found for the movies on Wikipedia and IMDb, respectively. The text in the two columns is similar, however, they are often written in different tones and thus provide context on a movie in a different manner of linguistic expression. Further, sometimes the text in one column may mention a feature of the plot that is not present in the other column. For example, consider the following plot extracts from &lt;em&gt;The Godfather&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Wikipedia: “On the day of his only daughter’s wedding, Vito Corleone…”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IMDb: “In late summer 1945, guests are gathered for the wedding reception of Don Vito Corleone’s daughter Connie”&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;While the Wikipedia plot only mentions it is the day of the daughter’s wedding, the IMDb plot also mentions the year of the scene and the name of the daughter. We can combine them to avoid the overheads in computation associated with extra columns to process.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Combine wiki_plot and imdb_plot into a single column
movies_df[&amp;quot;plot&amp;quot;] = movies_df[&amp;quot;wiki_plot&amp;quot;].astype(str) + &amp;quot;\n&amp;quot; + \
                    movies_df[&amp;quot;imdb_plot&amp;quot;].astype(str)
                    
movies_df.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   rank  ...                                               plot
0     0  ...  On the day of his only daughter&amp;#39;s wedding, Vit...
1     1  ...  In 1947, banker Andy Dufresne is convicted of ...
2     2  ...  In 1939, the Germans move Polish Jews into the...
3     3  ...  In a brief scene in 1964, an aging, overweight...
4     4  ...  It is early December 1941. American expatriate...

[5 rows x 6 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="tokenization"&gt;Tokenization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Tokenization is the process by which we break down articles into individual sentences or words, as needed. We can also use the regular expression (Regex) method to remove tokens that are entirely numeric values or punctuation to retain only words with meaning.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As an example, we will perform tokenization on a part of Godfather’s plot. Notice that quotation marks and numbers were removed in the output.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Tokenize a paragraph into sentences and store in sent_tokenized
sent_tokenized = [sent for sent in nltk.sent_tokenize(&amp;quot;&amp;quot;&amp;quot;
                        Today (May 19, 2016) is his only daughter&amp;#39;s wedding. 
                        Vito Corleone is the Godfather.
                        &amp;quot;&amp;quot;&amp;quot;)]
                        
# Word Tokenize first sentence from sent_tokenized, save as words_tokenized

words_tokenized = [word for word in nltk.word_tokenize(sent_tokenized[0])]

# Remove tokens that do not contain any letters from words_tokenized
import re

filtered = [word for word in words_tokenized if re.search(&amp;#39;[a-zA-Z]&amp;#39;, word)]

# Display filtered words to observe words after tokenization
filtered&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;Today&amp;#39;, &amp;#39;May&amp;#39;, &amp;#39;is&amp;#39;, &amp;#39;his&amp;#39;, &amp;#39;only&amp;#39;, &amp;#39;daughter&amp;#39;, &amp;quot;&amp;#39;s&amp;quot;, &amp;#39;wedding&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="stemming"&gt;Stemming&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Stemming is the process by which we bring down a word from its different forms to the root word (or to stem). This helps us establish meaning to different forms of the same words without having to deal with each form separately. For example, the words ‘fishing’, ‘fished’, and ‘fisher’ all get stemmed to the word ‘fish’.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Consider the following sentences:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;“Young William Wallace witnesses the treachery of Longshanks” - &lt;em&gt;Gladiator&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“escapes to the city walls only to witness Cicero’s death” - &lt;em&gt;Braveheart&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Instead of building separate dictionary entries for both witnesses and witness, which mean the same thing outside of quantity, stemming them reduces them to ‘wit’.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There are different algorithms available for stemming such as the Porter Stemmer and Snowball Stemmer. Here, we will use Snowball Stemmer.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Import the SnowballStemmer to perform stemming
from nltk.stem.snowball import SnowballStemmer

# Create an English language SnowballStemmer object
stemmer = SnowballStemmer(&amp;quot;english&amp;quot;)

# Print filtered to observe words without stemming
print(&amp;quot;Without stemming: &amp;quot;, filtered)

# Stem the words from filtered and store in stemmed_words&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Without stemming:  [&amp;#39;Today&amp;#39;, &amp;#39;May&amp;#39;, &amp;#39;is&amp;#39;, &amp;#39;his&amp;#39;, &amp;#39;only&amp;#39;, &amp;#39;daughter&amp;#39;, &amp;quot;&amp;#39;s&amp;quot;, &amp;#39;wedding&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;stemmed_words = [stemmer.stem(t) for t in filtered]

# Print the stemmed_words to observe words after stemming
print(&amp;quot;After stemming:   &amp;quot;, stemmed_words)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;After stemming:    [&amp;#39;today&amp;#39;, &amp;#39;may&amp;#39;, &amp;#39;is&amp;#39;, &amp;#39;his&amp;#39;, &amp;#39;onli&amp;#39;, &amp;#39;daughter&amp;#39;, &amp;quot;&amp;#39;s&amp;quot;, &amp;#39;wed&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="tokenization-and-stemming-together"&gt;Tokenization and Stemming together&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We are now able to tokenize and stem sentences. But we may have to use the two functions repeatedly one after the other to handle a large amount of data, hence we can think of wrapping them in a function and passing the text to be tokenized and stemmed as the function argument. Then we can pass the new wrapping function, which shall perform both tokenizing and stemming instead of just tokenizing, as the tokenizer argument while creating the TF-IDF vector of the text (we will get there to what TF-IDF means).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All the words are in their root form, which will lead to a better establishment of meaning as some of the non-root forms may not be present in the NLTK training corpus.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Define a function to perform both stemming and tokenization
def tokenize_and_stem(text):
    
    # Tokenize by sentence, then by word
    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]
    
    # Filter out raw tokens to remove noise
    filtered_tokens = [token for token in tokens if re.search(&amp;#39;[a-zA-Z]&amp;#39;, token)]
    
    # Stem the filtered_tokens
    stems = [stemmer.stem(t) for t in filtered_tokens]
    
    return stems

words_stemmed = tokenize_and_stem(&amp;quot;Today (May 19, 2016) is his only daughter&amp;#39;s wedding.&amp;quot;)
print(words_stemmed)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;#39;today&amp;#39;, &amp;#39;may&amp;#39;, &amp;#39;is&amp;#39;, &amp;#39;his&amp;#39;, &amp;#39;onli&amp;#39;, &amp;#39;daughter&amp;#39;, &amp;quot;&amp;#39;s&amp;quot;, &amp;#39;wed&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="create-tf-idf-vectorizer"&gt;Create TF-IDF Vectorizer&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Computers do not &lt;em&gt;understand&lt;/em&gt; text. These are machines only capable of understanding numbers and performing numerical computation. Hence, we must convert our textual plot summaries to numbers for the computer to be able to extract meaning from them. One simple method of doing this would be to count all the occurrences of each word in the entire vocabulary and return the counts in a vector. This method is called &lt;code&gt;CountVectorizer.&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Consider the word ‘the’. It appears quite frequently in almost all movie plots and will have a high count in each case. However, “the” could hardly be counted as the movie plot itself. For that, &lt;a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf"&gt;Term Frequency-Inverse Document Frequency&lt;/a&gt; (TF-IDF) is one method that overcomes the shortcomings of &lt;code&gt;CountVectorizer&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In TF-IDF, frequency of a word is the measure of how often it appears in a document, while the Inverse Document Frequency is the parameter which reduces the importance of a word if it frequently appears in several documents. In simplest terms, TF-IDF recognizes words which are unique and important to any given document.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Import TfidfVectorizer to create TF-IDF vectors
from sklearn.feature_extraction.text import TfidfVectorizer

# Instantiate TfidfVectorizer object with stopwords and tokenizer
# parameters for efficient processing of text
tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,
                                 min_df=0.2, stop_words=&amp;#39;english&amp;#39;,
                                 use_idf=True, tokenizer=tokenize_and_stem,
                                 ngram_range=(1,3))&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="fit-transform-tf-idf-vectorizer"&gt;Fit transform TF-IDF Vectorizer&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Once we create a TF-IDF Vectorizer, we must fit the text to it and then transform the text to produce the corresponding numeric form of the data which the computer will be able to understand and derive meaning from. To do this, we use the &lt;code&gt;fit_transform()&lt;/code&gt; method of the &lt;code&gt;TfidfVectorizer&lt;/code&gt; object.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the TF-IDF object, there is a parameter called &lt;code&gt;stopwords&lt;/code&gt;. Stopwords are those words in a given text which do not contribute considerably towards the meaning of the sentence and are generally grammatical filler words. For example, in the sentence ‘Dorothy Gale lives with her dog Toto on the farm of her Aunt Em and Uncle Henry’, we could drop the words ‘her’ and ‘the’, and still have a similar overall meaning to the sentence. Thus, ‘her’ and ‘the’ are stopwords and can be conveniently dropped from the sentence.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On setting the stopwords to ‘english’, we direct the vectorizer to drop all stopwords from a pre-defined list of English language stopwords present in the nltk module. Another parameter, &lt;code&gt;ngram_range&lt;/code&gt;, defines the length of the ngrams to be formed while vectorizing the text.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Fit and transform the tfidf_vectorizer with the &amp;quot;plot&amp;quot; of each movie
# to create a vector representation of the plot summaries
tfidf_matrix = tfidf_vectorizer.fit_transform([x for x in movies_df[&amp;quot;plot&amp;quot;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;C:\Users\tarid\AppData\Roaming\Python\Python38\site-packages\sklearn\feature_extraction\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [&amp;#39;abov&amp;#39;, &amp;#39;afterward&amp;#39;, &amp;#39;alon&amp;#39;, &amp;#39;alreadi&amp;#39;, &amp;#39;alway&amp;#39;, &amp;#39;ani&amp;#39;, &amp;#39;anoth&amp;#39;, &amp;#39;anyon&amp;#39;, &amp;#39;anyth&amp;#39;, &amp;#39;anywher&amp;#39;, &amp;#39;becam&amp;#39;, &amp;#39;becaus&amp;#39;, &amp;#39;becom&amp;#39;, &amp;#39;befor&amp;#39;, &amp;#39;besid&amp;#39;, &amp;#39;cri&amp;#39;, &amp;#39;describ&amp;#39;, &amp;#39;dure&amp;#39;, &amp;#39;els&amp;#39;, &amp;#39;elsewher&amp;#39;, &amp;#39;empti&amp;#39;, &amp;#39;everi&amp;#39;, &amp;#39;everyon&amp;#39;, &amp;#39;everyth&amp;#39;, &amp;#39;everywher&amp;#39;, &amp;#39;fifti&amp;#39;, &amp;#39;forti&amp;#39;, &amp;#39;henc&amp;#39;, &amp;#39;hereaft&amp;#39;, &amp;#39;herebi&amp;#39;, &amp;#39;howev&amp;#39;, &amp;#39;hundr&amp;#39;, &amp;#39;inde&amp;#39;, &amp;#39;mani&amp;#39;, &amp;#39;meanwhil&amp;#39;, &amp;#39;moreov&amp;#39;, &amp;#39;nobodi&amp;#39;, &amp;#39;noon&amp;#39;, &amp;#39;noth&amp;#39;, &amp;#39;nowher&amp;#39;, &amp;#39;onc&amp;#39;, &amp;#39;onli&amp;#39;, &amp;#39;otherwis&amp;#39;, &amp;#39;ourselv&amp;#39;, &amp;#39;perhap&amp;#39;, &amp;#39;pleas&amp;#39;, &amp;#39;sever&amp;#39;, &amp;#39;sinc&amp;#39;, &amp;#39;sincer&amp;#39;, &amp;#39;sixti&amp;#39;, &amp;#39;someon&amp;#39;, &amp;#39;someth&amp;#39;, &amp;#39;sometim&amp;#39;, &amp;#39;somewher&amp;#39;, &amp;#39;themselv&amp;#39;, &amp;#39;thenc&amp;#39;, &amp;#39;thereaft&amp;#39;, &amp;#39;therebi&amp;#39;, &amp;#39;therefor&amp;#39;, &amp;#39;togeth&amp;#39;, &amp;#39;twelv&amp;#39;, &amp;#39;twenti&amp;#39;, &amp;#39;veri&amp;#39;, &amp;#39;whatev&amp;#39;, &amp;#39;whenc&amp;#39;, &amp;#39;whenev&amp;#39;, &amp;#39;wherea&amp;#39;, &amp;#39;whereaft&amp;#39;, &amp;#39;wherebi&amp;#39;, &amp;#39;wherev&amp;#39;, &amp;#39;whi&amp;#39;, &amp;#39;yourselv&amp;#39;] not in stop_words.
  warnings.warn(&amp;#39;Your stop_words may be inconsistent with &amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(tfidf_matrix.shape)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(100, 564)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="import-k-means-and-create-clusters"&gt;Import K-Means and create clusters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To determine how closely one movie is related to the other by the help of unsupervised machine learning, we can use clustering techniques. Clustering is the method of grouping together a number of items such that they exhibit similar properties. According to the measure of similarity desired, a given sample of items can have one or more clusters.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A good basis of clustering in our data set could be the genre of the movies. Say we could have a cluster ‘0’ which holds movies of the ‘Drama’ genre, and ‘1’ for the ‘Adventure’ genre.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;K-means is an algorithm which helps us to implement clustering in Python. The name derives from its method of implementation: the given sample is divided into &lt;strong&gt;&lt;em&gt;K&lt;/em&gt;&lt;/strong&gt; clusters where each cluster is denoted by the &lt;strong&gt;&lt;em&gt;mean&lt;/em&gt;&lt;/strong&gt; of all the items lying in that cluster.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Here, we will examine how many movies we have in each of the five clusters we specified; then, we will visualize them with a category plot.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Import k-means to perform clustering
from sklearn.cluster import KMeans

# Create a KMeans object with 5 clusters and save as km
km = KMeans(n_clusters=5)

# Fit the k-means object with tfidf_matrix
km.fit(tfidf_matrix)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;KMeans(n_clusters=5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;clusters = km.labels_.tolist()

# Create a column cluster to denote the generated cluster for each movie
movies_df[&amp;quot;cluster&amp;quot;] = clusters

# Display number of films per cluster (clusters from 0 to 4)
movies_df[&amp;#39;cluster&amp;#39;].value_counts() &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3    31
1    27
0    22
4    13
2     7
Name: cluster, dtype: int64&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;import seaborn as sns
import matplotlib.pyplot as plt

#convert the cluster list into a dataframe
clusters_df = pd.DataFrame(clusters, columns = [&amp;#39;cluster_group&amp;#39;])

sns.set_theme(style=&amp;quot;whitegrid&amp;quot;)
sns.catplot(x=&amp;quot;cluster_group&amp;quot;, kind=&amp;quot;count&amp;quot;, data=clusters_df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b8411d42bbc_files/figure-html/unnamed-chunk-11-1.png" width="242" data-distill-preview=1 /&gt;&lt;/p&gt;
&lt;h2 id="calculate-similarity-distance"&gt;Calculate similarity distance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;By using &lt;code&gt;countvectorizer&lt;/code&gt;, we can turn a sentence into numbers for the computer to calculate similarity distance with the cosine similarity measurement (it is basically a number that indicates how closely related the two sets of words are).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Import cosine_similarity to calculate similarity of movie plots
from sklearn.metrics.pairwise import cosine_similarity

# Calculate the similarity distance
similarity_distance = 1 - cosine_similarity(tfidf_matrix)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="import-matplotlib-linkage-and-dendrograms"&gt;Import Matplotlib, Linkage, and Dendrograms&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We will then create a dendrogram of the movie title based on its plot similarity to visualize the level of similarity between our data points.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Dendrograms help visualize the results of hierarchical clustering, which is an alternative to k-means clustering. Two pairs of movies at the same level of hierarchical clustering are expected to have similar strength of similarity between the corresponding pairs of movies.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The more similar the two movies are, the closer they will be together as they travel down the dendrogram path. The plot is a little large to accommodate the number of data points, so you might need to zoom in to see which movie is similar to which.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Import modules necessary to plot dendrogram
from scipy.cluster.hierarchy import linkage, dendrogram

# Create mergings matrix 
mergings = linkage(similarity_distance, method=&amp;#39;complete&amp;#39;)

# Plot the dendrogram, using title as label column
dendrogram_ = dendrogram(mergings,
               labels=[x for x in movies_df[&amp;quot;title&amp;quot;]],
               leaf_rotation=90,
               leaf_font_size=16,
)

# Adjust the plot
fig = plt.gcf()
_ = [lbl.set_color(&amp;#39;r&amp;#39;) for lbl in plt.gca().get_xmajorticklabels()]
fig.set_size_inches(120, 50)

# Show the plotted dendrogram
plt.grid(False)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b8411d42bbc_files/figure-html/unnamed-chunk-13-3.png" width="11520" /&gt;&lt;/p&gt;
&lt;h2 id="concluding-remark"&gt;Concluding remark&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;While I am not an expert in movie critique, the movie plot data is a good venue to practice text cleaning with tokenization and stemming. The TF-IDF method is also widely implemented to extract meaningful information from textual data in general. Lastly, clustering is also a useful exploratory machine learning method to gain insights from unlabeled data to inform our decisions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This post combines both Narutal Language Processing and Machine Learning techniques to calculate similarity score between sets of words. This method can be used to establish a groundwork for a recommendation system that we often seen in popular sites such as Netflix or Spotify by grouping movies or musics together to recommend them to users. As always, thank you very much for reading!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>97ed681d6820c89f6bb644175087be55</distill:md5>
      <category>Python</category>
      <category>Natural Language Processing</category>
      <category>Unsupervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-29-movie-similarity</guid>
      <pubDate>Wed, 29 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-29-movie-similarity/movie-similarity_files/figure-html5/unnamed-chunk-11-1.png" medium="image" type="image/png" width="484" height="483"/>
    </item>
    <item>
      <title>Missing Data Analysis</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-27-missingdata</link>
      <description>


&lt;h2 id="looking-for-what-that-is-not-there"&gt;Looking for what that is not there&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;“If I had eight hours to chop down a tree, I’d spend six sharpening my axe.” - Abraham Lincoln via &lt;a href="https://www.guilford.com/books/Principles-and-Practice-of-Structural-Equation-Modeling/Rex-Kline/9781462523344"&gt;Kline (2016)&lt;/a&gt;. This adage is appropriate to set the tone for this post, as well as applicable to most things in general, including working with data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;My professors taught me that real data never works, and my experience attested to their statement countless times as I iterated over the data work procedure of importing, cleaning, model building, model tuning, and communicating results. One thing about it that I used to find frustrating is the data I got if oftentimes incomplete (or partly missing).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Missing data are values that should have been recorded but were not. The best way to treat missing data is not to have them, but unfortunately, real data is oftentimes &lt;del&gt;ugly&lt;/del&gt; unorganized. Missing data could potentially caused by nonresponse in surveys, or technical issues with data-collecting equipment.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;My previous posts were about visualizing data that we have, but this time, we will be visualizing things that we ‘do not’ have (aka missing data), as well as discussing about ways we can deal with them via complete case analysis or imputation.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2021-12-27-missingdatamissingpic.jpg" style="width:40.0%" alt="" /&gt;
&lt;p class="caption"&gt;Photo by Adam Lingelbach&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="import-and-read-the-data-set"&gt;Import and read the data set&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;As usual, we will begin by importing essential libraries and load in the data set to preprocess it.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(foreign) #To read SPSS data
library(tidyverse) #datawork toolbox
library(dlookr) #for missing data diagnosis
library(visdat) #for overall missingness visualization
library(naniar) #for missingness visualization
library(VIM) #for donor-based imputation
library(simputation) #for model-based imputation
library(mice) #for multiple imputation&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;#Import the data set
PISA_CAN &amp;lt;-read.spss(&amp;quot;PISA2018CAN.sav&amp;quot;,to.data.frame = TRUE, use.value.labels = FALSE)

#Subset and rename the variables
PISA_Subsetted &amp;lt;-  PISA_CAN %&amp;gt;% 
  select(REPEAT, FEMALE = ST004D01T, ESCS, DAYSKIP = ST062Q01TA,
         CLASSSKIP = ST062Q02TA, LATE = ST062Q03TA,
         BEINGBULLIED, DISCLIMA, ADAPTIVITY)

#Recode variables into factor
PISA_Subsetted$DAYSKIP &amp;lt;-as.factor(PISA_Subsetted$DAYSKIP)
PISA_Subsetted$CLASSSKIP &amp;lt;-as.factor(PISA_Subsetted$CLASSSKIP)
PISA_Subsetted$LATE &amp;lt;-as.factor(PISA_Subsetted$LATE)
PISA_Subsetted$FEMALE &amp;lt;-as.factor(PISA_Subsetted$FEMALE)
PISA_Subsetted$REPEAT &amp;lt;-as.factor(PISA_Subsetted$REPEAT)

# Renaming factor levels with dplyr
PISA_Subsetted$FEMALE &amp;lt;- recode_factor(PISA_Subsetted$FEMALE, 
                                       &amp;quot;1&amp;quot; = &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot; = &amp;quot;0&amp;quot;)

glimpse(PISA_Subsetted)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Rows: 22,653
Columns: 9
$ REPEAT       &amp;lt;fct&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~
$ FEMALE       &amp;lt;fct&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~
$ ESCS         &amp;lt;dbl&amp;gt; -0.7302, 0.3078, 0.5059, 1.1147, 1.3626, -0.857~
$ DAYSKIP      &amp;lt;fct&amp;gt; 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 2, 1, 2,~
$ CLASSSKIP    &amp;lt;fct&amp;gt; 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 3, 1, 1, 2, 2, 2,~
$ LATE         &amp;lt;fct&amp;gt; 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 3,~
$ BEINGBULLIED &amp;lt;dbl&amp;gt; 1.2618, 1.7669, 0.1462, -0.7823, 0.2907, 0.7703~
$ DISCLIMA     &amp;lt;dbl&amp;gt; -0.4186, -1.4179, 0.6019, -0.4995, -0.1045, 1.0~
$ ADAPTIVITY   &amp;lt;dbl&amp;gt; -0.4708, 0.6350, -0.5786, -0.5786, -0.0763, 0.5~&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The data set in this post was a Canadian student data subsetted from the Programme for Internal Student Assessment (PISA), which is an international assessment that measures 15-year-old students’ reading, mathematics, and science literacy every three years.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;From the &lt;code&gt;glimpse&lt;/code&gt; call above, our dataset has 9 variables and 22,653 data points.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="check-for-missing-data"&gt;Check for missing data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First, we will use the &lt;code&gt;dlookr&lt;/code&gt; package to diagnose missingness of the data set, as well as plot missing data map with &lt;code&gt;vis_miss&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The plot provides a specific visualization of the amount of missing data, showing in black the location of missing values, and also providing information on the overall percentage of missing values overall (in the legend), and in each variable.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;dlookr::diagnose(PISA_Subsetted)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 9 x 6
  variables    types   missing_count missing_percent unique_count
  &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;           &amp;lt;int&amp;gt;           &amp;lt;dbl&amp;gt;        &amp;lt;int&amp;gt;
1 REPEAT       factor           1926         8.50               3
2 FEMALE       factor              2         0.00883            3
3 ESCS         numeric          1163         5.13           15366
4 DAYSKIP      factor           3341        14.7                5
5 CLASSSKIP    factor           3330        14.7                5
6 LATE         factor           3316        14.6                5
7 BEINGBULLIED numeric          3833        16.9               63
8 DISCLIMA     numeric          1300         5.74             934
9 ADAPTIVITY   numeric          2197         9.70              65
# ... with 1 more variable: unique_rate &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;visdat::vis_miss(PISA_Subsetted, sort_miss = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b84658852cb_files/figure-html/unnamed-chunk-5-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If we are curious about the proportion of missing data by groups, we can also group the dataset by our categorical variable of interest, say, gender, before examining the missingness ratio.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_Subsetted %&amp;gt;% group_by (FEMALE) %&amp;gt;%
  miss_var_summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 24 x 4
# Groups:   FEMALE [3]
   FEMALE variable     n_miss pct_miss
   &amp;lt;fct&amp;gt;  &amp;lt;chr&amp;gt;         &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;
 1 1      BEINGBULLIED   1651    14.6 
 2 1      DAYSKIP        1451    12.8 
 3 1      CLASSSKIP      1445    12.8 
 4 1      LATE           1430    12.6 
 5 1      ADAPTIVITY      907     8.02
 6 1      REPEAT          812     7.18
 7 1      DISCLIMA        563     4.98
 8 1      ESCS            527     4.66
 9 0      BEINGBULLIED   2180    19.2 
10 0      DAYSKIP        1888    16.6 
# ... with 14 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="types-of-missing-data"&gt;Types of Missing Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Yes, we know now that our data is missing, but not all missing data are created (or not created, pun wholeheartedly intended) equal. There are three types of missing data, MCAR, MAR, and MNAR.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Missing Completely at Random (MCAR)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Locations of missing values in the dataset are purely random. they do not depend on any other data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For example, if a doctor forgets to record the age of every tenth patient entering an ICU, the presence of missing value would not depend on the characteristic of the patients.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Missing at Random (MAR)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Locations of missing values in the dataset depend on some other, observed data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Data are considered as MAR if the probability of missingness is unrelated to the actual value on that variable after controlling for the other variables in the dataset&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In survey data, high-income respondents are less likely to inform the researcher about the number of properties owned.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Below is an example of MAR missingness. See that &lt;code&gt;sea_temp&lt;/code&gt; and &lt;code&gt;air_temp&lt;/code&gt; are missing at a certain part of the year. Maybe the measuring tools broke down or something before they got them fixed.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;oceanbuoys %&amp;gt;% arrange(year) %&amp;gt;% vis_miss()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b84658852cb_files/figure-html/unnamed-chunk-7-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Missing Not at Random (MNAR)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If it is not MCAR or MAR, it is probably MNAR. This is the most tricky type of missingness to handle.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The missing values depend on both characteristics of the data and also on missing values. In this case, determining the mechanism of the generation of missing value is difficult.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Missing values for a variable like blood pressure may partially depend on the values of blood pressure as patients who have low blood pressure are less likely to get their blood pressure checked at frequently.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="visualize-missing-data"&gt;Visualize Missing data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Okay, now we know what missing data is, and what are types of missing data, here are some ways we can visualize them so that we know their patterns and what they are up to.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="missing-pattern-wupset-plot"&gt;Missing pattern w/Upset plot&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An upset plot from the &lt;code&gt;UpSetR&lt;/code&gt; package can be used to visualize the patterns of missingness, or rather the combinations of missingness across cases and variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;gg_miss_upset(PISA_Subsetted, nsets = 9, nintersects = 15)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b84658852cb_files/figure-html/unnamed-chunk-8-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The small bar plot to the left indicated the amount of missingness in variables. Consistent with the missingness diagnosis, the variable &lt;code&gt;BEINGBULLIED&lt;/code&gt; has the most missing data, following by &lt;code&gt;DAYSKIP&lt;/code&gt; and &lt;code&gt;CLASSKIP&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The dot plot to the right showed combinations of variable that are missing in the data set. For example, there are 1,234 cases that have missing data in the variable &lt;code&gt;LATE&lt;/code&gt;, &lt;code&gt;CLASSKIP&lt;/code&gt;, &lt;code&gt;DAYSKIP&lt;/code&gt;, and &lt;code&gt;BEINGBULLIED&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The parameter &lt;code&gt;nsets&lt;/code&gt; looks at 9 sets of variables, while the parameter &lt;code&gt;nintersects&lt;/code&gt; looks at 15 variable combinations.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="general-visual-summaries-of-missing-data"&gt;General visual summaries of missing data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;This section demonstrates numerous ways to visualize missing data to determine their patterns.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="missingness-in-variables-with-gg_miss_var"&gt;Missingness in variables with &lt;code&gt;gg_miss_var&lt;/code&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;This plot shows the number of missing values in each variable in a dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_Subsetted %&amp;gt;% miss_var_table()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 9 x 3
  n_miss_in_var n_vars pct_vars
          &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;
1             2      1     11.1
2          1163      1     11.1
3          1300      1     11.1
4          1926      1     11.1
5          2197      1     11.1
6          3316      1     11.1
7          3330      1     11.1
8          3341      1     11.1
9          3833      1     11.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;gg_miss_var(PISA_Subsetted, show_pct = TRUE) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b84658852cb_files/figure-html/unnamed-chunk-9-1.png" width="672" /&gt;&lt;/p&gt;
&lt;h4 id="missingness-in-cases-with-gg_miss_case"&gt;Missingness in cases with &lt;code&gt;gg_miss_case&lt;/code&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;This plot shows the number of missing values in each case. For example, the table showed that there are 2 cases with 9 missing variables (i.e., no data in all variables), and there are 1050 cases with 8 missing variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_Subsetted %&amp;gt;% miss_case_table()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# A tibble: 10 x 3
   n_miss_in_case n_cases pct_cases
            &amp;lt;int&amp;gt;   &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;
 1              0   18327  80.9    
 2              1     870   3.84   
 3              2     140   0.618  
 4              3      81   0.358  
 5              4    1254   5.54   
 6              5      83   0.366  
 7              6     756   3.34   
 8              7      90   0.397  
 9              8    1050   4.64   
10              9       2   0.00883&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;gg_miss_case(PISA_Subsetted)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b84658852cb_files/figure-html/unnamed-chunk-10-1.png" width="672" /&gt;&lt;/p&gt;
&lt;h4 id="missingness-across-factors-with-gg_miss_fct"&gt;Missingness across factors with &lt;code&gt;gg_miss_fct&lt;/code&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;This plot shows the number of missingness in each column, broken down by a categorical variable from the dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;gg_miss_fct(x = PISA_Subsetted, fct = REPEAT) + 
  labs(title = &amp;quot;Missing data by the History of Class Repetition&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b84658852cb_files/figure-html/unnamed-chunk-11-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;gg_miss_fct(x = PISA_Subsetted, fct = LATE) + 
  labs(title = &amp;quot;Missing data by Lateness History&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b84658852cb_files/figure-html/unnamed-chunk-11-2.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The heatmap above showed the proportion of missing data we have in each response of the selected categorical variable; for example, the history of class repetition (&lt;code&gt;REPEAT&lt;/code&gt;), with &lt;code&gt;0&lt;/code&gt; as no, &lt;code&gt;1&lt;/code&gt; as yes, and &lt;code&gt;NA&lt;/code&gt; as missing.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="missingness-along-a-repeating-span-with-gg_miss_span"&gt;Missingness along a repeating span with &lt;code&gt;gg_miss_span&lt;/code&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;This plot showed the number of missings in a given span, or breaksize, for a single selected variable.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;gg_miss_span(PISA_Subsetted, REPEAT, span_every = 2000) +
  theme_dark()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b84658852cb_files/figure-html/unnamed-chunk-12-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The plot went over the data and showed us how many missing data we have every 2000 data points that it went through.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="cumulative-missing"&gt;Cumulative missing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;This plot showed the cumulative amount of missing value over the data set. A sharp increase in cumulative missing value could indicate missing patterns to be discovered.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_Subsetted %&amp;gt;% gg_miss_case_cumsum(breaks = 2000) + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b84658852cb_files/figure-html/unnamed-chunk-13-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This plot showed the cumulative amount of missing value over the variable. We could examine the relative proportion of missing values across variables via this plot.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_Subsetted %&amp;gt;% gg_miss_var_cumsum() + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b84658852cb_files/figure-html/unnamed-chunk-14-1.png" width="672" /&gt;&lt;/p&gt;
&lt;h2 id="what-should-we-do-with-the-missing-data"&gt;What should we do with the missing data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Now that we know we have missing data, there are numerous ways we can deal with it such as disregarding them with complete case analysis, or making educated guesses with imputation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Anyway, dealing with missing data helps minimizing bias in the data, maximizing the use of available information (We don’t want to throw away any of our hard-earned data), and increasing the chance of getting a good reliability estimates such as standard errors, confidence intervals, and &lt;em&gt;p&lt;/em&gt;-values.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="complete-case-analysis"&gt;Complete Case Analysis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Listwise deletion&lt;/strong&gt; is the method of deleting all cases with missing value, so that we get a clean and complete data set as a result, at the expense of losing a chunk of data in the process.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Listwise deletion is often a default way to handle missing data (e.g., SPSS).&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;This often results in losing 20% to 50% of the data.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_Listwise &amp;lt;- PISA_Subsetted[complete.cases(PISA_Subsetted), ]
glimpse(PISA_Listwise)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Rows: 18,327
Columns: 9
$ REPEAT       &amp;lt;fct&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~
$ FEMALE       &amp;lt;fct&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~
$ ESCS         &amp;lt;dbl&amp;gt; -0.7302, 0.3078, 0.5059, 1.1147, 1.3626, -0.857~
$ DAYSKIP      &amp;lt;fct&amp;gt; 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 2, 1, 2,~
$ CLASSSKIP    &amp;lt;fct&amp;gt; 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 3, 1, 1, 2, 2, 2,~
$ LATE         &amp;lt;fct&amp;gt; 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 3,~
$ BEINGBULLIED &amp;lt;dbl&amp;gt; 1.2618, 1.7669, 0.1462, -0.7823, 0.2907, 0.7703~
$ DISCLIMA     &amp;lt;dbl&amp;gt; -0.4186, -1.4179, 0.6019, -0.4995, -0.1045, 1.0~
$ ADAPTIVITY   &amp;lt;dbl&amp;gt; -0.4708, 0.6350, -0.5786, -0.5786, -0.0763, 0.5~&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Notice that the size of our dataset got reduced to 18,327 cases. This happaned from deleting all cases with missing value.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Pairwise deletion&lt;/strong&gt; is the method that deletes cases only if they have missing data on variables involved in a particular computation, so we can still retain the data for other analyses that do not involve variables that are missing. However, the effective sample size can vary from one analysis to another.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As a demonstration, we will calculate a covariance matrix using pairwise complete observation method.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;pairwise_var &amp;lt;- c(&amp;quot;BEINGBULLIED&amp;quot;, &amp;quot;DISCLIMA&amp;quot;)
cov(PISA_Subsetted[pairwise_var], use=&amp;quot;pairwise.complete.obs&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;             BEINGBULLIED   DISCLIMA
BEINGBULLIED    1.1315670 -0.1982891
DISCLIMA       -0.1982891  1.1373620&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;HOWEVER&lt;/strong&gt;, the bias caused by using listwise/pairwise deletion has been shown in simulations to grossly exaggerate or underestimate some effects.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Despite giving valid estimates when data are MCAR, the statistical power will be severely reduced when there is a lot of missingness. If the missingness is MAR or MNAR, removing them introduces bias to models built on these data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="missing-data-imputation"&gt;Missing Data Imputation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Other than disregarding them, we can replace the missing value with our best guess with imputation. There are three approaches we can use, donor-based imputation, model-based imputation, and multiple imputation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="donor-based-imputation"&gt;Donor-based imputation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Donor-based imputation replaces missing values based on other complete observations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="mean-imputation"&gt;Mean Imputation&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Mean imputation replaces all missing values with the mean of that variable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;First, we will create a binary indicator for whether each value was originally missing.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_meanimp &amp;lt;- PISA_Subsetted %&amp;gt;%
  mutate(DISCLIMA_imp = ifelse(is.na(DISCLIMA), TRUE, FALSE)) %&amp;gt;%
  mutate(ADAPTIVITY_imp = ifelse(is.na(ADAPTIVITY), TRUE, FALSE))

PISA_meanimp[c(&amp;quot;DISCLIMA_imp&amp;quot;,&amp;quot;ADAPTIVITY_imp&amp;quot;)] %&amp;gt;% head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  DISCLIMA_imp ADAPTIVITY_imp
1        FALSE          FALSE
2        FALSE          FALSE
3        FALSE          FALSE
4        FALSE          FALSE
5        FALSE          FALSE
6        FALSE          FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Replace missing values in DISCLIMA and ADAPTIVITY variables with their respective means.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_meanimp &amp;lt;- PISA_meanimp %&amp;gt;%
mutate(DISCLIMA = ifelse(is.na(DISCLIMA), mean(DISCLIMA, na.rm = TRUE), DISCLIMA)) %&amp;gt;%
mutate(ADAPTIVITY = ifelse(is.na(ADAPTIVITY), mean(ADAPTIVITY, na.rm = TRUE), ADAPTIVITY))

PISA_meanimp %&amp;gt;%select(DISCLIMA, ADAPTIVITY, DISCLIMA_imp, ADAPTIVITY_imp) %&amp;gt;%
head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  DISCLIMA ADAPTIVITY DISCLIMA_imp ADAPTIVITY_imp
1  -0.4186    -0.4708        FALSE          FALSE
2  -1.4179     0.6350        FALSE          FALSE
3   0.6019    -0.5786        FALSE          FALSE
4  -0.4995    -0.5786        FALSE          FALSE
5  -0.1045    -0.0763        FALSE          FALSE
6   1.0832     0.5464        FALSE          FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can try plotting the data on a margin plot to see the result of our mean imputation.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_meanimp %&amp;gt;% select(DISCLIMA, ADAPTIVITY, DISCLIMA_imp, ADAPTIVITY_imp) %&amp;gt;% marginplot(delimiter=&amp;quot;imp&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1b84658852cb_files/figure-html/unnamed-chunk-19-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You can see that all missing values were replaced by the mean of that variable. Yes, we got the data back, but what did it cost?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mean imputation destroys relationship between variables.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Models predicting one using the other will be fooled by the outlying imputed values and will produce biased results.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mean imputation &lt;del&gt;crushes&lt;/del&gt; takes away variance in the data, which could potentially underestimate all standard errors. This prevents reliable hypothesis testing and calculating confidence interval.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This method is not generally recommended, but to each their own. Use it at your own discretion. With the right justification from the literature, mean imputation can be a viable method in your analysis as well.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="k-nearest-neighborknn-imputation"&gt;K-Nearest Neighbor(kNN) Imputation&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For kNN imputation, we identify ‘k’ samples in the dataset that are similar or close in the space. Then we use these ‘k’ samples to estimate the value of the missing data points.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Basically, it is like you have a data point with missing values asks its neighbors what value do they have on the variable that it is missing. That data point then replace its missing values with the value of its nearest neighbor.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_kNNimp &amp;lt;- VIM::kNN(PISA_Subsetted, k = 6, variable = c(&amp;quot;DISCLIMA&amp;quot;, &amp;quot;ADAPTIVITY&amp;quot;))

PISA_kNNimp[c(&amp;quot;DISCLIMA&amp;quot;, &amp;quot;ADAPTIVITY&amp;quot;,&amp;quot;DISCLIMA_imp&amp;quot;,&amp;quot;ADAPTIVITY_imp&amp;quot;)] %&amp;gt;% head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  DISCLIMA ADAPTIVITY DISCLIMA_imp ADAPTIVITY_imp
1  -0.4186    -0.4708        FALSE          FALSE
2  -1.4179     0.6350        FALSE          FALSE
3   0.6019    -0.5786        FALSE          FALSE
4  -0.4995    -0.5786        FALSE          FALSE
5  -0.1045    -0.0763        FALSE          FALSE
6   1.0832     0.5464        FALSE          FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Note that there are two more columns added, DISCLIMA_imp and ADAPTIVITY_imp. The two added columns tell us if our variables of interest were imputed with the kNN method or not, with &lt;code&gt;TRUE&lt;/code&gt; indicates that the value was imputed and &lt;code&gt;FALSE&lt;/code&gt; indicates otherwise.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="model-based-imputation"&gt;Model-based imputation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For model-based imputation, missing values are predicted with a statistical or machine learning model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The model that we used depends on the type of the missing variable:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Continuous variables - linear regression&lt;/li&gt;
&lt;li&gt;Binary variables - logistic regression&lt;/li&gt;
&lt;li&gt;Categorical variables - multinomial logistic regression&lt;/li&gt;
&lt;li&gt;Count variables - Poisson regression&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="linear-regression-imputation"&gt;Linear Regression Imputation&lt;/h5&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_lmreg &amp;lt;- impute_lm(PISA_Subsetted, DISCLIMA + ADAPTIVITY ~.)

PISA_lmreg %&amp;gt;% 
  is.na() %&amp;gt;%
  colSums()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      REPEAT       FEMALE         ESCS      DAYSKIP    CLASSSKIP 
        1926            2         1163         3341         3330 
        LATE BEINGBULLIED     DISCLIMA   ADAPTIVITY 
        3316         3833         1278         2056 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Notice that we have managed to impute some cases of &lt;code&gt;DISCLIMA&lt;/code&gt; and &lt;code&gt;ADAPTIVITY&lt;/code&gt; based on the availability of other variables in the same case. However, if there is no other variable on that case (i.e., complete missing), the model won’t be able to predict the target value as there is no predictor available.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="logistic-regression-imputation"&gt;Logistic Regression Imputation&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Logistic regression imputation is similar to linear regression imputation, with a difference in the nature of missing value (Continuous vs Binary).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;missing_REPEAT &amp;lt;- is.na(PISA_Subsetted$REPEAT)
head(missing_REPEAT, 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[12] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_logregimp &amp;lt;- PISA_Subsetted

logreg_model &amp;lt;- glm(REPEAT ~ DAYSKIP + BEINGBULLIED + ESCS,
                data = PISA_Subsetted, family = binomial)


preds &amp;lt;- predict(logreg_model, type = &amp;quot;response&amp;quot;)

preds &amp;lt;- ifelse(preds &amp;gt;= 0.5, 1, 0)

PISA_logregimp[missing_REPEAT, &amp;quot;REPEAT&amp;quot;] &amp;lt;- preds[missing_REPEAT]


table(preds[missing_REPEAT])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
   0    1 
1669    1 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;table(PISA_Subsetted$REPEAT)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    0     1 
19575  1152 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_logregimp %&amp;gt;% 
  is.na() %&amp;gt;%
  colSums()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      REPEAT       FEMALE         ESCS      DAYSKIP    CLASSSKIP 
         256            2         1163         3341         3330 
        LATE BEINGBULLIED     DISCLIMA   ADAPTIVITY 
        3316         3833         1300         2197 &lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="multiple-imputation-by-chained-equation"&gt;Multiple Imputation by Chained Equation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Multiple Imputation by Chained Equation (MICE) - also known as sequential regression multiple imputation - is an emerging method in dealing with missing values by implementing the imputation multiple times as opposed to the single imputation methods mentioned above &lt;a href="https://onlinelibrary.wiley.com/doi/10.1002/mpr.329"&gt;(Azur et al., 2011)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With the right model, MICE was found to be effective in reducing bias, especially in a large data set with MCAR and MAR. The method basically imputed the missing value with a statistical model (say, linear regression) multiple times for different imputed values before pooling the results together for the final most likely value that the algorithm can come up.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The package &lt;code&gt;mice&lt;/code&gt; has several statistics and machine learning models we can use such as Predictive mean matching (pmm), Classification and Regression Tree (cart), and Random Forest Imputation (rf). Keep in mind that it is a best practice to justify our selected model in missing data imputation to make the analysis as less ‘black box’ as possible for explainability.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For this post, I will use the predictive mean matchmaking method that calculates the predicted value from a randomly drawn set of candidate donors that have the value closest to the missing entry. The assumption is the distribution of the missing cell is the same as the observed data of the candidate donors.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The rationale is that PMM produces little biased estimates when missing data is below 50% and not systematically missing in a large data set &lt;a href="https://stefvanbuuren.name/fimd/sec-pmm.html"&gt;(van Buuren, 2018)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mice_model &amp;lt;- mice(PISA_Subsetted, method=&amp;#39;pmm&amp;#39;, seed = 123)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 iter imp variable
  1   1  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  1   2  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  1   3  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  1   4  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  1   5  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  2   1  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  2   2  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  2   3  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  2   4  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  2   5  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  3   1  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  3   2  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  3   3  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  3   4  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  3   5  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  4   1  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  4   2  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  4   3  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  4   4  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  4   5  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  5   1  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  5   2  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  5   3  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  5   4  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY
  5   5  REPEAT  FEMALE  ESCS  DAYSKIP  CLASSSKIP  LATE  BEINGBULLIED  DISCLIMA  ADAPTIVITY&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;PISA_miceimp &amp;lt;- complete(mice_model)

psych::describe(PISA_Subsetted)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;             vars     n  mean   sd median trimmed  mad   min  max
REPEAT*         1 20727  1.06 0.23   1.00    1.00 0.00  1.00 2.00
FEMALE*         2 22651  1.50 0.50   2.00    1.50 0.00  1.00 2.00
ESCS            3 21490  0.38 0.83   0.48    0.41 0.87 -6.69 4.04
DAYSKIP*        4 19312  1.31 0.66   1.00    1.16 0.00  1.00 4.00
CLASSSKIP*      5 19323  1.44 0.76   1.00    1.27 0.00  1.00 4.00
LATE*           6 19337  1.82 0.98   2.00    1.65 1.48  1.00 4.00
BEINGBULLIED    7 18820  0.17 1.06   0.15    0.03 1.38 -0.78 3.86
DISCLIMA        8 21353 -0.12 1.07  -0.04   -0.11 1.00 -2.71 2.03
ADAPTIVITY      9 20456  0.18 1.08   0.19    0.20 0.98 -2.27 2.01
             range  skew kurtosis   se
REPEAT*       1.00  3.88    13.05 0.00
FEMALE*       1.00  0.00    -2.00 0.00
ESCS         10.72 -0.47     0.85 0.01
DAYSKIP*      3.00  2.41     5.80 0.00
CLASSSKIP*    3.00  1.84     2.87 0.01
LATE*         3.00  1.01    -0.06 0.01
BEINGBULLIED  4.64  0.88     0.31 0.01
DISCLIMA      4.75 -0.13     0.16 0.01
ADAPTIVITY    4.27 -0.18    -0.19 0.01&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;psych::describe(PISA_miceimp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;             vars     n  mean   sd median trimmed  mad   min  max
REPEAT*         1 22653  1.06 0.23   1.00    1.00 0.00  1.00 2.00
FEMALE*         2 22653  1.50 0.50   2.00    1.50 0.00  1.00 2.00
ESCS            3 22653  0.38 0.83   0.48    0.41 0.86 -6.69 4.04
DAYSKIP*        4 22653  1.32 0.67   1.00    1.17 0.00  1.00 4.00
CLASSSKIP*      5 22653  1.45 0.76   1.00    1.27 0.00  1.00 4.00
LATE*           6 22653  1.83 0.98   2.00    1.66 1.48  1.00 4.00
BEINGBULLIED    7 22653  0.18 1.07   0.15    0.04 1.38 -0.78 3.86
DISCLIMA        8 22653 -0.12 1.07  -0.04   -0.11 1.00 -2.71 2.03
ADAPTIVITY      9 22653  0.17 1.09   0.19    0.20 0.98 -2.27 2.01
             range  skew kurtosis   se
REPEAT*       1.00  3.84    12.74 0.00
FEMALE*       1.00  0.00    -2.00 0.00
ESCS         10.72 -0.47     0.84 0.01
DAYSKIP*      3.00  2.38     5.62 0.00
CLASSSKIP*    3.00  1.83     2.81 0.01
LATE*         3.00  1.00    -0.09 0.01
BEINGBULLIED  4.64  0.88     0.35 0.01
DISCLIMA      4.75 -0.13     0.15 0.01
ADAPTIVITY    4.27 -0.17    -0.20 0.01&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The message above shows that the algorithm went over the data set 5 times per iteration, with the total of 25 times in 5 iterations (5 x 5). In other words, the machine imputed the missing over and over again until the change becomes minimal to give us the most stable replacement value as possible.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There is no substantial difference in descriptive statistics of the pre-imputed and post imputed data set. Given that we gained 10% of our data back, it is a win for us.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="concluding-remark"&gt;Concluding Remark&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Data cleaning is a challenging, but necessary, process in data work. That is why it is important for us to know how to identify and deal with missing data appropriately before proceeding further into developing a statistical model and drawing conclusions from it. With a solid data preparation, combining with a thorough literature review, it is likely that we can draw meaningful conclusions from the data to inform our future decisions. The opposite is also true as well for poorly processed data sets. We wouldn’t want to waste our time and resources to know that the conclusion we draw is not well-supported.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A bit of controversial topic here. Non-methodologists might have some concerns that we cannot just make up the obtained scores. Like, what if the participants did not answer that question for a reason? How can we be sure that the number we generated will represent characteristics of the targeted population? The million-dollar question is, would you still do this, knowing that the number you generated might have some degree of error? Are you willing to trade authenticity of the data for the data point that might improve your statistical models? It is your task as a researcher and an informed individual to justify your choice in this matter, as well as other choices that you made in your endeavor.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Anyway, thank you so much for your read as always! Happy Holiday, everyone! I hope you have an awesome break! :)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>9e8924d0fa8f46fafe3590635e32c199</distill:md5>
      <category>R</category>
      <category>Data Visualization</category>
      <category>Statistics</category>
      <guid>https://taridwong.github.io/posts/2021-12-27-missingdata</guid>
      <pubDate>Mon, 27 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-27-missingdata/missingpic.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Applying Machine Learning to Audio Data: Visualization, Classification, and Recommendation</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data</link>
      <description>For this entry, I am trying my hands on audio data to extract its features for exploratory data analysis (EDA), using machine learning algorithms to perform music classification, and finally build up on that result to develop a recommendation system for music of similar characteristics.

(13 min read)</description>
      <category>Python</category>
      <category>Data Visualization</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data</guid>
      <pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data/applying-machine-learning-to-audio-data_files/figure-html5/unnamed-chunk-13-11.png" medium="image" type="image/png" width="3072" height="1152"/>
    </item>
    <item>
      <title>Interactive plots for Suicide Data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data</link>
      <description>For this entry, will be visualizing suicide data from 1958 to 2015 with interactive plots to communicate insights to non-technical audience.  

(14 min read)</description>
      <category>R</category>
      <category>Data Visualization</category>
      <category>Quantitative research</category>
      <guid>https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data</guid>
      <pubDate>Thu, 09 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data/preview.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Image Recognition with Artificial Neural Networks</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks</link>
      <description>In this entry, we will be developing a deep learning algorithm - a sub-field of machine learning inspired by the structure of human brain (neural networks) - to classify images of single digit number (0-9).  

(9 min read)</description>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <category>Deep Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks</guid>
      <pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks/deepnn.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Anomaly Detection with New York City taxi data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data</link>
      <description>In this entry, I will be conducting anomaly detection to identify points of anomaly in the taxi passengers data in New York City from July 2014 to January 2015 at half-hourly intervals.
 
 (4 min read)</description>
      <category>Unsupervised Machine Learning</category>
      <category>R</category>
      <guid>https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data</guid>
      <pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data/anomaly-detection-with-new-york-city-taxi-data_files/figure-html5/unnamed-chunk-9-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Crime mapping in San Francisco with police data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data</link>
      <description>In this entry, we will explore San Francisco crime data from 2016 to 2018 to understand the relationship between civilian-reported incidents of crime and police-reported incidents of crime. Along the way we will use table intersection methods to subset our data, aggregation methods to calculate important statistics, and simple visualizations to understand crime trends.  

(7 min read)</description>
      <category>Data Visualization</category>
      <category>R</category>
      <category>Quantitative research</category>
      <guid>https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data</guid>
      <pubDate>Tue, 30 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data/crime-mapping-in-san-francisco-with-police-data_files/figure-html5/unnamed-chunk-6-1.png" medium="image" type="image/png" width="1920" height="1920"/>
    </item>
    <item>
      <title>Exploring COVID-19 data from twitter with topic modeling</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds</link>
      <description>This entry focuses on the exploration of twitter data from Alberta's Chief Medical Officer of Health via word cloud and topic modeling to gain insights in characteristics of public health messaging during the COVID-19 pandemic.  

(7 min read)</description>
      <category>Python</category>
      <category>Natural Language Processing</category>
      <category>Unsupervised Machine Learning</category>
      <category>COVID-19</category>
      <guid>https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds</guid>
      <pubDate>Tue, 23 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds/word-cloud-pv.png" medium="image" type="image/png" width="1920" height="1920"/>
    </item>
    <item>
      <title>Finding a home among the paradigm push-back with Dialectical Pluralism</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-07-introducing-dialectical-pluralism</link>
      <description>This entry discusses the reconciliation of quantitative and qualitative worldviews amidst the paradigm wars with pluralistic stance.

(2 min read)</description>
      <category>Mixed methods research</category>
      <guid>https://taridwong.github.io/posts/2021-11-07-introducing-dialectical-pluralism</guid>
      <pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://www.relevantinsights.com/wp-content/uploads/2020/05/Qualitative-Quantitative-Research-For-New-Product-Development.png" medium="image" type="image/png"/>
    </item>
  </channel>
</rss>
