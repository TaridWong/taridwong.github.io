<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>Tarid Wongvorachan</title>
    <link>https://taridwong.github.io/</link>
    <atom:link href="https://taridwong.github.io/index.xml" rel="self" type="application/rss+xml"/>
    <description>Welcome to my data science blog!
</description>
    <generator>Distill</generator>
    <lastBuildDate>Sat, 06 Aug 2022 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Leveraging a Large-Scale Educational Data Set with Educational Data Mining</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-08-06-edm</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Hi Everyone. It’s been awhile since my last blog post. I have
been occupied with writing and research meeting, among other things. I
have had the opportunity to work with several large-scale data sets from
start to finish (i.e., planning research ideas, data cleaning,
interpreting patterns, and translating insights for the audience. That
is why I want to post some of my ideas to this blog to share with you
with it is like to work with data from one end to another. In this post,
I will be predicting students’ high school dropout rate through the
usage of a large-scale educational data set.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;My work is largely in the field of educational data mining (EDM),
which is the method of knowledge discovery from educational databases
(&lt;a
href="https://www.wiley.com/en-gb/Data+Mining+and+Learning+Analytics%3A+Applications+in+Educational+Research-p-9781118998236"&gt;Elatia
et al., 2016&lt;/a&gt;). Such data is usually extracted from sources such as
students’ interactive learning environment, computerized testing, and
large-scale assessment data repository (&lt;a
href="https://educationaldatamining.org/"&gt;International Educational Data
Mining Society, 2022&lt;/a&gt;). The data set I use in this posting is the
High School Longitudinal Study of 2009, which is a longitudinal data set
that tracks the transition of American youth from secondary schooling to
subsequent education and work roles.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The original data set has 4014 variables and 23,503 cases that
were collected from students’ base year (2009), first follow-up (2012),
2013 update collection (2013), high school transcripts (2013–2014), and
second follow-up (2016). First, I chose a handful of variables based on
theories that are relevant to the prediction of students’ school
dropout.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;After the initial screening, we have 67 variables left. Then, I
further removed responses that were not answered by students or their
parents to preserve data representation. I use &lt;a
href="https://boxuancui.github.io/DataExplorer/"&gt;&lt;code&gt;dataexplorer&lt;/code&gt;&lt;/a&gt;
package to examine types and missingness of the variables. Figure 1
below shows that the data set largely consists of categorical variables
then continuous variables. The data also has a bit of missing
data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-08-06-edmMissing.png" style="width:50.0%" alt="" /&gt;
&lt;p class="caption"&gt;Figure 1. Variable Type and Missing Data&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="data-preprocessing"&gt;Data Preprocessing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To further clean up the data, variables with more than 30%
missingness were removed, and the rest missing data was imputed with
Random Forest algorithm based on the &lt;a
href="https://cran.r-project.org/web/packages/mice/mice.pdf"&gt;multivariate
imputation by chained equation&lt;/a&gt; method. I have done everything in
advance to save time. Here is the cleaned data. I will also load the
following packages for data preprocessing.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tidyverse)
library(readxl)
library(Hmisc)
library(corrplot)

hsls_30_rf &amp;lt;-read_csv(&amp;quot;hsls_30percent_imputed_rf.csv&amp;quot;, col_names = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;At this point, the data set has 51 variables and 16137 cases. I
converted some variables into factors to reflect their nature with
&lt;code&gt;as.factor&lt;/code&gt; function. I also mapped correlation matrix of the
data set to examine variables that are not related to one another.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;hsls_30_rf &amp;lt;- hsls_30_rf %&amp;gt;% 
  as.data.frame() %&amp;gt;%
  mutate(across(c(X1SEX, X1RACE, X1MOMRESP, 
                  X1MOMEDU, X1MOMRACE, X1DADRESP, 
                  X1DADEDU, X1DADRACE, X1HHNUMBER, 
                  X1STUEDEXPCT, X1PAREDEXPCT, X1TMRACE, 
                  X1TMCERT, 
                  X1LOCALE, X1REGION, S1NOHWDN, 
                  S1NOPAPER, S1NOBOOKS, S1LATE, 
                  S1PAYOFF, S1GETINTOCLG, S1AFFORD, 
                  S1WORKING, S1FRNDGRADES, S1FRNDSCHOOL, 
                  S1FRNDCLASS, S1FRNDCLG, S1HRMHOMEWK, 
                  S1HRSHOMEWK, S1SUREHSGRAD, P1BEHAVE, 
                  P1ATTEND, P1PERFORM, P1HWOFTEN, 
                  X4EVERDROP, X4PSENRSTLV), as.factor))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;correlation_30_rf &amp;lt;-rcorr(as.matrix(hsls_30_rf))

corrplot(correlation_30_rf$r, type = &amp;quot;upper&amp;quot;, order = &amp;quot;hclust&amp;quot;, 
         p.mat = correlation_30_rf$P, insig = &amp;quot;pch&amp;quot;, pch = 4, pch.cex = 1,
         tl.col = &amp;quot;black&amp;quot;, tl.cex = 0.5, tl.srt = 90)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2022-08-06-edmcor-unclean.png" style="width:75.0%" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Based on the above correlation matrix and theoretical relevance,
variables that are recommended for removal are: &lt;code&gt;X1RACE&lt;/code&gt;,
&lt;code&gt;X1MOMRACE&lt;/code&gt;, &lt;code&gt;X1DADRACE&lt;/code&gt;, &lt;code&gt;X1LOCALE&lt;/code&gt;,
&lt;code&gt;P1HWOFTEN&lt;/code&gt;, &lt;code&gt;X1HHNUMBER&lt;/code&gt;, &lt;code&gt;X1TMCERT&lt;/code&gt;,
&lt;code&gt;X1REGION&lt;/code&gt;, &lt;code&gt;X1MOMRESP&lt;/code&gt;, &lt;code&gt;X1DADRESP&lt;/code&gt;,
&lt;code&gt;X1SEX&lt;/code&gt;, &lt;code&gt;X1TMRACE&lt;/code&gt;, &lt;code&gt;X1TSRACE&lt;/code&gt;,
&lt;code&gt;X1MTHUTI.&lt;/code&gt; They are removed because 1) they are not
theoretically related to the prediction of high school dropout and 2)
they have insignificant correlation that might negatively impact the
prediction result.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;hsls_30_rf_final &amp;lt;- hsls_30_rf %&amp;gt;% select(!c(X1RACE, X1MOMRACE, X1DADRACE, X1LOCALE, P1HWOFTEN, X1HHNUMBER, X1TMCERT, X1REGION, X1MOMRESP, X1DADRESP, X1SEX, X1TMRACE, X1MTHUTI))&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;After uncorrelated variables were removed, we have 38 variables
left. The new correlation matrix is as follows:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;correlation_30_rf_final &amp;lt;-rcorr(as.matrix(hsls_30_rf_final))

corrplot(correlation_30_rf_final$r, type = &amp;quot;upper&amp;quot;, order = &amp;quot;hclust&amp;quot;, 
         p.mat = correlation_30_rf_final$P, insig = &amp;quot;pch&amp;quot;, pch = 4, pch.cex = 1,
         tl.col = &amp;quot;black&amp;quot;, tl.cex = 0.5, tl.srt = 90)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2022-08-06-edmcor-clean.png" style="width:75.0%" /&gt;&lt;/p&gt;
&lt;h2 id="data-augmentation"&gt;Data Augmentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;After the initial data preprocessing in R, I will use Python to
perform machine learning. I personally use R for data
exploration/statistical analysis and Python for machine learning. First,
I will initiate Python environment and import necessary modules.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

from sklearn.manifold import TSNE

import warnings
warnings.filterwarnings(&amp;quot;ignore&amp;quot;)

RANDOM_STATE = 123&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Then, I will transfer the data set to Python environment because the
two languages run in parallel instead of on the same ground.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;df = r.hsls_30_rf_final

df[&amp;#39;X4EVERDROP&amp;#39;] = np.where(df[&amp;#39;X4EVERDROP&amp;#39;] == &amp;quot;0&amp;quot;, 0, 1)

df.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  X1MOMEDU X1DADEDU   X1SES  ...  P1PERFORM  X4EVERDROP  X4PSENRSTLV
0        5        5  1.5644  ...          1           0            1
1        3        2 -0.3699  ...          1           0            0
2        7        0  1.2741  ...          1           0            1
3        4        0  0.1495  ...          1           1            2
4        3        3  1.0639  ...          1           0            1

[5 rows x 38 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;From the data set, I will extract the predictors (X) and the
targeted variable (y). I will also check class proportion of the
targeted variable to see if they are balanced.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;X_extreme = df.drop(&amp;#39;X4EVERDROP&amp;#39;, axis=1)
y_extreme = df[&amp;#39;X4EVERDROP&amp;#39;]

print(&amp;quot;The proportion of target variable&amp;#39;s class :&amp;quot;, Counter(y_extreme))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The proportion of target variable&amp;#39;s class : Counter({0: 14133, 1: 2004})&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can see that the data is imbalanced. We have 14133 cases of
student who did not and 2004 student who dropped out of their high
school. Class imbalance problem in educational data sets could hamper
the accuracy of predictive models as many of them are designed on the
assumption that the predicted class is balanced (&lt;a
href="https://www.wiley.com/en-us/Imbalanced+Learning%3A+Foundations%2C+Algorithms%2C+and+Applications-p-9781118074626"&gt;He
&amp;amp; Ma, 2013&lt;/a&gt;). This problem is especially prevalent in the
prediction of high-stakes educational issues such as such as school
dropout or grade repetition, where discrepancy between two classes is
high due to its rare occurrence (&lt;a
href="https://www.mdpi.com/2227-7102/9/4/275"&gt;Barros et al.,
2019&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;I will visualize the imbalance with a t-Distributed Stochastic
Neighbor Embedding (tSNE) plot and a count plot.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;tsne = TSNE(n_components=2, random_state=RANDOM_STATE)

TSNE_result = tsne.fit_transform(X_extreme)

plt.figure(figsize=(12,8))
sns.scatterplot(TSNE_result[:,0], TSNE_result[:,1], hue=y_extreme, legend=&amp;#39;full&amp;#39;, palette=&amp;quot;hls&amp;quot;)

plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2022-08-06-edmtsne-pre.PNG" style="width:75.0%" /&gt;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;
sns.set_theme(style=&amp;quot;darkgrid&amp;quot;)
sns.countplot(x=&amp;quot;X4EVERDROP&amp;quot;, data = df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;AxesSubplot:xlabel=&amp;#39;X4EVERDROP&amp;#39;, ylabel=&amp;#39;count&amp;#39;&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file1aec3cf179e1_files/figure-html/unnamed-chunk-11-1.png" width="960" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In &lt;a href="https://taridwong.github.io/posts/2022-04-28-xai/"&gt;my
previous post&lt;/a&gt;, I used the combination of Synthetic Minority
Oversampling TEchnique (SMOTE) and Edited Nearest Neighbor (ENN). The
thing is, SMOTE+ENN only works with numerical variables. We have a lot
of categorical variables in this data set, so we need to find a
workaround for that. I will use SMOTE for nominal and continuous
variable (SMOTE-NC) and random undersampling instead instead.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;from imblearn.over_sampling import SMOTENC
from imblearn.under_sampling import RandomUnderSampler 
from sklearn.model_selection import train_test_split

smote_nc = SMOTENC(random_state=RANDOM_STATE, sampling_strategy=0.8,
                    categorical_features=[0, 1, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36])

rus_hybrid = RandomUnderSampler(random_state=RANDOM_STATE, sampling_strategy=&amp;#39;not minority&amp;#39;)

X_smote_extreme, y_smote_extreme = smote_nc.fit_resample(X_extreme, y_extreme)

X_hybrid_extreme, y_hybrid_extreme = rus_hybrid.fit_resample(X_smote_extreme, y_smote_extreme)

print(&amp;quot;For Y extreme :&amp;quot;, Counter(y_extreme))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;For Y extreme : Counter({0: 14133, 1: 2004})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;For Y smote extreme :&amp;quot;, Counter(y_smote_extreme))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;For Y smote extreme : Counter({0: 14133, 1: 11306})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;For Y hybrid extreme :&amp;quot;, Counter(y_hybrid_extreme))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;For Y hybrid extreme : Counter({0: 11306, 1: 11306})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;X_train_hybrid_ext, X_test_hybrid_ext, y_train_hybrid_ext, y_test_hybrid_ext = train_test_split(X_hybrid_extreme, y_hybrid_extreme, test_size = 0.30, random_state = RANDOM_STATE)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;After we finished augmenting the data, below is the result. We have
much more instances of student who dropped out of their high school as
seen from the tSNE plot and the count plot below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;TSNE_result = tsne.fit_transform(X_hybrid_extreme)

plt.figure(figsize=(12,8))
sns.scatterplot(TSNE_result[:,0], TSNE_result[:,1], hue=y_hybrid_extreme, legend=&amp;#39;full&amp;#39;, palette=&amp;quot;hls&amp;quot;)

plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2022-08-06-edmtsne-post.PNG" style="width:75.0%" /&gt;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;sns.set_theme(style=&amp;quot;darkgrid&amp;quot;)
sns.countplot(y_hybrid_extreme)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;AxesSubplot:xlabel=&amp;#39;X4EVERDROP&amp;#39;, ylabel=&amp;#39;count&amp;#39;&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file1aec3cf179e1_files/figure-html/unnamed-chunk-14-3.png" width="960" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Next, we will proceed to the classification stage. For many
classification algorithms such as XGBoost or Random Forest, you need to
transform categorical variables into numerical variables with label
encoding or one-hot encoding first. However, we have a lot of
categorical variables that may hamper the process. To circumvent this,
we will use &lt;a href="https://catboost.ai/"&gt;CatBoost&lt;/a&gt;, which is a
gradient boosting decision tree that supports categorical variables
without the need for data transformation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="classification"&gt;Classification&lt;/h2&gt;
&lt;pre class="python"&gt;&lt;code&gt;from sklearn.feature_selection import RFECV
from catboost import CatBoostClassifier
from sklearn.model_selection import RandomizedSearchCV&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;First, we will create a CatBoost object as well as a list of
hyperparameters to tune. We can use the default mode of CatBoost, but
tuning the algorithm makes the algorithm perform better. I will tune
tree depth, learning rate, and the number of iteration that the machine
learns. I use randomized grid search to tune the algorithm to save
time.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;CBC = CatBoostClassifier(random_state=RANDOM_STATE)

parameters = {&amp;#39;depth&amp;#39;         : [4,5,6,7,8,9,10],
              &amp;#39;learning_rate&amp;#39; : [0.01,0.02,0.03,0.04,0.05],
              &amp;#39;iterations&amp;#39;    : [10,20,30,40,50,60,70,80,90,100]
             }

cat_features = [0, 1, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36]

Cat_random = RandomizedSearchCV(estimator = CBC, 
                                param_distributions = parameters, 
                                n_iter = 10, cv = 3, verbose=0, 
                                random_state = RANDOM_STATE, error_score=&amp;#39;raise&amp;#39;)

Cat_random.fit(X_train_hybrid_ext, y_train_hybrid_ext, cat_features = cat_features)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0:  learn: 0.6798844    total: 48.4ms   remaining: 4.31s
1:  learn: 0.6658710    total: 95.7ms   remaining: 4.21s
2:  learn: 0.6523493    total: 148ms    remaining: 4.3s
3:  learn: 0.6413995    total: 201ms    remaining: 4.31s
4:  learn: 0.6278567    total: 276ms    remaining: 4.69s
5:  learn: 0.6186284    total: 361ms    remaining: 5.05s
6:  learn: 0.6089585    total: 468ms    remaining: 5.54s
7:  learn: 0.5985513    total: 553ms    remaining: 5.66s
8:  learn: 0.5884809    total: 693ms    remaining: 6.23s
9:  learn: 0.5785280    total: 824ms    remaining: 6.59s
10: learn: 0.5702242    total: 958ms    remaining: 6.88s
11: learn: 0.5597852    total: 1.06s    remaining: 6.87s
12: learn: 0.5527543    total: 1.16s    remaining: 6.85s
13: learn: 0.5439965    total: 1.25s    remaining: 6.79s
14: learn: 0.5354271    total: 1.35s    remaining: 6.78s
15: learn: 0.5295986    total: 1.49s    remaining: 6.9s
16: learn: 0.5235632    total: 1.63s    remaining: 7.01s
17: learn: 0.5180815    total: 1.75s    remaining: 7.01s
18: learn: 0.5128125    total: 1.85s    remaining: 6.92s
19: learn: 0.5060868    total: 1.95s    remaining: 6.82s
20: learn: 0.5001726    total: 2.14s    remaining: 7.03s
21: learn: 0.4957502    total: 2.27s    remaining: 7.03s
22: learn: 0.4914979    total: 2.44s    remaining: 7.11s
23: learn: 0.4883335    total: 2.59s    remaining: 7.11s
24: learn: 0.4843515    total: 2.73s    remaining: 7.1s
25: learn: 0.4806735    total: 2.85s    remaining: 7.02s
26: learn: 0.4774183    total: 3s   remaining: 6.99s
27: learn: 0.4752628    total: 3.12s    remaining: 6.91s
28: learn: 0.4723224    total: 3.23s    remaining: 6.79s
29: learn: 0.4696826    total: 3.34s    remaining: 6.69s
30: learn: 0.4639752    total: 3.46s    remaining: 6.59s
31: learn: 0.4616352    total: 3.57s    remaining: 6.47s
32: learn: 0.4574664    total: 3.67s    remaining: 6.33s
33: learn: 0.4550142    total: 3.77s    remaining: 6.22s
34: learn: 0.4517165    total: 3.88s    remaining: 6.1s
35: learn: 0.4482490    total: 3.98s    remaining: 5.96s
36: learn: 0.4452413    total: 4.08s    remaining: 5.84s
37: learn: 0.4427028    total: 4.18s    remaining: 5.72s
38: learn: 0.4398182    total: 4.27s    remaining: 5.59s
39: learn: 0.4386967    total: 4.33s    remaining: 5.41s
40: learn: 0.4366265    total: 4.42s    remaining: 5.28s
41: learn: 0.4340851    total: 4.5s remaining: 5.15s
42: learn: 0.4319768    total: 4.59s    remaining: 5.02s
43: learn: 0.4300976    total: 4.68s    remaining: 4.9s
44: learn: 0.4285674    total: 4.77s    remaining: 4.77s
45: learn: 0.4265530    total: 4.85s    remaining: 4.64s
46: learn: 0.4245068    total: 4.94s    remaining: 4.52s
47: learn: 0.4223266    total: 5.03s    remaining: 4.4s
48: learn: 0.4206147    total: 5.12s    remaining: 4.28s
49: learn: 0.4184022    total: 5.2s remaining: 4.16s
50: learn: 0.4170437    total: 5.28s    remaining: 4.04s
51: learn: 0.4149012    total: 5.37s    remaining: 3.92s
52: learn: 0.4137503    total: 5.47s    remaining: 3.82s
53: learn: 0.4117136    total: 5.56s    remaining: 3.71s
54: learn: 0.4089305    total: 5.65s    remaining: 3.6s
55: learn: 0.4066496    total: 5.73s    remaining: 3.48s
56: learn: 0.4057898    total: 5.82s    remaining: 3.37s
57: learn: 0.4046035    total: 5.9s remaining: 3.26s
58: learn: 0.4035047    total: 6s   remaining: 3.15s
59: learn: 0.4021765    total: 6.07s    remaining: 3.04s
60: learn: 0.3990097    total: 6.18s    remaining: 2.94s
61: learn: 0.3980288    total: 6.27s    remaining: 2.83s
62: learn: 0.3964750    total: 6.37s    remaining: 2.73s
63: learn: 0.3954893    total: 6.45s    remaining: 2.62s
64: learn: 0.3943143    total: 6.54s    remaining: 2.52s
65: learn: 0.3918841    total: 6.62s    remaining: 2.41s
66: learn: 0.3899150    total: 6.71s    remaining: 2.3s
67: learn: 0.3890711    total: 6.81s    remaining: 2.2s
68: learn: 0.3881179    total: 6.91s    remaining: 2.1s
69: learn: 0.3868025    total: 6.99s    remaining: 2s
70: learn: 0.3850559    total: 7.08s    remaining: 1.89s
71: learn: 0.3841090    total: 7.16s    remaining: 1.79s
72: learn: 0.3832880    total: 7.25s    remaining: 1.69s
73: learn: 0.3819060    total: 7.33s    remaining: 1.58s
74: learn: 0.3811645    total: 7.42s    remaining: 1.48s
75: learn: 0.3792427    total: 7.5s remaining: 1.38s
76: learn: 0.3789477    total: 7.55s    remaining: 1.27s
77: learn: 0.3771891    total: 7.64s    remaining: 1.18s
78: learn: 0.3749524    total: 7.73s    remaining: 1.07s
79: learn: 0.3734599    total: 7.82s    remaining: 977ms
80: learn: 0.3727318    total: 7.9s remaining: 878ms
81: learn: 0.3711936    total: 7.99s    remaining: 779ms
82: learn: 0.3704078    total: 8.08s    remaining: 681ms
83: learn: 0.3697014    total: 8.16s    remaining: 583ms
84: learn: 0.3688939    total: 8.24s    remaining: 485ms
85: learn: 0.3681721    total: 8.34s    remaining: 388ms
86: learn: 0.3675191    total: 8.42s    remaining: 290ms
87: learn: 0.3667829    total: 8.5s remaining: 193ms
88: learn: 0.3661064    total: 8.57s    remaining: 96.3ms
89: learn: 0.3654935    total: 8.66s    remaining: 0us
0:  learn: 0.6789438    total: 81.4ms   remaining: 7.24s
1:  learn: 0.6622984    total: 166ms    remaining: 7.3s
2:  learn: 0.6528820    total: 248ms    remaining: 7.19s
3:  learn: 0.6378865    total: 333ms    remaining: 7.15s
4:  learn: 0.6248902    total: 419ms    remaining: 7.13s
5:  learn: 0.6143424    total: 501ms    remaining: 7.02s
6:  learn: 0.6051266    total: 579ms    remaining: 6.87s
7:  learn: 0.5963677    total: 652ms    remaining: 6.68s
8:  learn: 0.5878444    total: 728ms    remaining: 6.55s
9:  learn: 0.5791189    total: 799ms    remaining: 6.39s
10: learn: 0.5699589    total: 885ms    remaining: 6.36s
11: learn: 0.5631775    total: 954ms    remaining: 6.2s
12: learn: 0.5549372    total: 1.03s    remaining: 6.11s
13: learn: 0.5489560    total: 1.1s remaining: 5.98s
14: learn: 0.5398350    total: 1.21s    remaining: 6.07s
15: learn: 0.5335363    total: 1.28s    remaining: 5.95s
16: learn: 0.5286711    total: 1.36s    remaining: 5.84s
17: learn: 0.5248677    total: 1.44s    remaining: 5.75s
18: learn: 0.5208806    total: 1.56s    remaining: 5.82s
19: learn: 0.5137198    total: 1.63s    remaining: 5.72s
20: learn: 0.5089934    total: 1.73s    remaining: 5.69s
21: learn: 0.5041599    total: 1.81s    remaining: 5.61s
22: learn: 0.4994555    total: 1.9s remaining: 5.55s
23: learn: 0.4933377    total: 1.98s    remaining: 5.45s
24: learn: 0.4888624    total: 2.07s    remaining: 5.38s
25: learn: 0.4848647    total: 2.15s    remaining: 5.29s
26: learn: 0.4806195    total: 2.24s    remaining: 5.22s
27: learn: 0.4782823    total: 2.32s    remaining: 5.13s
28: learn: 0.4737880    total: 2.4s remaining: 5.05s
29: learn: 0.4716028    total: 2.48s    remaining: 4.97s
30: learn: 0.4665243    total: 2.56s    remaining: 4.88s
31: learn: 0.4640058    total: 2.64s    remaining: 4.79s
32: learn: 0.4596530    total: 2.76s    remaining: 4.78s
33: learn: 0.4570517    total: 2.84s    remaining: 4.68s
34: learn: 0.4538448    total: 2.92s    remaining: 4.58s
35: learn: 0.4518482    total: 2.98s    remaining: 4.47s
36: learn: 0.4473907    total: 3.1s remaining: 4.44s
37: learn: 0.4444594    total: 3.18s    remaining: 4.35s
38: learn: 0.4429416    total: 3.25s    remaining: 4.26s
39: learn: 0.4405574    total: 3.33s    remaining: 4.16s
40: learn: 0.4384978    total: 3.42s    remaining: 4.09s
41: learn: 0.4357888    total: 3.5s remaining: 4s
42: learn: 0.4333328    total: 3.6s remaining: 3.93s
43: learn: 0.4311137    total: 3.68s    remaining: 3.84s
44: learn: 0.4287708    total: 3.77s    remaining: 3.77s
45: learn: 0.4263805    total: 3.85s    remaining: 3.68s
46: learn: 0.4243778    total: 3.94s    remaining: 3.6s
47: learn: 0.4226912    total: 4.04s    remaining: 3.54s
48: learn: 0.4201662    total: 4.13s    remaining: 3.46s
49: learn: 0.4183090    total: 4.22s    remaining: 3.37s
50: learn: 0.4161610    total: 4.31s    remaining: 3.29s
51: learn: 0.4148893    total: 4.4s remaining: 3.21s
52: learn: 0.4128819    total: 4.48s    remaining: 3.13s
53: learn: 0.4114953    total: 4.57s    remaining: 3.04s
54: learn: 0.4101021    total: 4.65s    remaining: 2.96s
55: learn: 0.4088186    total: 4.73s    remaining: 2.87s
56: learn: 0.4075637    total: 4.82s    remaining: 2.79s
57: learn: 0.4053970    total: 4.9s remaining: 2.7s
58: learn: 0.4030154    total: 4.98s    remaining: 2.62s
59: learn: 0.4017849    total: 5.05s    remaining: 2.53s
60: learn: 0.3997640    total: 5.14s    remaining: 2.44s
61: learn: 0.3975432    total: 5.21s    remaining: 2.35s
62: learn: 0.3954974    total: 5.29s    remaining: 2.27s
63: learn: 0.3942386    total: 5.38s    remaining: 2.18s
64: learn: 0.3935385    total: 5.46s    remaining: 2.1s
65: learn: 0.3914313    total: 5.53s    remaining: 2.01s
66: learn: 0.3905388    total: 5.62s    remaining: 1.93s
67: learn: 0.3896347    total: 5.71s    remaining: 1.85s
68: learn: 0.3882451    total: 5.81s    remaining: 1.77s
69: learn: 0.3871845    total: 5.89s    remaining: 1.68s
70: learn: 0.3863762    total: 5.97s    remaining: 1.6s
71: learn: 0.3847478    total: 6.05s    remaining: 1.51s
72: learn: 0.3837932    total: 6.15s    remaining: 1.43s
73: learn: 0.3830264    total: 6.24s    remaining: 1.35s
74: learn: 0.3813505    total: 6.3s remaining: 1.26s
75: learn: 0.3805847    total: 6.4s remaining: 1.18s
76: learn: 0.3779104    total: 6.48s    remaining: 1.09s
77: learn: 0.3762739    total: 6.58s    remaining: 1.01s
78: learn: 0.3740235    total: 6.66s    remaining: 927ms
79: learn: 0.3727875    total: 6.74s    remaining: 842ms
80: learn: 0.3719730    total: 6.82s    remaining: 758ms
81: learn: 0.3711517    total: 6.89s    remaining: 673ms
82: learn: 0.3700075    total: 6.97s    remaining: 588ms
83: learn: 0.3683987    total: 7.04s    remaining: 503ms
84: learn: 0.3678371    total: 7.12s    remaining: 419ms
85: learn: 0.3671215    total: 7.2s remaining: 335ms
86: learn: 0.3653367    total: 7.27s    remaining: 251ms
87: learn: 0.3636935    total: 7.35s    remaining: 167ms
88: learn: 0.3624679    total: 7.43s    remaining: 83.5ms
89: learn: 0.3618877    total: 7.51s    remaining: 0us
0:  learn: 0.6741178    total: 75.4ms   remaining: 6.71s
1:  learn: 0.6620692    total: 151ms    remaining: 6.62s
2:  learn: 0.6501160    total: 234ms    remaining: 6.77s
3:  learn: 0.6358792    total: 308ms    remaining: 6.62s
4:  learn: 0.6235486    total: 397ms    remaining: 6.74s
5:  learn: 0.6116855    total: 470ms    remaining: 6.58s
6:  learn: 0.6019227    total: 557ms    remaining: 6.61s
7:  learn: 0.5939390    total: 635ms    remaining: 6.5s
8:  learn: 0.5860754    total: 718ms    remaining: 6.46s
9:  learn: 0.5749311    total: 803ms    remaining: 6.42s
10: learn: 0.5670579    total: 882ms    remaining: 6.34s
11: learn: 0.5601169    total: 969ms    remaining: 6.3s
12: learn: 0.5547385    total: 1.05s    remaining: 6.22s
13: learn: 0.5484639    total: 1.13s    remaining: 6.11s
14: learn: 0.5433616    total: 1.21s    remaining: 6.04s
15: learn: 0.5375490    total: 1.28s    remaining: 5.94s
16: learn: 0.5315263    total: 1.36s    remaining: 5.83s
17: learn: 0.5271360    total: 1.44s    remaining: 5.75s
18: learn: 0.5221979    total: 1.51s    remaining: 5.66s
19: learn: 0.5140389    total: 1.59s    remaining: 5.55s
20: learn: 0.5081484    total: 1.66s    remaining: 5.46s
21: learn: 0.5036802    total: 1.74s    remaining: 5.38s
22: learn: 0.4989852    total: 1.82s    remaining: 5.31s
23: learn: 0.4962544    total: 1.9s remaining: 5.22s
24: learn: 0.4924226    total: 2s   remaining: 5.19s
25: learn: 0.4884310    total: 2.07s    remaining: 5.1s
26: learn: 0.4856478    total: 2.17s    remaining: 5.06s
27: learn: 0.4822786    total: 2.25s    remaining: 4.98s
28: learn: 0.4766774    total: 2.34s    remaining: 4.93s
29: learn: 0.4737081    total: 2.42s    remaining: 4.84s
30: learn: 0.4712776    total: 2.5s remaining: 4.77s
31: learn: 0.4675332    total: 2.58s    remaining: 4.69s
32: learn: 0.4643978    total: 2.67s    remaining: 4.61s
33: learn: 0.4613327    total: 2.74s    remaining: 4.52s
34: learn: 0.4571739    total: 2.83s    remaining: 4.45s
35: learn: 0.4542738    total: 2.91s    remaining: 4.36s
36: learn: 0.4517763    total: 3s   remaining: 4.3s
37: learn: 0.4492796    total: 3.08s    remaining: 4.22s
38: learn: 0.4465959    total: 3.17s    remaining: 4.14s
39: learn: 0.4429619    total: 3.25s    remaining: 4.06s
40: learn: 0.4405566    total: 3.33s    remaining: 3.98s
41: learn: 0.4380638    total: 3.4s remaining: 3.89s
42: learn: 0.4337473    total: 3.51s    remaining: 3.83s
43: learn: 0.4301822    total: 3.62s    remaining: 3.78s
44: learn: 0.4279891    total: 3.73s    remaining: 3.73s
45: learn: 0.4266312    total: 3.81s    remaining: 3.65s
46: learn: 0.4246471    total: 3.9s remaining: 3.56s
47: learn: 0.4225972    total: 3.98s    remaining: 3.48s
48: learn: 0.4210475    total: 4.07s    remaining: 3.4s
49: learn: 0.4192105    total: 4.14s    remaining: 3.31s
50: learn: 0.4180917    total: 4.22s    remaining: 3.23s
51: learn: 0.4162651    total: 4.29s    remaining: 3.13s
52: learn: 0.4145738    total: 4.37s    remaining: 3.05s
53: learn: 0.4135470    total: 4.45s    remaining: 2.96s
54: learn: 0.4113829    total: 4.53s    remaining: 2.88s
55: learn: 0.4083366    total: 4.61s    remaining: 2.8s
56: learn: 0.4071852    total: 4.68s    remaining: 2.71s
57: learn: 0.4060343    total: 4.75s    remaining: 2.62s
58: learn: 0.4049222    total: 4.83s    remaining: 2.54s
59: learn: 0.4015865    total: 4.91s    remaining: 2.45s
60: learn: 0.4005098    total: 5s   remaining: 2.38s
61: learn: 0.3994448    total: 5.08s    remaining: 2.29s
62: learn: 0.3982600    total: 5.16s    remaining: 2.21s
63: learn: 0.3967756    total: 5.24s    remaining: 2.13s
64: learn: 0.3956166    total: 5.31s    remaining: 2.04s
65: learn: 0.3932374    total: 5.38s    remaining: 1.96s
66: learn: 0.3920478    total: 5.46s    remaining: 1.87s
67: learn: 0.3910069    total: 5.54s    remaining: 1.79s
68: learn: 0.3888444    total: 5.61s    remaining: 1.71s
69: learn: 0.3876487    total: 5.71s    remaining: 1.63s
70: learn: 0.3867444    total: 5.79s    remaining: 1.55s
71: learn: 0.3858815    total: 5.85s    remaining: 1.46s
72: learn: 0.3849503    total: 5.93s    remaining: 1.38s
73: learn: 0.3840737    total: 6.01s    remaining: 1.3s
74: learn: 0.3833085    total: 6.09s    remaining: 1.22s
75: learn: 0.3824363    total: 6.16s    remaining: 1.14s
76: learn: 0.3801746    total: 6.25s    remaining: 1.06s
77: learn: 0.3787439    total: 6.33s    remaining: 974ms
78: learn: 0.3780631    total: 6.41s    remaining: 893ms
79: learn: 0.3770939    total: 6.49s    remaining: 811ms
80: learn: 0.3760120    total: 6.57s    remaining: 730ms
81: learn: 0.3747921    total: 6.64s    remaining: 648ms
82: learn: 0.3736694    total: 6.73s    remaining: 567ms
83: learn: 0.3723968    total: 6.81s    remaining: 486ms
84: learn: 0.3716942    total: 6.89s    remaining: 406ms
85: learn: 0.3707126    total: 6.98s    remaining: 325ms
86: learn: 0.3696831    total: 7.1s remaining: 245ms
87: learn: 0.3690034    total: 7.17s    remaining: 163ms
88: learn: 0.3672579    total: 7.26s    remaining: 81.6ms
89: learn: 0.3657457    total: 7.33s    remaining: 0us
0:  learn: 0.6891827    total: 68.8ms   remaining: 4.06s
1:  learn: 0.6832416    total: 135ms    remaining: 3.92s
2:  learn: 0.6799339    total: 236ms    remaining: 4.47s
3:  learn: 0.6753852    total: 300ms    remaining: 4.2s
4:  learn: 0.6713542    total: 369ms    remaining: 4.06s
5:  learn: 0.6680221    total: 441ms    remaining: 3.97s
6:  learn: 0.6643313    total: 514ms    remaining: 3.89s
7:  learn: 0.6603958    total: 582ms    remaining: 3.78s
8:  learn: 0.6566409    total: 654ms    remaining: 3.7s
9:  learn: 0.6523928    total: 716ms    remaining: 3.58s
10: learn: 0.6490019    total: 796ms    remaining: 3.55s
11: learn: 0.6450495    total: 867ms    remaining: 3.47s
12: learn: 0.6418979    total: 937ms    remaining: 3.39s
13: learn: 0.6369726    total: 1.01s    remaining: 3.31s
14: learn: 0.6337408    total: 1.07s    remaining: 3.23s
15: learn: 0.6291213    total: 1.14s    remaining: 3.14s
16: learn: 0.6253233    total: 1.21s    remaining: 3.06s
17: learn: 0.6216481    total: 1.27s    remaining: 2.96s
18: learn: 0.6185586    total: 1.34s    remaining: 2.89s
19: learn: 0.6146644    total: 1.41s    remaining: 2.81s
20: learn: 0.6117712    total: 1.47s    remaining: 2.74s
21: learn: 0.6085535    total: 1.53s    remaining: 2.65s
22: learn: 0.6063618    total: 1.6s remaining: 2.57s
23: learn: 0.6033707    total: 1.66s    remaining: 2.49s
24: learn: 0.6011590    total: 1.73s    remaining: 2.42s
25: learn: 0.5980825    total: 1.79s    remaining: 2.35s
26: learn: 0.5952362    total: 1.86s    remaining: 2.27s
27: learn: 0.5926361    total: 1.92s    remaining: 2.2s
28: learn: 0.5898130    total: 1.99s    remaining: 2.13s
29: learn: 0.5862358    total: 2.05s    remaining: 2.05s
30: learn: 0.5826405    total: 2.13s    remaining: 1.99s
31: learn: 0.5799910    total: 2.2s remaining: 1.92s
32: learn: 0.5773621    total: 2.27s    remaining: 1.86s
33: learn: 0.5753155    total: 2.34s    remaining: 1.79s
34: learn: 0.5721495    total: 2.42s    remaining: 1.73s
35: learn: 0.5699541    total: 2.49s    remaining: 1.66s
36: learn: 0.5683134    total: 2.56s    remaining: 1.59s
37: learn: 0.5662259    total: 2.62s    remaining: 1.52s
38: learn: 0.5631273    total: 2.69s    remaining: 1.45s
39: learn: 0.5611466    total: 2.75s    remaining: 1.38s
40: learn: 0.5585133    total: 2.84s    remaining: 1.31s
41: learn: 0.5561262    total: 2.9s remaining: 1.24s
42: learn: 0.5540212    total: 2.97s    remaining: 1.17s
43: learn: 0.5520581    total: 3.04s    remaining: 1.1s
44: learn: 0.5496373    total: 3.1s remaining: 1.03s
45: learn: 0.5469572    total: 3.16s    remaining: 963ms
46: learn: 0.5449205    total: 3.24s    remaining: 895ms
47: learn: 0.5422714    total: 3.31s    remaining: 827ms
48: learn: 0.5392169    total: 3.39s    remaining: 761ms
49: learn: 0.5373144    total: 3.46s    remaining: 692ms
50: learn: 0.5356060    total: 3.53s    remaining: 623ms
51: learn: 0.5343717    total: 3.59s    remaining: 553ms
52: learn: 0.5328772    total: 3.66s    remaining: 483ms
53: learn: 0.5313314    total: 3.71s    remaining: 413ms
54: learn: 0.5290473    total: 3.78s    remaining: 344ms
55: learn: 0.5278621    total: 3.83s    remaining: 274ms
56: learn: 0.5266591    total: 3.9s remaining: 205ms
57: learn: 0.5248089    total: 3.95s    remaining: 136ms
58: learn: 0.5229226    total: 4.01s    remaining: 68.1ms
59: learn: 0.5211238    total: 4.08s    remaining: 0us
0:  learn: 0.6883565    total: 61.3ms   remaining: 3.62s
1:  learn: 0.6836626    total: 119ms    remaining: 3.44s
2:  learn: 0.6803896    total: 182ms    remaining: 3.45s
3:  learn: 0.6767302    total: 245ms    remaining: 3.43s
4:  learn: 0.6723705    total: 314ms    remaining: 3.46s
5:  learn: 0.6683962    total: 380ms    remaining: 3.42s
6:  learn: 0.6641959    total: 452ms    remaining: 3.42s
7:  learn: 0.6606501    total: 519ms    remaining: 3.38s
8:  learn: 0.6578427    total: 591ms    remaining: 3.35s
9:  learn: 0.6533980    total: 661ms    remaining: 3.31s
10: learn: 0.6498456    total: 729ms    remaining: 3.25s
11: learn: 0.6461800    total: 794ms    remaining: 3.17s
12: learn: 0.6429715    total: 869ms    remaining: 3.14s
13: learn: 0.6392294    total: 929ms    remaining: 3.05s
14: learn: 0.6347095    total: 1.01s    remaining: 3.03s
15: learn: 0.6312490    total: 1.07s    remaining: 2.96s
16: learn: 0.6289288    total: 1.14s    remaining: 2.88s
17: learn: 0.6250441    total: 1.2s remaining: 2.81s
18: learn: 0.6216653    total: 1.27s    remaining: 2.74s
19: learn: 0.6193114    total: 1.34s    remaining: 2.67s
20: learn: 0.6160372    total: 1.41s    remaining: 2.61s
21: learn: 0.6129786    total: 1.47s    remaining: 2.55s
22: learn: 0.6098641    total: 1.54s    remaining: 2.48s
23: learn: 0.6060756    total: 1.61s    remaining: 2.42s
24: learn: 0.6032837    total: 1.69s    remaining: 2.37s
25: learn: 0.6006062    total: 1.76s    remaining: 2.3s
26: learn: 0.5963792    total: 1.83s    remaining: 2.24s
27: learn: 0.5935965    total: 1.9s remaining: 2.17s
28: learn: 0.5911082    total: 1.96s    remaining: 2.1s
29: learn: 0.5884068    total: 2.03s    remaining: 2.03s
30: learn: 0.5855242    total: 2.1s remaining: 1.97s
31: learn: 0.5836715    total: 2.18s    remaining: 1.9s
32: learn: 0.5807843    total: 2.24s    remaining: 1.83s
33: learn: 0.5787853    total: 2.31s    remaining: 1.77s
34: learn: 0.5749655    total: 2.38s    remaining: 1.7s
35: learn: 0.5725083    total: 2.45s    remaining: 1.63s
36: learn: 0.5706425    total: 2.52s    remaining: 1.57s
37: learn: 0.5684386    total: 2.6s remaining: 1.5s
38: learn: 0.5663417    total: 2.67s    remaining: 1.44s
39: learn: 0.5633125    total: 2.73s    remaining: 1.36s
40: learn: 0.5611171    total: 2.79s    remaining: 1.29s
41: learn: 0.5586256    total: 2.86s    remaining: 1.22s
42: learn: 0.5561923    total: 2.93s    remaining: 1.16s
43: learn: 0.5542360    total: 2.99s    remaining: 1.09s
44: learn: 0.5521751    total: 3.06s    remaining: 1.02s
45: learn: 0.5504413    total: 3.13s    remaining: 952ms
46: learn: 0.5488480    total: 3.19s    remaining: 882ms
47: learn: 0.5472914    total: 3.24s    remaining: 810ms
48: learn: 0.5452874    total: 3.31s    remaining: 743ms
49: learn: 0.5436439    total: 3.47s    remaining: 694ms
50: learn: 0.5411503    total: 3.58s    remaining: 632ms
51: learn: 0.5383509    total: 3.66s    remaining: 563ms
52: learn: 0.5366184    total: 3.73s    remaining: 493ms
53: learn: 0.5343908    total: 3.81s    remaining: 424ms
54: learn: 0.5330084    total: 3.89s    remaining: 354ms
55: learn: 0.5317342    total: 3.96s    remaining: 283ms
56: learn: 0.5304471    total: 4.03s    remaining: 212ms
57: learn: 0.5288729    total: 4.1s remaining: 141ms
58: learn: 0.5273035    total: 4.17s    remaining: 70.7ms
59: learn: 0.5255895    total: 4.24s    remaining: 0us
0:  learn: 0.6882838    total: 66.1ms   remaining: 3.9s
1:  learn: 0.6832112    total: 135ms    remaining: 3.92s
2:  learn: 0.6799919    total: 219ms    remaining: 4.16s
3:  learn: 0.6771048    total: 288ms    remaining: 4.03s
4:  learn: 0.6738181    total: 358ms    remaining: 3.94s
5:  learn: 0.6707208    total: 421ms    remaining: 3.79s
6:  learn: 0.6679867    total: 497ms    remaining: 3.76s
7:  learn: 0.6654101    total: 563ms    remaining: 3.66s
8:  learn: 0.6621987    total: 641ms    remaining: 3.63s
9:  learn: 0.6580098    total: 712ms    remaining: 3.56s
10: learn: 0.6546495    total: 780ms    remaining: 3.47s
11: learn: 0.6513982    total: 845ms    remaining: 3.38s
12: learn: 0.6474299    total: 915ms    remaining: 3.31s
13: learn: 0.6446765    total: 983ms    remaining: 3.23s
14: learn: 0.6409242    total: 1.07s    remaining: 3.2s
15: learn: 0.6362265    total: 1.13s    remaining: 3.12s
16: learn: 0.6332374    total: 1.21s    remaining: 3.06s
17: learn: 0.6290921    total: 1.28s    remaining: 2.98s
18: learn: 0.6258450    total: 1.36s    remaining: 2.93s
19: learn: 0.6214934    total: 1.42s    remaining: 2.84s
20: learn: 0.6191940    total: 1.5s remaining: 2.78s
21: learn: 0.6147269    total: 1.57s    remaining: 2.71s
22: learn: 0.6121340    total: 1.66s    remaining: 2.67s
23: learn: 0.6088729    total: 1.73s    remaining: 2.59s
24: learn: 0.6060512    total: 1.8s remaining: 2.52s
25: learn: 0.6030904    total: 1.88s    remaining: 2.45s
26: learn: 0.6007806    total: 1.95s    remaining: 2.38s
27: learn: 0.5986268    total: 2.01s    remaining: 2.3s
28: learn: 0.5959227    total: 2.08s    remaining: 2.22s
29: learn: 0.5929901    total: 2.14s    remaining: 2.14s
30: learn: 0.5910613    total: 2.21s    remaining: 2.06s
31: learn: 0.5886354    total: 2.27s    remaining: 1.99s
32: learn: 0.5849181    total: 2.34s    remaining: 1.92s
33: learn: 0.5823567    total: 2.4s remaining: 1.84s
34: learn: 0.5802187    total: 2.47s    remaining: 1.77s
35: learn: 0.5780069    total: 2.54s    remaining: 1.69s
36: learn: 0.5753076    total: 2.61s    remaining: 1.62s
37: learn: 0.5719202    total: 2.68s    remaining: 1.55s
38: learn: 0.5696997    total: 2.75s    remaining: 1.48s
39: learn: 0.5672249    total: 2.81s    remaining: 1.41s
40: learn: 0.5640378    total: 2.88s    remaining: 1.33s
41: learn: 0.5617436    total: 2.95s    remaining: 1.26s
42: learn: 0.5594742    total: 3.02s    remaining: 1.19s
43: learn: 0.5568258    total: 3.09s    remaining: 1.12s
44: learn: 0.5541508    total: 3.15s    remaining: 1.05s
45: learn: 0.5516927    total: 3.23s    remaining: 983ms
46: learn: 0.5494778    total: 3.31s    remaining: 915ms
47: learn: 0.5465194    total: 3.38s    remaining: 846ms
48: learn: 0.5445204    total: 3.45s    remaining: 775ms
49: learn: 0.5424344    total: 3.53s    remaining: 706ms
50: learn: 0.5407045    total: 3.61s    remaining: 637ms
51: learn: 0.5380486    total: 3.67s    remaining: 565ms
52: learn: 0.5352292    total: 3.75s    remaining: 495ms
53: learn: 0.5333757    total: 3.81s    remaining: 424ms
54: learn: 0.5322769    total: 3.88s    remaining: 353ms
55: learn: 0.5305465    total: 3.95s    remaining: 282ms
56: learn: 0.5291379    total: 4.03s    remaining: 212ms
57: learn: 0.5275452    total: 4.09s    remaining: 141ms
58: learn: 0.5256782    total: 4.17s    remaining: 70.6ms
59: learn: 0.5239801    total: 4.23s    remaining: 0us
0:  learn: 0.6780674    total: 57ms remaining: 5.64s
1:  learn: 0.6656423    total: 103ms    remaining: 5.03s
2:  learn: 0.6499618    total: 157ms    remaining: 5.09s
3:  learn: 0.6394232    total: 211ms    remaining: 5.06s
4:  learn: 0.6275185    total: 285ms    remaining: 5.41s
5:  learn: 0.6156624    total: 341ms    remaining: 5.35s
6:  learn: 0.6007392    total: 401ms    remaining: 5.33s
7:  learn: 0.5896390    total: 454ms    remaining: 5.22s
8:  learn: 0.5800070    total: 515ms    remaining: 5.2s
9:  learn: 0.5667137    total: 580ms    remaining: 5.22s
10: learn: 0.5561906    total: 642ms    remaining: 5.2s
11: learn: 0.5475448    total: 703ms    remaining: 5.15s
12: learn: 0.5399063    total: 762ms    remaining: 5.1s
13: learn: 0.5328563    total: 819ms    remaining: 5.03s
14: learn: 0.5269186    total: 884ms    remaining: 5.01s
15: learn: 0.5229059    total: 951ms    remaining: 4.99s
16: learn: 0.5166568    total: 1.01s    remaining: 4.93s
17: learn: 0.5116476    total: 1.11s    remaining: 5.06s
18: learn: 0.5060172    total: 1.17s    remaining: 4.98s
19: learn: 0.5000798    total: 1.24s    remaining: 4.96s
20: learn: 0.4957177    total: 1.3s remaining: 4.91s
21: learn: 0.4912718    total: 1.36s    remaining: 4.82s
22: learn: 0.4876625    total: 1.42s    remaining: 4.75s
23: learn: 0.4845066    total: 1.48s    remaining: 4.68s
24: learn: 0.4792528    total: 1.53s    remaining: 4.59s
25: learn: 0.4743935    total: 1.59s    remaining: 4.52s
26: learn: 0.4718634    total: 1.64s    remaining: 4.43s
27: learn: 0.4674427    total: 1.7s remaining: 4.36s
28: learn: 0.4652127    total: 1.75s    remaining: 4.29s
29: learn: 0.4634977    total: 1.8s remaining: 4.21s
30: learn: 0.4610593    total: 1.86s    remaining: 4.14s
31: learn: 0.4590537    total: 1.92s    remaining: 4.09s
32: learn: 0.4573011    total: 1.99s    remaining: 4.04s
33: learn: 0.4546308    total: 2.06s    remaining: 3.99s
34: learn: 0.4517896    total: 2.11s    remaining: 3.91s
35: learn: 0.4480228    total: 2.16s    remaining: 3.84s
36: learn: 0.4462477    total: 2.22s    remaining: 3.77s
37: learn: 0.4434832    total: 2.27s    remaining: 3.7s
38: learn: 0.4409929    total: 2.35s    remaining: 3.67s
39: learn: 0.4374450    total: 2.4s remaining: 3.59s
40: learn: 0.4350297    total: 2.45s    remaining: 3.53s
41: learn: 0.4329302    total: 2.51s    remaining: 3.47s
42: learn: 0.4297999    total: 2.57s    remaining: 3.41s
43: learn: 0.4281784    total: 2.62s    remaining: 3.33s
44: learn: 0.4271110    total: 2.67s    remaining: 3.26s
45: learn: 0.4259213    total: 2.74s    remaining: 3.21s
46: learn: 0.4248926    total: 2.79s    remaining: 3.15s
47: learn: 0.4224029    total: 2.86s    remaining: 3.1s
48: learn: 0.4210638    total: 2.92s    remaining: 3.04s
49: learn: 0.4198819    total: 2.98s    remaining: 2.98s
50: learn: 0.4189043    total: 3.04s    remaining: 2.92s
51: learn: 0.4179394    total: 3.1s remaining: 2.86s
52: learn: 0.4145549    total: 3.17s    remaining: 2.81s
53: learn: 0.4116847    total: 3.23s    remaining: 2.75s
54: learn: 0.4107215    total: 3.29s    remaining: 2.69s
55: learn: 0.4084986    total: 3.34s    remaining: 2.63s
56: learn: 0.4067278    total: 3.4s remaining: 2.56s
57: learn: 0.4058478    total: 3.45s    remaining: 2.5s
58: learn: 0.4050000    total: 3.5s remaining: 2.43s
59: learn: 0.4039232    total: 3.56s    remaining: 2.37s
60: learn: 0.4032417    total: 3.61s    remaining: 2.31s
61: learn: 0.4025092    total: 3.67s    remaining: 2.25s
62: learn: 0.4019447    total: 3.72s    remaining: 2.19s
63: learn: 0.3978013    total: 3.78s    remaining: 2.13s
64: learn: 0.3971572    total: 3.84s    remaining: 2.07s
65: learn: 0.3952749    total: 3.89s    remaining: 2s
66: learn: 0.3943706    total: 3.95s    remaining: 1.95s
67: learn: 0.3935298    total: 4.01s    remaining: 1.89s
68: learn: 0.3928379    total: 4.07s    remaining: 1.83s
69: learn: 0.3922042    total: 4.13s    remaining: 1.77s
70: learn: 0.3915744    total: 4.18s    remaining: 1.71s
71: learn: 0.3909309    total: 4.24s    remaining: 1.65s
72: learn: 0.3902807    total: 4.29s    remaining: 1.59s
73: learn: 0.3897191    total: 4.35s    remaining: 1.53s
74: learn: 0.3870658    total: 4.41s    remaining: 1.47s
75: learn: 0.3854908    total: 4.47s    remaining: 1.41s
76: learn: 0.3849864    total: 4.54s    remaining: 1.35s
77: learn: 0.3843592    total: 4.6s remaining: 1.3s
78: learn: 0.3837757    total: 4.66s    remaining: 1.24s
79: learn: 0.3834153    total: 4.72s    remaining: 1.18s
80: learn: 0.3820647    total: 4.79s    remaining: 1.12s
81: learn: 0.3815409    total: 4.84s    remaining: 1.06s
82: learn: 0.3809143    total: 4.9s remaining: 1s
83: learn: 0.3804150    total: 4.97s    remaining: 946ms
84: learn: 0.3783076    total: 5.05s    remaining: 892ms
85: learn: 0.3777422    total: 5.12s    remaining: 834ms
86: learn: 0.3773108    total: 5.18s    remaining: 774ms
87: learn: 0.3759948    total: 5.25s    remaining: 716ms
88: learn: 0.3733624    total: 5.31s    remaining: 656ms
89: learn: 0.3728116    total: 5.37s    remaining: 596ms
90: learn: 0.3722956    total: 5.43s    remaining: 537ms
91: learn: 0.3718745    total: 5.48s    remaining: 477ms
92: learn: 0.3709713    total: 5.54s    remaining: 417ms
93: learn: 0.3705749    total: 5.6s remaining: 357ms
94: learn: 0.3701589    total: 5.66s    remaining: 298ms
95: learn: 0.3696644    total: 5.71s    remaining: 238ms
96: learn: 0.3693308    total: 5.78s    remaining: 179ms
97: learn: 0.3689769    total: 5.84s    remaining: 119ms
98: learn: 0.3685521    total: 5.92s    remaining: 59.8ms
99: learn: 0.3681502    total: 5.99s    remaining: 0us
0:  learn: 0.6787347    total: 52.9ms   remaining: 5.24s
1:  learn: 0.6629308    total: 112ms    remaining: 5.49s
2:  learn: 0.6521186    total: 175ms    remaining: 5.66s
3:  learn: 0.6424923    total: 240ms    remaining: 5.75s
4:  learn: 0.6330328    total: 299ms    remaining: 5.67s
5:  learn: 0.6254169    total: 370ms    remaining: 5.8s
6:  learn: 0.6134426    total: 434ms    remaining: 5.76s
7:  learn: 0.6037043    total: 494ms    remaining: 5.68s
8:  learn: 0.5960655    total: 547ms    remaining: 5.53s
9:  learn: 0.5894701    total: 623ms    remaining: 5.61s
10: learn: 0.5801976    total: 703ms    remaining: 5.69s
11: learn: 0.5686701    total: 766ms    remaining: 5.62s
12: learn: 0.5604865    total: 824ms    remaining: 5.52s
13: learn: 0.5518040    total: 887ms    remaining: 5.45s
14: learn: 0.5465118    total: 950ms    remaining: 5.38s
15: learn: 0.5355247    total: 1.02s    remaining: 5.37s
16: learn: 0.5289492    total: 1.08s    remaining: 5.25s
17: learn: 0.5228831    total: 1.13s    remaining: 5.14s
18: learn: 0.5187277    total: 1.19s    remaining: 5.07s
19: learn: 0.5136091    total: 1.26s    remaining: 5.04s
20: learn: 0.5089900    total: 1.31s    remaining: 4.95s
21: learn: 0.5028772    total: 1.37s    remaining: 4.87s
22: learn: 0.4980801    total: 1.43s    remaining: 4.8s
23: learn: 0.4936861    total: 1.49s    remaining: 4.72s
24: learn: 0.4904537    total: 1.54s    remaining: 4.64s
25: learn: 0.4867408    total: 1.6s remaining: 4.56s
26: learn: 0.4836632    total: 1.67s    remaining: 4.5s
27: learn: 0.4810250    total: 1.72s    remaining: 4.42s
28: learn: 0.4770696    total: 1.78s    remaining: 4.35s
29: learn: 0.4747817    total: 1.85s    remaining: 4.33s
30: learn: 0.4702333    total: 1.92s    remaining: 4.26s
31: learn: 0.4681339    total: 1.98s    remaining: 4.2s
32: learn: 0.4661564    total: 2.03s    remaining: 4.13s
33: learn: 0.4627045    total: 2.09s    remaining: 4.06s
34: learn: 0.4593287    total: 2.15s    remaining: 3.99s
35: learn: 0.4577672    total: 2.21s    remaining: 3.92s
36: learn: 0.4541266    total: 2.26s    remaining: 3.85s
37: learn: 0.4509724    total: 2.31s    remaining: 3.78s
38: learn: 0.4492433    total: 2.37s    remaining: 3.71s
39: learn: 0.4454138    total: 2.44s    remaining: 3.66s
40: learn: 0.4440516    total: 2.5s remaining: 3.59s
41: learn: 0.4413669    total: 2.55s    remaining: 3.53s
42: learn: 0.4383825    total: 2.61s    remaining: 3.46s
43: learn: 0.4364719    total: 2.66s    remaining: 3.39s
44: learn: 0.4338141    total: 2.73s    remaining: 3.34s
45: learn: 0.4326615    total: 2.79s    remaining: 3.28s
46: learn: 0.4310823    total: 2.85s    remaining: 3.22s
47: learn: 0.4274267    total: 2.92s    remaining: 3.16s
48: learn: 0.4263108    total: 2.99s    remaining: 3.11s
49: learn: 0.4232592    total: 3.04s    remaining: 3.04s
50: learn: 0.4219268    total: 3.1s remaining: 2.97s
51: learn: 0.4209638    total: 3.15s    remaining: 2.9s
52: learn: 0.4189954    total: 3.21s    remaining: 2.84s
53: learn: 0.4156722    total: 3.26s    remaining: 2.78s
54: learn: 0.4141122    total: 3.33s    remaining: 2.72s
55: learn: 0.4131974    total: 3.38s    remaining: 2.65s
56: learn: 0.4091712    total: 3.44s    remaining: 2.59s
57: learn: 0.4073386    total: 3.5s remaining: 2.53s
58: learn: 0.4061417    total: 3.58s    remaining: 2.48s
59: learn: 0.4051938    total: 3.63s    remaining: 2.42s
60: learn: 0.4029454    total: 3.69s    remaining: 2.36s
61: learn: 0.4023675    total: 3.74s    remaining: 2.29s
62: learn: 0.4017586    total: 3.79s    remaining: 2.23s
63: learn: 0.4010712    total: 3.86s    remaining: 2.17s
64: learn: 0.3987395    total: 3.92s    remaining: 2.11s
65: learn: 0.3979852    total: 3.98s    remaining: 2.05s
66: learn: 0.3974124    total: 4.04s    remaining: 1.99s
67: learn: 0.3950991    total: 4.11s    remaining: 1.93s
68: learn: 0.3943587    total: 4.17s    remaining: 1.87s
69: learn: 0.3924520    total: 4.23s    remaining: 1.81s
70: learn: 0.3895471    total: 4.3s remaining: 1.76s
71: learn: 0.3887278    total: 4.37s    remaining: 1.7s
72: learn: 0.3879781    total: 4.42s    remaining: 1.64s
73: learn: 0.3863183    total: 4.48s    remaining: 1.57s
74: learn: 0.3858390    total: 4.54s    remaining: 1.51s
75: learn: 0.3837355    total: 4.61s    remaining: 1.46s
76: learn: 0.3831220    total: 4.67s    remaining: 1.4s
77: learn: 0.3825741    total: 4.74s    remaining: 1.34s
78: learn: 0.3819807    total: 4.8s remaining: 1.27s
79: learn: 0.3816032    total: 4.86s    remaining: 1.21s
80: learn: 0.3810282    total: 4.92s    remaining: 1.15s
81: learn: 0.3788452    total: 4.97s    remaining: 1.09s
82: learn: 0.3784096    total: 5.04s    remaining: 1.03s
83: learn: 0.3777615    total: 5.1s remaining: 971ms
84: learn: 0.3773304    total: 5.15s    remaining: 909ms
85: learn: 0.3768781    total: 5.2s remaining: 847ms
86: learn: 0.3744644    total: 5.26s    remaining: 786ms
87: learn: 0.3740947    total: 5.32s    remaining: 725ms
88: learn: 0.3737719    total: 5.38s    remaining: 665ms
89: learn: 0.3727752    total: 5.44s    remaining: 604ms
90: learn: 0.3724632    total: 5.5s remaining: 544ms
91: learn: 0.3720264    total: 5.55s    remaining: 483ms
92: learn: 0.3715933    total: 5.62s    remaining: 423ms
93: learn: 0.3701342    total: 5.7s remaining: 364ms
94: learn: 0.3697976    total: 5.76s    remaining: 303ms
95: learn: 0.3694249    total: 5.82s    remaining: 242ms
96: learn: 0.3674265    total: 5.88s    remaining: 182ms
97: learn: 0.3660059    total: 5.93s    remaining: 121ms
98: learn: 0.3650624    total: 5.99s    remaining: 60.5ms
99: learn: 0.3641319    total: 6.06s    remaining: 0us
0:  learn: 0.6755975    total: 58.4ms   remaining: 5.78s
1:  learn: 0.6640494    total: 111ms    remaining: 5.45s
2:  learn: 0.6479160    total: 169ms    remaining: 5.47s
3:  learn: 0.6353381    total: 225ms    remaining: 5.41s
4:  learn: 0.6256086    total: 275ms    remaining: 5.22s
5:  learn: 0.6123194    total: 334ms    remaining: 5.23s
6:  learn: 0.6004987    total: 387ms    remaining: 5.15s
7:  learn: 0.5877596    total: 439ms    remaining: 5.05s
8:  learn: 0.5787817    total: 492ms    remaining: 4.98s
9:  learn: 0.5685038    total: 550ms    remaining: 4.95s
10: learn: 0.5621513    total: 605ms    remaining: 4.89s
11: learn: 0.5530775    total: 669ms    remaining: 4.9s
12: learn: 0.5472004    total: 728ms    remaining: 4.87s
13: learn: 0.5408390    total: 783ms    remaining: 4.81s
14: learn: 0.5362334    total: 850ms    remaining: 4.82s
15: learn: 0.5291432    total: 902ms    remaining: 4.73s
16: learn: 0.5192261    total: 963ms    remaining: 4.7s
17: learn: 0.5160676    total: 1.01s    remaining: 4.62s
18: learn: 0.5102759    total: 1.08s    remaining: 4.62s
19: learn: 0.5053539    total: 1.14s    remaining: 4.57s
20: learn: 0.5018072    total: 1.21s    remaining: 4.54s
21: learn: 0.4984342    total: 1.26s    remaining: 4.46s
22: learn: 0.4946264    total: 1.31s    remaining: 4.38s
23: learn: 0.4895979    total: 1.36s    remaining: 4.31s
24: learn: 0.4864200    total: 1.44s    remaining: 4.32s
25: learn: 0.4833633    total: 1.49s    remaining: 4.24s
26: learn: 0.4787359    total: 1.55s    remaining: 4.2s
27: learn: 0.4742556    total: 1.61s    remaining: 4.14s
28: learn: 0.4722703    total: 1.67s    remaining: 4.09s
29: learn: 0.4680890    total: 1.72s    remaining: 4.01s
30: learn: 0.4646848    total: 1.79s    remaining: 3.99s
31: learn: 0.4624585    total: 1.84s    remaining: 3.92s
32: learn: 0.4603976    total: 1.91s    remaining: 3.87s
33: learn: 0.4586205    total: 1.95s    remaining: 3.79s
34: learn: 0.4554294    total: 2.01s    remaining: 3.74s
35: learn: 0.4521939    total: 2.07s    remaining: 3.68s
36: learn: 0.4490775    total: 2.13s    remaining: 3.63s
37: learn: 0.4476623    total: 2.19s    remaining: 3.57s
38: learn: 0.4459154    total: 2.24s    remaining: 3.5s
39: learn: 0.4442376    total: 2.29s    remaining: 3.44s
40: learn: 0.4415100    total: 2.35s    remaining: 3.38s
41: learn: 0.4402119    total: 2.4s remaining: 3.32s
42: learn: 0.4388914    total: 2.46s    remaining: 3.25s
43: learn: 0.4362361    total: 2.51s    remaining: 3.2s
44: learn: 0.4351212    total: 2.56s    remaining: 3.13s
45: learn: 0.4339355    total: 2.61s    remaining: 3.06s
46: learn: 0.4316856    total: 2.67s    remaining: 3s
47: learn: 0.4303935    total: 2.71s    remaining: 2.94s
48: learn: 0.4290825    total: 2.76s    remaining: 2.88s
49: learn: 0.4268229    total: 2.83s    remaining: 2.83s
50: learn: 0.4258701    total: 2.88s    remaining: 2.77s
51: learn: 0.4247373    total: 2.94s    remaining: 2.71s
52: learn: 0.4237794    total: 2.99s    remaining: 2.65s
53: learn: 0.4227810    total: 3.04s    remaining: 2.59s
54: learn: 0.4179922    total: 3.1s remaining: 2.53s
55: learn: 0.4168096    total: 3.15s    remaining: 2.47s
56: learn: 0.4159020    total: 3.19s    remaining: 2.41s
57: learn: 0.4149399    total: 3.25s    remaining: 2.35s
58: learn: 0.4110033    total: 3.3s remaining: 2.29s
59: learn: 0.4100967    total: 3.35s    remaining: 2.23s
60: learn: 0.4083306    total: 3.4s remaining: 2.18s
61: learn: 0.4072024    total: 3.46s    remaining: 2.12s
62: learn: 0.4065295    total: 3.52s    remaining: 2.06s
63: learn: 0.4058783    total: 3.56s    remaining: 2s
64: learn: 0.4039702    total: 3.62s    remaining: 1.95s
65: learn: 0.4022050    total: 3.67s    remaining: 1.89s
66: learn: 0.4003304    total: 3.72s    remaining: 1.83s
67: learn: 0.3997262    total: 3.78s    remaining: 1.78s
68: learn: 0.3982798    total: 3.84s    remaining: 1.73s
69: learn: 0.3976596    total: 3.91s    remaining: 1.68s
70: learn: 0.3971132    total: 3.96s    remaining: 1.62s
71: learn: 0.3964351    total: 4.04s    remaining: 1.57s
72: learn: 0.3945348    total: 4.1s remaining: 1.51s
73: learn: 0.3939425    total: 4.17s    remaining: 1.47s
74: learn: 0.3932463    total: 4.23s    remaining: 1.41s
75: learn: 0.3928738    total: 4.3s remaining: 1.36s
76: learn: 0.3911859    total: 4.36s    remaining: 1.3s
77: learn: 0.3907312    total: 4.42s    remaining: 1.25s
78: learn: 0.3896094    total: 4.47s    remaining: 1.19s
79: learn: 0.3890563    total: 4.53s    remaining: 1.13s
80: learn: 0.3884495    total: 4.58s    remaining: 1.07s
81: learn: 0.3878721    total: 4.63s    remaining: 1.02s
82: learn: 0.3870853    total: 4.71s    remaining: 964ms
83: learn: 0.3864268    total: 4.76s    remaining: 907ms
84: learn: 0.3836755    total: 4.81s    remaining: 849ms
85: learn: 0.3831474    total: 4.88s    remaining: 794ms
86: learn: 0.3824414    total: 4.93s    remaining: 736ms
87: learn: 0.3820372    total: 4.98s    remaining: 679ms
88: learn: 0.3811514    total: 5.06s    remaining: 625ms
89: learn: 0.3785666    total: 5.12s    remaining: 569ms
90: learn: 0.3777927    total: 5.17s    remaining: 512ms
91: learn: 0.3773483    total: 5.23s    remaining: 455ms
92: learn: 0.3768092    total: 5.29s    remaining: 398ms
93: learn: 0.3747709    total: 5.34s    remaining: 341ms
94: learn: 0.3738006    total: 5.39s    remaining: 284ms
95: learn: 0.3726374    total: 5.46s    remaining: 227ms
96: learn: 0.3714779    total: 5.51s    remaining: 170ms
97: learn: 0.3708958    total: 5.61s    remaining: 114ms
98: learn: 0.3704787    total: 5.66s    remaining: 57.1ms
99: learn: 0.3693287    total: 5.71s    remaining: 0us
0:  learn: 0.6854808    total: 47.1ms   remaining: 3.25s
1:  learn: 0.6789342    total: 86.4ms   remaining: 2.94s
2:  learn: 0.6710810    total: 137ms    remaining: 3.06s
3:  learn: 0.6652697    total: 181ms    remaining: 2.98s
4:  learn: 0.6584662    total: 227ms    remaining: 2.95s
5:  learn: 0.6511288    total: 275ms    remaining: 2.93s
6:  learn: 0.6465899    total: 315ms    remaining: 2.84s
7:  learn: 0.6414408    total: 359ms    remaining: 2.78s
8:  learn: 0.6351526    total: 405ms    remaining: 2.75s
9:  learn: 0.6288849    total: 449ms    remaining: 2.69s
10: learn: 0.6214170    total: 496ms    remaining: 2.66s
11: learn: 0.6170727    total: 543ms    remaining: 2.62s
12: learn: 0.6105342    total: 587ms    remaining: 2.58s
13: learn: 0.6051710    total: 636ms    remaining: 2.54s
14: learn: 0.5995441    total: 686ms    remaining: 2.52s
15: learn: 0.5955350    total: 731ms    remaining: 2.47s
16: learn: 0.5899862    total: 783ms    remaining: 2.44s
17: learn: 0.5845524    total: 833ms    remaining: 2.41s
18: learn: 0.5800335    total: 883ms    remaining: 2.37s
19: learn: 0.5748668    total: 933ms    remaining: 2.33s
20: learn: 0.5706926    total: 986ms    remaining: 2.3s
21: learn: 0.5644252    total: 1.04s    remaining: 2.26s
22: learn: 0.5606822    total: 1.09s    remaining: 2.23s
23: learn: 0.5553719    total: 1.14s    remaining: 2.18s
24: learn: 0.5526506    total: 1.18s    remaining: 2.13s
25: learn: 0.5490226    total: 1.24s    remaining: 2.09s
26: learn: 0.5437315    total: 1.28s    remaining: 2.04s
27: learn: 0.5406485    total: 1.33s    remaining: 2s
28: learn: 0.5372565    total: 1.38s    remaining: 1.96s
29: learn: 0.5327823    total: 1.43s    remaining: 1.91s
30: learn: 0.5298844    total: 1.48s    remaining: 1.86s
31: learn: 0.5265301    total: 1.53s    remaining: 1.82s
32: learn: 0.5244033    total: 1.57s    remaining: 1.76s
33: learn: 0.5204570    total: 1.63s    remaining: 1.73s
34: learn: 0.5186000    total: 1.69s    remaining: 1.69s
35: learn: 0.5166380    total: 1.74s    remaining: 1.64s
36: learn: 0.5138762    total: 1.79s    remaining: 1.59s
37: learn: 0.5113238    total: 1.84s    remaining: 1.55s
38: learn: 0.5079826    total: 1.89s    remaining: 1.51s
39: learn: 0.5063132    total: 1.94s    remaining: 1.45s
40: learn: 0.5034582    total: 1.99s    remaining: 1.41s
41: learn: 0.5016317    total: 2.04s    remaining: 1.36s
42: learn: 0.4989641    total: 2.09s    remaining: 1.31s
43: learn: 0.4968961    total: 2.15s    remaining: 1.27s
44: learn: 0.4955183    total: 2.21s    remaining: 1.23s
45: learn: 0.4940431    total: 2.25s    remaining: 1.18s
46: learn: 0.4912095    total: 2.3s remaining: 1.13s
47: learn: 0.4897417    total: 2.35s    remaining: 1.08s
48: learn: 0.4874051    total: 2.41s    remaining: 1.03s
49: learn: 0.4853273    total: 2.46s    remaining: 983ms
50: learn: 0.4830105    total: 2.51s    remaining: 935ms
51: learn: 0.4792982    total: 2.56s    remaining: 885ms
52: learn: 0.4773667    total: 2.6s remaining: 835ms
53: learn: 0.4751678    total: 2.66s    remaining: 788ms
54: learn: 0.4720366    total: 2.72s    remaining: 741ms
55: learn: 0.4702685    total: 2.79s    remaining: 699ms
56: learn: 0.4690686    total: 2.84s    remaining: 648ms
57: learn: 0.4670308    total: 2.88s    remaining: 597ms
58: learn: 0.4659758    total: 2.93s    remaining: 546ms
59: learn: 0.4648619    total: 2.98s    remaining: 497ms
60: learn: 0.4637065    total: 3.02s    remaining: 446ms
61: learn: 0.4616158    total: 3.07s    remaining: 396ms
62: learn: 0.4606571    total: 3.11s    remaining: 345ms
63: learn: 0.4580198    total: 3.16s    remaining: 296ms
64: learn: 0.4564792    total: 3.2s remaining: 246ms
65: learn: 0.4555989    total: 3.25s    remaining: 197ms
66: learn: 0.4544228    total: 3.29s    remaining: 147ms
67: learn: 0.4533427    total: 3.34s    remaining: 98.2ms
68: learn: 0.4518842    total: 3.38s    remaining: 49ms
69: learn: 0.4510303    total: 3.43s    remaining: 0us
0:  learn: 0.6858449    total: 48.6ms   remaining: 3.35s
1:  learn: 0.6778718    total: 97.3ms   remaining: 3.31s
2:  learn: 0.6719399    total: 142ms    remaining: 3.18s
3:  learn: 0.6664632    total: 192ms    remaining: 3.16s
4:  learn: 0.6591999    total: 242ms    remaining: 3.15s
5:  learn: 0.6538062    total: 287ms    remaining: 3.06s
6:  learn: 0.6478654    total: 338ms    remaining: 3.04s
7:  learn: 0.6426394    total: 387ms    remaining: 3s
8:  learn: 0.6359268    total: 436ms    remaining: 2.96s
9:  learn: 0.6298517    total: 489ms    remaining: 2.93s
10: learn: 0.6239089    total: 536ms    remaining: 2.88s
11: learn: 0.6164559    total: 587ms    remaining: 2.84s
12: learn: 0.6120916    total: 633ms    remaining: 2.78s
13: learn: 0.6081299    total: 680ms    remaining: 2.72s
14: learn: 0.6042900    total: 742ms    remaining: 2.72s
15: learn: 0.6012153    total: 789ms    remaining: 2.66s
16: learn: 0.5954841    total: 839ms    remaining: 2.61s
17: learn: 0.5903460    total: 889ms    remaining: 2.57s
18: learn: 0.5836323    total: 938ms    remaining: 2.52s
19: learn: 0.5803133    total: 982ms    remaining: 2.46s
20: learn: 0.5767913    total: 1.03s    remaining: 2.4s
21: learn: 0.5733150    total: 1.07s    remaining: 2.34s
22: learn: 0.5701430    total: 1.12s    remaining: 2.29s
23: learn: 0.5658711    total: 1.17s    remaining: 2.24s
24: learn: 0.5615695    total: 1.22s    remaining: 2.19s
25: learn: 0.5583077    total: 1.27s    remaining: 2.15s
26: learn: 0.5553097    total: 1.32s    remaining: 2.11s
27: learn: 0.5524697    total: 1.39s    remaining: 2.08s
28: learn: 0.5488063    total: 1.45s    remaining: 2.05s
29: learn: 0.5454557    total: 1.52s    remaining: 2.02s
30: learn: 0.5418739    total: 1.6s remaining: 2.01s
31: learn: 0.5372384    total: 1.65s    remaining: 1.96s
32: learn: 0.5339865    total: 1.72s    remaining: 1.92s
33: learn: 0.5300285    total: 1.77s    remaining: 1.87s
34: learn: 0.5281090    total: 1.82s    remaining: 1.82s
35: learn: 0.5250143    total: 1.87s    remaining: 1.77s
36: learn: 0.5220368    total: 1.93s    remaining: 1.72s
37: learn: 0.5189688    total: 1.98s    remaining: 1.67s
38: learn: 0.5161286    total: 2.03s    remaining: 1.61s
39: learn: 0.5132831    total: 2.08s    remaining: 1.56s
40: learn: 0.5106647    total: 2.13s    remaining: 1.5s
41: learn: 0.5089302    total: 2.17s    remaining: 1.45s
42: learn: 0.5070844    total: 2.22s    remaining: 1.39s
43: learn: 0.5044440    total: 2.29s    remaining: 1.35s
44: learn: 0.5024906    total: 2.34s    remaining: 1.3s
45: learn: 0.4986218    total: 2.39s    remaining: 1.25s
46: learn: 0.4959638    total: 2.45s    remaining: 1.2s
47: learn: 0.4937223    total: 2.51s    remaining: 1.15s
48: learn: 0.4906317    total: 2.57s    remaining: 1.1s
49: learn: 0.4879473    total: 2.63s    remaining: 1.05s
50: learn: 0.4855137    total: 2.68s    remaining: 998ms
51: learn: 0.4841537    total: 2.74s    remaining: 948ms
52: learn: 0.4825360    total: 2.79s    remaining: 896ms
53: learn: 0.4809738    total: 2.85s    remaining: 845ms
54: learn: 0.4797089    total: 2.9s remaining: 791ms
55: learn: 0.4785221    total: 2.96s    remaining: 739ms
56: learn: 0.4771580    total: 3s   remaining: 685ms
57: learn: 0.4757980    total: 3.05s    remaining: 631ms
58: learn: 0.4746702    total: 3.11s    remaining: 580ms
59: learn: 0.4734499    total: 3.17s    remaining: 528ms
60: learn: 0.4714873    total: 3.22s    remaining: 475ms
61: learn: 0.4704030    total: 3.27s    remaining: 422ms
62: learn: 0.4691505    total: 3.32s    remaining: 369ms
63: learn: 0.4682198    total: 3.39s    remaining: 317ms
64: learn: 0.4674057    total: 3.44s    remaining: 264ms
65: learn: 0.4662900    total: 3.48s    remaining: 211ms
66: learn: 0.4646490    total: 3.54s    remaining: 158ms
67: learn: 0.4637058    total: 3.58s    remaining: 105ms
68: learn: 0.4626845    total: 3.63s    remaining: 52.6ms
69: learn: 0.4617493    total: 3.67s    remaining: 0us
0:  learn: 0.6842231    total: 45.5ms   remaining: 3.14s
1:  learn: 0.6781632    total: 90.2ms   remaining: 3.07s
2:  learn: 0.6688444    total: 140ms    remaining: 3.12s
3:  learn: 0.6616521    total: 190ms    remaining: 3.13s
4:  learn: 0.6564388    total: 266ms    remaining: 3.45s
5:  learn: 0.6512287    total: 312ms    remaining: 3.33s
6:  learn: 0.6456828    total: 358ms    remaining: 3.22s
7:  learn: 0.6406841    total: 407ms    remaining: 3.16s
8:  learn: 0.6343532    total: 465ms    remaining: 3.15s
9:  learn: 0.6305959    total: 526ms    remaining: 3.16s
10: learn: 0.6247688    total: 575ms    remaining: 3.08s
11: learn: 0.6188381    total: 627ms    remaining: 3.03s
12: learn: 0.6147997    total: 678ms    remaining: 2.97s
13: learn: 0.6101684    total: 743ms    remaining: 2.97s
14: learn: 0.6038660    total: 795ms    remaining: 2.91s
15: learn: 0.5987693    total: 848ms    remaining: 2.86s
16: learn: 0.5932746    total: 902ms    remaining: 2.81s
17: learn: 0.5881489    total: 953ms    remaining: 2.75s
18: learn: 0.5816394    total: 1.01s    remaining: 2.72s
19: learn: 0.5752610    total: 1.07s    remaining: 2.67s
20: learn: 0.5723198    total: 1.14s    remaining: 2.67s
21: learn: 0.5674529    total: 1.19s    remaining: 2.59s
22: learn: 0.5631400    total: 1.24s    remaining: 2.53s
23: learn: 0.5587330    total: 1.28s    remaining: 2.46s
24: learn: 0.5548008    total: 1.33s    remaining: 2.4s
25: learn: 0.5512702    total: 1.38s    remaining: 2.33s
26: learn: 0.5484515    total: 1.42s    remaining: 2.27s
27: learn: 0.5449410    total: 1.48s    remaining: 2.21s
28: learn: 0.5409617    total: 1.52s    remaining: 2.16s
29: learn: 0.5375626    total: 1.58s    remaining: 2.1s
30: learn: 0.5345770    total: 1.63s    remaining: 2.04s
31: learn: 0.5322886    total: 1.67s    remaining: 1.98s
32: learn: 0.5300507    total: 1.72s    remaining: 1.92s
33: learn: 0.5282686    total: 1.76s    remaining: 1.86s
34: learn: 0.5253057    total: 1.81s    remaining: 1.81s
35: learn: 0.5223315    total: 1.86s    remaining: 1.75s
36: learn: 0.5194561    total: 1.9s remaining: 1.7s
37: learn: 0.5176734    total: 1.95s    remaining: 1.65s
38: learn: 0.5151183    total: 2s   remaining: 1.59s
39: learn: 0.5121722    total: 2.05s    remaining: 1.54s
40: learn: 0.5087551    total: 2.1s remaining: 1.49s
41: learn: 0.5061796    total: 2.15s    remaining: 1.44s
42: learn: 0.5041603    total: 2.2s remaining: 1.38s
43: learn: 0.5008195    total: 2.25s    remaining: 1.33s
44: learn: 0.4981981    total: 2.3s remaining: 1.28s
45: learn: 0.4961635    total: 2.35s    remaining: 1.23s
46: learn: 0.4943916    total: 2.42s    remaining: 1.18s
47: learn: 0.4928103    total: 2.47s    remaining: 1.13s
48: learn: 0.4912153    total: 2.52s    remaining: 1.08s
49: learn: 0.4891246    total: 2.57s    remaining: 1.03s
50: learn: 0.4876870    total: 2.63s    remaining: 980ms
51: learn: 0.4860567    total: 2.68s    remaining: 928ms
52: learn: 0.4847793    total: 2.73s    remaining: 876ms
53: learn: 0.4837673    total: 2.77s    remaining: 822ms
54: learn: 0.4817976    total: 2.82s    remaining: 769ms
55: learn: 0.4803827    total: 2.86s    remaining: 716ms
56: learn: 0.4791580    total: 2.91s    remaining: 664ms
57: learn: 0.4770999    total: 2.96s    remaining: 612ms
58: learn: 0.4760986    total: 3.02s    remaining: 563ms
59: learn: 0.4739569    total: 3.07s    remaining: 512ms
60: learn: 0.4719559    total: 3.12s    remaining: 460ms
61: learn: 0.4708329    total: 3.17s    remaining: 409ms
62: learn: 0.4696806    total: 3.21s    remaining: 357ms
63: learn: 0.4681599    total: 3.26s    remaining: 306ms
64: learn: 0.4672418    total: 3.31s    remaining: 254ms
65: learn: 0.4653938    total: 3.36s    remaining: 204ms
66: learn: 0.4643815    total: 3.41s    remaining: 153ms
67: learn: 0.4634560    total: 3.46s    remaining: 102ms
68: learn: 0.4620317    total: 3.52s    remaining: 51ms
69: learn: 0.4608927    total: 3.57s    remaining: 0us
0:  learn: 0.6820864    total: 108ms    remaining: 975ms
1:  learn: 0.6717586    total: 224ms    remaining: 896ms
2:  learn: 0.6653624    total: 339ms    remaining: 791ms
3:  learn: 0.6535887    total: 459ms    remaining: 689ms
4:  learn: 0.6436591    total: 580ms    remaining: 580ms
5:  learn: 0.6357576    total: 699ms    remaining: 466ms
6:  learn: 0.6283024    total: 839ms    remaining: 359ms
7:  learn: 0.6206824    total: 970ms    remaining: 242ms
8:  learn: 0.6120411    total: 1.08s    remaining: 120ms
9:  learn: 0.6045663    total: 1.2s remaining: 0us
0:  learn: 0.6812725    total: 106ms    remaining: 950ms
1:  learn: 0.6682593    total: 214ms    remaining: 858ms
2:  learn: 0.6600218    total: 328ms    remaining: 765ms
3:  learn: 0.6502116    total: 427ms    remaining: 641ms
4:  learn: 0.6418241    total: 561ms    remaining: 561ms
5:  learn: 0.6320138    total: 669ms    remaining: 446ms
6:  learn: 0.6246990    total: 777ms    remaining: 333ms
7:  learn: 0.6172527    total: 882ms    remaining: 221ms
8:  learn: 0.6107522    total: 994ms    remaining: 110ms
9:  learn: 0.6046327    total: 1.11s    remaining: 0us
0:  learn: 0.6778932    total: 105ms    remaining: 945ms
1:  learn: 0.6699917    total: 208ms    remaining: 831ms
2:  learn: 0.6630856    total: 323ms    remaining: 754ms
3:  learn: 0.6541323    total: 421ms    remaining: 631ms
4:  learn: 0.6465458    total: 535ms    remaining: 535ms
5:  learn: 0.6383303    total: 648ms    remaining: 432ms
6:  learn: 0.6319545    total: 766ms    remaining: 328ms
7:  learn: 0.6249272    total: 906ms    remaining: 226ms
8:  learn: 0.6182948    total: 1.01s    remaining: 112ms
9:  learn: 0.6095659    total: 1.12s    remaining: 0us
0:  learn: 0.6788336    total: 78ms remaining: 702ms
1:  learn: 0.6641919    total: 157ms    remaining: 630ms
2:  learn: 0.6520778    total: 243ms    remaining: 567ms
3:  learn: 0.6378013    total: 321ms    remaining: 481ms
4:  learn: 0.6280943    total: 404ms    remaining: 404ms
5:  learn: 0.6183372    total: 489ms    remaining: 326ms
6:  learn: 0.6045327    total: 582ms    remaining: 250ms
7:  learn: 0.5970878    total: 671ms    remaining: 168ms
8:  learn: 0.5855487    total: 764ms    remaining: 84.8ms
9:  learn: 0.5745267    total: 850ms    remaining: 0us
0:  learn: 0.6754175    total: 80.7ms   remaining: 726ms
1:  learn: 0.6657609    total: 163ms    remaining: 652ms
2:  learn: 0.6497813    total: 278ms    remaining: 648ms
3:  learn: 0.6385731    total: 360ms    remaining: 540ms
4:  learn: 0.6243063    total: 453ms    remaining: 453ms
5:  learn: 0.6121638    total: 543ms    remaining: 362ms
6:  learn: 0.6001453    total: 629ms    remaining: 270ms
7:  learn: 0.5917983    total: 715ms    remaining: 179ms
8:  learn: 0.5795535    total: 802ms    remaining: 89.1ms
9:  learn: 0.5703570    total: 887ms    remaining: 0us
0:  learn: 0.6736660    total: 81.1ms   remaining: 730ms
1:  learn: 0.6632992    total: 163ms    remaining: 653ms
2:  learn: 0.6518791    total: 251ms    remaining: 587ms
3:  learn: 0.6420412    total: 344ms    remaining: 517ms
4:  learn: 0.6300900    total: 469ms    remaining: 469ms
5:  learn: 0.6202904    total: 556ms    remaining: 371ms
6:  learn: 0.6090917    total: 661ms    remaining: 283ms
7:  learn: 0.6016861    total: 764ms    remaining: 191ms
8:  learn: 0.5888547    total: 854ms    remaining: 94.9ms
9:  learn: 0.5786878    total: 951ms    remaining: 0us
0:  learn: 0.6870159    total: 204ms    remaining: 14.1s
1:  learn: 0.6828002    total: 413ms    remaining: 14.1s
2:  learn: 0.6777351    total: 619ms    remaining: 13.8s
3:  learn: 0.6726369    total: 831ms    remaining: 13.7s
4:  learn: 0.6657397    total: 1.06s    remaining: 13.8s
5:  learn: 0.6593904    total: 1.29s    remaining: 13.7s
6:  learn: 0.6543954    total: 1.53s    remaining: 13.8s
7:  learn: 0.6501775    total: 1.76s    remaining: 13.6s
8:  learn: 0.6455502    total: 1.98s    remaining: 13.4s
9:  learn: 0.6401297    total: 2.21s    remaining: 13.2s
10: learn: 0.6358096    total: 2.43s    remaining: 13.1s
11: learn: 0.6307778    total: 2.66s    remaining: 12.8s
12: learn: 0.6262099    total: 2.88s    remaining: 12.6s
13: learn: 0.6222219    total: 3.11s    remaining: 12.4s
14: learn: 0.6177531    total: 3.34s    remaining: 12.2s
15: learn: 0.6144299    total: 3.58s    remaining: 12.1s
16: learn: 0.6097591    total: 3.8s remaining: 11.8s
17: learn: 0.6060434    total: 4.03s    remaining: 11.6s
18: learn: 0.6016303    total: 4.24s    remaining: 11.4s
19: learn: 0.5982357    total: 4.46s    remaining: 11.2s
20: learn: 0.5944408    total: 4.7s remaining: 11s
21: learn: 0.5904644    total: 4.93s    remaining: 10.8s
22: learn: 0.5864407    total: 5.15s    remaining: 10.5s
23: learn: 0.5830545    total: 5.37s    remaining: 10.3s
24: learn: 0.5795563    total: 5.6s remaining: 10.1s
25: learn: 0.5759693    total: 5.83s    remaining: 9.86s
26: learn: 0.5728544    total: 6.05s    remaining: 9.64s
27: learn: 0.5695674    total: 6.3s remaining: 9.45s
28: learn: 0.5663685    total: 6.52s    remaining: 9.21s
29: learn: 0.5634423    total: 6.75s    remaining: 9.01s
30: learn: 0.5609288    total: 6.99s    remaining: 8.79s
31: learn: 0.5570840    total: 7.23s    remaining: 8.59s
32: learn: 0.5529839    total: 7.45s    remaining: 8.36s
33: learn: 0.5504068    total: 7.68s    remaining: 8.13s
34: learn: 0.5476205    total: 7.92s    remaining: 7.92s
35: learn: 0.5446545    total: 8.15s    remaining: 7.7s
36: learn: 0.5419831    total: 8.38s    remaining: 7.47s
37: learn: 0.5380515    total: 8.61s    remaining: 7.25s
38: learn: 0.5342235    total: 8.84s    remaining: 7.03s
39: learn: 0.5308639    total: 9.06s    remaining: 6.8s
40: learn: 0.5278329    total: 9.29s    remaining: 6.57s
41: learn: 0.5255189    total: 9.53s    remaining: 6.35s
42: learn: 0.5228636    total: 9.76s    remaining: 6.13s
43: learn: 0.5199856    total: 9.99s    remaining: 5.9s
44: learn: 0.5169885    total: 10.2s    remaining: 5.67s
45: learn: 0.5145012    total: 10.4s    remaining: 5.44s
46: learn: 0.5123452    total: 10.7s    remaining: 5.21s
47: learn: 0.5101961    total: 10.9s    remaining: 4.99s
48: learn: 0.5079208    total: 11.1s    remaining: 4.76s
49: learn: 0.5054776    total: 11.3s    remaining: 4.53s
50: learn: 0.5032262    total: 11.6s    remaining: 4.31s
51: learn: 0.5011123    total: 11.8s    remaining: 4.08s
52: learn: 0.4990094    total: 12s  remaining: 3.85s
53: learn: 0.4964270    total: 12.2s    remaining: 3.63s
54: learn: 0.4943683    total: 12.5s    remaining: 3.4s
55: learn: 0.4922463    total: 12.7s    remaining: 3.17s
56: learn: 0.4904286    total: 12.9s    remaining: 2.94s
57: learn: 0.4879954    total: 13.2s    remaining: 2.72s
58: learn: 0.4860014    total: 13.4s    remaining: 2.5s
59: learn: 0.4842943    total: 13.6s    remaining: 2.27s
60: learn: 0.4824070    total: 13.9s    remaining: 2.04s
61: learn: 0.4800439    total: 14.1s    remaining: 1.82s
62: learn: 0.4783785    total: 14.3s    remaining: 1.59s
63: learn: 0.4759009    total: 14.5s    remaining: 1.36s
64: learn: 0.4739579    total: 14.8s    remaining: 1.14s
65: learn: 0.4718904    total: 15s  remaining: 907ms
66: learn: 0.4704376    total: 15.2s    remaining: 681ms
67: learn: 0.4687568    total: 15.4s    remaining: 454ms
68: learn: 0.4675015    total: 15.7s    remaining: 227ms
69: learn: 0.4661309    total: 15.9s    remaining: 0us
0:  learn: 0.6867160    total: 205ms    remaining: 14.1s
1:  learn: 0.6817983    total: 409ms    remaining: 13.9s
2:  learn: 0.6763436    total: 616ms    remaining: 13.8s
3:  learn: 0.6707665    total: 842ms    remaining: 13.9s
4:  learn: 0.6655522    total: 1.07s    remaining: 14s
5:  learn: 0.6595188    total: 1.3s remaining: 13.8s
6:  learn: 0.6541515    total: 1.52s    remaining: 13.7s
7:  learn: 0.6491243    total: 1.76s    remaining: 13.6s
8:  learn: 0.6447512    total: 1.98s    remaining: 13.4s
9:  learn: 0.6392623    total: 2.21s    remaining: 13.2s
10: learn: 0.6346267    total: 2.4s remaining: 12.9s
11: learn: 0.6298721    total: 2.62s    remaining: 12.7s
12: learn: 0.6248463    total: 2.83s    remaining: 12.4s
13: learn: 0.6205205    total: 3.06s    remaining: 12.2s
14: learn: 0.6170393    total: 3.29s    remaining: 12.1s
15: learn: 0.6130706    total: 3.52s    remaining: 11.9s
16: learn: 0.6087552    total: 3.75s    remaining: 11.7s
17: learn: 0.6044712    total: 3.97s    remaining: 11.5s
18: learn: 0.6013824    total: 4.2s remaining: 11.3s
19: learn: 0.5975130    total: 4.43s    remaining: 11.1s
20: learn: 0.5936635    total: 4.65s    remaining: 10.9s
21: learn: 0.5902263    total: 4.88s    remaining: 10.7s
22: learn: 0.5872904    total: 5.12s    remaining: 10.5s
23: learn: 0.5843652    total: 5.35s    remaining: 10.3s
24: learn: 0.5793123    total: 5.57s    remaining: 10s
25: learn: 0.5761650    total: 5.8s remaining: 9.81s
26: learn: 0.5729976    total: 6.03s    remaining: 9.61s
27: learn: 0.5698271    total: 6.25s    remaining: 9.38s
28: learn: 0.5652784    total: 6.48s    remaining: 9.16s
29: learn: 0.5622549    total: 6.7s remaining: 8.93s
30: learn: 0.5596275    total: 6.92s    remaining: 8.71s
31: learn: 0.5564927    total: 7.15s    remaining: 8.49s
32: learn: 0.5535853    total: 7.38s    remaining: 8.28s
33: learn: 0.5510388    total: 7.62s    remaining: 8.06s
34: learn: 0.5488515    total: 7.84s    remaining: 7.84s
35: learn: 0.5457913    total: 8.08s    remaining: 7.63s
36: learn: 0.5426674    total: 8.31s    remaining: 7.41s
37: learn: 0.5399439    total: 8.53s    remaining: 7.19s
38: learn: 0.5369225    total: 8.75s    remaining: 6.96s
39: learn: 0.5336043    total: 8.97s    remaining: 6.73s
40: learn: 0.5303811    total: 9.2s remaining: 6.51s
41: learn: 0.5277906    total: 9.43s    remaining: 6.29s
42: learn: 0.5240578    total: 9.66s    remaining: 6.07s
43: learn: 0.5219273    total: 9.87s    remaining: 5.83s
44: learn: 0.5193463    total: 10.1s    remaining: 5.61s
45: learn: 0.5172282    total: 10.3s    remaining: 5.4s
46: learn: 0.5144832    total: 10.6s    remaining: 5.18s
47: learn: 0.5121899    total: 10.8s    remaining: 4.95s
48: learn: 0.5101026    total: 11s  remaining: 4.73s
49: learn: 0.5079813    total: 11.3s    remaining: 4.5s
50: learn: 0.5051492    total: 11.5s    remaining: 4.28s
51: learn: 0.5027441    total: 11.7s    remaining: 4.05s
52: learn: 0.5009599    total: 11.9s    remaining: 3.83s
53: learn: 0.4981433    total: 12.2s    remaining: 3.6s
54: learn: 0.4962993    total: 12.4s    remaining: 3.38s
55: learn: 0.4936682    total: 12.6s    remaining: 3.15s
56: learn: 0.4916918    total: 12.8s    remaining: 2.93s
57: learn: 0.4891126    total: 13.1s    remaining: 2.7s
58: learn: 0.4869325    total: 13.3s    remaining: 2.48s
59: learn: 0.4848522    total: 13.5s    remaining: 2.26s
60: learn: 0.4829020    total: 13.8s    remaining: 2.03s
61: learn: 0.4812473    total: 14s  remaining: 1.81s
62: learn: 0.4795380    total: 14.2s    remaining: 1.58s
63: learn: 0.4775164    total: 14.5s    remaining: 1.36s
64: learn: 0.4757711    total: 14.7s    remaining: 1.13s
65: learn: 0.4738938    total: 14.9s    remaining: 904ms
66: learn: 0.4717643    total: 15.1s    remaining: 678ms
67: learn: 0.4699461    total: 15.4s    remaining: 452ms
68: learn: 0.4679298    total: 15.6s    remaining: 226ms
69: learn: 0.4661554    total: 15.8s    remaining: 0us
0:  learn: 0.6856073    total: 207ms    remaining: 14.3s
1:  learn: 0.6813658    total: 442ms    remaining: 15s
2:  learn: 0.6763579    total: 675ms    remaining: 15.1s
3:  learn: 0.6714679    total: 918ms    remaining: 15.1s
4:  learn: 0.6660735    total: 1.14s    remaining: 14.8s
5:  learn: 0.6590186    total: 1.37s    remaining: 14.6s
6:  learn: 0.6539830    total: 1.61s    remaining: 14.5s
7:  learn: 0.6491584    total: 1.83s    remaining: 14.2s
8:  learn: 0.6447890    total: 2.08s    remaining: 14.1s
9:  learn: 0.6392684    total: 2.32s    remaining: 13.9s
10: learn: 0.6345571    total: 2.54s    remaining: 13.7s
11: learn: 0.6311585    total: 2.77s    remaining: 13.4s
12: learn: 0.6257382    total: 3s   remaining: 13.2s
13: learn: 0.6210128    total: 3.23s    remaining: 12.9s
14: learn: 0.6167537    total: 3.46s    remaining: 12.7s
15: learn: 0.6117552    total: 3.69s    remaining: 12.5s
16: learn: 0.6068090    total: 3.9s remaining: 12.2s
17: learn: 0.6026308    total: 4.13s    remaining: 11.9s
18: learn: 0.5994881    total: 4.36s    remaining: 11.7s
19: learn: 0.5950413    total: 4.6s remaining: 11.5s
20: learn: 0.5911287    total: 4.83s    remaining: 11.3s
21: learn: 0.5879195    total: 5.06s    remaining: 11s
22: learn: 0.5846708    total: 5.29s    remaining: 10.8s
23: learn: 0.5808136    total: 5.51s    remaining: 10.6s
24: learn: 0.5771825    total: 5.74s    remaining: 10.3s
25: learn: 0.5729025    total: 5.95s    remaining: 10.1s
26: learn: 0.5697713    total: 6.19s    remaining: 9.86s
27: learn: 0.5665399    total: 6.46s    remaining: 9.69s
28: learn: 0.5630845    total: 6.72s    remaining: 9.51s
29: learn: 0.5605371    total: 6.97s    remaining: 9.3s
30: learn: 0.5576014    total: 7.21s    remaining: 9.06s
31: learn: 0.5548633    total: 7.44s    remaining: 8.83s
32: learn: 0.5512260    total: 7.66s    remaining: 8.59s
33: learn: 0.5485768    total: 7.9s remaining: 8.37s
34: learn: 0.5459721    total: 8.13s    remaining: 8.13s
35: learn: 0.5433590    total: 8.36s    remaining: 7.89s
36: learn: 0.5404806    total: 8.57s    remaining: 7.65s
37: learn: 0.5376623    total: 8.8s remaining: 7.42s
38: learn: 0.5352702    total: 9.02s    remaining: 7.17s
39: learn: 0.5318593    total: 9.24s    remaining: 6.93s
40: learn: 0.5287440    total: 9.47s    remaining: 6.7s
41: learn: 0.5263023    total: 9.69s    remaining: 6.46s
42: learn: 0.5238787    total: 9.92s    remaining: 6.23s
43: learn: 0.5211830    total: 10.1s    remaining: 6s
44: learn: 0.5192002    total: 10.4s    remaining: 5.77s
45: learn: 0.5169270    total: 10.6s    remaining: 5.53s
46: learn: 0.5140122    total: 10.8s    remaining: 5.3s
47: learn: 0.5117085    total: 11.1s    remaining: 5.07s
48: learn: 0.5090780    total: 11.3s    remaining: 4.84s
49: learn: 0.5069044    total: 11.5s    remaining: 4.61s
50: learn: 0.5048360    total: 11.8s    remaining: 4.38s
51: learn: 0.5030125    total: 12s  remaining: 4.15s
52: learn: 0.5011650    total: 12.2s    remaining: 3.92s
53: learn: 0.4981879    total: 12.5s    remaining: 3.69s
54: learn: 0.4960101    total: 12.7s    remaining: 3.46s
55: learn: 0.4932337    total: 12.9s    remaining: 3.23s
56: learn: 0.4912376    total: 13.1s    remaining: 3s
57: learn: 0.4887712    total: 13.4s    remaining: 2.76s
58: learn: 0.4868622    total: 13.6s    remaining: 2.53s
59: learn: 0.4853113    total: 13.8s    remaining: 2.3s
60: learn: 0.4832912    total: 14s  remaining: 2.07s
61: learn: 0.4806880    total: 14.3s    remaining: 1.84s
62: learn: 0.4787688    total: 14.5s    remaining: 1.61s
63: learn: 0.4769648    total: 14.7s    remaining: 1.38s
64: learn: 0.4750955    total: 14.9s    remaining: 1.15s
65: learn: 0.4730873    total: 15.2s    remaining: 919ms
66: learn: 0.4714787    total: 15.4s    remaining: 690ms
67: learn: 0.4698039    total: 15.6s    remaining: 460ms
68: learn: 0.4680462    total: 15.9s    remaining: 230ms
69: learn: 0.4665342    total: 16.1s    remaining: 0us
0:  learn: 0.6788336    total: 79.2ms   remaining: 1.5s
1:  learn: 0.6641919    total: 167ms    remaining: 1.5s
2:  learn: 0.6520778    total: 253ms    remaining: 1.43s
3:  learn: 0.6378013    total: 331ms    remaining: 1.32s
4:  learn: 0.6280943    total: 423ms    remaining: 1.27s
5:  learn: 0.6183372    total: 518ms    remaining: 1.21s
6:  learn: 0.6045327    total: 615ms    remaining: 1.14s
7:  learn: 0.5970878    total: 699ms    remaining: 1.05s
8:  learn: 0.5855487    total: 811ms    remaining: 992ms
9:  learn: 0.5745267    total: 905ms    remaining: 905ms
10: learn: 0.5635858    total: 996ms    remaining: 815ms
11: learn: 0.5562680    total: 1.08s    remaining: 720ms
12: learn: 0.5494956    total: 1.17s    remaining: 632ms
13: learn: 0.5434607    total: 1.26s    remaining: 542ms
14: learn: 0.5366582    total: 1.36s    remaining: 454ms
15: learn: 0.5306572    total: 1.45s    remaining: 362ms
16: learn: 0.5249508    total: 1.55s    remaining: 273ms
17: learn: 0.5189863    total: 1.63s    remaining: 182ms
18: learn: 0.5142055    total: 1.72s    remaining: 90.3ms
19: learn: 0.5078640    total: 1.8s remaining: 0us
0:  learn: 0.6754175    total: 77.1ms   remaining: 1.46s
1:  learn: 0.6657609    total: 157ms    remaining: 1.42s
2:  learn: 0.6497813    total: 241ms    remaining: 1.36s
3:  learn: 0.6385731    total: 335ms    remaining: 1.34s
4:  learn: 0.6243063    total: 425ms    remaining: 1.27s
5:  learn: 0.6121638    total: 519ms    remaining: 1.21s
6:  learn: 0.6001453    total: 615ms    remaining: 1.14s
7:  learn: 0.5917983    total: 706ms    remaining: 1.06s
8:  learn: 0.5795535    total: 804ms    remaining: 982ms
9:  learn: 0.5703570    total: 897ms    remaining: 897ms
10: learn: 0.5607939    total: 989ms    remaining: 809ms
11: learn: 0.5511210    total: 1.07s    remaining: 715ms
12: learn: 0.5461723    total: 1.16s    remaining: 624ms
13: learn: 0.5394976    total: 1.25s    remaining: 538ms
14: learn: 0.5329465    total: 1.34s    remaining: 448ms
15: learn: 0.5261160    total: 1.43s    remaining: 359ms
16: learn: 0.5212908    total: 1.53s    remaining: 271ms
17: learn: 0.5171155    total: 1.62s    remaining: 180ms
18: learn: 0.5100957    total: 1.71s    remaining: 90.1ms
19: learn: 0.5040100    total: 1.8s remaining: 0us
0:  learn: 0.6736660    total: 79ms remaining: 1.5s
1:  learn: 0.6632992    total: 166ms    remaining: 1.49s
2:  learn: 0.6518791    total: 259ms    remaining: 1.47s
3:  learn: 0.6420412    total: 347ms    remaining: 1.39s
4:  learn: 0.6300900    total: 445ms    remaining: 1.33s
5:  learn: 0.6202904    total: 544ms    remaining: 1.27s
6:  learn: 0.6090917    total: 634ms    remaining: 1.18s
7:  learn: 0.6016861    total: 720ms    remaining: 1.08s
8:  learn: 0.5888547    total: 817ms    remaining: 999ms
9:  learn: 0.5786878    total: 903ms    remaining: 903ms
10: learn: 0.5690928    total: 997ms    remaining: 815ms
11: learn: 0.5605502    total: 1.1s remaining: 735ms
12: learn: 0.5552464    total: 1.19s    remaining: 641ms
13: learn: 0.5451069    total: 1.28s    remaining: 549ms
14: learn: 0.5388667    total: 1.37s    remaining: 456ms
15: learn: 0.5330966    total: 1.46s    remaining: 364ms
16: learn: 0.5254825    total: 1.54s    remaining: 273ms
17: learn: 0.5204746    total: 1.62s    remaining: 180ms
18: learn: 0.5137134    total: 1.71s    remaining: 90.1ms
19: learn: 0.5077613    total: 1.8s remaining: 0us
0:  learn: 0.6661865    total: 99.8ms   remaining: 7.88s
1:  learn: 0.6442901    total: 211ms    remaining: 8.22s
2:  learn: 0.6296108    total: 319ms    remaining: 8.18s
3:  learn: 0.6041061    total: 427ms    remaining: 8.11s
4:  learn: 0.5846803    total: 541ms    remaining: 8.12s
5:  learn: 0.5694868    total: 653ms    remaining: 8.06s
6:  learn: 0.5564804    total: 760ms    remaining: 7.92s
7:  learn: 0.5362603    total: 867ms    remaining: 7.8s
8:  learn: 0.5249824    total: 978ms    remaining: 7.72s
9:  learn: 0.5156833    total: 1.1s remaining: 7.74s
10: learn: 0.5074373    total: 1.22s    remaining: 7.65s
11: learn: 0.4966087    total: 1.34s    remaining: 7.61s
12: learn: 0.4843238    total: 1.46s    remaining: 7.5s
13: learn: 0.4751487    total: 1.56s    remaining: 7.37s
14: learn: 0.4667416    total: 1.67s    remaining: 7.26s
15: learn: 0.4601999    total: 1.78s    remaining: 7.13s
16: learn: 0.4506270    total: 1.89s    remaining: 7s
17: learn: 0.4443271    total: 2s   remaining: 6.88s
18: learn: 0.4364742    total: 2.1s remaining: 6.74s
19: learn: 0.4287354    total: 2.21s    remaining: 6.62s
20: learn: 0.4225620    total: 2.32s    remaining: 6.52s
21: learn: 0.4179536    total: 2.44s    remaining: 6.42s
22: learn: 0.4146607    total: 2.56s    remaining: 6.33s
23: learn: 0.4116159    total: 2.68s    remaining: 6.26s
24: learn: 0.4072502    total: 2.79s    remaining: 6.13s
25: learn: 0.4033408    total: 2.91s    remaining: 6.04s
26: learn: 0.3992202    total: 3.04s    remaining: 5.96s
27: learn: 0.3950707    total: 3.15s    remaining: 5.85s
28: learn: 0.3920537    total: 3.29s    remaining: 5.79s
29: learn: 0.3892721    total: 3.41s    remaining: 5.69s
30: learn: 0.3878469    total: 3.47s    remaining: 5.49s
31: learn: 0.3853206    total: 3.58s    remaining: 5.36s
32: learn: 0.3825435    total: 3.68s    remaining: 5.24s
33: learn: 0.3797944    total: 3.81s    remaining: 5.16s
34: learn: 0.3776274    total: 3.92s    remaining: 5.04s
35: learn: 0.3749218    total: 4.04s    remaining: 4.94s
36: learn: 0.3730313    total: 4.16s    remaining: 4.83s
37: learn: 0.3697761    total: 4.27s    remaining: 4.72s
38: learn: 0.3679260    total: 4.4s remaining: 4.63s
39: learn: 0.3655824    total: 4.51s    remaining: 4.51s
40: learn: 0.3625396    total: 4.63s    remaining: 4.4s
41: learn: 0.3606557    total: 4.74s    remaining: 4.29s
42: learn: 0.3586780    total: 4.86s    remaining: 4.18s
43: learn: 0.3569128    total: 4.97s    remaining: 4.07s
44: learn: 0.3547151    total: 5.09s    remaining: 3.96s
45: learn: 0.3530752    total: 5.22s    remaining: 3.86s
46: learn: 0.3516675    total: 5.36s    remaining: 3.76s
47: learn: 0.3500730    total: 5.47s    remaining: 3.65s
48: learn: 0.3482186    total: 5.59s    remaining: 3.53s
49: learn: 0.3469220    total: 5.69s    remaining: 3.42s
50: learn: 0.3451109    total: 5.81s    remaining: 3.3s
51: learn: 0.3426953    total: 5.92s    remaining: 3.19s
52: learn: 0.3411557    total: 6.03s    remaining: 3.07s
53: learn: 0.3385266    total: 6.15s    remaining: 2.96s
54: learn: 0.3368309    total: 6.27s    remaining: 2.85s
55: learn: 0.3338607    total: 6.39s    remaining: 2.74s
56: learn: 0.3305002    total: 6.52s    remaining: 2.63s
57: learn: 0.3288964    total: 6.64s    remaining: 2.52s
58: learn: 0.3273422    total: 6.75s    remaining: 2.4s
59: learn: 0.3262397    total: 6.88s    remaining: 2.29s
60: learn: 0.3245780    total: 7s   remaining: 2.18s
61: learn: 0.3225558    total: 7.12s    remaining: 2.07s
62: learn: 0.3211791    total: 7.24s    remaining: 1.95s
63: learn: 0.3197995    total: 7.34s    remaining: 1.83s
64: learn: 0.3185145    total: 7.47s    remaining: 1.73s
65: learn: 0.3174419    total: 7.59s    remaining: 1.61s
66: learn: 0.3148381    total: 7.71s    remaining: 1.5s
67: learn: 0.3134987    total: 7.81s    remaining: 1.38s
68: learn: 0.3118657    total: 7.92s    remaining: 1.26s
69: learn: 0.3106146    total: 8.03s    remaining: 1.15s
70: learn: 0.3095362    total: 8.15s    remaining: 1.03s
71: learn: 0.3084200    total: 8.28s    remaining: 920ms
72: learn: 0.3071627    total: 8.41s    remaining: 806ms
73: learn: 0.3056916    total: 8.54s    remaining: 692ms
74: learn: 0.3042122    total: 8.66s    remaining: 577ms
75: learn: 0.3030383    total: 8.77s    remaining: 462ms
76: learn: 0.3018889    total: 8.89s    remaining: 346ms
77: learn: 0.3005356    total: 9.17s    remaining: 235ms
78: learn: 0.2986284    total: 9.32s    remaining: 118ms
79: learn: 0.2973939    total: 9.46s    remaining: 0us
0:  learn: 0.6643290    total: 116ms    remaining: 9.17s
1:  learn: 0.6454704    total: 252ms    remaining: 9.82s
2:  learn: 0.6267988    total: 381ms    remaining: 9.77s
3:  learn: 0.6045659    total: 504ms    remaining: 9.57s
4:  learn: 0.5871720    total: 632ms    remaining: 9.48s
5:  learn: 0.5693765    total: 775ms    remaining: 9.56s
6:  learn: 0.5530513    total: 906ms    remaining: 9.45s
7:  learn: 0.5386130    total: 1.03s    remaining: 9.28s
8:  learn: 0.5240635    total: 1.16s    remaining: 9.13s
9:  learn: 0.5159297    total: 1.31s    remaining: 9.15s
10: learn: 0.5085569    total: 1.44s    remaining: 9s
11: learn: 0.5003657    total: 1.57s    remaining: 8.88s
12: learn: 0.4913964    total: 1.67s    remaining: 8.63s
13: learn: 0.4835945    total: 1.78s    remaining: 8.41s
14: learn: 0.4764090    total: 1.9s remaining: 8.23s
15: learn: 0.4699141    total: 2.02s    remaining: 8.1s
16: learn: 0.4647210    total: 2.13s    remaining: 7.9s
17: learn: 0.4583236    total: 2.25s    remaining: 7.75s
18: learn: 0.4507625    total: 2.36s    remaining: 7.57s
19: learn: 0.4434956    total: 2.49s    remaining: 7.47s
20: learn: 0.4369761    total: 2.62s    remaining: 7.35s
21: learn: 0.4315171    total: 2.73s    remaining: 7.21s
22: learn: 0.4271261    total: 2.87s    remaining: 7.11s
23: learn: 0.4231828    total: 2.99s    remaining: 6.99s
24: learn: 0.4176501    total: 3.12s    remaining: 6.86s
25: learn: 0.4142370    total: 3.23s    remaining: 6.72s
26: learn: 0.4112396    total: 3.36s    remaining: 6.6s
27: learn: 0.4080917    total: 3.48s    remaining: 6.47s
28: learn: 0.4024743    total: 3.6s remaining: 6.33s
29: learn: 0.3995273    total: 3.71s    remaining: 6.19s
30: learn: 0.3932476    total: 3.83s    remaining: 6.05s
31: learn: 0.3922751    total: 3.87s    remaining: 5.81s
32: learn: 0.3900948    total: 4.01s    remaining: 5.71s
33: learn: 0.3873775    total: 4.13s    remaining: 5.58s
34: learn: 0.3846040    total: 4.24s    remaining: 5.45s
35: learn: 0.3808225    total: 4.35s    remaining: 5.31s
36: learn: 0.3775607    total: 4.46s    remaining: 5.18s
37: learn: 0.3753230    total: 4.58s    remaining: 5.06s
38: learn: 0.3731365    total: 4.72s    remaining: 4.96s
39: learn: 0.3709968    total: 4.83s    remaining: 4.83s
40: learn: 0.3669675    total: 4.95s    remaining: 4.71s
41: learn: 0.3630283    total: 5.08s    remaining: 4.6s
42: learn: 0.3595623    total: 5.21s    remaining: 4.49s
43: learn: 0.3582165    total: 5.33s    remaining: 4.36s
44: learn: 0.3565920    total: 5.47s    remaining: 4.26s
45: learn: 0.3545405    total: 5.65s    remaining: 4.18s
46: learn: 0.3517645    total: 5.76s    remaining: 4.04s
47: learn: 0.3496004    total: 5.86s    remaining: 3.91s
48: learn: 0.3469607    total: 5.97s    remaining: 3.78s
49: learn: 0.3450449    total: 6.09s    remaining: 3.65s
50: learn: 0.3409540    total: 6.2s remaining: 3.52s
51: learn: 0.3393811    total: 6.31s    remaining: 3.4s
52: learn: 0.3370172    total: 6.42s    remaining: 3.27s
53: learn: 0.3350501    total: 6.53s    remaining: 3.14s
54: learn: 0.3336867    total: 6.65s    remaining: 3.02s
55: learn: 0.3326568    total: 6.77s    remaining: 2.9s
56: learn: 0.3305981    total: 6.9s remaining: 2.78s
57: learn: 0.3291170    total: 7.01s    remaining: 2.66s
58: learn: 0.3267238    total: 7.12s    remaining: 2.54s
59: learn: 0.3254530    total: 7.25s    remaining: 2.42s
60: learn: 0.3240547    total: 7.36s    remaining: 2.29s
61: learn: 0.3227733    total: 7.47s    remaining: 2.17s
62: learn: 0.3208503    total: 7.59s    remaining: 2.05s
63: learn: 0.3192012    total: 7.71s    remaining: 1.93s
64: learn: 0.3183069    total: 7.83s    remaining: 1.81s
65: learn: 0.3166566    total: 7.95s    remaining: 1.69s
66: learn: 0.3146433    total: 8.06s    remaining: 1.56s
67: learn: 0.3127463    total: 8.17s    remaining: 1.44s
68: learn: 0.3114390    total: 8.29s    remaining: 1.32s
69: learn: 0.3101015    total: 8.4s remaining: 1.2s
70: learn: 0.3092973    total: 8.52s    remaining: 1.08s
71: learn: 0.3082230    total: 8.64s    remaining: 961ms
72: learn: 0.3068674    total: 8.76s    remaining: 840ms
73: learn: 0.3050543    total: 8.89s    remaining: 721ms
74: learn: 0.3042546    total: 9.01s    remaining: 601ms
75: learn: 0.3032710    total: 9.13s    remaining: 480ms
76: learn: 0.3022287    total: 9.25s    remaining: 360ms
77: learn: 0.3010763    total: 9.36s    remaining: 240ms
78: learn: 0.3000623    total: 9.48s    remaining: 120ms
79: learn: 0.2986710    total: 9.61s    remaining: 0us
0:  learn: 0.6562429    total: 97.9ms   remaining: 7.73s
1:  learn: 0.6335780    total: 202ms    remaining: 7.88s
2:  learn: 0.6186449    total: 329ms    remaining: 8.44s
3:  learn: 0.6001972    total: 462ms    remaining: 8.77s
4:  learn: 0.5860519    total: 572ms    remaining: 8.58s
5:  learn: 0.5703638    total: 689ms    remaining: 8.49s
6:  learn: 0.5581365    total: 830ms    remaining: 8.65s
7:  learn: 0.5453623    total: 938ms    remaining: 8.44s
8:  learn: 0.5348661    total: 1.06s    remaining: 8.34s
9:  learn: 0.5213708    total: 1.17s    remaining: 8.21s
10: learn: 0.5124921    total: 1.29s    remaining: 8.08s
11: learn: 0.5043456    total: 1.4s remaining: 7.93s
12: learn: 0.4954577    total: 1.51s    remaining: 7.79s
13: learn: 0.4879698    total: 1.64s    remaining: 7.72s
14: learn: 0.4769865    total: 1.76s    remaining: 7.61s
15: learn: 0.4697842    total: 1.87s    remaining: 7.49s
16: learn: 0.4611150    total: 1.98s    remaining: 7.33s
17: learn: 0.4545033    total: 2.1s remaining: 7.24s
18: learn: 0.4492953    total: 2.22s    remaining: 7.12s
19: learn: 0.4422798    total: 2.33s    remaining: 6.99s
20: learn: 0.4376908    total: 2.44s    remaining: 6.84s
21: learn: 0.4331509    total: 2.57s    remaining: 6.77s
22: learn: 0.4293290    total: 2.69s    remaining: 6.66s
23: learn: 0.4249046    total: 2.82s    remaining: 6.58s
24: learn: 0.4221392    total: 2.96s    remaining: 6.51s
25: learn: 0.4183462    total: 3.07s    remaining: 6.38s
26: learn: 0.4133569    total: 3.19s    remaining: 6.26s
27: learn: 0.4094390    total: 3.31s    remaining: 6.14s
28: learn: 0.4065924    total: 3.42s    remaining: 6.02s
29: learn: 0.4036392    total: 3.54s    remaining: 5.9s
30: learn: 0.3975599    total: 3.68s    remaining: 5.81s
31: learn: 0.3945920    total: 3.79s    remaining: 5.68s
32: learn: 0.3920131    total: 3.91s    remaining: 5.56s
33: learn: 0.3894455    total: 4.01s    remaining: 5.43s
34: learn: 0.3871030    total: 4.13s    remaining: 5.3s
35: learn: 0.3852647    total: 4.25s    remaining: 5.19s
36: learn: 0.3819382    total: 4.36s    remaining: 5.07s
37: learn: 0.3796748    total: 4.48s    remaining: 4.95s
38: learn: 0.3773800    total: 4.59s    remaining: 4.83s
39: learn: 0.3748525    total: 4.75s    remaining: 4.75s
40: learn: 0.3725744    total: 4.88s    remaining: 4.64s
41: learn: 0.3689606    total: 5.03s    remaining: 4.55s
42: learn: 0.3665860    total: 5.17s    remaining: 4.45s
43: learn: 0.3626355    total: 5.3s remaining: 4.34s
44: learn: 0.3595666    total: 5.43s    remaining: 4.22s
45: learn: 0.3567451    total: 5.55s    remaining: 4.1s
46: learn: 0.3537801    total: 5.67s    remaining: 3.98s
47: learn: 0.3515180    total: 5.8s remaining: 3.87s
48: learn: 0.3484534    total: 5.94s    remaining: 3.76s
49: learn: 0.3460548    total: 6.09s    remaining: 3.65s
50: learn: 0.3446576    total: 6.21s    remaining: 3.53s
51: learn: 0.3422310    total: 6.33s    remaining: 3.41s
52: learn: 0.3402758    total: 6.45s    remaining: 3.28s
53: learn: 0.3388304    total: 6.56s    remaining: 3.16s
54: learn: 0.3369734    total: 6.69s    remaining: 3.04s
55: learn: 0.3352839    total: 6.81s    remaining: 2.92s
56: learn: 0.3338208    total: 6.93s    remaining: 2.8s
57: learn: 0.3321561    total: 7.06s    remaining: 2.68s
58: learn: 0.3305014    total: 7.21s    remaining: 2.57s
59: learn: 0.3292382    total: 7.33s    remaining: 2.44s
60: learn: 0.3277298    total: 7.45s    remaining: 2.32s
61: learn: 0.3264114    total: 7.57s    remaining: 2.2s
62: learn: 0.3242588    total: 7.69s    remaining: 2.08s
63: learn: 0.3229963    total: 7.8s remaining: 1.95s
64: learn: 0.3218746    total: 7.92s    remaining: 1.83s
65: learn: 0.3198690    total: 8.04s    remaining: 1.71s
66: learn: 0.3186453    total: 8.17s    remaining: 1.58s
67: learn: 0.3171265    total: 8.31s    remaining: 1.47s
68: learn: 0.3158560    total: 8.44s    remaining: 1.35s
69: learn: 0.3146361    total: 8.55s    remaining: 1.22s
70: learn: 0.3126894    total: 8.67s    remaining: 1.1s
71: learn: 0.3115763    total: 8.81s    remaining: 979ms
72: learn: 0.3098584    total: 8.93s    remaining: 857ms
73: learn: 0.3088046    total: 9.06s    remaining: 734ms
74: learn: 0.3078307    total: 9.18s    remaining: 612ms
75: learn: 0.3053759    total: 9.31s    remaining: 490ms
76: learn: 0.3038827    total: 9.43s    remaining: 367ms
77: learn: 0.3023627    total: 9.55s    remaining: 245ms
78: learn: 0.3009514    total: 9.66s    remaining: 122ms
79: learn: 0.2996157    total: 9.8s remaining: 0us
0:  learn: 0.6870159    total: 199ms    remaining: 1.79s
1:  learn: 0.6828002    total: 408ms    remaining: 1.63s
2:  learn: 0.6777351    total: 638ms    remaining: 1.49s
3:  learn: 0.6726369    total: 871ms    remaining: 1.31s
4:  learn: 0.6657397    total: 1.1s remaining: 1.1s
5:  learn: 0.6593904    total: 1.34s    remaining: 892ms
6:  learn: 0.6543954    total: 1.57s    remaining: 672ms
7:  learn: 0.6501775    total: 1.79s    remaining: 449ms
8:  learn: 0.6455502    total: 2.02s    remaining: 224ms
9:  learn: 0.6401297    total: 2.24s    remaining: 0us
0:  learn: 0.6867160    total: 219ms    remaining: 1.97s
1:  learn: 0.6817983    total: 441ms    remaining: 1.76s
2:  learn: 0.6763436    total: 665ms    remaining: 1.55s
3:  learn: 0.6707665    total: 895ms    remaining: 1.34s
4:  learn: 0.6655522    total: 1.11s    remaining: 1.11s
5:  learn: 0.6595188    total: 1.35s    remaining: 901ms
6:  learn: 0.6541515    total: 1.57s    remaining: 674ms
7:  learn: 0.6491243    total: 1.8s remaining: 451ms
8:  learn: 0.6447512    total: 2.03s    remaining: 225ms
9:  learn: 0.6392623    total: 2.26s    remaining: 0us
0:  learn: 0.6856073    total: 217ms    remaining: 1.95s
1:  learn: 0.6813658    total: 448ms    remaining: 1.79s
2:  learn: 0.6763579    total: 677ms    remaining: 1.58s
3:  learn: 0.6714679    total: 916ms    remaining: 1.37s
4:  learn: 0.6660735    total: 1.14s    remaining: 1.14s
5:  learn: 0.6590186    total: 1.37s    remaining: 913ms
6:  learn: 0.6539830    total: 1.59s    remaining: 681ms
7:  learn: 0.6491584    total: 1.81s    remaining: 454ms
8:  learn: 0.6447890    total: 2.04s    remaining: 227ms
9:  learn: 0.6392684    total: 2.26s    remaining: 0us
0:  learn: 0.6665327    total: 111ms    remaining: 8.8s
1:  learn: 0.6464428    total: 243ms    remaining: 9.47s
2:  learn: 0.6274255    total: 375ms    remaining: 9.63s
3:  learn: 0.6038900    total: 518ms    remaining: 9.84s
4:  learn: 0.5853056    total: 641ms    remaining: 9.62s
5:  learn: 0.5711453    total: 767ms    remaining: 9.46s
6:  learn: 0.5518932    total: 903ms    remaining: 9.42s
7:  learn: 0.5371312    total: 1.04s    remaining: 9.35s
8:  learn: 0.5246082    total: 1.17s    remaining: 9.22s
9:  learn: 0.5108871    total: 1.3s remaining: 9.11s
10: learn: 0.5017540    total: 1.45s    remaining: 9.11s
11: learn: 0.4914030    total: 1.59s    remaining: 9.01s
12: learn: 0.4829017    total: 1.72s    remaining: 8.85s
13: learn: 0.4753740    total: 1.84s    remaining: 8.67s
14: learn: 0.4684517    total: 1.97s    remaining: 8.55s
15: learn: 0.4595894    total: 2.13s    remaining: 8.5s
16: learn: 0.4516042    total: 2.25s    remaining: 8.35s
17: learn: 0.4452736    total: 2.37s    remaining: 8.16s
18: learn: 0.4382162    total: 2.49s    remaining: 8s
19: learn: 0.4326054    total: 2.64s    remaining: 7.91s
20: learn: 0.4268372    total: 2.76s    remaining: 7.77s
21: learn: 0.4202041    total: 2.88s    remaining: 7.59s
22: learn: 0.4167642    total: 3.02s    remaining: 7.47s
23: learn: 0.4122437    total: 3.17s    remaining: 7.4s
24: learn: 0.4093793    total: 3.31s    remaining: 7.29s
25: learn: 0.4039919    total: 3.44s    remaining: 7.15s
26: learn: 0.4004563    total: 3.58s    remaining: 7.02s
27: learn: 0.3975197    total: 3.7s remaining: 6.87s
28: learn: 0.3932144    total: 3.82s    remaining: 6.71s
29: learn: 0.3904852    total: 3.93s    remaining: 6.55s
30: learn: 0.3880708    total: 4.05s    remaining: 6.4s
31: learn: 0.3859222    total: 4.17s    remaining: 6.25s
32: learn: 0.3831005    total: 4.29s    remaining: 6.11s
33: learn: 0.3810144    total: 4.43s    remaining: 5.99s
34: learn: 0.3785504    total: 4.54s    remaining: 5.84s
35: learn: 0.3761624    total: 4.66s    remaining: 5.7s
36: learn: 0.3737088    total: 4.79s    remaining: 5.56s
37: learn: 0.3712488    total: 4.9s remaining: 5.42s
38: learn: 0.3685311    total: 5.03s    remaining: 5.29s
39: learn: 0.3662991    total: 5.16s    remaining: 5.16s
40: learn: 0.3634880    total: 5.3s remaining: 5.04s
41: learn: 0.3606838    total: 5.43s    remaining: 4.91s
42: learn: 0.3587348    total: 5.56s    remaining: 4.79s
43: learn: 0.3567398    total: 5.68s    remaining: 4.65s
44: learn: 0.3548707    total: 5.8s remaining: 4.51s
45: learn: 0.3520839    total: 5.95s    remaining: 4.4s
46: learn: 0.3492787    total: 6.09s    remaining: 4.28s
47: learn: 0.3467724    total: 6.22s    remaining: 4.15s
48: learn: 0.3430803    total: 6.35s    remaining: 4.01s
49: learn: 0.3411632    total: 6.47s    remaining: 3.88s
50: learn: 0.3396453    total: 6.63s    remaining: 3.77s
51: learn: 0.3377685    total: 6.79s    remaining: 3.66s
52: learn: 0.3362602    total: 6.93s    remaining: 3.53s
53: learn: 0.3349568    total: 7.07s    remaining: 3.4s
54: learn: 0.3336092    total: 7.22s    remaining: 3.28s
55: learn: 0.3318186    total: 7.38s    remaining: 3.17s
56: learn: 0.3287766    total: 7.58s    remaining: 3.06s
57: learn: 0.3268753    total: 7.76s    remaining: 2.94s
58: learn: 0.3251920    total: 7.93s    remaining: 2.82s
59: learn: 0.3227292    total: 8.09s    remaining: 2.7s
60: learn: 0.3213152    total: 8.28s    remaining: 2.58s
61: learn: 0.3192716    total: 8.47s    remaining: 2.46s
62: learn: 0.3183261    total: 8.65s    remaining: 2.33s
63: learn: 0.3171622    total: 8.84s    remaining: 2.21s
64: learn: 0.3159563    total: 9.02s    remaining: 2.08s
65: learn: 0.3143367    total: 9.22s    remaining: 1.96s
66: learn: 0.3133019    total: 9.41s    remaining: 1.83s
67: learn: 0.3122973    total: 9.56s    remaining: 1.69s
68: learn: 0.3103480    total: 9.72s    remaining: 1.55s
69: learn: 0.3093429    total: 9.87s    remaining: 1.41s
70: learn: 0.3081943    total: 10s  remaining: 1.27s
71: learn: 0.3067472    total: 10.2s    remaining: 1.13s
72: learn: 0.3058445    total: 10.3s    remaining: 987ms
73: learn: 0.3047133    total: 10.5s    remaining: 849ms
74: learn: 0.3035925    total: 10.6s    remaining: 707ms
75: learn: 0.3022633    total: 10.7s    remaining: 565ms
76: learn: 0.3011552    total: 10.9s    remaining: 424ms
77: learn: 0.2998250    total: 11s  remaining: 282ms
78: learn: 0.2985347    total: 11.2s    remaining: 141ms
79: learn: 0.2977599    total: 11.3s    remaining: 0us
RandomizedSearchCV(cv=3, error_score=&amp;#39;raise&amp;#39;,
                   estimator=&amp;lt;catboost.core.CatBoostClassifier object at 0x0000018B43171A60&amp;gt;,
                   param_distributions={&amp;#39;depth&amp;#39;: [4, 5, 6, 7, 8, 9, 10],
                                        &amp;#39;iterations&amp;#39;: [10, 20, 30, 40, 50, 60,
                                                       70, 80, 90, 100],
                                        &amp;#39;learning_rate&amp;#39;: [0.01, 0.02, 0.03,
                                                          0.04, 0.05]},
                   random_state=123)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;After the tuning, I will print out the grid search result. The best
hyperparameter values we have are learning_rate = 0.05, iterations = 80,
and depth = 8. I will then let the machine learn from the data by
fitting the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot; Results from Grid Search &amp;quot; )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; Results from Grid Search &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;\n The best estimator across ALL searched params:\n&amp;quot;,Cat_random.best_estimator_)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 The best estimator across ALL searched params:
 &amp;lt;catboost.core.CatBoostClassifier object at 0x0000018B3A424C10&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;\n The best score across ALL searched params:\n&amp;quot;,Cat_random.best_score_)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 The best score across ALL searched params:
 0.8718094516047511&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;\n The best parameters across ALL searched params:\n&amp;quot;,Cat_random.best_params_)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 The best parameters across ALL searched params:
 {&amp;#39;learning_rate&amp;#39;: 0.05, &amp;#39;iterations&amp;#39;: 80, &amp;#39;depth&amp;#39;: 8}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;CBC_tuned = CatBoostClassifier(learning_rate = 0.05, iterations = 80, depth = 8, random_state=RANDOM_STATE)

CBC_tuned.fit(X_train_hybrid_ext, y_train_hybrid_ext, cat_features = cat_features)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0:  learn: 0.6665327    total: 82.6ms   remaining: 6.53s
1:  learn: 0.6464428    total: 179ms    remaining: 6.98s
2:  learn: 0.6274255    total: 283ms    remaining: 7.27s
3:  learn: 0.6038900    total: 463ms    remaining: 8.8s
4:  learn: 0.5853056    total: 683ms    remaining: 10.3s
5:  learn: 0.5711453    total: 849ms    remaining: 10.5s
6:  learn: 0.5518932    total: 1.02s    remaining: 10.6s
7:  learn: 0.5371312    total: 1.19s    remaining: 10.7s
8:  learn: 0.5246082    total: 1.38s    remaining: 10.9s
9:  learn: 0.5108871    total: 1.54s    remaining: 10.8s
10: learn: 0.5017540    total: 1.72s    remaining: 10.8s
11: learn: 0.4914030    total: 1.88s    remaining: 10.7s
12: learn: 0.4829017    total: 2.07s    remaining: 10.7s
13: learn: 0.4753740    total: 2.25s    remaining: 10.6s
14: learn: 0.4684517    total: 2.42s    remaining: 10.5s
15: learn: 0.4595894    total: 2.6s remaining: 10.4s
16: learn: 0.4516042    total: 2.79s    remaining: 10.3s
17: learn: 0.4452736    total: 2.98s    remaining: 10.2s
18: learn: 0.4382162    total: 3.14s    remaining: 10.1s
19: learn: 0.4326054    total: 3.31s    remaining: 9.94s
20: learn: 0.4268372    total: 3.48s    remaining: 9.76s
21: learn: 0.4202041    total: 3.62s    remaining: 9.55s
22: learn: 0.4167642    total: 3.77s    remaining: 9.34s
23: learn: 0.4122437    total: 3.91s    remaining: 9.13s
24: learn: 0.4093793    total: 4.07s    remaining: 8.96s
25: learn: 0.4039919    total: 4.22s    remaining: 8.77s
26: learn: 0.4004563    total: 4.37s    remaining: 8.57s
27: learn: 0.3975197    total: 4.52s    remaining: 8.39s
28: learn: 0.3932144    total: 4.67s    remaining: 8.21s
29: learn: 0.3904852    total: 4.84s    remaining: 8.07s
30: learn: 0.3880708    total: 5.01s    remaining: 7.92s
31: learn: 0.3859222    total: 5.2s remaining: 7.81s
32: learn: 0.3831005    total: 5.39s    remaining: 7.67s
33: learn: 0.3810144    total: 5.57s    remaining: 7.53s
34: learn: 0.3785504    total: 5.74s    remaining: 7.38s
35: learn: 0.3761624    total: 5.93s    remaining: 7.25s
36: learn: 0.3737088    total: 6.11s    remaining: 7.1s
37: learn: 0.3712488    total: 6.29s    remaining: 6.95s
38: learn: 0.3685311    total: 6.46s    remaining: 6.79s
39: learn: 0.3662991    total: 6.64s    remaining: 6.64s
40: learn: 0.3634880    total: 6.81s    remaining: 6.47s
41: learn: 0.3606838    total: 6.97s    remaining: 6.31s
42: learn: 0.3587348    total: 7.12s    remaining: 6.13s
43: learn: 0.3567398    total: 7.27s    remaining: 5.95s
44: learn: 0.3548707    total: 7.42s    remaining: 5.77s
45: learn: 0.3520839    total: 7.55s    remaining: 5.58s
46: learn: 0.3492787    total: 7.68s    remaining: 5.39s
47: learn: 0.3467724    total: 7.8s remaining: 5.2s
48: learn: 0.3430803    total: 7.94s    remaining: 5.02s
49: learn: 0.3411632    total: 8.07s    remaining: 4.84s
50: learn: 0.3396453    total: 8.2s remaining: 4.66s
51: learn: 0.3377685    total: 8.35s    remaining: 4.49s
52: learn: 0.3362602    total: 8.48s    remaining: 4.32s
53: learn: 0.3349568    total: 8.6s remaining: 4.14s
54: learn: 0.3336092    total: 8.73s    remaining: 3.97s
55: learn: 0.3318186    total: 8.84s    remaining: 3.79s
56: learn: 0.3287766    total: 8.98s    remaining: 3.62s
57: learn: 0.3268753    total: 9.1s remaining: 3.45s
58: learn: 0.3251920    total: 9.23s    remaining: 3.29s
59: learn: 0.3227292    total: 9.35s    remaining: 3.12s
60: learn: 0.3213152    total: 9.49s    remaining: 2.96s
61: learn: 0.3192716    total: 9.62s    remaining: 2.79s
62: learn: 0.3183261    total: 9.73s    remaining: 2.63s
63: learn: 0.3171622    total: 9.85s    remaining: 2.46s
64: learn: 0.3159563    total: 9.98s    remaining: 2.3s
65: learn: 0.3143367    total: 10.1s    remaining: 2.14s
66: learn: 0.3133019    total: 10.3s    remaining: 1.99s
67: learn: 0.3122973    total: 10.4s    remaining: 1.83s
68: learn: 0.3103480    total: 10.5s    remaining: 1.68s
69: learn: 0.3093429    total: 10.7s    remaining: 1.52s
70: learn: 0.3081943    total: 10.8s    remaining: 1.37s
71: learn: 0.3067472    total: 10.9s    remaining: 1.21s
72: learn: 0.3058445    total: 11.1s    remaining: 1.06s
73: learn: 0.3047133    total: 11.2s    remaining: 910ms
74: learn: 0.3035925    total: 11.4s    remaining: 759ms
75: learn: 0.3022633    total: 11.5s    remaining: 607ms
76: learn: 0.3011552    total: 11.7s    remaining: 456ms
77: learn: 0.2998250    total: 11.8s    remaining: 303ms
78: learn: 0.2985347    total: 12s  remaining: 151ms
79: learn: 0.2977599    total: 12.1s    remaining: 0us
&amp;lt;catboost.core.CatBoostClassifier object at 0x0000018B3A96BF70&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We have 38 variables. We can use all of them, but we can also
further reduce them for to look for the most relevant variables to the
model. We can trim the variable with recursive feature elimination
(RFE), which is a feature selection method that fits the model and
remove the weakest feature (or predictor) iteratively until the optimal
number of features is found (&lt;a
href="https://link.springer.com/content/pdf/10.1023/A:1012487302797.pdf"&gt;Guyon
et al., 2022&lt;/a&gt;). Note that this process is entirely data-driven,
meaning that the machine decides which variable solely based on the
data, not the theory. In this post, I use a variant of RFE called RFE
with cross validation (RFECV) that selects the best subset of features
based on the cross-validation score of the model. RFECV is a
bit&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I have computed RFECV in advance to save time. Below is the
result. Performance of the model jumped at 20 features and fluctuated
after that, meaning that the optimal number of features is 20.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;rfecv_model = RFECV(estimator=CBC_tuned, step=1, cv=5 ,scoring=&amp;#39;accuracy&amp;#39;)
rfecv = rfecv_model.fit(X_train_hybrid_ext, y_train_hybrid_ext)

print(&amp;#39;Optimal number of features :&amp;#39;, rfecv.n_features_)
print(&amp;#39;Best features :&amp;#39;, X_train_hybrid_ext.columns[rfecv.support_])
print(&amp;#39;Original features :&amp;#39;, X_train_hybrid_ext.columns)

plt.figure(figsize=(10, 15), dpi=800)
plt.xlabel(&amp;quot;Number of features selected&amp;quot;)
plt.ylabel(&amp;quot;Cross validation score \n of number of selected features&amp;quot;)
plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2022-08-06-edmrfecv.PNG" style="width:50.0%" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The optimal set of features are &lt;code&gt;X1MOMEDU&lt;/code&gt;,
&lt;code&gt;X1DADEDU&lt;/code&gt;, &lt;code&gt;X1MTHEFF&lt;/code&gt;, &lt;code&gt;X1SCIUTI&lt;/code&gt;,
&lt;code&gt;X1SCIEFF&lt;/code&gt;, &lt;code&gt;X1SCHOOLBEL&lt;/code&gt;,
&lt;code&gt;X1SCHOOLENG&lt;/code&gt;, &lt;code&gt;X1STUEDEXPCT&lt;/code&gt;,
&lt;code&gt;X1SCHOOLCLI&lt;/code&gt;, &lt;code&gt;X1COUPERCOU&lt;/code&gt;,
&lt;code&gt;X1COUPERPRI&lt;/code&gt;, &lt;code&gt;X3TGPA9TH&lt;/code&gt;, &lt;code&gt;S1NOHWDN&lt;/code&gt;,
&lt;code&gt;S1NOPAPER&lt;/code&gt;, &lt;code&gt;S1GETINTOCLG&lt;/code&gt;,
&lt;code&gt;S1WORKING&lt;/code&gt;, &lt;code&gt;S1HRMHOMEWK&lt;/code&gt;,
&lt;code&gt;S1HRSHOMEWK&lt;/code&gt;, &lt;code&gt;S1HROTHHOMWK&lt;/code&gt;,
&lt;code&gt;X4PSENRSTLV&lt;/code&gt;. I will reduce the number of variable based on
the RFECV result and create a training and a testing data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;X_hybrid_extreme_trim = X_hybrid_extreme[[&amp;#39;X1MOMEDU&amp;#39;, &amp;#39;X1DADEDU&amp;#39;, &amp;#39;X1MTHEFF&amp;#39;, &amp;#39;X1SCIUTI&amp;#39;, &amp;#39;X1SCIEFF&amp;#39;,&amp;#39;X1SCHOOLBEL&amp;#39;, &amp;#39;X1SCHOOLENG&amp;#39;, &amp;#39;X1STUEDEXPCT&amp;#39;, &amp;#39;X1SCHOOLCLI&amp;#39;,
&amp;#39;X1COUPERCOU&amp;#39;, &amp;#39;X1COUPERPRI&amp;#39;, &amp;#39;X3TGPA9TH&amp;#39;, &amp;#39;S1NOHWDN&amp;#39;, &amp;#39;S1NOPAPER&amp;#39;,
&amp;#39;S1GETINTOCLG&amp;#39;, &amp;#39;S1WORKING&amp;#39;, &amp;#39;S1HRMHOMEWK&amp;#39;, &amp;#39;S1HRSHOMEWK&amp;#39;, &amp;#39;S1HROTHHOMWK&amp;#39;, &amp;#39;X4PSENRSTLV&amp;#39;]]

X_train_hybrid_ext, X_test_hybrid_ext, y_train_hybrid_ext, y_test_hybrid_ext = train_test_split(X_hybrid_extreme_trim, y_hybrid_extreme, test_size = 0.30, random_state = RANDOM_STATE)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Then, I will fit the CatBoost model I created earlier with this new
data set and use it to predict the testing data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;cat_features_post_trim = [0, 1, 7, 12, 13, 14, 15,16, 17, 19]

CBC_tuned.fit(X_train_hybrid_ext, y_train_hybrid_ext, cat_features = cat_features_post_trim)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0:  learn: 0.6630968    total: 82.3ms   remaining: 6.5s
1:  learn: 0.6347421    total: 167ms    remaining: 6.51s
2:  learn: 0.6135360    total: 252ms    remaining: 6.46s
3:  learn: 0.5958557    total: 355ms    remaining: 6.75s
4:  learn: 0.5789939    total: 461ms    remaining: 6.91s
5:  learn: 0.5568907    total: 551ms    remaining: 6.79s
6:  learn: 0.5378680    total: 675ms    remaining: 7.04s
7:  learn: 0.5226018    total: 768ms    remaining: 6.91s
8:  learn: 0.5087498    total: 871ms    remaining: 6.87s
9:  learn: 0.5009270    total: 977ms    remaining: 6.84s
10: learn: 0.4841922    total: 1.06s    remaining: 6.68s
11: learn: 0.4746953    total: 1.16s    remaining: 6.56s
12: learn: 0.4681778    total: 1.27s    remaining: 6.54s
13: learn: 0.4612541    total: 1.37s    remaining: 6.46s
14: learn: 0.4532992    total: 1.48s    remaining: 6.42s
15: learn: 0.4450863    total: 1.58s    remaining: 6.32s
16: learn: 0.4375271    total: 1.7s remaining: 6.31s
17: learn: 0.4308885    total: 1.81s    remaining: 6.25s
18: learn: 0.4254128    total: 1.92s    remaining: 6.18s
19: learn: 0.4213425    total: 2.04s    remaining: 6.14s
20: learn: 0.4177958    total: 2.16s    remaining: 6.08s
21: learn: 0.4131347    total: 2.26s    remaining: 5.96s
22: learn: 0.4089653    total: 2.37s    remaining: 5.86s
23: learn: 0.4055672    total: 2.48s    remaining: 5.78s
24: learn: 0.4030118    total: 2.58s    remaining: 5.69s
25: learn: 0.3997708    total: 2.7s remaining: 5.6s
26: learn: 0.3945504    total: 2.8s remaining: 5.49s
27: learn: 0.3909571    total: 2.92s    remaining: 5.42s
28: learn: 0.3874681    total: 3.03s    remaining: 5.32s
29: learn: 0.3842159    total: 3.13s    remaining: 5.22s
30: learn: 0.3817295    total: 3.23s    remaining: 5.11s
31: learn: 0.3793464    total: 3.34s    remaining: 5s
32: learn: 0.3760956    total: 3.44s    remaining: 4.9s
33: learn: 0.3744325    total: 3.54s    remaining: 4.78s
34: learn: 0.3711698    total: 3.65s    remaining: 4.7s
35: learn: 0.3690638    total: 3.76s    remaining: 4.6s
36: learn: 0.3659461    total: 3.88s    remaining: 4.5s
37: learn: 0.3637044    total: 3.99s    remaining: 4.41s
38: learn: 0.3621688    total: 4.11s    remaining: 4.32s
39: learn: 0.3601269    total: 4.23s    remaining: 4.23s
40: learn: 0.3580019    total: 4.34s    remaining: 4.13s
41: learn: 0.3567636    total: 4.47s    remaining: 4.04s
42: learn: 0.3544761    total: 4.57s    remaining: 3.93s
43: learn: 0.3505973    total: 4.69s    remaining: 3.83s
44: learn: 0.3495149    total: 4.8s remaining: 3.73s
45: learn: 0.3476525    total: 4.93s    remaining: 3.64s
46: learn: 0.3462577    total: 5.04s    remaining: 3.54s
47: learn: 0.3445400    total: 5.15s    remaining: 3.43s
48: learn: 0.3427600    total: 5.24s    remaining: 3.32s
49: learn: 0.3408402    total: 5.34s    remaining: 3.21s
50: learn: 0.3381710    total: 5.45s    remaining: 3.1s
51: learn: 0.3364197    total: 5.58s    remaining: 3s
52: learn: 0.3339518    total: 5.69s    remaining: 2.9s
53: learn: 0.3311018    total: 5.8s remaining: 2.79s
54: learn: 0.3294152    total: 5.91s    remaining: 2.69s
55: learn: 0.3280687    total: 6.03s    remaining: 2.58s
56: learn: 0.3269127    total: 6.16s    remaining: 2.48s
57: learn: 0.3252681    total: 6.27s    remaining: 2.38s
58: learn: 0.3243089    total: 6.37s    remaining: 2.27s
59: learn: 0.3212165    total: 6.49s    remaining: 2.16s
60: learn: 0.3201323    total: 6.63s    remaining: 2.06s
61: learn: 0.3189120    total: 6.74s    remaining: 1.96s
62: learn: 0.3178599    total: 6.84s    remaining: 1.85s
63: learn: 0.3169985    total: 6.95s    remaining: 1.74s
64: learn: 0.3154376    total: 7.06s    remaining: 1.63s
65: learn: 0.3142858    total: 7.17s    remaining: 1.52s
66: learn: 0.3130732    total: 7.3s remaining: 1.42s
67: learn: 0.3115498    total: 7.4s remaining: 1.3s
68: learn: 0.3106751    total: 7.49s    remaining: 1.19s
69: learn: 0.3104327    total: 7.54s    remaining: 1.08s
70: learn: 0.3094918    total: 7.59s    remaining: 962ms
71: learn: 0.3084260    total: 7.7s remaining: 856ms
72: learn: 0.3070031    total: 7.81s    remaining: 749ms
73: learn: 0.3052639    total: 7.92s    remaining: 642ms
74: learn: 0.3043254    total: 8.02s    remaining: 535ms
75: learn: 0.3038159    total: 8.12s    remaining: 427ms
76: learn: 0.3029093    total: 8.23s    remaining: 321ms
77: learn: 0.3014591    total: 8.35s    remaining: 214ms
78: learn: 0.2992314    total: 8.46s    remaining: 107ms
79: learn: 0.2984094    total: 8.58s    remaining: 0us
&amp;lt;catboost.core.CatBoostClassifier object at 0x0000018B3A96BF70&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Results from classification report are satisfactory as seen from the
macro average of precision, recall, and f1-score. I also show the
receiver operating characteristic curve below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;from sklearn.metrics import roc_auc_score
from sklearn.metrics import classification_report

pred_ext = CBC_tuned.predict(X_test_hybrid_ext)

print(classification_report(y_test_hybrid_ext, pred_ext))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;              precision    recall  f1-score   support

           0       0.86      0.92      0.88      3351
           1       0.91      0.85      0.88      3433

    accuracy                           0.88      6784
   macro avg       0.88      0.88      0.88      6784
weighted avg       0.88      0.88      0.88      6784&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;roc_auc_score(y_test_hybrid_ext, pred_ext)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.8827700805886101&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;from sklearn import metrics

y_pred_proba_cat = CBC_tuned.predict_proba(X_test_hybrid_ext)[::,1]
fpr_cat, tpr_cat, _ = metrics.roc_curve(y_test_hybrid_ext,  y_pred_proba_cat)

auc_cat = metrics.roc_auc_score(y_test_hybrid_ext, y_pred_proba_cat)

#create ROC curve
plt.plot(fpr_cat,tpr_cat, label=&amp;quot;ROC_AUC=&amp;quot;+str(auc_cat.round(3)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;lt;matplotlib.lines.Line2D object at 0x0000018B3A946DC0&amp;gt;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.legend(loc=&amp;quot;lower right&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.legend.Legend object at 0x0000018B3A946E20&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;True Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;True Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.xlabel(&amp;#39;False Positive Rate&amp;#39;)

# displaying the title&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;False Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Area Under Curve&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Area Under Curve&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file1aec3cf179e1_files/figure-html/unnamed-chunk-23-5.png" width="960" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I also visualize feature importance of the model below. The most
impactful predictor to students’ high school dropout is their last year
GPA, followed by hours spent doing homework on typical school days, and
their self-efficacy in mathematics.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;from matplotlib.pyplot import figure

importances_cat = pd.Series(CBC_tuned.feature_importances_, index = X_hybrid_extreme_trim.columns)

sorted_importance_cat = importances_cat.sort_values()

#Horizontal bar plot
sorted_importance_cat.plot(kind=&amp;#39;barh&amp;#39;, color=&amp;#39;lightgreen&amp;#39;); 
plt.xlabel(&amp;#39;Feature Importance Score&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;Feature Importance Score&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;Features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;Features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Visualizing Important Features&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Visualizing Important Features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.rcParams[&amp;quot;figure.figsize&amp;quot;] = (8, 4)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file1aec3cf179e1_files/figure-html/unnamed-chunk-24-7.png" width="960" /&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The point of this post is to demonstrate how EDM can be used with
large-scale educational data to derive insights and potentially apply it
to practice. We started out with a lot of variables (4014), then we
reduce it based on the relevant theory to 67, based on missing data to
51, based on correlation coefficient to 38, and based on RFECV to
20.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We might want to select variables that are actionable for the
model to be meaningful. For example, saying that a student is likely to
dropout of their high school because of their socio-economic status
might not be as helpful because you cannot change their family income in
a matter of days or months. However, saying that their GPA and hours
spent on home work are influencing factors might allow students to
adjust their learning behavior.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With a meaningful model, an early warning system can be developed
to alert teachers of potential under-performing students for an early
intervention. However, I do not mean that results from the model is
perfect. It should be used in conjunction with other indicators such as
student record, parents’ observation, and behavior note. As education
goes online or semi-online, records of student data can be leveraged to
better understand them and ultimately benefit the teaching
practice.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>53861e41352b60b38bf303d190c7b01f</distill:md5>
      <category>R</category>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-08-06-edm</guid>
      <pubDate>Sat, 06 Aug 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-08-06-edm/rfecv_preview.png" medium="image" type="image/png" width="1920" height="2880"/>
    </item>
    <item>
      <title>Item Response Theory</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-05-15-irt</link>
      <description>


&lt;h2 id="introduction-to-item-response-theory"&gt;Introduction to Item
Response Theory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://taridwong.github.io/posts/2022-01-15-ctt/"&gt;My
previous post on Classical Test Theory (CTT)&lt;/a&gt; discussed how it has
several disadvantages that limit its interpretation to a certain group
of population and therefore reduces its utility to test development.
Specifically, generalizability of the test scores from CTT is quite
limited due to item/test dependency; item parameters such as item
difficulty, item discrimination, and reliability estimates are dependent
upon test scores, which are derived from a group of sample (&lt;a
href="https://www.jstor.org/stable/41219505?casa_token=PZl4ti6FHboAAAAA%3AEsuXjTVftCbh7pnyqQVhGv5cCEG9nAWMNV36MC7nXp5svyUaJhXBQG6jYtcmYNNf3S3b7HkhG48gjFTDo_ftMvXvm6vwRAuQwiEretfpuoKpwFiUQw&amp;amp;seq=1"&gt;DeVellis,
2006&lt;/a&gt;). If we change our sample, those parameters might change. Also,
If we administer two forms of the same test (i.e., form A and form B) to
the same examinee, we still cannot guarantee that they will obtain the
same score on both tests. Raw scores of a CTT-based test do not reflect
learning progress of an examinee as CTT-based scores are not comparable
across time (&lt;a
href="https://www.jstor.org/stable/41219505?casa_token=PZl4ti6FHboAAAAA%3AEsuXjTVftCbh7pnyqQVhGv5cCEG9nAWMNV36MC7nXp5svyUaJhXBQG6jYtcmYNNf3S3b7HkhG48gjFTDo_ftMvXvm6vwRAuQwiEretfpuoKpwFiUQw&amp;amp;seq=1"&gt;DeVellis,
2006&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Such limitations can be addressed by Item Response Theory (IRT).
IRT is able to link information from test items to examinee performance
on the same scale to provide information on the specific domain of
interest and ability of the examinee (θ) (&lt;a
href="https://www.routledge.com/Handbook-of-Item-Response-Theory-Volume-3-Applications/Linden/p/book/9780367221188"&gt;Hambleton
&amp;amp; Zenisky, 2018&lt;/a&gt;). The relationship between observable items and
examinee performance is explained through &lt;em&gt;Item Characteristic Curve
(ICC)&lt;/em&gt;, which explains the probability of getting an item(s)
correctly given the current ability level and parameter (&lt;a
href="https://doi.org/10.1111/j.1745-3992.1993.tb00543.x"&gt;Hambleton
&amp;amp; Jones, 1993&lt;/a&gt;). Therefore, IRT allows researchers to predict
examinees’ expected test score given their ability level. In this post,
I will be examining characteristics of test items based on the IRT
framework. The R packages I will be using are &lt;a
href="https://cran.r-project.org/web/packages/ltm/ltm.pdf"&gt;&lt;code&gt;ltm&lt;/code&gt;&lt;/a&gt;
and &lt;a
href="https://cran.r-project.org/web/packages/mirt/mirt.pdf"&gt;&lt;code&gt;mirt&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(ltm)
library(mirt)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;First, let’s load in a data set. I will be using the data from Law
School Admission Test (LSAT), N = 1000, 5 items. The data can be called
with &lt;code&gt;data(LSAT).&lt;/code&gt;As an initial step, we can use
&lt;code&gt;ltm::descript&lt;/code&gt; for descriptive statistics.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;data(LSAT)
ltm::descript(LSAT)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Descriptive statistics for the &amp;#39;LSAT&amp;#39; data-set

Sample:
 5 items and 1000 sample units; 0 missing values

Proportions for each level of response:
           0     1  logit
Item 1 0.076 0.924 2.4980
Item 2 0.291 0.709 0.8905
Item 3 0.447 0.553 0.2128
Item 4 0.237 0.763 1.1692
Item 5 0.130 0.870 1.9010


Frequencies of total scores:
     0  1  2   3   4   5
Freq 3 20 85 237 357 298


Point Biserial correlation with Total Score:
       Included Excluded
Item 1   0.3620   0.1128
Item 2   0.5668   0.1532
Item 3   0.6184   0.1728
Item 4   0.5344   0.1444
Item 5   0.4354   0.1216


Cronbach&amp;#39;s alpha:
                  value
All Items        0.2950
Excluding Item 1 0.2754
Excluding Item 2 0.2376
Excluding Item 3 0.2168
Excluding Item 4 0.2459
Excluding Item 5 0.2663


Pairwise Associations:
   Item i Item j p.value
1       1      5   0.565
2       1      4   0.208
3       3      5   0.113
4       2      4   0.059
5       1      2   0.028
6       2      5   0.009
7       1      3   0.003
8       4      5   0.002
9       3      4   7e-04
10      2      3   4e-04&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;From the output above, inspection of non significant results can be
used to reveal ‘problematic’ items in pairwise association. Latent
variable models assume that the high associations between items can be
explained by a set of latent variables, so any pair of items that is not
related to each other violates this assumption. Additionally, Item 1
seems to be the easiest item as seen from its highest proportion of
correct response.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="dichotomous-item"&gt;Dichotomous Item&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We will be performing IRT analyses on dichotomous items, which are
items that only have two possible scores of incorrect (0) and correct
(1). The three most common dichotomous IRT models are Rasch/1-parameter
logistics model (1PL), 2-parameter logistics model (2PL), and
3-parameter logistics model(3PL).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="rasch-model-1pl"&gt;Rasch Model (1PL)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We will fit the original Rasch model, which fixes the item
discrimination (aka &lt;em&gt;a&lt;/em&gt; parameter) of all items to 1 to the data.
The 1PL (also called &lt;em&gt;Rasch model&lt;/em&gt;) model describes test items in
terms of only one parameter, &lt;em&gt;item difficulty&lt;/em&gt; (aka &lt;em&gt;b&lt;/em&gt;
parameter). Item difficulty is simply how hard an item is (how high does
one’s latent ability level need to be in order to have a 50% chance of
getting the item right?). &lt;em&gt;b-parameter&lt;/em&gt; is estimated for each
item of the test.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;ltm::rasch()&lt;/code&gt; assumes equal a-parameter across
items with an estimated value. In order to impose the constraint = 1,
the &lt;code&gt;constraint&lt;/code&gt; argument is used. This argument accepts a
two-column matrix where the first column denotes the parameter and the
second column indicates the value at which the corresponding parameter
should be fixed.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_rasch &amp;lt;- rasch(LSAT, constraint = cbind(length(LSAT) + 1, 1))

summary(mod_rasch)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = cbind(length(LSAT) + 1, 1))

Model Summary:
   log.Lik      AIC      BIC
 -2473.054 4956.108 4980.646

Coefficients:
                value std.err   z.vals
Dffclt.Item 1 -2.8720  0.1287 -22.3066
Dffclt.Item 2 -1.0630  0.0821 -12.9458
Dffclt.Item 3 -0.2576  0.0766  -3.3635
Dffclt.Item 4 -1.3881  0.0865 -16.0478
Dffclt.Item 5 -2.2188  0.1048 -21.1660
Dscrmn         1.0000      NA       NA

Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 6.3e-05 
quasi-Newton: BFGS &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The results of the descriptive analysis are also validated by the
model fit, where items 3 and 1 are the most difficult and the easiest
respectively (the lower the &lt;em&gt;b&lt;/em&gt;-parameter value, the easier). The
parameter estimates can be transformed to probability estimates using
the &lt;code&gt;coef()&lt;/code&gt; method&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;coef(mod_rasch, prob = TRUE, order = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           Dffclt Dscrmn P(x=1|z=0)
Item 1 -2.8719712      1  0.9464434
Item 5 -2.2187785      1  0.9019232
Item 4 -1.3880588      1  0.8002822
Item 2 -1.0630294      1  0.7432690
Item 3 -0.2576109      1  0.5640489&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The last column denotes the probability of a positive response
(getting the item correctly) to the &lt;em&gt;i&lt;/em&gt;th item for the average
individual. The argument &lt;code&gt;order = TRUE&lt;/code&gt; indicates the output
to sort the items according to the difficulty estimates. In order to
check the fit of the model to the data, the argument
&lt;code&gt;GoF.rasch()&lt;/code&gt; and &lt;code&gt;margins()&lt;/code&gt; can be used. The
former argument performs a parametric Bootstrap goodness-of-fit test
using Pearson’s Chi-square statistics, while the latter examines the
two- and three-way chi-square residual analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;GoF.rasch(mod_rasch, B = 199) # B = Bootstrap sample&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Bootstrap Goodness-of-Fit using Pearson chi-squared

Call:
rasch(data = LSAT, constraint = cbind(length(LSAT) + 1, 1))

Tobs: 30.6 
# data-sets: 200 
p-value: 0.26 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Based on 200 data-points, the non significant p-value suggests an
acceptable fit of the model. The null hypothesis states that the
observed data and the model fit with each other (no differences). Now,
for two-way margin analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_rasch)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = cbind(length(LSAT) + 1, 1))

Fit on the Two-Way Margins

Response: (0,0)
  Item i Item j Obs   Exp (O-E)^2/E  
1      2      4  81 98.69      3.17  
2      1      5  12 18.45      2.25  
3      3      5  67 80.04      2.12  

Response: (1,0)
  Item i Item j Obs    Exp (O-E)^2/E  
1      3      5  63  51.62      2.51  
2      2      4 156 139.78      1.88  
3      3      4 108  99.42      0.74  

Response: (0,1)
  Item i Item j Obs    Exp (O-E)^2/E  
1      2      4 210 193.47      1.41  
2      2      3 135 125.07      0.79  
3      1      4  53  47.24      0.70  

Response: (1,1)
  Item i Item j Obs    Exp (O-E)^2/E  
1      2      4 553 568.06      0.40  
2      3      5 490 501.43      0.26  
3      2      3 418 427.98      0.23  &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;From the above output, using the 3.5 rule of thumb, the value of all
two-way combinations are below the cut-off (same way of how statistical
hypothesis works) and therefore indicate a good fit to the two-way
margins. Next, we will examine the fit to the three-way margins.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_rasch, type = &amp;quot;three-way&amp;quot;, nprint = 2) #nprint returns 2 highest residual values for each combinations&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = cbind(length(LSAT) + 1, 1))

Fit on the Three-Way Margins

Response: (0,0,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E    
1      2      3      4  48 66.07      4.94 ***
2      1      3      5   6 13.58      4.23 ***

Response: (1,0,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      2      4  70 82.01      1.76  
2      2      4      5  28 22.75      1.21  

Response: (0,1,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      2      5   3  7.73      2.90  
2      3      4      5  37 45.58      1.61  

Response: (1,1,0)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      3      4      5  48  36.91      3.33  
2      1      2      4 144 126.35      2.47  

Response: (0,0,1)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      3      5  41 34.58      1.19  
2      2      4      5  64 72.26      0.94  

Response: (1,0,1)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      1      2      4 190 174.87      1.31  
2      1      2      3 126 114.66      1.12  

Response: (0,1,1)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      2      5  42 34.35      1.70  
2      1      4      5  46 38.23      1.58  

Response: (1,1,1)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      3      4      5 397 416.73      0.93  
2      2      3      4 343 361.18      0.91  

&amp;#39;***&amp;#39; denotes a chi-squared residual greater than 3.5 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The three-way margins suggest a problematic fit for two triplets of
items, both containing item 3. We can try fitting the unconstrained
version of Rasch model (not fixing the &lt;em&gt;a&lt;/em&gt;-parameter to 1) to see
the difference. This time, no need for the &lt;code&gt;constraint&lt;/code&gt;
argument.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_1pl &amp;lt;- rasch(LSAT, constraint = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;After fitting the 1PL model, we will request for Item
Characteristics Curve (ICC), Item Information Curve (IIC), Test
Information Function (TIF), Latent Ability Curve of the examinees, and
Uni-dimensionality Plot of the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mfrow = c(2, 3))

plot(mod_1pl, type=c(&amp;quot;ICC&amp;quot;), 
     legend = TRUE, cx = &amp;quot;bottomright&amp;quot;, lwd = 2, 
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;1PL Item Characteristics Curve&amp;quot;)

plot(mod_1pl,type=c(&amp;quot;IIC&amp;quot;),
     legend = TRUE, cx = &amp;quot;topright&amp;quot;, lwd = 2,
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;1PL Item Information Curve&amp;quot;)

plot(mod_1pl,type=c(&amp;quot;IIC&amp;quot;),items=c(0), lwd = 2,
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;Test Information Function&amp;quot;)

plot(0:1, 0:1, type = &amp;quot;n&amp;quot;, ann = FALSE, axes = FALSE)
info1 &amp;lt;- information(mod_1pl, c(-4, 0)) #Item information on the negative area of theta (-4 to 0)
info2 &amp;lt;- information(mod_1pl, c(0, 4)) #Item information on the positive area of theta (0 to 4)
text(0.5, 0.5, labels = paste(&amp;quot;Total Information:&amp;quot;, round(info1$InfoTotal, 3),
                               &amp;quot;\n\nInformation in (-4, 0):&amp;quot;, round(info1$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info1$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;),
                               &amp;quot;\n\nInformation in (0, 4):&amp;quot;, round(info2$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info2$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;)))

theta.rasch&amp;lt;-ltm::factor.scores(mod_1pl)
summary(theta.rasch$score.dat$z1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-1.9104 -0.9594 -0.4660 -0.6867 -0.4660  0.5930 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(theta.rasch, main = &amp;quot;Latent Ability of the Examinee&amp;quot;)

unitest_1pl &amp;lt;- unidimTest(mod_1pl,LSAT)
plot(unitest_1pl, type = &amp;quot;b&amp;quot;, pch = 1:2, main = &amp;quot;Modified Parallel Analysis Plot&amp;quot;)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Real Data&amp;quot;, &amp;quot;Average Simulated Data&amp;quot;), lty = 1, 
    pch = 1:2, col = 1:2, bty = &amp;quot;n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1aec7b91e18_files/figure-html/unnamed-chunk-11-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The first plot is ICC of out 1PL model. ICC shows the
relationship between examinee ability (θ) and the probability of
examinees answering an item correctly based on their ability. On ICC,
item discrimination is represented by the steepness of the curve, and
item difficulty is represented by the position where the probability of
getting the item correct is 0.5.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Try comparing item 1 and item 3. For item 1, the point where the
probability of getting the item correctly is 0.5 matches with the
ability point -4 on the X-axis. However, for item 3, the point where the
probability of getting the item correctly is 0.5 matches with the
ability point 0 on the X-axis. In other words, you need more ability to
get item 3 correct than item 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The second plot is IIC. IIC shows how much “information” about
the latent trait ability an item can provide. Item information curves
peak at the point of difficulty value, where the item has the highest
discrimination and the probability of answering the item correctly is
0.5. To put it in plain language, a very difficult item will provide
very little information about persons with low ability (because the item
is already too hard), and very easy items will provide little
information about persons with high ability levels (because it is too
easy).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The third plot is TIF of the whole test. This is simply the sum
of the individual IICs above. The curves shows how much information this
test offers in terms of ability level of examinees. Ideally, we want a
test which provides fairly good coverage of a wide range of latent
ability levels. Otherwise, the test is only good at identifying a
limited range of examinees. The current TIF shows that more information
is yielded around examinees with -2 ability level. The test could use
more items for people with high ability (more difficult item is needed).
In particular, the amount of Test Information for ability levels in the
interval (-4 - 0) is almost 60%, and the item that seems to distinguish
between respondents with higher ability levels is item 3 as it is the
most difficult item.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Latent Ability Curve shows the distribution of examinee’s
latent ability level. The plot shows that most examinees are located
around 0 to 1 ability level.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;unitest_1pl&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Unidimensionality Check using Modified Parallel Analysis

Call:
rasch(data = LSAT, constraint = NULL)

Matrix of tertachoric correlations
       Item 1 Item 2 Item 3 Item 4 Item 5
Item 1 1.0000 0.1703 0.2275 0.1072 0.0665
Item 2 0.1703 1.0000 0.1891 0.1111 0.1724
Item 3 0.2275 0.1891 1.0000 0.1867 0.1055
Item 4 0.1072 0.1111 0.1867 1.0000 0.2009
Item 5 0.0665 0.1724 0.1055 0.2009 1.0000

Alternative hypothesis: the second eigenvalue of the observed data is substantially larger 
            than the second eigenvalue of data under the assumed IRT model

Second eigenvalue in the observed data: 0.2254
Average of second eigenvalues in Monte Carlo samples: 0.256
Monte Carlo samples: 100
p-value: 0.6535&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The last plot is a test for unidimensionality of the test with
modified parallel analysis &lt;a
href="https://psycnet.apa.org/record/1983-31736-001"&gt;(Drasgpw &amp;amp;
Lissak, 1983&lt;/a&gt;). The output above shows that the result is
non-significant, meaning that the 1PL model fits the data well and we
are actually measuring a single trait here. The data is a law school
test, so it should be measuring contents about law, not maths or
English. The unidimensionality analysis shows that the test is measuring
what it is intended to measure.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;summary(mod_1pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = NULL)

Model Summary:
   log.Lik      AIC      BIC
 -2466.938 4945.875 4975.322

Coefficients:
                value std.err   z.vals
Dffclt.Item 1 -3.6153  0.3266 -11.0680
Dffclt.Item 2 -1.3224  0.1422  -9.3009
Dffclt.Item 3 -0.3176  0.0977  -3.2518
Dffclt.Item 4 -1.7301  0.1691 -10.2290
Dffclt.Item 5 -2.7802  0.2510 -11.0743
Dscrmn         0.7551  0.0694  10.8757

Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 2.9e-05 
quasi-Newton: BFGS &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;coef(mod_1pl, prob = TRUE, order = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           Dffclt    Dscrmn P(x=1|z=0)
Item 1 -3.6152665 0.7551347  0.9387746
Item 5 -2.7801716 0.7551347  0.8908453
Item 4 -1.7300903 0.7551347  0.7869187
Item 2 -1.3224208 0.7551347  0.7307844
Item 3 -0.3176306 0.7551347  0.5596777&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The output above suggests that the discrimination parameter of our
unconstrained model is different from 1, meaning that our constrained
and unconstrained Rasch models are different. The difference can be
tested with a likelihood ratio test using &lt;code&gt;anova().&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;anova(mod_rasch, mod_1pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 Likelihood Ratio Table
              AIC     BIC  log.Lik   LRT df p.value
mod_rasch 4956.11 4980.65 -2473.05                 
mod_1pl   4945.88 4975.32 -2466.94 12.23  1  &amp;lt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;By comparing model summary of the constrained and unconstrained
version, the latter is more suitable for the LSAT data due to its
smaller Akaike’s Information Criterion (AIC) and Bayesian Information
Criterion (BIC) values. AIC and BIC are measures of model performance
that account for model complexity. AIC is a measure that determines
which model fits the data better. The lower the score, the better fit
the model is. Similarly for BIC, the score measures complexity of the
model. BIC penalizes the model more for its complexity, meaning that
more complex models will have a worse (larger) score and will, in turn,
be less likely to be selected.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can double check the result by testing the unconstrained model
with the three-way margins, which yields a problematic fit with the
constrained model.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_1pl, type = &amp;quot;three-way&amp;quot;, nprint = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = NULL)

Fit on the Three-Way Margins

Response: (0,0,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      3      5   6  9.40      1.23  
2      3      4      5  30 25.85      0.67  

Response: (1,0,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      2      4      5  28 22.75      1.21  
2      2      3      4  81 74.44      0.58  

Response: (0,1,0)
  Item i Item j Item k Obs  Exp (O-E)^2/E  
1      1      2      5   3 7.58      2.76  
2      1      3      4   5 9.21      1.92  

Response: (1,1,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      2      4      5  51 57.49      0.73  
2      3      4      5  48 42.75      0.64  

Response: (0,0,1)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      1      3      5  41  33.07      1.90  
2      2      3      4 108 101.28      0.45  

Response: (1,0,1)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      2      3      4 210 218.91      0.36  
2      1      2      4 190 185.56      0.11  

Response: (0,1,1)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      3      5  23 28.38      1.02  
2      1      4      5  46 42.51      0.29  

Response: (1,1,1)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      1      2      4 520 526.36      0.08  
2      1      2      3 398 393.30      0.06  &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The new three-way margins suggests a good fit with the unconstrained
Rasch model. Finally, we investigate two more possible extensions of the
unconstrained Rasch model, the two-parameter logistic (2PL) model that
assumes a different discrimination parameter per item, and Rasch model
that incorporates a guessing parameter (3PL).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="parameter-logistics-model-2pl"&gt;2 Parameter Logistics Model
(2PL)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The 2PL model has the same equation as the 1PL model, but unlike
1PL, 2PL allows item discrimination and item difficulty to vary across
items instead of fixing it to a constant value.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The 2PL model can also be fitted with &lt;code&gt;ltm()&lt;/code&gt;.The
formula of &lt;code&gt;ltm()&lt;/code&gt; is two-sided, where its left is either a
data frame or a matrix, and its right allows only &lt;code&gt;z1&lt;/code&gt; and/or
&lt;code&gt;z2&lt;/code&gt;. Latent variables with &lt;code&gt;z2&lt;/code&gt; serves in the
case of interaction.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_2pl &amp;lt;- ltm(LSAT ~ z1)
summary(mod_2pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
ltm(formula = LSAT ~ z1)

Model Summary:
   log.Lik      AIC      BIC
 -2466.653 4953.307 5002.384

Coefficients:
                value std.err  z.vals
Dffclt.Item 1 -3.3597  0.8669 -3.8754
Dffclt.Item 2 -1.3696  0.3073 -4.4565
Dffclt.Item 3 -0.2799  0.0997 -2.8083
Dffclt.Item 4 -1.8659  0.4341 -4.2982
Dffclt.Item 5 -3.1236  0.8700 -3.5904
Dscrmn.Item 1  0.8254  0.2581  3.1983
Dscrmn.Item 2  0.7229  0.1867  3.8721
Dscrmn.Item 3  0.8905  0.2326  3.8281
Dscrmn.Item 4  0.6886  0.1852  3.7186
Dscrmn.Item 5  0.6575  0.2100  3.1306

Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 0.024 
quasi-Newton: BFGS &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The output above shows that the model estimated both item difficulty
and item discrimination. Next, we can try comparing our 1PL model with
the newly fitted 2PL model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#compare
anova(mod_1pl, mod_2pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 Likelihood Ratio Table
            AIC     BIC  log.Lik  LRT df p.value
mod_1pl 4945.88 4975.32 -2466.94                
mod_2pl 4953.31 5002.38 -2466.65 0.57  4   0.967&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The comparison suggested no significant difference between both
models. Next, we ca nrequest for ICC, IIC, TIF, Latent Ability
Distribution, and Unidimensionality Plot like we did with the 1PL
model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mfrow = c(2, 3))

plot(mod_2pl, type=c(&amp;quot;ICC&amp;quot;), 
     legend = TRUE, cx = &amp;quot;bottomright&amp;quot;, lwd = 2, 
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;2PL Item Characteristics Curve&amp;quot;)

plot(mod_2pl,type=c(&amp;quot;IIC&amp;quot;),
     legend = TRUE, cx = &amp;quot;topright&amp;quot;, lwd = 2,
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;2PL Item Information Curve&amp;quot;)

plot(mod_2pl,type=c(&amp;quot;IIC&amp;quot;),items=c(0), lwd = 2,
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;Test Information Function&amp;quot;)

plot(0:1, 0:1, type = &amp;quot;n&amp;quot;, ann = FALSE, axes = FALSE)
info1_2pl &amp;lt;- information(mod_2pl, c(-4, 0)) #Item information on the negative area of theta (-4 to 0)
info2_2pl &amp;lt;- information(mod_2pl, c(0, 4)) #Item information on the positive area of theta (0 to 4)
text(0.5, 0.5, labels = paste(&amp;quot;Total Information:&amp;quot;, round(info1_2pl$InfoTotal, 3),
                               &amp;quot;\n\nInformation in (-4, 0):&amp;quot;, round(info1_2pl$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info1_2pl$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;),
                               &amp;quot;\n\nInformation in (0, 4):&amp;quot;, round(info2_2pl$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info2_2pl$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;)))

theta.2pl&amp;lt;-ltm::factor.scores(mod_2pl)
summary(theta.2pl$score.dat$z1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-1.8953 -1.0026 -0.5397 -0.6629 -0.3572  0.6064 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(theta.2pl, main = &amp;quot;Latent ability scores of the participants 2PL&amp;quot;)

unitest_2pl &amp;lt;- unidimTest(mod_2pl,LSAT)
plot(unitest_2pl, type = &amp;quot;b&amp;quot;, pch = 1:2)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Real Data&amp;quot;, &amp;quot;Average Simulated Data&amp;quot;), lty = 1, 
    pch = 1:2, col = 1:2, bty = &amp;quot;n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1aec7b91e18_files/figure-html/unnamed-chunk-18-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ICC and IIC of the two models are different, meaning that when we
allow item discrimination to vary, characteristics and yielded
information of each item also changed accordingly.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;unitest_2pl&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Unidimensionality Check using Modified Parallel Analysis

Call:
ltm(formula = LSAT ~ z1)

Matrix of tertachoric correlations
       Item 1 Item 2 Item 3 Item 4 Item 5
Item 1 1.0000 0.1703 0.2275 0.1072 0.0665
Item 2 0.1703 1.0000 0.1891 0.1111 0.1724
Item 3 0.2275 0.1891 1.0000 0.1867 0.1055
Item 4 0.1072 0.1111 0.1867 1.0000 0.2009
Item 5 0.0665 0.1724 0.1055 0.2009 1.0000

Alternative hypothesis: the second eigenvalue of the observed data is substantially larger 
            than the second eigenvalue of data under the assumed IRT model

Second eigenvalue in the observed data: 0.2254
Average of second eigenvalues in Monte Carlo samples: 0.256
Monte Carlo samples: 100
p-value: 0.6832&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The above results of unidimensionality testing also suggests that
the test measures only one construct. When considering two models
together, it might be more preferable for us to choose the 1PL model as
there is no difference between both 1PL and 2PL; however, by nature, 1PL
model is more simple and easier to explain comparing to 2PL. Let us try
fitting the data to a 3PL model just in case.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="parameter-logistics-model-3pl"&gt;3 Parameter Logistics Model
(3PL)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The 3PL model is very similar to the 2PL model; however, the model
includes an additional parameter: lower asymptote (also known as the
guessing parameter). Under this model, individuals with zero ability
have a nonzero chance of correctly answering any item just by guessing
randomly. A 3PL model can be fitted with &lt;code&gt;tpm()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_3pl &amp;lt;- tpm(LSAT, type = &amp;quot;latent.trait&amp;quot;, max.guessing = 1)

summary(mod_3pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
tpm(data = LSAT, type = &amp;quot;latent.trait&amp;quot;, max.guessing = 1)

Model Summary:
  log.Lik      AIC      BIC
 -2466.66 4963.319 5036.935

Coefficients:
                value std.err  z.vals
Gussng.Item 1  0.0374  0.8650  0.0432
Gussng.Item 2  0.0777  2.5282  0.0307
Gussng.Item 3  0.0118  0.2815  0.0419
Gussng.Item 4  0.0353  0.5769  0.0612
Gussng.Item 5  0.0532  1.5596  0.0341
Dffclt.Item 1 -3.2965  1.7788 -1.8532
Dffclt.Item 2 -1.1451  7.5166 -0.1523
Dffclt.Item 3 -0.2490  0.7527 -0.3308
Dffclt.Item 4 -1.7658  1.6162 -1.0925
Dffclt.Item 5 -2.9902  4.0606 -0.7364
Dscrmn.Item 1  0.8286  0.2877  2.8797
Dscrmn.Item 2  0.7604  1.3774  0.5520
Dscrmn.Item 3  0.9016  0.4190  2.1516
Dscrmn.Item 4  0.7007  0.2574  2.7219
Dscrmn.Item 5  0.6658  0.3282  2.0284

Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Optimizer: optim (BFGS)
Convergence: 0 
max(|grad|): 0.028 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The model summary above suggested that all three item parameters of
item discrimination (&lt;em&gt;a&lt;/em&gt;), item difficulty (&lt;em&gt;b&lt;/em&gt;), and item
guessing (&lt;em&gt;c&lt;/em&gt;) are allowed to vary. Like what we did, we can try
requesting for ICC, IIC, TIF, latent ability distribution, and
unidimensionality plot.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mfrow = c(2, 3))

plot(mod_3pl, type=c(&amp;quot;ICC&amp;quot;), 
     legend = TRUE, cx = &amp;quot;bottomright&amp;quot;, lwd = 2, 
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;3PL Item Characteristics Curve&amp;quot;)

plot(mod_3pl,type=c(&amp;quot;IIC&amp;quot;),
     legend = TRUE, cx = &amp;quot;topright&amp;quot;, lwd = 2,
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;3PL Item Information Curve&amp;quot;)

plot(mod_3pl,type=c(&amp;quot;IIC&amp;quot;),items=c(0))

plot(0:1, 0:1, type = &amp;quot;n&amp;quot;, ann = FALSE, axes = FALSE)
info1_3pl &amp;lt;- information(mod_3pl, c(-4, 0)) #Item information on the negative area of theta (-4 to 0)
info2_3pl &amp;lt;- information(mod_3pl, c(0, 4)) #Item information on the positive area of theta (0 to 4)
text(0.5, 0.5, labels = paste(&amp;quot;Total Information:&amp;quot;, round(info1_3pl$InfoTotal, 3),
                               &amp;quot;\n\nInformation in (-4, 0):&amp;quot;, round(info1_3pl$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info1_3pl$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;),
                               &amp;quot;\n\nInformation in (0, 4):&amp;quot;, round(info2_3pl$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info2_3pl$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;)))

theta.3pl&amp;lt;-ltm::factor.scores(mod_3pl)
summary(theta.3pl$score.dat$z1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-1.8706 -0.9992 -0.5368 -0.6584 -0.3590  0.6116 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(theta.3pl, main = &amp;quot;Latent ability scores of the participants 3PL&amp;quot;)

unitest_3pl &amp;lt;- unidimTest(mod_3pl,LSAT)
plot(unitest_3pl, type = &amp;quot;b&amp;quot;, pch = 1:2)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Real Data&amp;quot;, &amp;quot;Average Simulated Data&amp;quot;), lty = 1, 
    pch = 1:2, col = 1:2, bty = &amp;quot;n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1aec7b91e18_files/figure-html/unnamed-chunk-21-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;unitest_3pl&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Unidimensionality Check using Modified Parallel Analysis

Call:
tpm(data = LSAT, type = &amp;quot;latent.trait&amp;quot;, max.guessing = 1)

Matrix of tertachoric correlations
       Item 1 Item 2 Item 3 Item 4 Item 5
Item 1 1.0000 0.1703 0.2275 0.1072 0.0665
Item 2 0.1703 1.0000 0.1891 0.1111 0.1724
Item 3 0.2275 0.1891 1.0000 0.1867 0.1055
Item 4 0.1072 0.1111 0.1867 1.0000 0.2009
Item 5 0.0665 0.1724 0.1055 0.2009 1.0000

Alternative hypothesis: the second eigenvalue of the observed data is substantially larger 
            than the second eigenvalue of data under the assumed IRT model

Second eigenvalue in the observed data: 0.2254
Average of second eigenvalues in Monte Carlo samples: 0.257
Monte Carlo samples: 100
p-value: 0.6436&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The unidimensionality testing above suggested non-significant
result, meaning that the test measures one construct as intended.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;anova(mod_1pl, mod_3pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 Likelihood Ratio Table
            AIC     BIC  log.Lik  LRT df p.value
mod_1pl 4945.88 4975.32 -2466.94                
mod_3pl 4963.32 5036.94 -2466.66 0.56  9       1&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The comparison between the 1PL and the 3PL model also suggests no
statistical differences. The 3PL model also has larger AIC and BIC;
therefore, it is more preferable for us to use 1PL model with this data.
Finally, we can request for ability estimates for all response pattern
with the &lt;code&gt;factor.scores()&lt;/code&gt; function.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;factor.scores(mod_1pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = NULL)

Scoring Method: Empirical Bayes

Factor-Scores for observed response patterns:
   Item 1 Item 2 Item 3 Item 4 Item 5 Obs     Exp     z1 se.z1
1       0      0      0      0      0   3   2.364 -1.910 0.790
2       0      0      0      0      1   6   5.468 -1.439 0.793
3       0      0      0      1      0   2   2.474 -1.439 0.793
4       0      0      0      1      1  11   8.249 -0.959 0.801
5       0      0      1      0      0   1   0.852 -1.439 0.793
6       0      0      1      0      1   1   2.839 -0.959 0.801
7       0      0      1      1      0   3   1.285 -0.959 0.801
8       0      0      1      1      1   4   6.222 -0.466 0.816
9       0      1      0      0      0   1   1.819 -1.439 0.793
10      0      1      0      0      1   8   6.063 -0.959 0.801
11      0      1      0      1      1  16  13.288 -0.466 0.816
12      0      1      1      0      1   3   4.574 -0.466 0.816
13      0      1      1      1      0   2   2.070 -0.466 0.816
14      0      1      1      1      1  15  14.749  0.049 0.836
15      1      0      0      0      0  10  10.273 -1.439 0.793
16      1      0      0      0      1  29  34.249 -0.959 0.801
17      1      0      0      1      0  14  15.498 -0.959 0.801
18      1      0      0      1      1  81  75.060 -0.466 0.816
19      1      0      1      0      0   3   5.334 -0.959 0.801
20      1      0      1      0      1  28  25.834 -0.466 0.816
21      1      0      1      1      0  15  11.690 -0.466 0.816
22      1      0      1      1      1  80  83.310  0.049 0.836
23      1      1      0      0      0  16  11.391 -0.959 0.801
24      1      1      0      0      1  56  55.171 -0.466 0.816
25      1      1      0      1      0  21  24.965 -0.466 0.816
26      1      1      0      1      1 173 177.918  0.049 0.836
27      1      1      1      0      0  11   8.592 -0.466 0.816
28      1      1      1      0      1  61  61.235  0.049 0.836
29      1      1      1      1      0  28  27.709  0.049 0.836
30      1      1      1      1      1 298 295.767  0.593 0.862&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;By default, &lt;code&gt;factor.scores()&lt;/code&gt; produces ability estimates
for the observed response patterns (every combination available); if
ability estimates are required for non observed or specific response
patterns, these could be specified using the &lt;code&gt;resp.patterns&lt;/code&gt;
argument.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;factor.scores(mod_1pl, resp.patterns = rbind(c(1,1,1,1,1), c(0,0,0,0,0)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = NULL)

Scoring Method: Empirical Bayes

Factor-Scores for specified response patterns:
  Item 1 Item 2 Item 3 Item 4 Item 5 Obs     Exp     z1 se.z1
1      1      1      1      1      1 298 295.767  0.593 0.862
2      0      0      0      0      0   3   2.364 -1.910 0.790&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The specified response patterns above are for examinees who got
all items correctly and incorrectly respectively. The results suggested
that the examinee who got all item correctly has ability level of 0.50
and the examinee who got all item incorrectly has ability level of
-1.91.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The model we discussed so far, Rasch, 1PL,2PL, 3PL are all
suitable for dichotomous test items (True/False), but what if item
responses have more than 2 categories like in a survey (i.e., 1 =
Strongly disagree 2 = Disagree 3 = Agree 4 = Strongly Agree)? This is
when we use polytomous IRT models.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="polytomous-item"&gt;Polytomous Item&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The data we consider here comes from the Environment section of the
1990 British Social Attitudes Survey, N = 291, 6 items, 3 ordinal
response options. The data can be loaded with
&lt;code&gt;data(Environment)&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;data(Environment)
descript(Environment)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Descriptive statistics for the &amp;#39;Environment&amp;#39; data-set

Sample:
 6 items and 291 sample units; 0 missing values

Proportions for each level of response:
             very concerned slightly concerned not very concerned
LeadPetrol           0.6151             0.3265             0.0584
RiverSea             0.8007             0.1753             0.0241
RadioWaste           0.7457             0.1924             0.0619
AirPollution         0.6495             0.3196             0.0309
Chemicals            0.7491             0.1924             0.0584
Nuclear              0.5155             0.3265             0.1581


Frequencies of total scores:
      6  7  8  9 10 11 12 13 14 15 16 17 18
Freq 96 51 37 27 26 18 13  7  6  6  1  1  2


Cronbach&amp;#39;s alpha:
                        value
All Items              0.8215
Excluding LeadPetrol   0.8218
Excluding RiverSea     0.7990
Excluding RadioWaste   0.7767
Excluding AirPollution 0.7751
Excluding Chemicals    0.7790
Excluding Nuclear      0.8058


Pairwise Associations:
   Item i Item j p.value
1       1      2   0.001
2       1      3   0.001
3       1      4   0.001
4       1      5   0.001
5       1      6   0.001
6       2      3   0.001
7       2      4   0.001
8       2      5   0.001
9       2      6   0.001
10      3      4   0.001&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;From the descriptive output, the first response, “&lt;em&gt;very
concerned&lt;/em&gt;”, has the highest frequency. The p-values for the
pairwise associations indicate significant associations between all
items. An alternative method to explore the degree of association
between pairs of items can be done with &lt;code&gt;rcor.test()&lt;/code&gt; for
non-parametric correlation coefficients.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;rcor.test(Environment, method = &amp;quot;kendall&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
             LeadPetrol RiverSea RadioWaste AirPollution Chemicals
LeadPetrol    *****      0.385    0.260      0.457        0.305   
RiverSea     &amp;lt;0.001      *****    0.399      0.548        0.403   
RadioWaste   &amp;lt;0.001     &amp;lt;0.001    *****      0.506        0.623   
AirPollution &amp;lt;0.001     &amp;lt;0.001   &amp;lt;0.001      *****        0.504   
Chemicals    &amp;lt;0.001     &amp;lt;0.001   &amp;lt;0.001     &amp;lt;0.001        *****   
Nuclear      &amp;lt;0.001     &amp;lt;0.001   &amp;lt;0.001     &amp;lt;0.001       &amp;lt;0.001   
             Nuclear
LeadPetrol    0.279 
RiverSea      0.320 
RadioWaste    0.484 
AirPollution  0.382 
Chemicals     0.463 
Nuclear       ***** 

upper diagonal part contains correlation coefficient estimates 
lower diagonal part contains corresponding p-values&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;rcor.test()&lt;/code&gt; provides two options for nonparametric
calculation, Kendall’s &lt;em&gt;Tau&lt;/em&gt; and Spearman’s &lt;em&gt;rho&lt;/em&gt; in
&lt;code&gt;method&lt;/code&gt; argument. Initially, we will fit the partial credit
model (PCM), which is a Polytomous version of the Rasch model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="partial-credit-model"&gt;Partial Credit Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;PCM fixes the discrimination parameter of all item as 1 in the
same way as what Rasch’s model does. The threshold (or the parameter
that represents the trait level necessary for an examinee to have 50% to
pick a response category) of PCM is allowed to vary.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For the &lt;code&gt;gpcm()&lt;/code&gt; function, If
&lt;code&gt;constraint = "rasch"&lt;/code&gt;, then the discrimination parameter is
assumed equal for all items and fixed at one. If
&lt;code&gt;constraint = "1PL"&lt;/code&gt;, then the discrimination parameter βi is
assumed equal for all items but is estimated. Here, we are fixing all
discrimination parameter to 1.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_pcm_rasch &amp;lt;- gpcm(Environment, constraint = &amp;quot;rasch&amp;quot;)
summary(mod_pcm_rasch)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
gpcm(data = Environment, constraint = &amp;quot;rasch&amp;quot;)

Model Summary:
   log.Lik      AIC      BIC
 -1147.176 2318.351 2362.431

Coefficients:
$LeadPetrol
        value std.err z.value
Catgr.1 0.680   0.153   4.450
Catgr.2 2.785   0.292   9.541
Dscrmn  1.000      NA      NA

$RiverSea
        value std.err z.value
Catgr.1 1.822   0.180  10.149
Catgr.2 3.385   0.435   7.781
Dscrmn  1.000      NA      NA

$RadioWaste
        value std.err z.value
Catgr.1 1.542   0.174   8.879
Catgr.2 2.328   0.302   7.709
Dscrmn  1.000      NA      NA

$AirPollution
        value std.err z.value
Catgr.1 0.822   0.153   5.363
Catgr.2 3.517   0.376   9.343
Dscrmn  1.000      NA      NA

$Chemicals
        value std.err z.value
Catgr.1 1.555   0.174   8.949
Catgr.2 2.399   0.308   7.788
Dscrmn  1.000      NA      NA

$Nuclear
        value std.err z.value
Catgr.1 0.316   0.156   2.029
Catgr.2 1.498   0.208   7.218
Dscrmn  1.000      NA      NA


Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 0.0079 
optimizer: nlminb &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The model summary provides AIC and BIC of the model. Same as what we
did with our dichotomous data, we can request for parameter estimates of
each item.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;coef(mod_pcm_rasch, prob = TRUE, order = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;             Catgr.1 Catgr.2 Dscrmn
LeadPetrol     0.680   2.785      1
RiverSea       1.822   3.385      1
RadioWaste     1.542   2.328      1
AirPollution   0.822   3.517      1
Chemicals      1.555   2.399      1
Nuclear        0.316   1.498      1&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;In order to check the fit of the model to the data, the argument
&lt;code&gt;GoF.gpcm&lt;/code&gt; and &lt;code&gt;margins()&lt;/code&gt; can be used.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;GoF.gpcm(mod_pcm_rasch, B = 199) # B = Bootstrap sample&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Parametric Bootstrap Approximation to Pearson chi-squared Goodness-of-Fit Measure

Call:
gpcm(data = Environment, constraint = &amp;quot;rasch&amp;quot;)

Tobs: 1001.41 
# data-sets: 200 
p-value: 0.1 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Based on 200 data-points, the non significant p-value suggests an
acceptable fit of the model. The null hypothesis states that the
observed data and the model fit with each other (no differences). We can
move on to the two-way margin analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_pcm_rasch)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
gpcm(data = Environment, constraint = &amp;quot;rasch&amp;quot;)

Fit on the Two-Way Margins

             LeadPetrol RiverSea RadioWaste AirPollution Chemicals
LeadPetrol   &amp;lt;NA&amp;gt;       35.47     3.04      34.43         7.41    
RiverSea     ***        &amp;lt;NA&amp;gt;     20.07      77.50        18.75    
RadioWaste                       &amp;lt;NA&amp;gt;       44.64        68.12    
AirPollution ***        ***      ***        &amp;lt;NA&amp;gt;         33.29    
Chemicals                        ***        ***          &amp;lt;NA&amp;gt;     
Nuclear                          ***                              
             Nuclear
LeadPetrol    4.71  
RiverSea     10.36  
RadioWaste   43.35  
AirPollution 16.56  
Chemicals    27.68  
Nuclear      &amp;lt;NA&amp;gt;   

&amp;#39;***&amp;#39; denotes pairs of items with lack-of-fit&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The upper diagonal part of the output contains the residuals, and
the lower diagonal part indicates the pairs for which the residuals
exceed the threshold value. The two-way margin analysis above suggests
problematic fit of the data with the PCM model, meaning that PCM might
not be suitable for this data. We can try using the next model, the
Graded Response Model (GRM).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="graded-response-model"&gt;Graded Response Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;GRM is the polytomous version of the 2PL model (&lt;a
href="https://www.frontiersin.org/articles/10.3389/feduc.2021.721963/full"&gt;Dai
et al., 2021&lt;/a&gt;). Despite able to constrain the discrimination
parameter, GRM works differently than PCM. PCM estimates separate
category response parameters for each item, while the GRM model further
assumes that the thresholds for category response are also equal across
items (&lt;a
href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4520411/#:~:text=The%20difference%20between%20the%20two,are%20also%20equal%20across%20items."&gt;Nguyen
et al., 2014&lt;/a&gt;). Initially, we can try fitting the constrained version
of Graded Response Model (GRM) that assumes equal &lt;em&gt;a&lt;/em&gt; parameter
across items (similar to Rasch model). The model is fitted by
&lt;code&gt;grm()&lt;/code&gt; as follows&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_grm &amp;lt;- grm(Environment, constrained = TRUE)
summary(mod_grm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment, constrained = TRUE)

Model Summary:
   log.Lik      AIC      BIC
 -1106.193 2238.386 2286.139

Coefficients:
$LeadPetrol
        value
Extrmt1 0.395
Extrmt2 1.988
Dscrmn  2.218

$RiverSea
        value
Extrmt1 1.060
Extrmt2 2.560
Dscrmn  2.218

$RadioWaste
        value
Extrmt1 0.832
Extrmt2 1.997
Dscrmn  2.218

$AirPollution
        value
Extrmt1 0.483
Extrmt2 2.448
Dscrmn  2.218

$Chemicals
        value
Extrmt1 0.855
Extrmt2 2.048
Dscrmn  2.218

$Nuclear
        value
Extrmt1 0.062
Extrmt2 1.266
Dscrmn  2.218


Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 0.0049 
quasi-Newton: BFGS &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;If standard errors for the parameter estimates are required, you can
add the argument &lt;code&gt;Hessian = T&lt;/code&gt; to the function
&lt;code&gt;grm()&lt;/code&gt;. Similarly to our dichotomous case, the fit of the
model can be checked using &lt;code&gt;margins()&lt;/code&gt; for two-way
margins.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_grm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment, constrained = TRUE)

Fit on the Two-Way Margins

             LeadPetrol RiverSea RadioWaste AirPollution Chemicals
LeadPetrol   -          10.03     9.98       5.19         7.85    
RiverSea                -         5.06      17.12         2.56    
RadioWaste                       -           6.78        20.60    
AirPollution                                -             4.49    
Chemicals                                                -        
Nuclear                                                           
             Nuclear
LeadPetrol   16.93  
RiverSea      7.14  
RadioWaste   12.09  
AirPollution  4.57  
Chemicals     3.85  
Nuclear      -      &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The output above looks good as there is no indication of poor fit.
Next, we will try with the three-way margins.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_grm, type = &amp;quot;three&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment, constrained = TRUE)

Fit on the Three-Way Margins

   Item i Item j Item k (O-E)^2/E  
1       1      2      3     28.52  
2       1      2      4     34.26  
3       1      2      5     29.91  
4       1      2      6     42.74  
5       1      3      4     33.03  
6       1      3      5     66.72  
7       1      3      6     65.31  
8       1      4      5     25.48  
9       1      4      6     34.46  
10      1      5      6     39.49  
11      2      3      4     29.63  
12      2      3      5     37.74  
13      2      3      6     32.50  
14      2      4      5     27.08  
15      2      4      6     36.77  
16      2      5      6     19.49  
17      3      4      5     38.99  
18      3      4      6     26.91  
19      3      5      6     39.62  
20      4      5      6     22.25  &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Both the two- and three-way residuals show a good fit of the
constrained model to the data, but checking the fit of the model in the
margins does not correspond to an overall goodness-of-fit test. As a
result, we will fit the unconstrained version of the GRM as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_grm_unconstrained &amp;lt;- grm(Environment) #unconstrained GRM

summary(mod_grm_unconstrained)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment)

Model Summary:
   log.Lik      AIC      BIC
 -1090.404 2216.807 2282.927

Coefficients:
$LeadPetrol
        value
Extrmt1 0.487
Extrmt2 2.584
Dscrmn  1.378

$RiverSea
        value
Extrmt1 1.058
Extrmt2 2.499
Dscrmn  2.341

$RadioWaste
        value
Extrmt1 0.779
Extrmt2 1.793
Dscrmn  3.123

$AirPollution
        value
Extrmt1 0.457
Extrmt2 2.157
Dscrmn  3.283

$Chemicals
        value
Extrmt1 0.809
Extrmt2 1.868
Dscrmn  2.947

$Nuclear
        value
Extrmt1 0.073
Extrmt2 1.427
Dscrmn  1.761


Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 0.003 
quasi-Newton: BFGS &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can use a likelihood ratio test to check if the unconstrained
version GRM is better than its constrained one.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;anova(mod_grm, mod_grm_unconstrained)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 Likelihood Ratio Table
                          AIC     BIC  log.Lik   LRT df p.value
mod_grm               2238.39 2286.14 -1106.19                 
mod_grm_unconstrained 2216.81 2282.93 -1090.40 31.58  5  &amp;lt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The likelihood ratio test indicates that the unconstrained GRM is
preferable for the Environment data. We can plot the Item Characteristic
Curve (ICC) of all 6 items, Item Information Curve (IIC), and Test
Information Curve (TIC) below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mar=c(2,6,2,2), mfrow = c(3, 3))

plot(mod_grm_unconstrained, lwd = 2, cex = 0.6, legend = TRUE, cx = &amp;quot;left&amp;quot;, 
     xlab = &amp;quot;Latent Trait&amp;quot;, cex.main = 0.7, cex.lab = 0.7, cex.axis = 1)

##############################################################

plot(mod_grm_unconstrained, type = &amp;quot;IIC&amp;quot;, lwd = 2, cex = 0.6, legend = TRUE, cx = &amp;quot;topleft&amp;quot;,
     xlab = &amp;quot;Latent Trait&amp;quot;, cex.main = 0.7, cex.lab = 0.7, cex.axis = 1)

plot(mod_grm_unconstrained, type = &amp;quot;IIC&amp;quot;, items = 0, lwd = 2, xlab = &amp;quot;Latent Trait&amp;quot;,cex.main = 0.7, cex.lab = 0.7, cex.axis = 1)

info3 &amp;lt;- information(mod_grm_unconstrained, c(-4, 0))
info4 &amp;lt;- information(mod_grm_unconstrained, c(0, 4))

text(-1.9, 8, labels = paste(&amp;quot;Information in (-4, 0):&amp;quot;,
                             paste(round(100 * info3$PropRange, 1), &amp;quot;%&amp;quot;, sep = &amp;quot;&amp;quot;),
                             &amp;quot;\n\nInformation in (0, 4):&amp;quot;,
                             paste(round(100 * info4$PropRange, 1), &amp;quot;%&amp;quot;, sep = &amp;quot;&amp;quot;)), cex = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1aec7b91e18_files/figure-html/unnamed-chunk-37-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;From the Item Characteristic Curve, we observe that there is low
probability of endorsing the the first option, “very concerned”, for
relatively high latent trait levels, which means that the questions
asked are not considered as major environmental issues by the
respondent.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Test Information Curve also tells us that the test provides
89% of the total information for high latent trait levels.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally, the Item Information Curve indicates that items in
LeadPetrol and Nuclear provide little information in the whole latent
trait continuum. We can check this in detail using
&lt;code&gt;information()&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;information(mod_grm_unconstrained, c(-4, 4))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment)

Total Information = 26.97
Information in (-4, 4) = 26.7 (98.97%)
Based on all the items&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;For item 1 and item 6&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;information(mod_grm_unconstrained, c(-4, 4), items = c(1, 6)) #for item 1 and 6&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment)

Total Information = 5.36
Information in (-4, 4) = 5.17 (96.38%)
Based on items 1, 6&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We observe that item 1 and 6 provide only the 5.36% of the total
information (from the total of 26.97); Thus, they could probably be
excluded from a similar future study. Finally, a useful comparison is to
plot the ICC of each response option separately.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mar=c(2,6,2,2), mfrow = c(2, 2))

plot(mod_grm_unconstrained, category = 1, lwd = 2, cex = 0.7, legend = TRUE, cx = -4.5,
     cy = 0.85, xlab = &amp;quot;Latent Trait&amp;quot;, cex.main = 0.7, cex.lab = 0.7,
     cex.axis = 1)

for (ctg in 2:3) 
  {
  plot(mod_grm_unconstrained, category = ctg, lwd = 2, cex = 0.5, annot = FALSE,
      xlab = &amp;quot;Latent Trait&amp;quot;, cex.main = 0.7, cex.lab = 0.7,
      cex.axis = 1)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file1aec7b91e18_files/figure-html/unnamed-chunk-40-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;From the plot, the response option for RadioWaste and Chemicals have
nearly identical characteristic curves for all categories, indicating
that these two items are probably regarded to have the same effect on
the environment.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;So far, we have discussed what IRT is, including its model for
dichotomous and polytomous test items. There are several models for us
to choose from, and each model has its parameter we can adjust to suit
our needs as well depending on how complex we want our model to be.
There is no right or wrong answer model selection. For example, if we
want the model to be as simple as possible with a dichotomous test (say,
a math test), we could go for Rasch model. If we want our model to be
more realistic, we may want to use 2PL or 3PL, which comes with expenses
such as the need for more sample size or more difficulty to fit the
model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, given how useful IRT is in analyzing test items with
several unique parameters, it doesn’t mean that we need to disregard the
concept of CTT in our practice. Both theories have its own contribution
and usefulness. For example, CTT is more practical to implement in the
classroom setting where there is small amount of students and there is
no need to investigate items at a deeper level. For example, if we want
to use a classroom assessment for formative purposes (i.e., a practice
quiz to help students prepare for the final exam), using CTT might be
sufficient. If we want to develop a large-scale test to measure students
at a national level, maybe IRT might be more appropriate to improve the
test with a more realistic model.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a
href="https://d1wqtxts1xzle7.cloudfront.net/47596077/ITEMS_Module_16-with-cover-page-v2.pdf?Expires=1652597267&amp;amp;Signature=dRiumBzQzxIiFeQvRCtF3cuBjpeaui-mWZXaskGDaQYlwenB7ZZVx0dBUxSnl47nyQnveBh0ZBHdY3IbzNnPHZqAhAOdIN~QIQVCfSQ11d0ZV3mef~kmo8nEzXj459sakTPeZCeTwZSVxklbbtIE52mzRfFItBw4ZF70kJMT9S3FC9YaI~lDDXN5YUbb4VL7ZgCH-lsBIthNbTSDUXxqGuwgeHgxKfotwPdlQRr5iLEEwlLkea2Fr1zow4uLrNg38yK31MdNg55m73x9iwJZA2s~6wlj9IfsWINVJlwPoIOjT5Zm0bIuyIfzGbUKRk~3AiXLjThGUjIi0wlc1Y6flQ__&amp;amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"&gt;Hambleton
and Jones (1993&lt;/a&gt;) did a really great job in comparing the difference
between CTT and IRT in their work. I have presented the table below for
your information.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;The Comparison between CTT and IRT Models (&lt;a
href="https://d1wqtxts1xzle7.cloudfront.net/47596077/ITEMS_Module_16-with-cover-page-v2.pdf?Expires=1652597267&amp;amp;Signature=dRiumBzQzxIiFeQvRCtF3cuBjpeaui-mWZXaskGDaQYlwenB7ZZVx0dBUxSnl47nyQnveBh0ZBHdY3IbzNnPHZqAhAOdIN~QIQVCfSQ11d0ZV3mef~kmo8nEzXj459sakTPeZCeTwZSVxklbbtIE52mzRfFItBw4ZF70kJMT9S3FC9YaI~lDDXN5YUbb4VL7ZgCH-lsBIthNbTSDUXxqGuwgeHgxKfotwPdlQRr5iLEEwlLkea2Fr1zow4uLrNg38yK31MdNg55m73x9iwJZA2s~6wlj9IfsWINVJlwPoIOjT5Zm0bIuyIfzGbUKRk~3AiXLjThGUjIi0wlc1Y6flQ__&amp;amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"&gt;Hambleton
&amp;amp; Jones, 1993&lt;/a&gt;, p.43)&lt;/caption&gt;
&lt;colgroup&gt;
&lt;col width="24%" /&gt;
&lt;col width="34%" /&gt;
&lt;col width="41%" /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th&gt;Area&lt;/th&gt;
&lt;th&gt;Classical Test Theory&lt;/th&gt;
&lt;th&gt;Item Response Theory&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;Model&lt;/td&gt;
&lt;td&gt;Linear&lt;/td&gt;
&lt;td&gt;Non-linear&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;Level&lt;/td&gt;
&lt;td&gt;Test&lt;/td&gt;
&lt;td&gt;Item&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;Assumptions&lt;/td&gt;
&lt;td&gt;Weak (i.e., easy to fit with test data)&lt;/td&gt;
&lt;td&gt;Strong (i.e., more difficult to meet with test data)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;Item-Ability Relationship&lt;/td&gt;
&lt;td&gt;Not Specified&lt;/td&gt;
&lt;td&gt;Item Characteristics Function&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;Examinee Ability&lt;/td&gt;
&lt;td&gt;Represented by test scores or estimated true scores&lt;/td&gt;
&lt;td&gt;Represented by latent ability (Theta/θ)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;Invariance of item and person statistics&lt;/td&gt;
&lt;td&gt;Unavailable / item and person parameters are sample dependent&lt;/td&gt;
&lt;td&gt;Item and person parameters are sample dependent if the model fits
the data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;Item Statistics&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;em&gt;p&lt;/em&gt; = item difficulty&lt;/p&gt;
&lt;p&gt;&lt;em&gt;r&lt;/em&gt; = item discrimination&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;Item discrimination (&lt;em&gt;a&lt;/em&gt;), Item difficulty (&lt;em&gt;b&lt;/em&gt;),
guessing parameter (&lt;em&gt;c&lt;/em&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;Sample Size (for item parameter estimation)&lt;/td&gt;
&lt;td&gt;200-500&lt;/td&gt;
&lt;td&gt;More than 500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Similar to CTT, IRT can be used to develop tests, scales,
surveys, or other measurement tools. The model can analyze item-level
data of both dichotomous (i.e., exams with true/false) and polytomous
(i.e., surveys with no right/wrong answers) tests to provide information
on sensitivity of measurement across a range of latent trait. Knowing
information like item difficulty, item discrimination, and item guessing
is useful when building tests as we can examine which item is a good
item and which item is a not-so-useful one. For example, a test item
that is easy to guess might not be appropriate because everyone can do
it, so it doesn’t really measure anything.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IRT also allows us to put ability levels and difficulties of
items into the same scale to match the trait levels of a target
population. If we want a test to measure someone with “just enough”
knowledge to pass (say, a driver license exam), we can build a test to
measure people with low to medium knowledge. I mean, a taxi driver or a
racing driver know how to drive a car, and that is enough. However, if
we want to develop a test to select the best-of-the-best candidates for
scholarship selection, we might want to build a difficult test to
separate low-to-mid tier students to high performance students. Anyway,
that is all for this post. Thank you so much for reading this! Have a
good day!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>2530d599a3222722a8076a89ee971d4a</distill:md5>
      <category>R</category>
      <category>Psychometric</category>
      <guid>https://taridwong.github.io/posts/2022-05-15-irt</guid>
      <pubDate>Sun, 15 May 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-05-15-irt/IIC.png" medium="image" type="image/png" width="656" height="551"/>
    </item>
    <item>
      <title>Making Sense of Machine Learning with Explanable Artificial Intelligence</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-28-xai</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In my &lt;a
href="https://taridwong.github.io/posts/2022-04-09-ensemble/"&gt;previous
post&lt;/a&gt; on ensemble machine learning models, I mentioned that one major
drawback in the artificial intelligence (AI) field is &lt;a
href="https://www.technologyreview.com/2017/04/11/5113/the-dark-secret-at-the-heart-of-ai/"&gt;the
black box problem&lt;/a&gt;, which hampers interpretability of the results
from complex algorithms such as Random Forest or Extreme Gradient
Boosting. Not knowing how the algorithm works behind the prediction
could reduce applicability of the method itself as the audience can’t
fully comprehend the result and therefore unable to use it to inform
their decisions; this problem could therefore damage trust from the
stakeholders (users, policy makers, general audience) to the field as
well &lt;a href="https://doi.org/10.1175/BAMS-D-18-0195.1"&gt;(McGovern et
al., 2019)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On the developer’s side, fully understanding the machine learning
models through the explanable approach (aka the white-box approach)
allows developers to identify potential problems such as &lt;a
href="https://www.kaggle.com/code/alexisbcook/data-leakage/tutorial"&gt;data
leakage&lt;/a&gt; in the algorithm and fix (or debug) it with relative ease
(&lt;a
href="https://ieeexplore.ieee.org/abstract/document/8882211"&gt;Loyola-Gonzalez,
2019)&lt;/a&gt;. Further, knowing which variable affects the prediction the
most can inform feature engineering to reduce model complexity and
direct future data collection as well by focusing on collecting the
variables that matter &lt;a
href="https://www.kaggle.com/code/dansbecker/use-cases-for-model-insights"&gt;(Becker
&amp;amp; Cook, 2021)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On stakeholder’s side, it is important to emphasize model
explanability especially in industries such as healthcare, finances, and
military to foster trust between the people inside and outside of the
field that could lead to the extent that the result is used to inform
decisions made by humans such as financial credit approval &lt;a
href="https://www.kaggle.com/code/dansbecker/use-cases-for-model-insights"&gt;(Becker
&amp;amp; Cook, 2021&lt;/a&gt;; &lt;a
href="https://ieeexplore.ieee.org/abstract/document/8882211"&gt;Loyola-Gonzalez,
2019)&lt;/a&gt;. Clearly Understanding how, where, and why the model also
benefits the model itself as users are able to identify potential
problems in its performance and provide the develoeprs with their
feedback &lt;a
href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=9401991"&gt;(Velez
et al., 2021)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The above examples knowing how to extract human-understandable
insights from a complex machine learning model is important, especially
in social science data where the theoretical part is as important as the
methodological and the practical part. For that reason, I will be
applying the methods of Explanable Artificial Intelligence (XAI) to
extract interpretable insights from a classification model that predicts
students’ grade repetition. We will begin by setting up the environment
as usual.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

from imblearn.combine import SMOTEENN

from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score

import warnings
warnings.filterwarnings(&amp;quot;ignore&amp;quot;)

RANDOM_STATE = 123&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;I will be using the same data set as my &lt;a
href="https://taridwong.github.io/posts/2022-02-27-statlearning/"&gt;previous
post about statistical learning&lt;/a&gt;, namely the Programme for
International Student Assessment (PISA) 2018 (&lt;a
href="https://www.oecd.org/education/pisa-2018-results-volume-i-5f07c754-en.htm"&gt;OECD,
2019&lt;/a&gt;). However, the set of variables that I am examining will be
different as PISA contains several school-related variables that can be
shifted as the researcher sees fit. For this post, I will predict
students’ class repetition from 25 predictors (or features as called in
the field of machine learning) such as students’ socio-economic status,
history of bullying involvement, and their learning motivation. The data
is collected from Thai student in 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;df = pd.read_csv(&amp;quot;PISA_TH.csv&amp;quot;)

X = df.drop(&amp;#39;REPEAT&amp;#39;, axis=1)
y = df[&amp;#39;REPEAT&amp;#39;]

df.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   REPEAT    ESCS  DAYSKIP  ...  Invest_effort  WEALTH  Home_resource
0       0 -0.7914        1  ...              6  0.0721        -1.4469
1       0  0.8188        1  ...              8 -0.3429         1.1793
2       0  0.4509        1  ...             10  0.3031         1.1793
3       0  0.7086        1  ...             10 -0.5893        -0.1357
4       0  0.8361        1  ...             10  0.5406         1.1793

[5 rows x 25 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="addressing-sample-imbalance"&gt;Addressing Sample Imbalance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The problem is that our targeted variable is imbalance; that is,
the number of students who repeated a class is smaller than the number
of students who did not. This situation makes sense in the real-world
data as normal samples are usually more prevalent than the abnormal
ones, but it is undesirable in the machine learning scenario as the
model could recognize minority samples as unimportant and therefore
disregard them as noises &lt;a
href="https://arxiv.org/pdf/1106.1813.pdf"&gt;(Chawla et al., 2022)&lt;/a&gt;. As
a result, the model could give misleadingly optimistic performance on
classification datasets as it classifies only students who did not
repeat a class.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;See the t-Distributed Stochastic Neighbor Embedding (tSNE) plot
below for the visualization. There isn’t much samples of repeaters in
contrary to non-repeater students. Plus, the pattern is not prominent
enough as the blut dots (repeaters) stay very close to the red dots
(non-repeaters). This could make the pattern difficult to be learned by
the machine due to its ambiguity. One way we can mitigate this problem
is to perform data augmentation via oversampling and undersampling,
which synthesizes more minority samples and deletes or merges majority
samples to improve performance of the machine (&lt;a
href="https://ieeexplore.ieee.org/abstract/document/9034624?casa_token=P33Jkz0x1zEAAAAA:Xtz22PhKDSZ_ktb6X7w-Le7PHkxHwfzzRzvrL3qJcJIDwmyaAMizIr1lUBSK5Lpz1qyk4Ls"&gt;Budhiman
et al., 2019&lt;/a&gt;; &lt;a
href="https://ieeexplore.ieee.org/abstract/document/7797091"&gt;Wong et
al;., 2016&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;Counter(y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Counter({0: 8044, 1: 589})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;tsne = TSNE(n_components=2, random_state=RANDOM_STATE)

TSNE_result = tsne.fit_transform(X)

plt.figure(figsize=(12,8))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1200x800 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;sns.scatterplot(TSNE_result[:,0], TSNE_result[:,1], hue=y, legend=&amp;#39;full&amp;#39;, palette=&amp;quot;hls&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;AxesSubplot:&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To balance the data, I will use both oversampling and
undersampling. Normal oversampling methods duplicates minority samples
for more sample size; however, this approach does not add any more
information to the model (more of the same, basically). Instead, we can
&lt;em&gt;synthesize&lt;/em&gt; minority samples by creating samples that are
&lt;em&gt;similar&lt;/em&gt; to the existing minority samples; this technique is
named as &lt;strong&gt;Synthetic Minority Oversampling TEchnique
(SMOTE)&lt;/strong&gt; &lt;a
href="https://www.wiley.com/en-us/Imbalanced+Learning%3A+Foundations%2C+Algorithms%2C+and+Applications-p-9781118074626"&gt;(He
and Ma, 2013)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Also, we can further enhance the effectiveness of SMOTE by adding
undersampling into the process &lt;a
href="https://arxiv.org/pdf/1106.1813.pdf"&gt;(Chawla et al., 2022)&lt;/a&gt;.
Instead of randomly delete our majority samples, we will use the
&lt;strong&gt;Edited Nearest Neighbor (ENN)&lt;/strong&gt; method, which deletes
data points based on their neighbors to make the difference between
majority and minority samples &lt;a
href="https://www.researchgate.net/profile/Duke-T-J-Ludera/publication/348663430_Credit_Card_Fraud_Detection_by_Combining_Synthetic_Minority_Oversampling_and_Edited_Nearest_Neighbours/links/6009f844a6fdccdcb86fc68c/Credit-Card-Fraud-Detection-by-Combining-Synthetic-Minority-Oversampling-and-Edited-Nearest-Neighbours.pdf"&gt;(Ludera,
2021)&lt;/a&gt;. The combination of these two techniques is called &lt;a
href="https://imbalanced-learn.org/stable/auto_examples/combine/plot_comparison_combine.html#sphx-glr-auto-examples-combine-plot-comparison-combine-py"&gt;&lt;strong&gt;SMOTEENN&lt;/strong&gt;&lt;/a&gt;
See Figure 1 for the example of ENN from &lt;a
href="https://doi.org/10.1016/j.ins.2009.02.011"&gt;Guan et al.,
(2009)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-xaienn.png" style="width:70.0%" alt="" /&gt;
&lt;p class="caption"&gt;ENN Editing with 1-NN Classifier. No copyright
infringement is intended&lt;/p&gt;
&lt;/div&gt;
&lt;pre class="python"&gt;&lt;code&gt;smote_enn = SMOTEENN(random_state=RANDOM_STATE, sampling_strategy = &amp;#39;minority&amp;#39;, n_jobs=-1)

X_resampled, y_resampled = smote_enn.fit_resample(X, y)

Counter(y_resampled)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Counter({1: 8040, 0: 4794})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;tsne = TSNE(n_components=2, random_state=RANDOM_STATE)

TSNE_result = tsne.fit_transform(X_resampled)

plt.figure(figsize=(12,8))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1200x800 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;sns.scatterplot(TSNE_result[:,0], TSNE_result[:,1], hue=y_resampled, legend=&amp;#39;full&amp;#39;, palette=&amp;quot;hls&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;AxesSubplot:&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The second tSNE plot shows a more noticable pattern between student
repeaters and non-repeaters. The number of repeaters is increased while
the number of non-repeaters is decreased. Next, we can put our augmented
data into the Random Forest model for prediction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="random-forest-ensemble"&gt;Random Forest Ensemble&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We will be splitting the data set into a training and a testing set
as usual. For a quick recap, Random Forest is a machine learning model
that consists of several unique and uncorrelated decision trees; hence
the word Random in its name. Those trees work together to improve the
predictive accuracy of that dataset than a single decision tree &lt;a
href="https://mitpress.mit.edu/books/introduction-machine-learning-second-edition"&gt;(Kubat,
2017)&lt;/a&gt;. The model will be evaluated with the repeated stratified
10-folds technique to test our model prediction on different sets of
unseen data to ensure its accuracy, especially in the case of imbalanced
data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;CV = RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=RANDOM_STATE)


X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size = 0.30, 
                                                    random_state = RANDOM_STATE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;# random forest model creation
clf_rfc = RandomForestClassifier(random_state=RANDOM_STATE)
clf_rfc.fit(X_train, y_train)

# predictions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;RandomForestClassifier(random_state=123)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;rfc_predict = clf_rfc.predict(X_test)

rfc_cv_score = cross_val_score(clf_rfc, X_resampled, y_resampled, cv=CV, scoring=&amp;#39;roc_auc&amp;#39;)

print(&amp;quot;=== All AUC Scores ===&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;=== All AUC Scores ===&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(rfc_cv_score)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[0.98907675 0.99357898 0.99228597 0.99276793 0.99409399 0.99378369
 0.9931644  0.9901627  0.98967714 0.99287747 0.99384328 0.99252177
 0.99452348 0.99187137 0.99040419 0.99201929 0.9896265  0.99140778
 0.99115331 0.99457436]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;#39;\n&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;=== Mean AUC Score ===&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;=== Mean AUC Score ===&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;Mean AUC Score - RandForest: &amp;quot;, rfc_cv_score.mean())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean AUC Score - RandForest:  0.9921707177188173&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;#define metrics for normal RF
from sklearn import metrics

y_pred_proba_rf = clf_rfc.predict_proba(X_test)[::,1]
fpr_rf, tpr_rf, _ = metrics.roc_curve(y_test,  y_pred_proba_rf)

auc_rf = metrics.roc_auc_score(y_test, y_pred_proba_rf)
plt.plot(fpr_rf,tpr_rf, label=&amp;quot;AUC for Random Forest Classifier = &amp;quot;+str(auc_rf.round(3)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;lt;matplotlib.lines.Line2D object at 0x0000018B3AA0EBB0&amp;gt;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.legend(loc=&amp;quot;lower right&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.legend.Legend object at 0x0000018B3AA0E850&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;True Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;True Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.xlabel(&amp;#39;False Positive Rate&amp;#39;)
           &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;False Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Receiver-Operator Curve (ROC)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Receiver-Operator Curve (ROC)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file1aec426d66a9_files/figure-html/unnamed-chunk-10-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The results will be evaluated with the Receiver Operator
Characteristic (ROC) curve, which shows the diagnostic ability of binary
classifiers. One approach to use ROC is to evaluate its Area Under Curve
(AUC), which measures of the ability of a classifier to distinguish
between classes and is used as a summary of the ROC curve. The higher
the AUC, the better the performance of the model at distinguishing
between the positive and negative classes. The mean of 20 rounds of
testing (randomly splitting the data into 10 stratified parts, repeated
it for 2 times) looks good is around 0.99, meaning that there is a 99%
chance that the model is able to correctly predict which student is a
reapeater and which is not based on the data used to train the machine.
Now we know that the model works well with our data, let us move on to
interpreting it with XAI techniques.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="explaining-ai"&gt;Explaining AI&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;XAI is a set of methods that allows a machine learning model and its
results understandable to human in terms of how it works in terms of
prediction, including the impace of variables to the prediction results
&lt;a
href="https://link.springer.com/book/10.1007/978-3-030-68640-6"&gt;(Gianfagna
&amp;amp; Di Cecco, 2021)&lt;/a&gt;. The XAI methods that we will extract insights
are permutation importance, partial dependence plot, and Shapley
Additive explanations (SHAP) values.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="permutation-importance"&gt;Permutation Importance&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;One of the most basic questions we might ask of a model is: What
features have the biggest impact on predictions? This quention could be
answered through the examination of &lt;strong&gt;feature importance&lt;/strong&gt;.
There are multiple ways to measure feature importance. One way is to
extract the feature importance plot from the model itself as
demonstrated below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Create a pd.Series of features importances
importances_rf = pd.Series(clf_rfc.feature_importances_, index = X_resampled.columns)

# Sort importances_rf
sorted_importance_rf = importances_rf.sort_values()

#Horizontal bar plot
sorted_importance_rf.plot(kind=&amp;#39;barh&amp;#39;, color=&amp;#39;lightgreen&amp;#39;); 
plt.xlabel(&amp;#39;Feature Importance Score&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;Feature Importance Score&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;Features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;Features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Visualizing Important Features&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Visualizing Important Features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file1aec426d66a9_files/figure-html/unnamed-chunk-11-3.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Another way, which we will focus on in this post, is to use the
permutation importance score from the area of XAI. Permutation
importance is calculated by asking the following question: “If I
randomly shuffle a single column of the validation data, leaving the
target and all other columns in place, how would that affect the
accuracy of predictions in that now-shuffled data?”. Randomly
re-ordering a single column should cause less accurate predictions,
since the resulting data no longer corresponds to anything observed in
the real world. Model accuracy especially suffers if we shuffle a column
that the model relied on heavily for predictions. In our case, if we
mess with the “BEINGBULLIED” variable, the model would be severely
affected by the reduced prediction accuracy. The same would happen to
the variable “Parent_emosup”, “Positive_feel” and so forth as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;import eli5
from eli5.sklearn import PermutationImportance

FEATURES = X_test.columns.tolist()

perm = PermutationImportance(clf_rfc, random_state=RANDOM_STATE).fit(X_test, y_test)
eli5.show_weights(perm, feature_names = FEATURES, top = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;IPython.core.display.HTML object&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-xaiper-imp.png" style="width:40.0%" alt="" /&gt;
&lt;p class="caption"&gt;Permutation Importance&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The permutation importance results are consistent with the
feature importance score we extracted from the model. The values towards
the top are the most important features, and those towards the bottom
matter least. The first number in each row shows how much model
performance decreased with a random shuffling (in this case, using
“accuracy” as the performance metric). Like most things in data science,
there is some randomness to the exact performance change from a
shuffling a column. We measure the amount of randomness in our
permutation importance calculation by repeating the process with
multiple shuffles. The number after the ± measures how performance
varied from one-reshuffling to the next.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In our example, the most important feature was “BEINGBULLIED”,
which is the index of exposure to bullying. The index was constructed
from questions that ask if students have experienced bullying in the
past 12 months from statements such as “Other students left me out of
things on purpose”; “Other students made fun of me”; “I was threatened
by other students”. Positive values on this scale indicate that the
student was more exposed to bullying at school than the average student
in OECD countries; negative values on this scale indicate that the
student was less exposed to bullying at school than the average student
across OECD countries. This result is consistent with the literature
that students’ grade repetition is associated with the likelihood of
being bullied (&lt;a
href="https://journals.plos.org/Plosmedicine/article?id=10.1371/journal.pmed.1003846"&gt;Lian
et al., 2021&lt;/a&gt;; &lt;a
href="https://www.tandfonline.com/doi/full/10.1080/21683603.2019.1699215?casa_token=OB1MKY8CNuMAAAAA%3A5gwLS94ZgeACsQtZhKmiDLGtJUCu_qUMTtNyy_ftGl14WekcRoUErjezdZcOvI1s6bJEg_HYFIo"&gt;Ozada
Nazim &amp;amp; Duyan, 2019&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="partial-dependence-plots"&gt;Partial Dependence Plots&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;While feature importance shows what variables most affect
predictions, partial dependence plots show how a feature affects
predictions. For our case, partial dependence plots can be used to
answer questions such as “Controlling for all variables, what impact
does the index of exposure to bullying have on the prediction of grade
repetition?”. The interpretation of partial dependence plot is somewhat
similar to the interpretation of linear or logistic regression. On this
plot, The y axis is interpreted as change in the prediction from what it
would be predicted at the baseline or leftmost value. A blue shaded area
indicates level of confidence.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The plot below indicates that being subjected to bullying (as
reflected by having positive value of the variable) increases the
likelihood of students to repeat a grade. Positive values in this index
indicate that the student is more exposed to bullying at school than the
average student in OECD countries. Negative values in this index
indicate that the student is less exposed to bullying at school than the
average student in OECD countries; therefore, having zero does not mean
students did not experience any form of bullying, but rather
experiencing bullying to some degree (i.e., being bullied a bit).
However, the predicting power does not change much after 0, meaning that
the amount of exposure to bullying does not matter in predicting
students’ grade repetition.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;from pdpbox import pdp

pdp_bullied = pdp.pdp_isolate(model=clf_rfc, dataset=X_test, model_features=FEATURES, feature=&amp;#39;BEINGBULLIED&amp;#39;)

pdp.pdp_plot(pdp_bullied, &amp;#39;BEINGBULLIED&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(&amp;lt;Figure size 1500x950 with 2 Axes&amp;gt;, {&amp;#39;title_ax&amp;#39;: &amp;lt;AxesSubplot:&amp;gt;, &amp;#39;pdp_ax&amp;#39;: &amp;lt;AxesSubplot:xlabel=&amp;#39;BEINGBULLIED&amp;#39;&amp;gt;})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file1aec426d66a9_files/figure-html/unnamed-chunk-13-5.png" width="1440" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Partial Dependence Plots can also be used to examine interactions
between variables as well. The graph below shows predictions for any
combination of students’ exposure to bullying and the amount of
emotional support from parents. The prediction power is highest when
students score 0 in the index of exposure to bullying (i.e., being
bullied a bit) and having scores on the index of parents’ emotional
support between -1.7 to +0.5. Positive values on this scale mean that
students perceived greater levels of emotional support from their
parents than did the average student across OECD countries while
negative value means otherwise. Having higher exposure to bullying
reduces prediction power of the model as indicated by the changing color
from yellow to green, and when the score in the index of emotional
support reaches 1, the score of the exposure to bullying index becomes
less matter as the prediction power reduces.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;features_to_plot = [&amp;#39;BEINGBULLIED&amp;#39;, &amp;#39;Parent_emosup&amp;#39;]

inter1  =  pdp.pdp_interact(model=clf_rfc, dataset=X_test, model_features=FEATURES, features=features_to_plot)

pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type=&amp;#39;contour&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(&amp;lt;Figure size 750x950 with 3 Axes&amp;gt;, {&amp;#39;title_ax&amp;#39;: &amp;lt;AxesSubplot:&amp;gt;, &amp;#39;pdp_inter_ax&amp;#39;: &amp;lt;AxesSubplot:xlabel=&amp;#39;BEINGBULLIED&amp;#39;, ylabel=&amp;#39;Parent_emosup&amp;#39;&amp;gt;})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file1aec426d66a9_files/figure-html/unnamed-chunk-14-7.png" width="720" data-distill-preview=1 /&gt;&lt;/p&gt;
&lt;h3 id="shap-values"&gt;SHAP Values&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Finally, SHAP value allows us to interpret the prediction at a
fine-grained level to the components of individual predictions to show
the impact of each feature. For our case, CHAP value can be used to
answer questions like “On what basis did the model predict that student
A is likely to repeat a grade?”. The plot is quite straightforward to
interpret. The red part shows what increases the likelihood of repeating
a grade, and the blue part shows what decreases the likelihood of
repeating a grade.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On the plot below, the prediction is at the base alue of 0.60,
meaning that it is the average of the model output. For this particular
student, their likelihood to repeat a grade is increased by being
exposed to bullying (BEINGBULLIED), having mediocre emotional support
from parents (Parent_emosup), and having poor overall social standing as
indicated by -0.9 the variable the index of socio-economic, social and
cultural status (ESCS). However, the likelihood is decreased by their
educational resources at home (Home_resource), having cooperative class
(Stu_coop), their parents’ occupational status (Parent_occupation), and
having low record of class skipping (CLASSKIP).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-xaishap_10th_java.png" style="width:85.0%" alt="" /&gt;
&lt;p class="caption"&gt;SHAP Value for a prediction&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id="shap-summary-plot"&gt;SHAP Summary Plot&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In addition to the breakdown of each individual prediction, we
can also visualize groups of SHAP values with SHAP summary plot and SHAP
dependence contribution plot. SHAP summary plots give us an overview of
feature importance and what is driving the prediction. This plot is made
of many dots. Each dot has three characteristics as follows: a)
horizontal location (the x-axis) that indicates whether the effect of
that value caused a higher or lower prediction; b) vertical location
(the y-axis) that indicates the variable name, in order of importance
from top to bottom. c) Gradient color indicates the original value for
that variable. In booleans (i.e., yes/no variable), it will take two
colors, but in number it can contain the whole spectrum.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For example, the left most point in the ‘Parent_emosup’ row is
red in color, meaning that for that particular student, having greater
levels of emotional support from their parents reduces their likelihood
of repeating a grade by roughly 0.3. Seeing variables have a wide spread
in range can be inferred that permutation importance is high; however,
it is best to use permutation importance to measure which variable is
important to the prediction.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Some features such as &lt;code&gt;Home_resource&lt;/code&gt; (educational
resource at home) have reasonably clear separation between the blue and
pink dots, which implies a straightforward meaning that the increase in
the variable value lower (i.e., more resource) the likelihood of
repeating a grade while the decrease in educational resource impacts the
variable in the other direction (higher chance to repeat a
grade).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, some variables such as &lt;code&gt;Stu_coop&lt;/code&gt; (the degree
of cooperativeness within classrooms) have blue and pink dots jumbled
together, suggesting that the increase in this variable leads to higher
predictions, and other times it leads to a lower prediction. In other
words, both high and low values of the variable can have both positive
and negative effects on the prediction. The most likely explanation for
this “jumbling” of effects is that the variable (in this case
&lt;code&gt;Stu_coop&lt;/code&gt;) has an interaction effect with other variables.
For example, there may be some situations where cooperating with other
students lead to &lt;a
href="https://www.simplypsychology.org/social-loafing.html"&gt;social
loafing&lt;/a&gt; - when stduents contribute less effort when working as a
group, and therefore learns less. This interaction needs further
investigation.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;shap_values_summary = explainer.shap_values(X_test)
shap.summary_plot(shap_values[1], X_test)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-xaishap_summary.png" style="width:45.0%" alt="" /&gt;
&lt;p class="caption"&gt;SHAP Summary Plot&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id="shap-dependence-contribution-plot"&gt;SHAP Dependence Contribution
Plot&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The earlier Partial Dependence Plots to show how a single feature
impacts predictions. This is insightful and relevant for many real-world
use cases. The interpretation is also friendly to non-technical audience
as well. However, there is a lot that we still don’t know; for example,
what is the distribution of effects? Is the effect of having a certain
value pretty constant, or does it vary a lot depending on the values of
other feaures. SHAP dependence contribution plots provide a similar
insight to the partial dependence plot, but they add a lot more detail.
The plot shows scatter dots that explain how the effect a single feature
has on the predictions made by the model. The plot can be read as
follows: a) The x-axis is the value of the feature; b) The y-axis is the
SHAP value for that feature, which represents how much knowing that
feature’s value changes the output of the model prediction; c) The color
corresponds to a second feature that may have an interaction effect with
the feature we are plotting.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The plot below shows the relatively flat trend of the
&lt;code&gt;BEINGBULLIED&lt;/code&gt; feature, meaning that this variable does
impact the prediction regardless of the value; this trend is consistent
with the partial dependence plot shown earlier in the post. However,
there is a sign of interaction as there are points with similar value
that produce different outcome. See the left of the 2D pane, for
example. For some students, being less exposed to bullying gives them
more chance to repeat a grade while some students got less chance. There
might be other features that interact with this variable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;While the primary trend is that being bullied increases the
chance of repeating a grade , there are some variations that can be
explained by the interaction of features as well. For a concrete
explanation, see the right of the 2D pane. Being positioned overthere
means that those students experience a lot of bullying, but their chance
of repeating a grade is relatively lower than those who experience less
bullying. One explanation is that some of those students have positive
feelings for themselves (indicated by the red color), which could make
them more resilient toward being bullied.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;shap.dependence_plot(&amp;#39;BEINGBULLIED&amp;#39;, shap_values_summary[1], X_test, interaction_index=&amp;quot;Positive_feel&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-xaishap_dependence_bulliedXpositivefeel.png" style="width:60.0%"
alt="" /&gt;
&lt;p class="caption"&gt;SHAP Dependence Contribution Plot&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;What we have done so far is making a prediction with a Random Forest
Ensemble model, which has high predictive power at the price of being
challenging to explain due to its complexity (&lt;a
href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2822360/"&gt;Zhang &amp;amp;
Wang, 2009&lt;/a&gt;). XAI tools such as permutation importance, partial
dependence plot, and SHAP values, allow us to understand outputs of the
model at various levels from the overall picture to fine-grained
individual cases. Knowing how predictions are made also allow
establishes venues for future studies as well. XAI results are important
to bridge the knowledge gap between technical (e.g., developers) and
non-technical (e.g., customers, users) audiences, which could build
trust and confidence when putting the AI models into the actual use. XAI
also helps an organization develop a responsible approach to AI
development by avoiding the reliance on results that we do not
understand to inform our decisions.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;However, note that XAI is not perfect. Its results are
context-dependent, meaning that if the context changes, so does the
result (&lt;a
href="https://www.sciencedirect.com/science/article/pii/S0740624X21001027"&gt;de
Brujin et al., 2021&lt;/a&gt;). The prediction and how it happens can only be
used as a factor to be considered along with other lines of evidence
such as expert opinion, counter explanations, and potential
consequences. Regardless, XAI is still a useful too to have in expanding
the knowledge we get from machine learning. Thank you very much for
reading!&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>f40f1e89b04d8b250adbd706645111a3</distill:md5>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-28-xai</guid>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-28-xai/xai_files/figure-html5/unnamed-chunk-14-11.png" medium="image" type="image/png" width="1440" height="1824"/>
    </item>
    <item>
      <title>Addressing Data Imbalance with Semi-Supervised Learning</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-28-semisupervised</link>
      <description>For this post, I will use semi-supervised learning approach to perform a classification task with a highly imbalance data.  

(7 min read)</description>
      <category>Python</category>
      <category>Unsupervised Machine Learning</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-28-semisupervised</guid>
      <pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-28-semisupervised/semi-ml.png" medium="image" type="image/png" width="900" height="450"/>
    </item>
    <item>
      <title>Examining Customer Cluster with Unsupervised Machine Learning</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-18-unsupervisedml</link>
      <description>In this post, I will be using two unsupervised learning techniques with a data set, namely K-means clustering and Hierarchical clustering, to determine groups of customers from their age, income, and spending behavior data.  

(8 min read)</description>
      <category>Python</category>
      <category>Unsupervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-18-unsupervisedml</guid>
      <pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-18-unsupervisedml/unsupervisedml_files/figure-html5/unnamed-chunk-14-19.png" medium="image" type="image/png" width="2880" height="1152"/>
    </item>
    <item>
      <title>Combining Multiple Machine Learning Models with the Ensemble Methods</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-09-ensemble</link>
      <description>This entry explores different ways to combine supervised machine learning models to maximize their predictive capability.  

(13 min read)</description>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-09-ensemble</guid>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-09-ensemble/robot.png" medium="image" type="image/png" width="626" height="528"/>
    </item>
    <item>
      <title>Examining PISA 2018 Data Set with Statistical Learning Approach</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-02-27-statlearning</link>
      <description>In this post, I will be developing two statistical learning models, namely Linear Regression and Polynomial Regression, and apply it to Thai student data from the Programme for International Student Assessment (PISA) 2018 data set to examine the impact of classroom competition and cooperation to students' academic performance.  

(14 min read)</description>
      <category>R</category>
      <category>Statistics</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-02-27-statlearning</guid>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-02-27-statlearning/statlearn.jpeg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Classical Test Theory in R</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-01-15-ctt</link>
      <description>For this post, I will be analyzing characteristics of test items based on the framework of Classical Test Theory (CTT).

(13 min read)</description>
      <category>R</category>
      <category>Psychometric</category>
      <guid>https://taridwong.github.io/posts/2022-01-15-ctt</guid>
      <pubDate>Sat, 15 Jan 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-01-15-ctt/ctt_files/figure-html5/unnamed-chunk-28-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Examining the Big 5 personality Dataset with factor analysis</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-01-01-efacfa</link>
      <description>For this entry, I will be examining the Big 5 personality Inventory data set with Exploratory Data Analysis to identify potential structures of personality trait and verify them with Confirmatory Factor Analysis.

(8 min read)</description>
      <category>R</category>
      <category>Statistics</category>
      <guid>https://taridwong.github.io/posts/2022-01-01-efacfa</guid>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-01-01-efacfa/corrmatrix.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Measuring Text Similarity with Movie plot data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-29-movie-similarity</link>
      <description>For this post, I will be analyzing textual data of movie plots to determine their similarity with TF-IDF and Clustering.

(7 min read)</description>
      <category>Python</category>
      <category>Natural Language Processing</category>
      <category>Unsupervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-29-movie-similarity</guid>
      <pubDate>Wed, 29 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-29-movie-similarity/movie-similarity_files/figure-html5/unnamed-chunk-11-1.png" medium="image" type="image/png" width="484" height="483"/>
    </item>
    <item>
      <title>Missing Data Analysis</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-27-missingdata</link>
      <description>For this post, I will examine missing data in a large-scale dataset and discuss about numerous ways we can clean them as a part of data preparation.

(10 min read)</description>
      <category>R</category>
      <category>Data Visualization</category>
      <category>Statistics</category>
      <guid>https://taridwong.github.io/posts/2021-12-27-missingdata</guid>
      <pubDate>Mon, 27 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-27-missingdata/missingpic.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Applying Machine Learning to Audio Data: Visualization, Classification, and Recommendation</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data</link>
      <description>For this entry, I am trying my hands on audio data to extract its features for exploratory data analysis (EDA), using machine learning algorithms to perform music classification, and finally build up on that result to develop a recommendation system for music of similar characteristics.

(13 min read)</description>
      <category>Python</category>
      <category>Data Visualization</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data</guid>
      <pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data/applying-machine-learning-to-audio-data_files/figure-html5/unnamed-chunk-13-11.png" medium="image" type="image/png" width="3072" height="1152"/>
    </item>
    <item>
      <title>Interactive plots for Suicide Data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data</link>
      <description>For this entry, will be visualizing suicide data from 1958 to 2015 with interactive plots to communicate insights to non-technical audience.  

(14 min read)</description>
      <category>R</category>
      <category>Data Visualization</category>
      <category>Quantitative research</category>
      <guid>https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data</guid>
      <pubDate>Thu, 09 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data/preview.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Image Recognition with Artificial Neural Networks</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks</link>
      <description>In this entry, we will be developing a deep learning algorithm - a sub-field of machine learning inspired by the structure of human brain (neural networks) - to classify images of single digit number (0-9).  

(9 min read)</description>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <category>Deep Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks</guid>
      <pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks/deepnn.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Anomaly Detection with New York City taxi data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data</link>
      <description>In this entry, I will be conducting anomaly detection to identify points of anomaly in the taxi passengers data in New York City from July 2014 to January 2015 at half-hourly intervals.
 
 (4 min read)</description>
      <category>Unsupervised Machine Learning</category>
      <category>R</category>
      <guid>https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data</guid>
      <pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data/anomaly-detection-with-new-york-city-taxi-data_files/figure-html5/unnamed-chunk-9-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Crime mapping in San Francisco with police data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data</link>
      <description>In this entry, we will explore San Francisco crime data from 2016 to 2018 to understand the relationship between civilian-reported incidents of crime and police-reported incidents of crime. Along the way we will use table intersection methods to subset our data, aggregation methods to calculate important statistics, and simple visualizations to understand crime trends.  

(7 min read)</description>
      <category>Data Visualization</category>
      <category>R</category>
      <category>Quantitative research</category>
      <guid>https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data</guid>
      <pubDate>Tue, 30 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data/crime-mapping-in-san-francisco-with-police-data_files/figure-html5/unnamed-chunk-6-1.png" medium="image" type="image/png" width="1920" height="1920"/>
    </item>
    <item>
      <title>Exploring COVID-19 data from twitter with topic modeling</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds</link>
      <description>


&lt;h2 id="covid-19-situation-in-alberta-canada"&gt;COVID-19 situation in
Alberta, Canada&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The province of Alberta, Canada, has suffered from the COVID-19
pandemic like all other places. Alberta has gone through cycles of
reopening and returning to provincial lock down since April 2020. The
province, however, has lifted almost all restrictions and enacted its
reopening plan on the recent &lt;a
href="https://calgary.ctvnews.ca/alberta-moves-to-stage-3-of-reopening-plan-on-canada-day-1.5475913"&gt;Canada
day&lt;/a&gt; when 70% of Alberta population has received at least one dose of
&lt;a
href="https://www.canada.ca/en/public-health/services/diseases/coronavirus-disease-covid-19/vaccines.html"&gt;approved
COVID-19 vaccination&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The navigation of the province through this pandemic was led by
the Alberta’s Chief Medical Officer of Health, &lt;a
href="https://www.alberta.ca/office-of-the-chief-medical-officer-of-health.aspx"&gt;Dr. Deena
Hinshaw&lt;/a&gt;. Dr.Hinshaw usually held public health briefings almost
every day during wave 1 to wave 3 of the pandemic, but her communication
channel has changed in wave 4 as less public health briefing was held
and more tweets were posted on the &lt;a
href="https://twitter.com/CMOH_Alberta"&gt;her account&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For that, I believe we could use &lt;a
href="https://en.wikipedia.org/wiki/Natural_language_processing"&gt;Natural
Language Processing (NLP)&lt;/a&gt; techniques to extract themes and
characteristics from Dr.Hinshaw’s tweet to examine the essence of public
health messages since the provincial reopening date, specifically from
July 1st to October 31st, 2021.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="text-mining-and-word-cloud-fundamentals"&gt;Text mining and word
cloud fundamentals&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For this post, we will use text mining and word clouds to
initially explore characteristics of the data set. Text mining is an
exploratory method for textual data under Natural Language Processing
(NLP), a branch of Artificial Intelligence concerning the understanding
of words and spoken texts. NLP is also a type of unsupervised machine
learning approach to discover hidden structures in the data to inform
decisions made by experts of the subject matter.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Word cloud is also a popular way to to communicate findings from
textual data in a visually engaging way. The more frequent a word appear
in the data set (or corpus) the bigger that word will be in the
cloud.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will use Python to perform this analysis on R platform with
&lt;code&gt;reticulate::repl_python()&lt;/code&gt;. First of all, we will be
importing necessary modules and twitter data set that we mined from
Dr. Hinshaw’s account with &lt;code&gt;pd.read_csv&lt;/code&gt;. There are 538
tweets in total, and we can print out examples of the tweets via
&lt;code&gt;tweets_df.Text.head(5)&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;#Import necessary modules

import numpy as np #for numpy array
import pandas as pd #for data reading and processing
import matplotlib.pyplot as plt #for plotting
import re #for Regex text cleaning
from wordcloud import WordCloud, STOPWORDS #for word clouds
from nltk.stem import WordNetLemmatizer #to reduce text to base form
from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation as LDA #for topic modeling
import warnings

warnings.filterwarnings(&amp;quot;ignore&amp;quot;) #suppress the warning that Python kindly gave me

tweets_df = pd.read_csv(&amp;quot;text-query-tweets.csv&amp;quot;)

tweets_df.shape

# Print out the first rows of papers&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(538, 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(tweets_df.Text.head(5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    We all have the ability to take small actions ...
1    As we head into Halloween weekend, I encourage...
2    Sadly, 9 new deaths related to COVID-19 were a...
3    Over the past 24 hours, we ID’d 603 new cases ...
4    Here is a summary of the latest #COVID19AB num...
Name: Text, dtype: object&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="lets-clean-the-text-first"&gt;Let’s clean the text first&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;After we imported our data into the system, we have to clean our
data to get rid of textual elements that we do not need such as
punctuation, numbers, as well as convert all words to lower case.
Painful as it may be, this has to be done. It took me days (not that
much, but I felt it that way) to clean all of this and make sure that no
junk is left behind (well, there could be. Do let me know if you find
any).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The phrase “Garbage in, garbage out” is really applicable here in
data work context. If you let any junk (corrupted data) in, the most you
will get is processed junk. After we cleaned the text, let us print them
out again to see what they look like. All numbers are gone. All texts
are in lowercase. All URLs and punctuation is gone. Good
riddance!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ATTN nerds: Note that in the code below, we will pass the
original &lt;code&gt;Text&lt;/code&gt; column in &lt;code&gt;tweets_df&lt;/code&gt; to the
&lt;code&gt;re.sub&lt;/code&gt; function only once. For the second cleaning function
onward, we will pass &lt;code&gt;tweets_df['Text_processed']&lt;/code&gt; instead to
stack our text cleaning results on the same column. Yes, I wrote this to
remind myself because I struggled on it for hours (half an hour,
actually).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;
#remove all numbers from the text with list comprehension
tweets_df[&amp;#39;Text_processed&amp;#39;] = tweets_df[&amp;#39;Text&amp;#39;].map(lambda x: re.sub(r&amp;#39;[0-9]+&amp;#39;, &amp;#39;&amp;#39;, x))

# Remove punctuation
tweets_df[&amp;#39;Text_processed&amp;#39;] = tweets_df[&amp;#39;Text_processed&amp;#39;].map(lambda x: re.sub(r&amp;#39;[^\w\s\,\.!?]&amp;#39;, &amp;#39;&amp;#39;, x))

# Convert the tweets to lowercase
tweets_df[&amp;#39;Text_processed&amp;#39;] = tweets_df[&amp;#39;Text_processed&amp;#39;].map(lambda x: x.lower())

#Clean out URLs
tweets_df[&amp;#39;Text_processed&amp;#39;] = tweets_df[&amp;#39;Text_processed&amp;#39;].map(lambda x: re.sub(r&amp;quot;http\S+&amp;quot;, &amp;quot;&amp;quot;, x))

# Print the processed titles of the first rows 
print(tweets_df[&amp;#39;Text_processed&amp;#39;].head())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    we all have the ability to take small actions ...
1    as we head into halloween weekend, i encourage...
2    sadly,  new deaths related to covid were also ...
3    over the past  hours, we idd  new cases amp co...
4    here is a summary of the latest covidab number...
Name: Text_processed, dtype: object&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="so-this-is-whats-happening-over-time"&gt;So this is what’s
happening over time&lt;/h2&gt;
&lt;pre class="python"&gt;&lt;code&gt;
#Change datetime format to datetime
tweets_df[&amp;#39;Datetime&amp;#39;] = pd.to_datetime(tweets_df[&amp;#39;Datetime&amp;#39;])

#Extract month from datetime
tweets_df[&amp;#39;Month&amp;#39;] = tweets_df[&amp;#39;Datetime&amp;#39;].dt.month

# Group the papers by year
groups = tweets_df.groupby(&amp;#39;Month&amp;#39;)

# Determine the size of each group
counts = groups.size()

# Visualize the counts as a bar plot

# Vertical lines
plt.axvline(x = 7.0, color = &amp;#39;forestgreen&amp;#39;, label = &amp;#39;The reopening date&amp;#39;, linestyle=&amp;#39;--&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.lines.Line2D object at 0x0000018B853DE040&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.axvline(x = 8.0, color = &amp;#39;firebrick&amp;#39;, label = &amp;#39;Wave 4 started&amp;#39;, linestyle=&amp;#39;--&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.lines.Line2D object at 0x0000018B853DE5E0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.legend(bbox_to_anchor = (1.0, 1), loc = &amp;#39;upper right&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.legend.Legend object at 0x0000018B853D1FD0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Tweet count across months&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Tweet count across months&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;quot;Tweet count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;Tweet count&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.xlabel(&amp;quot;Month&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;Month&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;counts.plot()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;AxesSubplot:title={&amp;#39;center&amp;#39;:&amp;#39;Tweet count across months&amp;#39;}, xlabel=&amp;#39;Month&amp;#39;, ylabel=&amp;#39;Tweet count&amp;#39;&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file1aec12bd489_files/figure-html/unnamed-chunk-5-1.png" width="720" style="display: block; margin: auto;" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The line plot above represents tweet counts across months after the
provincial reopening date. The x-axis indicates months and the y-axis
indicates the number of twitter post of Dr. Hinshaw. The number of tweet
dropped slightly from July to August as cases decreased, but wave 4 of
the pandemic started in August as cases were on the rise again. We can
see that the number of cases aligns with the number of tweets posted on
Dr. Hinshaw’s account.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="lets-see-the-big-picture-with-word-cloud"&gt;Let’s see the big
picture with word cloud&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Now that we know the frequency of tweets over months, we can plot a
word cloud from our processed text to see the big picture of twitter
data. There are 114,362 words in total after combining all 538 tweets
together. The word cloud below suggests that “covid” was mentioned the
most during the past four months, following by “vaccine”, “new cases”,
and “unvaccinated”.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;
text_all = &amp;quot; &amp;quot;.join(tweet for tweet in tweets_df.Text_processed)
print (&amp;quot;There are {} words in the combination of all tweets&amp;quot;.format(len(text_all)))

#lemmatize all words&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;There are 114362 words in the combination of all tweets&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;lemmatizer = WordNetLemmatizer()
text_all = &amp;quot;&amp;quot;.join([lemmatizer.lemmatize(i) for i in text_all])

# Create Stopword list:
stopwords_cloud = set(STOPWORDS)
stopwords_cloud.update([&amp;quot;https://&amp;quot;, &amp;quot;(/)&amp;quot;, &amp;quot;Online:&amp;quot;, 
                        &amp;quot;Twitter:&amp;quot;, &amp;quot;Join&amp;quot;, &amp;quot;us&amp;quot;, &amp;quot;virtually&amp;quot;,
                        &amp;quot;pm&amp;quot;, &amp;quot;:&amp;quot;, &amp;quot;https&amp;quot;, &amp;quot;t&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;co&amp;quot;, &amp;quot;amp&amp;quot;, &amp;quot;will&amp;quot;])
                      
#Generate a word cloud image
wordcloud_tweet = WordCloud(stopwords=stopwords_cloud, background_color=&amp;quot;white&amp;quot;,random_state=7).generate(text_all)

#Display the generated image:
#the matplotlib way:
plt.figure(figsize=[10,10])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1000x1000 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.imshow(wordcloud_tweet, interpolation=&amp;#39;bilinear&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage object at 0x0000018B85405430&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.axis(&amp;quot;off&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(-0.5, 399.5, 199.5, -0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file1aec12bd489_files/figure-html/unnamed-chunk-6-3.png" width="960" style="display: block; margin: auto;" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The thing is, word cloud can only provide a rough visual
presentation for the characteristics of our textual data. We would need
to dive a little bit deeper to graphs and numbers to examine what is
truly going on. Let us visualize them all on a bar plot.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2
id="common-word-bar-plot-and-text-preprocessing-for-topic-modeling"&gt;Common
word bar plot and text preprocessing for topic modeling&lt;/h2&gt;
&lt;pre class="python"&gt;&lt;code&gt;
# Helper function to count common words

def plot_10_most_common_words(count_data, tfidf_vectorizer):
    import matplotlib.pyplot as plt
    words = tfidf_vectorizer.get_feature_names()
    total_counts = np.zeros(len(words))
    for t in count_data:
        total_counts+=t.toarray()[0]
    
    count_dict = (zip(words, total_counts))
    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]
    words = [w[0] for w in count_dict]
    counts = [w[1] for w in count_dict]
    x_pos = np.arange(len(words)) 

    plt.bar(x_pos, counts,align=&amp;#39;center&amp;#39;)
    plt.xticks(x_pos, words, rotation=90) 
    plt.xlabel(&amp;#39;words&amp;#39;)
    plt.ylabel(&amp;#39;counts&amp;#39;)
    plt.title(&amp;#39;10 most common words&amp;#39;)
    plt.show()

#Make your own list of stop words
my_additional_stop_words = (&amp;quot;https://&amp;quot;, &amp;quot;(/)&amp;quot;, &amp;quot;➡Online:&amp;quot;, 
                        &amp;quot;➡Twitter:&amp;quot;, &amp;quot;Join&amp;quot;, &amp;quot;us&amp;quot;, &amp;quot;virtually&amp;quot;,
                        &amp;quot;pm&amp;quot;, &amp;quot;:&amp;quot;, &amp;quot;https&amp;quot;, &amp;quot;t&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;co&amp;quot;, &amp;quot;amp&amp;quot;, &amp;quot;today&amp;quot;, &amp;quot;new&amp;quot;, &amp;quot;covid&amp;quot;,
                        &amp;quot;covidab&amp;quot;, &amp;quot;hours&amp;quot;, &amp;quot;completed&amp;quot;)
                        
stop_words_lda = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)    

# Initialize the count vectorizer with the English stop words
tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words_lda)

# Fit and transform the processed titles
count_data = tfidf_vectorizer.fit_transform(tweets_df[&amp;#39;Text_processed&amp;#39;])

# Visualise the 10 most common words
plot_10_most_common_words(count_data, tfidf_vectorizer)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file1aec12bd489_files/figure-html/unnamed-chunk-7-5.png" width="960" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The bar plot above gave us a more detailed information of which
word occurs more frequently than the others based on the term
frequency–inverse document frequency (TFIDF) statistics. TFIDF gives
each word a weight that reflects its importance to a document. For
TFIDF, words that occur too frequent like “the” provides little meaning
while words rarely occur doesn’t tell us much as well. We are taking
about the COVID-19 pandemic here, so it is obvious that “vaccinated” is
going to be mentioned the most in Dr. Hinshaw’s tweet. “Cases” and
“unvaccinated” seem to be reasonable to be mentioned as the second- and
third most important words as the government of Alberta has been putting
more effort in identifying more cases in the province and encourage
unvaccinated individuals to get their vaccine.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will also create a &lt;code&gt;tfidf_vectorizer&lt;/code&gt; model with
our own list of stopwords (or words that have little meaning such as
“is, am, are”) to prepare our data for Latent Dirichlet Allocation (LDA)
topic modeling.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2
id="finally-lets-see-potential-topics-from-dr.-hinshaws-tweet"&gt;Finally,
let’s see potential topics from Dr. Hinshaw’s tweet&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Latent Dirichlet Allocation is a powerful natural language
processing technique that discovers hidden patterns in topic from
unstructured textual data with statistical models &lt;a
href="https://link.springer.com/content/pdf/10.1007/s11042-018-6894-4.pdf"&gt;(Jelodar
et al., 2019)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Here, we can use LDA to discover potential topics among the sea
of tweets posted by Dr. Hinshaw to find out what she talked about since
the provincial reopening and wave 4 of the pandemic. I have specified
the model to extract 8 topics from the data, with 5 words per topics.
Note that these numbers are arbitrary chosen.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If we extracted too few topics, we might not be able to capture
the whole picture of the data. On the other hand, extracting too much
topics could just give us more of the same overlapping themes. We need
to find the middle ground.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;
# Helper function to print out the topics
def print_topics(model, tfidf_vectorizer, n_top_words):
    words = tfidf_vectorizer.get_feature_names()
    for topic_idx, topic in enumerate(model.components_):
        print(&amp;quot;\nTopic #%d:&amp;quot; % topic_idx)
        print(&amp;quot; &amp;quot;.join([words[i]
                        for i in topic.argsort()[:-n_top_words - 1:-1]]))
                        
#How many topic and words per topic we want to see
number_topics = 8
number_words = 5 
                      
# Create and fit the LDA model
lda = LDA(n_components=number_topics, random_state = 1)
lda.fit(count_data)

# Print the topics found by the LDA model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;LatentDirichletAllocation(n_components=8, random_state=1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print_topics(lda, tfidf_vectorizer, number_words)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Topic #0:
twitter online join video transcript

Topic #1:
possible information protection protect soon

Topic #2:
vaccines protect vaccine dose book

Topic #3:
cases tests partially unvaccinated idd

Topic #4:
reported deaths sadly condolences alberta

Topic #5:
oct age steps important dr

Topic #6:
matter pandemic report continue health

Topic #7:
ahs participating prevent book available&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="wrapping-up-here.-what-can-we-conclude"&gt;Wrapping up here. What
can we conclude?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The topics we discovered above can be inferred as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Topic 0: An invitation for the general population to join a live
update video on Twitter.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Topic 1: The availability of possible information on COVID-19
protection&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Topic 2: Encouragement to book for a vaccination for more
protection.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Topic 3: The proportion of unvaxxed vs vaxxed vs partiallyvaxxed
patients.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Topic 4: Covid-related death.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The insights that we gained could also be further supported by
opinion from public health experts as they could provide information at
a greater depth into their field.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;From what we have discussed so far, we can see that with the
right tool, LDA for our case, we could take advantage of the vast
availability of textual data that revolves around us in our everyday
lives and use that information to deepen our understanding of social
phenomena. We could explore how students opinion changed from pre- to
post-COVID era, or we could use this technique to media transcription of
social events such as political protests, election speech, or even
product review in the marketing field. Thank you for your
reading!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>150e32d23689517567eba0ddc47378ec</distill:md5>
      <category>Python</category>
      <category>Natural Language Processing</category>
      <category>Unsupervised Machine Learning</category>
      <category>COVID-19</category>
      <guid>https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds</guid>
      <pubDate>Tue, 23 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds/word-cloud-pv.png" medium="image" type="image/png" width="1920" height="1920"/>
    </item>
    <item>
      <title>Finding a home among the paradigm push-back with Dialectical Pluralism</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-07-introducing-dialectical-pluralism</link>
      <description>This entry discusses the reconciliation of quantitative and qualitative worldviews amidst the paradigm wars with pluralistic stance.

(2 min read)</description>
      <category>Mixed methods research</category>
      <guid>https://taridwong.github.io/posts/2021-11-07-introducing-dialectical-pluralism</guid>
      <pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://www.relevantinsights.com/wp-content/uploads/2020/05/Qualitative-Quantitative-Research-For-New-Product-Development.png" medium="image" type="image/png"/>
    </item>
  </channel>
</rss>
