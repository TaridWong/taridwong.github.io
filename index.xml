<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>Tarid Wongvorachan</title>
    <link>https://taridwong.github.io/</link>
    <atom:link href="https://taridwong.github.io/index.xml" rel="self" type="application/rss+xml"/>
    <description>Welcome to my data science blog!
</description>
    <generator>Distill</generator>
    <lastBuildDate>Sat, 06 Aug 2022 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Leveraging a Large-Scale Educational Data Set with Educational Data Mining</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-08-06-edm</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Hi Everyone. It’s been awhile since my last blog post. I have
been occupied with writing and research meeting, among other things. I
have had the opportunity to work with several large-scale data sets from
start to finish (i.e., planning research ideas, data cleaning,
interpreting patterns, and translating insights for the audience. That
is why I want to post some of my ideas to this blog to share with you
with it is like to work with data from one end to another. In this post,
I will be predicting students’ high school dropout rate through the
usage of a large-scale educational data set.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;My work is largely in the field of educational data mining (EDM),
which is the method of knowledge discovery from educational databases
(&lt;a
href="https://www.wiley.com/en-gb/Data+Mining+and+Learning+Analytics%3A+Applications+in+Educational+Research-p-9781118998236"&gt;Elatia
et al., 2016&lt;/a&gt;). Such data is usually extracted from sources such as
students’ interactive learning environment, computerized testing, and
large-scale assessment data repository (&lt;a
href="https://educationaldatamining.org/"&gt;International Educational Data
Mining Society, 2022&lt;/a&gt;). The data set I use in this posting is the
High School Longitudinal Study of 2009, which is a longitudinal data set
that tracks the transition of American youth from secondary schooling to
subsequent education and work roles.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The original data set has 4014 variables and 23,503 cases that
were collected from students’ base year (2009), first follow-up (2012),
2013 update collection (2013), high school transcripts (2013–2014), and
second follow-up (2016). First, I chose a handful of variables based on
theories that are relevant to the prediction of students’ school
dropout.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;After the initial screening, we have 67 variables left. Then, I
further removed responses that were not answered by students or their
parents to preserve data representation. I use &lt;a
href="https://boxuancui.github.io/DataExplorer/"&gt;&lt;code&gt;dataexplorer&lt;/code&gt;&lt;/a&gt;
package to examine types and missingness of the variables. Figure 1
below shows that the data set largely consists of categorical variables
then continuous variables. The data also has a bit of missing
data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-08-06-edmMissing.png" style="width:50.0%" alt="" /&gt;
&lt;p class="caption"&gt;Figure 1. Variable Type and Missing Data&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="data-preprocessing"&gt;Data Preprocessing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To further clean up the data, variables with more than 30%
missingness were removed, and the rest missing data was imputed with
Random Forest algorithm based on the &lt;a
href="https://cran.r-project.org/web/packages/mice/mice.pdf"&gt;multivariate
imputation by chained equation&lt;/a&gt; method. I have done everything in
advance to save time. Here is the cleaned data. I will also load the
following packages for data preprocessing.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tidyverse)
library(readxl)
library(Hmisc)
library(corrplot)

hsls_30_rf &amp;lt;-read_csv(&amp;quot;hsls_30percent_imputed_rf.csv&amp;quot;, col_names = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;At this point, the data set has 51 variables and 16137 cases. I
converted some variables into factors to reflect their nature with
&lt;code&gt;as.factor&lt;/code&gt; function. I also mapped correlation matrix of the
data set to examine variables that are not related to one another.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;hsls_30_rf &amp;lt;- hsls_30_rf %&amp;gt;% 
  as.data.frame() %&amp;gt;%
  mutate(across(c(X1SEX, X1RACE, X1MOMRESP, 
                  X1MOMEDU, X1MOMRACE, X1DADRESP, 
                  X1DADEDU, X1DADRACE, X1HHNUMBER, 
                  X1STUEDEXPCT, X1PAREDEXPCT, X1TMRACE, 
                  X1TMCERT, 
                  X1LOCALE, X1REGION, S1NOHWDN, 
                  S1NOPAPER, S1NOBOOKS, S1LATE, 
                  S1PAYOFF, S1GETINTOCLG, S1AFFORD, 
                  S1WORKING, S1FRNDGRADES, S1FRNDSCHOOL, 
                  S1FRNDCLASS, S1FRNDCLG, S1HRMHOMEWK, 
                  S1HRSHOMEWK, S1SUREHSGRAD, P1BEHAVE, 
                  P1ATTEND, P1PERFORM, P1HWOFTEN, 
                  X4EVERDROP, X4PSENRSTLV), as.factor))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;correlation_30_rf &amp;lt;-rcorr(as.matrix(hsls_30_rf))

corrplot(correlation_30_rf$r, type = &amp;quot;upper&amp;quot;, order = &amp;quot;hclust&amp;quot;, 
         p.mat = correlation_30_rf$P, insig = &amp;quot;pch&amp;quot;, pch = 4, pch.cex = 1,
         tl.col = &amp;quot;black&amp;quot;, tl.cex = 0.5, tl.srt = 90)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2022-08-06-edmcor-unclean.png" style="width:75.0%" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Based on the above correlation matrix and theoretical relevance,
variables that are recommended for removal are: &lt;code&gt;X1RACE&lt;/code&gt;,
&lt;code&gt;X1MOMRACE&lt;/code&gt;, &lt;code&gt;X1DADRACE&lt;/code&gt;, &lt;code&gt;X1LOCALE&lt;/code&gt;,
&lt;code&gt;P1HWOFTEN&lt;/code&gt;, &lt;code&gt;X1HHNUMBER&lt;/code&gt;, &lt;code&gt;X1TMCERT&lt;/code&gt;,
&lt;code&gt;X1REGION&lt;/code&gt;, &lt;code&gt;X1MOMRESP&lt;/code&gt;, &lt;code&gt;X1DADRESP&lt;/code&gt;,
&lt;code&gt;X1SEX&lt;/code&gt;, &lt;code&gt;X1TMRACE&lt;/code&gt;, &lt;code&gt;X1TSRACE&lt;/code&gt;,
&lt;code&gt;X1MTHUTI.&lt;/code&gt; They are removed because 1) they are not
theoretically related to the prediction of high school dropout and 2)
they have insignificant correlation that might negatively impact the
prediction result.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;hsls_30_rf_final &amp;lt;- hsls_30_rf %&amp;gt;% select(!c(X1RACE, X1MOMRACE, X1DADRACE, X1LOCALE, P1HWOFTEN, X1HHNUMBER, X1TMCERT, X1REGION, X1MOMRESP, X1DADRESP, X1SEX, X1TMRACE, X1MTHUTI))&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;After uncorrelated variables were removed, we have 38 variables
left. The new correlation matrix is as follows:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;correlation_30_rf_final &amp;lt;-rcorr(as.matrix(hsls_30_rf_final))

corrplot(correlation_30_rf_final$r, type = &amp;quot;upper&amp;quot;, order = &amp;quot;hclust&amp;quot;, 
         p.mat = correlation_30_rf_final$P, insig = &amp;quot;pch&amp;quot;, pch = 4, pch.cex = 1,
         tl.col = &amp;quot;black&amp;quot;, tl.cex = 0.5, tl.srt = 90)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2022-08-06-edmcor-clean.png" style="width:75.0%" /&gt;&lt;/p&gt;
&lt;h2 id="data-augmentation"&gt;Data Augmentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;After the initial data preprocessing in R, I will use Python to
perform machine learning. I personally use R for data
exploration/statistical analysis and Python for machine learning. First,
I will initiate Python environment and import necessary modules.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

from sklearn.manifold import TSNE

import warnings
warnings.filterwarnings(&amp;quot;ignore&amp;quot;)

RANDOM_STATE = 123&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Then, I will transfer the data set to Python environment because the
two languages run in parallel instead of on the same ground.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;df = r.hsls_30_rf_final

df[&amp;#39;X4EVERDROP&amp;#39;] = np.where(df[&amp;#39;X4EVERDROP&amp;#39;] == &amp;quot;0&amp;quot;, 0, 1)

df.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  X1MOMEDU X1DADEDU   X1SES  ...  P1PERFORM  X4EVERDROP  X4PSENRSTLV
0        5        5  1.5644  ...          1           0            1
1        3        2 -0.3699  ...          1           0            0
2        7        0  1.2741  ...          1           0            1
3        4        0  0.1495  ...          1           1            2
4        3        3  1.0639  ...          1           0            1

[5 rows x 38 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;From the data set, I will extract the predictors (X) and the
targeted variable (y). I will also check class proportion of the
targeted variable to see if they are balanced.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;X_extreme = df.drop(&amp;#39;X4EVERDROP&amp;#39;, axis=1)
y_extreme = df[&amp;#39;X4EVERDROP&amp;#39;]

print(&amp;quot;The proportion of target variable&amp;#39;s class :&amp;quot;, Counter(y_extreme))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The proportion of target variable&amp;#39;s class : Counter({0: 14133, 1: 2004})&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can see that the data is imbalanced. We have 14133 cases of
student who did not and 2004 student who dropped out of their high
school. Class imbalance problem in educational data sets could hamper
the accuracy of predictive models as many of them are designed on the
assumption that the predicted class is balanced (&lt;a
href="https://www.wiley.com/en-us/Imbalanced+Learning%3A+Foundations%2C+Algorithms%2C+and+Applications-p-9781118074626"&gt;He
&amp;amp; Ma, 2013&lt;/a&gt;). This problem is especially prevalent in the
prediction of high-stakes educational issues such as such as school
dropout or grade repetition, where discrepancy between two classes is
high due to its rare occurrence (&lt;a
href="https://www.mdpi.com/2227-7102/9/4/275"&gt;Barros et al.,
2019&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;I will visualize the imbalance with a t-Distributed Stochastic
Neighbor Embedding (tSNE) plot and a count plot.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;tsne = TSNE(n_components=2, random_state=RANDOM_STATE)

TSNE_result = tsne.fit_transform(X_extreme)

plt.figure(figsize=(12,8))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1200x800 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;sns.scatterplot(TSNE_result[:,0], TSNE_result[:,1], hue=y_extreme, legend=&amp;#39;full&amp;#39;, palette=&amp;quot;hls&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;AxesSubplot:&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file366c53d3573_files/figure-html/unnamed-chunk-10-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;
sns.set_theme(style=&amp;quot;darkgrid&amp;quot;)
sns.countplot(x=&amp;quot;X4EVERDROP&amp;quot;, data = df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;AxesSubplot:xlabel=&amp;#39;X4EVERDROP&amp;#39;, ylabel=&amp;#39;count&amp;#39;&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file366c53d3573_files/figure-html/unnamed-chunk-11-3.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In &lt;a href="https://taridwong.github.io/posts/2022-04-28-xai/"&gt;my
previous post&lt;/a&gt;, I used the combination of Synthetic Minority
Oversampling TEchnique (SMOTE) and Edited Nearest Neighbor (ENN). The
thing is, SMOTE+ENN only works with numerical variables. We have a lot
of categorical variables in this data set, so we need to find a
workaround for that. I will use SMOTE for nominal and continuous
variable (SMOTE-NC) and random undersampling instead instead.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;from imblearn.over_sampling import SMOTENC
from imblearn.under_sampling import RandomUnderSampler 
from sklearn.model_selection import train_test_split

smote_nc = SMOTENC(random_state=RANDOM_STATE, sampling_strategy=0.8,
                    categorical_features=[0, 1, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36])

rus_hybrid = RandomUnderSampler(random_state=RANDOM_STATE, sampling_strategy=&amp;#39;not minority&amp;#39;)

X_smote_extreme, y_smote_extreme = smote_nc.fit_resample(X_extreme, y_extreme)

X_hybrid_extreme, y_hybrid_extreme = rus_hybrid.fit_resample(X_smote_extreme, y_smote_extreme)

print(&amp;quot;For Y extreme :&amp;quot;, Counter(y_extreme))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;For Y extreme : Counter({0: 14133, 1: 2004})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;For Y smote extreme :&amp;quot;, Counter(y_smote_extreme))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;For Y smote extreme : Counter({0: 14133, 1: 11306})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;For Y hybrid extreme :&amp;quot;, Counter(y_hybrid_extreme))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;For Y hybrid extreme : Counter({0: 11306, 1: 11306})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;X_train_hybrid_ext, X_test_hybrid_ext, y_train_hybrid_ext, y_test_hybrid_ext = train_test_split(X_hybrid_extreme, y_hybrid_extreme, test_size = 0.30, random_state = RANDOM_STATE)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;After we finished augmenting the data, below is the result. We have
much more instances of student who dropped out of their high school as
seen from the tSNE plot and the count plot below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;TSNE_result = tsne.fit_transform(X_hybrid_extreme)

plt.figure(figsize=(12,8))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1200x800 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;sns.scatterplot(TSNE_result[:,0], TSNE_result[:,1], hue=y_hybrid_extreme, legend=&amp;#39;full&amp;#39;, palette=&amp;quot;hls&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;AxesSubplot:&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file366c53d3573_files/figure-html/unnamed-chunk-13-5.png" width="1152" /&gt;&lt;/p&gt;
&lt;pre class="python"&gt;&lt;code&gt;sns.set_theme(style=&amp;quot;darkgrid&amp;quot;)
sns.countplot(y_hybrid_extreme)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;AxesSubplot:xlabel=&amp;#39;X4EVERDROP&amp;#39;, ylabel=&amp;#39;count&amp;#39;&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file366c53d3573_files/figure-html/unnamed-chunk-14-7.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Next, we will proceed to the classification stage. For many
classification algorithms such as XGBoost or Random Forest, you need to
transform categorical variables into numerical variables with label
encoding or one-hot encoding first. However, we have a lot of
categorical variables that may hamper the process. To circumvent this,
we will use &lt;a href="https://catboost.ai/"&gt;CatBoost&lt;/a&gt;, which is a
gradient boosting decision tree that supports categorical variables
without the need for data transformation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="classification"&gt;Classification&lt;/h2&gt;
&lt;pre class="python"&gt;&lt;code&gt;from sklearn.feature_selection import RFECV
from catboost import CatBoostClassifier
from sklearn.model_selection import RandomizedSearchCV&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;First, we will create a CatBoost object as well as a list of
hyperparameters to tune. We can use the default mode of CatBoost, but
tuning the algorithm makes the algorithm perform better. I will tune
tree depth, learning rate, and the number of iteration that the machine
learns. I use randomized grid search to tune the algorithm to save
time.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;CBC = CatBoostClassifier(random_state=RANDOM_STATE)

parameters = {&amp;#39;depth&amp;#39;         : [4,5,6,7,8,9,10],
              &amp;#39;learning_rate&amp;#39; : [0.01,0.02,0.03,0.04,0.05],
              &amp;#39;iterations&amp;#39;    : [10,20,30,40,50,60,70,80,90,100]
             }

cat_features = [0, 1, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36]

Cat_random = RandomizedSearchCV(estimator = CBC, 
                                param_distributions = parameters, 
                                n_iter = 10, cv = 3, verbose=0, 
                                random_state = RANDOM_STATE, error_score=&amp;#39;raise&amp;#39;)

Cat_random.fit(X_train_hybrid_ext, y_train_hybrid_ext, cat_features = cat_features)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0:  learn: 0.6798844    total: 43.6ms   remaining: 3.88s
1:  learn: 0.6658710    total: 86.2ms   remaining: 3.79s
2:  learn: 0.6523493    total: 147ms    remaining: 4.25s
3:  learn: 0.6413995    total: 188ms    remaining: 4.04s
4:  learn: 0.6278567    total: 235ms    remaining: 3.99s
5:  learn: 0.6186284    total: 286ms    remaining: 4.01s
6:  learn: 0.6089585    total: 333ms    remaining: 3.95s
7:  learn: 0.5985513    total: 384ms    remaining: 3.94s
8:  learn: 0.5884809    total: 463ms    remaining: 4.17s
9:  learn: 0.5785280    total: 514ms    remaining: 4.11s
10: learn: 0.5702242    total: 563ms    remaining: 4.05s
11: learn: 0.5597852    total: 610ms    remaining: 3.97s
12: learn: 0.5527543    total: 668ms    remaining: 3.96s
13: learn: 0.5439965    total: 723ms    remaining: 3.92s
14: learn: 0.5354271    total: 799ms    remaining: 3.99s
15: learn: 0.5295986    total: 846ms    remaining: 3.91s
16: learn: 0.5235632    total: 907ms    remaining: 3.9s
17: learn: 0.5180815    total: 960ms    remaining: 3.84s
18: learn: 0.5128125    total: 1.01s    remaining: 3.79s
19: learn: 0.5060868    total: 1.06s    remaining: 3.72s
20: learn: 0.5001726    total: 1.12s    remaining: 3.67s
21: learn: 0.4957502    total: 1.17s    remaining: 3.62s
22: learn: 0.4914979    total: 1.22s    remaining: 3.56s
23: learn: 0.4883335    total: 1.27s    remaining: 3.5s
24: learn: 0.4843515    total: 1.33s    remaining: 3.45s
25: learn: 0.4806735    total: 1.38s    remaining: 3.39s
26: learn: 0.4774183    total: 1.46s    remaining: 3.4s
27: learn: 0.4752628    total: 1.5s remaining: 3.33s
28: learn: 0.4723224    total: 1.57s    remaining: 3.31s
29: learn: 0.4696826    total: 1.62s    remaining: 3.24s
30: learn: 0.4639752    total: 1.67s    remaining: 3.18s
31: learn: 0.4616352    total: 1.72s    remaining: 3.12s
32: learn: 0.4574664    total: 1.78s    remaining: 3.07s
33: learn: 0.4550142    total: 1.83s    remaining: 3.01s
34: learn: 0.4517165    total: 1.88s    remaining: 2.96s
35: learn: 0.4482490    total: 1.93s    remaining: 2.9s
36: learn: 0.4452413    total: 1.99s    remaining: 2.85s
37: learn: 0.4427028    total: 2.04s    remaining: 2.79s
38: learn: 0.4398182    total: 2.14s    remaining: 2.8s
39: learn: 0.4386967    total: 2.17s    remaining: 2.71s
40: learn: 0.4366265    total: 2.23s    remaining: 2.67s
41: learn: 0.4340851    total: 2.28s    remaining: 2.61s
42: learn: 0.4319768    total: 2.34s    remaining: 2.56s
43: learn: 0.4300976    total: 2.39s    remaining: 2.5s
44: learn: 0.4285674    total: 2.45s    remaining: 2.45s
45: learn: 0.4265530    total: 2.52s    remaining: 2.4s
46: learn: 0.4245068    total: 2.57s    remaining: 2.35s
47: learn: 0.4223266    total: 2.62s    remaining: 2.29s
48: learn: 0.4206147    total: 2.68s    remaining: 2.24s
49: learn: 0.4184022    total: 2.74s    remaining: 2.19s
50: learn: 0.4170437    total: 2.8s remaining: 2.14s
51: learn: 0.4149012    total: 2.87s    remaining: 2.09s
52: learn: 0.4137503    total: 2.92s    remaining: 2.04s
53: learn: 0.4117136    total: 2.97s    remaining: 1.98s
54: learn: 0.4089305    total: 3.03s    remaining: 1.93s
55: learn: 0.4066496    total: 3.08s    remaining: 1.87s
56: learn: 0.4057898    total: 3.15s    remaining: 1.82s
57: learn: 0.4046035    total: 3.2s remaining: 1.77s
58: learn: 0.4035047    total: 3.26s    remaining: 1.71s
59: learn: 0.4021765    total: 3.31s    remaining: 1.66s
60: learn: 0.3990097    total: 3.4s remaining: 1.62s
61: learn: 0.3980288    total: 3.46s    remaining: 1.56s
62: learn: 0.3964750    total: 3.51s    remaining: 1.5s
63: learn: 0.3954893    total: 3.56s    remaining: 1.45s
64: learn: 0.3943143    total: 3.63s    remaining: 1.4s
65: learn: 0.3918841    total: 3.68s    remaining: 1.34s
66: learn: 0.3899150    total: 3.74s    remaining: 1.28s
67: learn: 0.3890711    total: 3.79s    remaining: 1.23s
68: learn: 0.3881179    total: 3.85s    remaining: 1.17s
69: learn: 0.3868025    total: 3.91s    remaining: 1.12s
70: learn: 0.3850559    total: 3.99s    remaining: 1.07s
71: learn: 0.3841090    total: 4.04s    remaining: 1.01s
72: learn: 0.3832880    total: 4.11s    remaining: 957ms
73: learn: 0.3819060    total: 4.16s    remaining: 901ms
74: learn: 0.3811645    total: 4.22s    remaining: 843ms
75: learn: 0.3792427    total: 4.27s    remaining: 786ms
76: learn: 0.3789477    total: 4.3s remaining: 726ms
77: learn: 0.3771891    total: 4.35s    remaining: 669ms
78: learn: 0.3749524    total: 4.4s remaining: 613ms
79: learn: 0.3734599    total: 4.47s    remaining: 558ms
80: learn: 0.3727318    total: 4.52s    remaining: 503ms
81: learn: 0.3711936    total: 4.58s    remaining: 447ms
82: learn: 0.3704078    total: 4.64s    remaining: 392ms
83: learn: 0.3697014    total: 4.7s remaining: 336ms
84: learn: 0.3688939    total: 4.76s    remaining: 280ms
85: learn: 0.3681721    total: 4.82s    remaining: 224ms
86: learn: 0.3675191    total: 4.87s    remaining: 168ms
87: learn: 0.3667829    total: 4.96s    remaining: 113ms
88: learn: 0.3661064    total: 5.01s    remaining: 56.3ms
89: learn: 0.3654935    total: 5.07s    remaining: 0us
0:  learn: 0.6789438    total: 47.8ms   remaining: 4.25s
1:  learn: 0.6622984    total: 100ms    remaining: 4.41s
2:  learn: 0.6528820    total: 162ms    remaining: 4.71s
3:  learn: 0.6378865    total: 217ms    remaining: 4.67s
4:  learn: 0.6248902    total: 271ms    remaining: 4.6s
5:  learn: 0.6143424    total: 327ms    remaining: 4.58s
6:  learn: 0.6051266    total: 385ms    remaining: 4.56s
7:  learn: 0.5963677    total: 435ms    remaining: 4.46s
8:  learn: 0.5878444    total: 491ms    remaining: 4.41s
9:  learn: 0.5791189    total: 543ms    remaining: 4.34s
10: learn: 0.5699589    total: 601ms    remaining: 4.31s
11: learn: 0.5631775    total: 658ms    remaining: 4.27s
12: learn: 0.5549372    total: 710ms    remaining: 4.21s
13: learn: 0.5489560    total: 763ms    remaining: 4.14s
14: learn: 0.5398350    total: 826ms    remaining: 4.13s
15: learn: 0.5335363    total: 885ms    remaining: 4.09s
16: learn: 0.5286711    total: 943ms    remaining: 4.05s
17: learn: 0.5248677    total: 998ms    remaining: 3.99s
18: learn: 0.5208806    total: 1.07s    remaining: 4s
19: learn: 0.5137198    total: 1.13s    remaining: 3.96s
20: learn: 0.5089934    total: 1.19s    remaining: 3.93s
21: learn: 0.5041599    total: 1.25s    remaining: 3.87s
22: learn: 0.4994555    total: 1.3s remaining: 3.8s
23: learn: 0.4933377    total: 1.36s    remaining: 3.74s
24: learn: 0.4888624    total: 1.42s    remaining: 3.7s
25: learn: 0.4848647    total: 1.49s    remaining: 3.66s
26: learn: 0.4806195    total: 1.56s    remaining: 3.65s
27: learn: 0.4782823    total: 1.62s    remaining: 3.58s
28: learn: 0.4737880    total: 1.68s    remaining: 3.53s
29: learn: 0.4716028    total: 1.74s    remaining: 3.47s
30: learn: 0.4665243    total: 1.8s remaining: 3.43s
31: learn: 0.4640058    total: 1.86s    remaining: 3.37s
32: learn: 0.4596530    total: 1.91s    remaining: 3.3s
33: learn: 0.4570517    total: 1.96s    remaining: 3.23s
34: learn: 0.4538448    total: 2.03s    remaining: 3.19s
35: learn: 0.4518482    total: 2.07s    remaining: 3.1s
36: learn: 0.4473907    total: 2.12s    remaining: 3.04s
37: learn: 0.4444594    total: 2.19s    remaining: 3s
38: learn: 0.4429416    total: 2.25s    remaining: 2.94s
39: learn: 0.4405574    total: 2.3s remaining: 2.88s
40: learn: 0.4384978    total: 2.36s    remaining: 2.82s
41: learn: 0.4357888    total: 2.41s    remaining: 2.75s
42: learn: 0.4333328    total: 2.47s    remaining: 2.7s
43: learn: 0.4311137    total: 2.52s    remaining: 2.64s
44: learn: 0.4287708    total: 2.58s    remaining: 2.58s
45: learn: 0.4263805    total: 2.64s    remaining: 2.52s
46: learn: 0.4243778    total: 2.72s    remaining: 2.49s
47: learn: 0.4226912    total: 2.77s    remaining: 2.42s
48: learn: 0.4201662    total: 2.83s    remaining: 2.37s
49: learn: 0.4183090    total: 2.88s    remaining: 2.31s
50: learn: 0.4161610    total: 2.94s    remaining: 2.25s
51: learn: 0.4148893    total: 3s   remaining: 2.19s
52: learn: 0.4128819    total: 3.07s    remaining: 2.14s
53: learn: 0.4114953    total: 3.13s    remaining: 2.09s
54: learn: 0.4101021    total: 3.2s remaining: 2.04s
55: learn: 0.4088186    total: 3.25s    remaining: 1.97s
56: learn: 0.4075637    total: 3.33s    remaining: 1.93s
57: learn: 0.4053970    total: 3.39s    remaining: 1.87s
58: learn: 0.4030154    total: 3.45s    remaining: 1.81s
59: learn: 0.4017849    total: 3.5s remaining: 1.75s
60: learn: 0.3997640    total: 3.59s    remaining: 1.7s
61: learn: 0.3975432    total: 3.64s    remaining: 1.65s
62: learn: 0.3954974    total: 3.71s    remaining: 1.59s
63: learn: 0.3942386    total: 3.75s    remaining: 1.52s
64: learn: 0.3935385    total: 3.81s    remaining: 1.47s
65: learn: 0.3914313    total: 3.86s    remaining: 1.41s
66: learn: 0.3905388    total: 3.92s    remaining: 1.34s
67: learn: 0.3896347    total: 3.97s    remaining: 1.28s
68: learn: 0.3882451    total: 4.02s    remaining: 1.22s
69: learn: 0.3871845    total: 4.09s    remaining: 1.17s
70: learn: 0.3863762    total: 4.15s    remaining: 1.11s
71: learn: 0.3847478    total: 4.2s remaining: 1.05s
72: learn: 0.3837932    total: 4.26s    remaining: 993ms
73: learn: 0.3830264    total: 4.32s    remaining: 934ms
74: learn: 0.3813505    total: 4.36s    remaining: 871ms
75: learn: 0.3805847    total: 4.41s    remaining: 813ms
76: learn: 0.3779104    total: 4.47s    remaining: 755ms
77: learn: 0.3762739    total: 4.52s    remaining: 696ms
78: learn: 0.3740235    total: 4.58s    remaining: 638ms
79: learn: 0.3727875    total: 4.63s    remaining: 579ms
80: learn: 0.3719730    total: 4.7s remaining: 522ms
81: learn: 0.3711517    total: 4.75s    remaining: 464ms
82: learn: 0.3700075    total: 4.8s remaining: 405ms
83: learn: 0.3683987    total: 4.87s    remaining: 348ms
84: learn: 0.3678371    total: 5.05s    remaining: 297ms
85: learn: 0.3671215    total: 5.13s    remaining: 238ms
86: learn: 0.3653367    total: 5.19s    remaining: 179ms
87: learn: 0.3636935    total: 5.29s    remaining: 120ms
88: learn: 0.3624679    total: 5.34s    remaining: 60ms
89: learn: 0.3618877    total: 5.43s    remaining: 0us
0:  learn: 0.6741178    total: 52.8ms   remaining: 4.7s
1:  learn: 0.6620692    total: 105ms    remaining: 4.6s
2:  learn: 0.6501160    total: 156ms    remaining: 4.52s
3:  learn: 0.6358792    total: 206ms    remaining: 4.43s
4:  learn: 0.6235486    total: 257ms    remaining: 4.37s
5:  learn: 0.6116855    total: 310ms    remaining: 4.34s
6:  learn: 0.6019227    total: 394ms    remaining: 4.67s
7:  learn: 0.5939390    total: 446ms    remaining: 4.57s
8:  learn: 0.5860754    total: 510ms    remaining: 4.59s
9:  learn: 0.5749311    total: 568ms    remaining: 4.54s
10: learn: 0.5670579    total: 622ms    remaining: 4.47s
11: learn: 0.5601169    total: 674ms    remaining: 4.38s
12: learn: 0.5547385    total: 756ms    remaining: 4.47s
13: learn: 0.5484639    total: 810ms    remaining: 4.4s
14: learn: 0.5433616    total: 868ms    remaining: 4.34s
15: learn: 0.5375490    total: 917ms    remaining: 4.24s
16: learn: 0.5315263    total: 972ms    remaining: 4.17s
17: learn: 0.5271360    total: 1.03s    remaining: 4.11s
18: learn: 0.5221979    total: 1.09s    remaining: 4.07s
19: learn: 0.5140389    total: 1.14s    remaining: 3.99s
20: learn: 0.5081484    total: 1.22s    remaining: 4s
21: learn: 0.5036802    total: 1.27s    remaining: 3.93s
22: learn: 0.4989852    total: 1.33s    remaining: 3.86s
23: learn: 0.4962544    total: 1.38s    remaining: 3.78s
24: learn: 0.4924226    total: 1.43s    remaining: 3.73s
25: learn: 0.4884310    total: 1.49s    remaining: 3.67s
26: learn: 0.4856478    total: 1.56s    remaining: 3.65s
27: learn: 0.4822786    total: 1.62s    remaining: 3.59s
28: learn: 0.4766774    total: 1.69s    remaining: 3.54s
29: learn: 0.4737081    total: 1.74s    remaining: 3.48s
30: learn: 0.4712776    total: 1.81s    remaining: 3.44s
31: learn: 0.4675332    total: 1.88s    remaining: 3.4s
32: learn: 0.4643978    total: 1.95s    remaining: 3.36s
33: learn: 0.4613327    total: 2s   remaining: 3.29s
34: learn: 0.4571739    total: 2.06s    remaining: 3.23s
35: learn: 0.4542738    total: 2.11s    remaining: 3.16s
36: learn: 0.4517763    total: 2.17s    remaining: 3.1s
37: learn: 0.4492796    total: 2.23s    remaining: 3.04s
38: learn: 0.4465959    total: 2.29s    remaining: 3s
39: learn: 0.4429619    total: 2.35s    remaining: 2.94s
40: learn: 0.4405566    total: 2.4s remaining: 2.87s
41: learn: 0.4380638    total: 2.46s    remaining: 2.82s
42: learn: 0.4337473    total: 2.53s    remaining: 2.77s
43: learn: 0.4301822    total: 2.59s    remaining: 2.71s
44: learn: 0.4279891    total: 2.66s    remaining: 2.66s
45: learn: 0.4266312    total: 2.72s    remaining: 2.6s
46: learn: 0.4246471    total: 2.78s    remaining: 2.54s
47: learn: 0.4225972    total: 2.84s    remaining: 2.48s
48: learn: 0.4210475    total: 2.91s    remaining: 2.43s
49: learn: 0.4192105    total: 2.97s    remaining: 2.38s
50: learn: 0.4180917    total: 3.04s    remaining: 2.33s
51: learn: 0.4162651    total: 3.11s    remaining: 2.27s
52: learn: 0.4145738    total: 3.17s    remaining: 2.21s
53: learn: 0.4135470    total: 3.23s    remaining: 2.15s
54: learn: 0.4113829    total: 3.29s    remaining: 2.09s
55: learn: 0.4083366    total: 3.35s    remaining: 2.04s
56: learn: 0.4071852    total: 3.42s    remaining: 1.98s
57: learn: 0.4060343    total: 3.49s    remaining: 1.92s
58: learn: 0.4049222    total: 3.56s    remaining: 1.87s
59: learn: 0.4015865    total: 3.62s    remaining: 1.81s
60: learn: 0.4005098    total: 3.69s    remaining: 1.75s
61: learn: 0.3994448    total: 3.76s    remaining: 1.7s
62: learn: 0.3982600    total: 3.85s    remaining: 1.65s
63: learn: 0.3967756    total: 3.9s remaining: 1.58s
64: learn: 0.3956166    total: 3.97s    remaining: 1.52s
65: learn: 0.3932374    total: 4.03s    remaining: 1.46s
66: learn: 0.3920478    total: 4.09s    remaining: 1.4s
67: learn: 0.3910069    total: 4.15s    remaining: 1.34s
68: learn: 0.3888444    total: 4.21s    remaining: 1.28s
69: learn: 0.3876487    total: 4.26s    remaining: 1.22s
70: learn: 0.3867444    total: 4.33s    remaining: 1.16s
71: learn: 0.3858815    total: 4.39s    remaining: 1.1s
72: learn: 0.3849503    total: 4.45s    remaining: 1.04s
73: learn: 0.3840737    total: 4.5s remaining: 974ms
74: learn: 0.3833085    total: 4.57s    remaining: 913ms
75: learn: 0.3824363    total: 4.62s    remaining: 851ms
76: learn: 0.3801746    total: 4.68s    remaining: 790ms
77: learn: 0.3787439    total: 4.73s    remaining: 728ms
78: learn: 0.3780631    total: 4.79s    remaining: 667ms
79: learn: 0.3770939    total: 4.84s    remaining: 605ms
80: learn: 0.3760120    total: 4.9s remaining: 545ms
81: learn: 0.3747921    total: 4.95s    remaining: 483ms
82: learn: 0.3736694    total: 5.01s    remaining: 422ms
83: learn: 0.3723968    total: 5.06s    remaining: 361ms
84: learn: 0.3716942    total: 5.12s    remaining: 301ms
85: learn: 0.3707126    total: 5.17s    remaining: 240ms
86: learn: 0.3696831    total: 5.22s    remaining: 180ms
87: learn: 0.3690034    total: 5.28s    remaining: 120ms
88: learn: 0.3672579    total: 5.33s    remaining: 60ms
89: learn: 0.3657457    total: 5.39s    remaining: 0us
0:  learn: 0.6891827    total: 42.1ms   remaining: 2.48s
1:  learn: 0.6832416    total: 88.4ms   remaining: 2.56s
2:  learn: 0.6799339    total: 135ms    remaining: 2.56s
3:  learn: 0.6753852    total: 178ms    remaining: 2.5s
4:  learn: 0.6713542    total: 230ms    remaining: 2.53s
5:  learn: 0.6680221    total: 276ms    remaining: 2.48s
6:  learn: 0.6643313    total: 322ms    remaining: 2.43s
7:  learn: 0.6603958    total: 372ms    remaining: 2.42s
8:  learn: 0.6566409    total: 421ms    remaining: 2.39s
9:  learn: 0.6523928    total: 467ms    remaining: 2.33s
10: learn: 0.6490019    total: 516ms    remaining: 2.3s
11: learn: 0.6450495    total: 561ms    remaining: 2.24s
12: learn: 0.6418979    total: 620ms    remaining: 2.24s
13: learn: 0.6369726    total: 668ms    remaining: 2.2s
14: learn: 0.6337408    total: 718ms    remaining: 2.15s
15: learn: 0.6291213    total: 761ms    remaining: 2.09s
16: learn: 0.6253233    total: 808ms    remaining: 2.04s
17: learn: 0.6216481    total: 851ms    remaining: 1.99s
18: learn: 0.6185586    total: 916ms    remaining: 1.98s
19: learn: 0.6146644    total: 961ms    remaining: 1.92s
20: learn: 0.6117712    total: 1.02s    remaining: 1.9s
21: learn: 0.6085535    total: 1.07s    remaining: 1.85s
22: learn: 0.6063618    total: 1.12s    remaining: 1.79s
23: learn: 0.6033707    total: 1.16s    remaining: 1.73s
24: learn: 0.6011590    total: 1.2s remaining: 1.68s
25: learn: 0.5980825    total: 1.25s    remaining: 1.63s
26: learn: 0.5952362    total: 1.3s remaining: 1.58s
27: learn: 0.5926361    total: 1.34s    remaining: 1.53s
28: learn: 0.5898130    total: 1.39s    remaining: 1.49s
29: learn: 0.5862358    total: 1.44s    remaining: 1.44s
30: learn: 0.5826405    total: 1.53s    remaining: 1.43s
31: learn: 0.5799910    total: 1.57s    remaining: 1.38s
32: learn: 0.5773621    total: 1.62s    remaining: 1.33s
33: learn: 0.5753155    total: 1.67s    remaining: 1.27s
34: learn: 0.5721495    total: 1.73s    remaining: 1.23s
35: learn: 0.5699541    total: 1.77s    remaining: 1.18s
36: learn: 0.5683134    total: 1.82s    remaining: 1.13s
37: learn: 0.5662259    total: 1.86s    remaining: 1.08s
38: learn: 0.5631273    total: 1.92s    remaining: 1.03s
39: learn: 0.5611466    total: 1.97s    remaining: 985ms
40: learn: 0.5585133    total: 2.02s    remaining: 935ms
41: learn: 0.5561262    total: 2.07s    remaining: 888ms
42: learn: 0.5540212    total: 2.12s    remaining: 836ms
43: learn: 0.5520581    total: 2.17s    remaining: 790ms
44: learn: 0.5496373    total: 2.23s    remaining: 742ms
45: learn: 0.5469572    total: 2.27s    remaining: 691ms
46: learn: 0.5449205    total: 2.31s    remaining: 640ms
47: learn: 0.5422714    total: 2.36s    remaining: 590ms
48: learn: 0.5392169    total: 2.41s    remaining: 541ms
49: learn: 0.5373144    total: 2.46s    remaining: 491ms
50: learn: 0.5356060    total: 2.5s remaining: 441ms
51: learn: 0.5343717    total: 2.56s    remaining: 393ms
52: learn: 0.5328772    total: 2.6s remaining: 344ms
53: learn: 0.5313314    total: 2.64s    remaining: 294ms
54: learn: 0.5290473    total: 2.69s    remaining: 244ms
55: learn: 0.5278621    total: 2.74s    remaining: 195ms
56: learn: 0.5266591    total: 2.78s    remaining: 146ms
57: learn: 0.5248089    total: 2.84s    remaining: 97.9ms
58: learn: 0.5229226    total: 2.9s remaining: 49.1ms
59: learn: 0.5211238    total: 2.94s    remaining: 0us
0:  learn: 0.6883565    total: 41.9ms   remaining: 2.47s
1:  learn: 0.6836626    total: 85.6ms   remaining: 2.48s
2:  learn: 0.6803896    total: 127ms    remaining: 2.4s
3:  learn: 0.6767302    total: 171ms    remaining: 2.4s
4:  learn: 0.6723705    total: 238ms    remaining: 2.62s
5:  learn: 0.6683962    total: 283ms    remaining: 2.55s
6:  learn: 0.6641959    total: 327ms    remaining: 2.48s
7:  learn: 0.6606501    total: 383ms    remaining: 2.49s
8:  learn: 0.6578427    total: 426ms    remaining: 2.41s
9:  learn: 0.6533980    total: 479ms    remaining: 2.4s
10: learn: 0.6498456    total: 529ms    remaining: 2.35s
11: learn: 0.6461800    total: 570ms    remaining: 2.28s
12: learn: 0.6429715    total: 625ms    remaining: 2.26s
13: learn: 0.6392294    total: 673ms    remaining: 2.21s
14: learn: 0.6347095    total: 722ms    remaining: 2.17s
15: learn: 0.6312490    total: 770ms    remaining: 2.12s
16: learn: 0.6289288    total: 821ms    remaining: 2.08s
17: learn: 0.6250441    total: 864ms    remaining: 2.02s
18: learn: 0.6216653    total: 919ms    remaining: 1.98s
19: learn: 0.6193114    total: 971ms    remaining: 1.94s
20: learn: 0.6160372    total: 1.02s    remaining: 1.89s
21: learn: 0.6129786    total: 1.07s    remaining: 1.84s
22: learn: 0.6098641    total: 1.11s    remaining: 1.79s
23: learn: 0.6060756    total: 1.16s    remaining: 1.74s
24: learn: 0.6032837    total: 1.21s    remaining: 1.69s
25: learn: 0.6006062    total: 1.25s    remaining: 1.64s
26: learn: 0.5963792    total: 1.3s remaining: 1.59s
27: learn: 0.5935965    total: 1.35s    remaining: 1.54s
28: learn: 0.5911082    total: 1.43s    remaining: 1.52s
29: learn: 0.5884068    total: 1.47s    remaining: 1.47s
30: learn: 0.5855242    total: 1.52s    remaining: 1.42s
31: learn: 0.5836715    total: 1.56s    remaining: 1.37s
32: learn: 0.5807843    total: 1.64s    remaining: 1.34s
33: learn: 0.5787853    total: 1.68s    remaining: 1.28s
34: learn: 0.5749655    total: 1.73s    remaining: 1.24s
35: learn: 0.5725083    total: 1.77s    remaining: 1.18s
36: learn: 0.5706425    total: 1.82s    remaining: 1.13s
37: learn: 0.5684386    total: 1.87s    remaining: 1.08s
38: learn: 0.5663417    total: 1.91s    remaining: 1.03s
39: learn: 0.5633125    total: 1.95s    remaining: 977ms
40: learn: 0.5611171    total: 2s   remaining: 928ms
41: learn: 0.5586256    total: 2.04s    remaining: 876ms
42: learn: 0.5561923    total: 2.09s    remaining: 827ms
43: learn: 0.5542360    total: 2.13s    remaining: 776ms
44: learn: 0.5521751    total: 2.18s    remaining: 726ms
45: learn: 0.5504413    total: 2.22s    remaining: 676ms
46: learn: 0.5488480    total: 2.27s    remaining: 627ms
47: learn: 0.5472914    total: 2.31s    remaining: 579ms
48: learn: 0.5452874    total: 2.36s    remaining: 530ms
49: learn: 0.5436439    total: 2.41s    remaining: 481ms
50: learn: 0.5411503    total: 2.48s    remaining: 437ms
51: learn: 0.5383509    total: 2.52s    remaining: 388ms
52: learn: 0.5366184    total: 2.57s    remaining: 339ms
53: learn: 0.5343908    total: 2.61s    remaining: 290ms
54: learn: 0.5330084    total: 2.65s    remaining: 241ms
55: learn: 0.5317342    total: 2.71s    remaining: 194ms
56: learn: 0.5304471    total: 2.79s    remaining: 147ms
57: learn: 0.5288729    total: 2.84s    remaining: 97.8ms
58: learn: 0.5273035    total: 2.88s    remaining: 48.8ms
59: learn: 0.5255895    total: 2.93s    remaining: 0us
0:  learn: 0.6882838    total: 59.7ms   remaining: 3.52s
1:  learn: 0.6832112    total: 108ms    remaining: 3.13s
2:  learn: 0.6799919    total: 187ms    remaining: 3.54s
3:  learn: 0.6771048    total: 229ms    remaining: 3.2s
4:  learn: 0.6738181    total: 281ms    remaining: 3.09s
5:  learn: 0.6707208    total: 336ms    remaining: 3.02s
6:  learn: 0.6679867    total: 405ms    remaining: 3.07s
7:  learn: 0.6654101    total: 466ms    remaining: 3.03s
8:  learn: 0.6621987    total: 520ms    remaining: 2.95s
9:  learn: 0.6580098    total: 572ms    remaining: 2.86s
10: learn: 0.6546495    total: 634ms    remaining: 2.83s
11: learn: 0.6513982    total: 686ms    remaining: 2.74s
12: learn: 0.6474299    total: 746ms    remaining: 2.7s
13: learn: 0.6446765    total: 798ms    remaining: 2.62s
14: learn: 0.6409242    total: 864ms    remaining: 2.59s
15: learn: 0.6362265    total: 916ms    remaining: 2.52s
16: learn: 0.6332374    total: 964ms    remaining: 2.44s
17: learn: 0.6290921    total: 1.01s    remaining: 2.36s
18: learn: 0.6258450    total: 1.06s    remaining: 2.29s
19: learn: 0.6214934    total: 1.1s remaining: 2.2s
20: learn: 0.6191940    total: 1.15s    remaining: 2.13s
21: learn: 0.6147269    total: 1.2s remaining: 2.06s
22: learn: 0.6121340    total: 1.24s    remaining: 2s
23: learn: 0.6088729    total: 1.28s    remaining: 1.93s
24: learn: 0.6060512    total: 1.35s    remaining: 1.89s
25: learn: 0.6030904    total: 1.39s    remaining: 1.82s
26: learn: 0.6007806    total: 1.44s    remaining: 1.76s
27: learn: 0.5986268    total: 1.48s    remaining: 1.7s
28: learn: 0.5959227    total: 1.53s    remaining: 1.63s
29: learn: 0.5929901    total: 1.57s    remaining: 1.57s
30: learn: 0.5910613    total: 1.62s    remaining: 1.51s
31: learn: 0.5886354    total: 1.67s    remaining: 1.46s
32: learn: 0.5849181    total: 1.71s    remaining: 1.4s
33: learn: 0.5823567    total: 1.76s    remaining: 1.34s
34: learn: 0.5802187    total: 1.81s    remaining: 1.29s
35: learn: 0.5780069    total: 1.85s    remaining: 1.23s
36: learn: 0.5753076    total: 1.9s remaining: 1.18s
37: learn: 0.5719202    total: 1.95s    remaining: 1.13s
38: learn: 0.5696997    total: 2s   remaining: 1.07s
39: learn: 0.5672249    total: 2.04s    remaining: 1.02s
40: learn: 0.5640378    total: 2.09s    remaining: 968ms
41: learn: 0.5617436    total: 2.13s    remaining: 914ms
42: learn: 0.5594742    total: 2.18s    remaining: 863ms
43: learn: 0.5568258    total: 2.23s    remaining: 810ms
44: learn: 0.5541508    total: 2.27s    remaining: 758ms
45: learn: 0.5516927    total: 2.32s    remaining: 706ms
46: learn: 0.5494778    total: 2.38s    remaining: 658ms
47: learn: 0.5465194    total: 2.42s    remaining: 606ms
48: learn: 0.5445204    total: 2.47s    remaining: 554ms
49: learn: 0.5424344    total: 2.51s    remaining: 503ms
50: learn: 0.5407045    total: 2.56s    remaining: 453ms
51: learn: 0.5380486    total: 2.61s    remaining: 401ms
52: learn: 0.5352292    total: 2.66s    remaining: 351ms
53: learn: 0.5333757    total: 2.7s remaining: 300ms
54: learn: 0.5322769    total: 2.75s    remaining: 250ms
55: learn: 0.5305465    total: 2.79s    remaining: 199ms
56: learn: 0.5291379    total: 2.84s    remaining: 149ms
57: learn: 0.5275452    total: 2.88s    remaining: 99.3ms
58: learn: 0.5256782    total: 2.93s    remaining: 49.6ms
59: learn: 0.5239801    total: 2.97s    remaining: 0us
0:  learn: 0.6780674    total: 36.5ms   remaining: 3.61s
1:  learn: 0.6656423    total: 72.2ms   remaining: 3.54s
2:  learn: 0.6499618    total: 109ms    remaining: 3.54s
3:  learn: 0.6394232    total: 144ms    remaining: 3.45s
4:  learn: 0.6275185    total: 192ms    remaining: 3.64s
5:  learn: 0.6156624    total: 232ms    remaining: 3.63s
6:  learn: 0.6007392    total: 271ms    remaining: 3.6s
7:  learn: 0.5896390    total: 332ms    remaining: 3.82s
8:  learn: 0.5800070    total: 376ms    remaining: 3.8s
9:  learn: 0.5667137    total: 415ms    remaining: 3.73s
10: learn: 0.5561906    total: 460ms    remaining: 3.72s
11: learn: 0.5475448    total: 524ms    remaining: 3.84s
12: learn: 0.5399063    total: 582ms    remaining: 3.89s
13: learn: 0.5328563    total: 659ms    remaining: 4.05s
14: learn: 0.5269186    total: 698ms    remaining: 3.96s
15: learn: 0.5229059    total: 751ms    remaining: 3.94s
16: learn: 0.5166568    total: 793ms    remaining: 3.87s
17: learn: 0.5116476    total: 835ms    remaining: 3.8s
18: learn: 0.5060172    total: 877ms    remaining: 3.74s
19: learn: 0.5000798    total: 918ms    remaining: 3.67s
20: learn: 0.4957177    total: 973ms    remaining: 3.66s
21: learn: 0.4912718    total: 1.01s    remaining: 3.59s
22: learn: 0.4876625    total: 1.05s    remaining: 3.53s
23: learn: 0.4845066    total: 1.09s    remaining: 3.45s
24: learn: 0.4792528    total: 1.13s    remaining: 3.39s
25: learn: 0.4743935    total: 1.17s    remaining: 3.33s
26: learn: 0.4718634    total: 1.22s    remaining: 3.31s
27: learn: 0.4674427    total: 1.31s    remaining: 3.36s
28: learn: 0.4652127    total: 1.36s    remaining: 3.33s
29: learn: 0.4634977    total: 1.4s remaining: 3.27s
30: learn: 0.4610593    total: 1.45s    remaining: 3.23s
31: learn: 0.4590537    total: 1.49s    remaining: 3.16s
32: learn: 0.4573011    total: 1.53s    remaining: 3.1s
33: learn: 0.4546308    total: 1.59s    remaining: 3.08s
34: learn: 0.4517896    total: 1.62s    remaining: 3.01s
35: learn: 0.4480228    total: 1.66s    remaining: 2.96s
36: learn: 0.4462477    total: 1.7s remaining: 2.89s
37: learn: 0.4434832    total: 1.77s    remaining: 2.89s
38: learn: 0.4409929    total: 1.82s    remaining: 2.84s
39: learn: 0.4374450    total: 1.86s    remaining: 2.79s
40: learn: 0.4350297    total: 1.9s remaining: 2.73s
41: learn: 0.4329302    total: 1.94s    remaining: 2.68s
42: learn: 0.4297999    total: 2.02s    remaining: 2.68s
43: learn: 0.4281784    total: 2.06s    remaining: 2.62s
44: learn: 0.4271110    total: 2.1s remaining: 2.57s
45: learn: 0.4259213    total: 2.19s    remaining: 2.57s
46: learn: 0.4248926    total: 2.22s    remaining: 2.5s
47: learn: 0.4224029    total: 2.26s    remaining: 2.45s
48: learn: 0.4210638    total: 2.3s remaining: 2.39s
49: learn: 0.4198819    total: 2.34s    remaining: 2.34s
50: learn: 0.4189043    total: 2.37s    remaining: 2.28s
51: learn: 0.4179394    total: 2.41s    remaining: 2.23s
52: learn: 0.4145549    total: 2.45s    remaining: 2.17s
53: learn: 0.4116847    total: 2.49s    remaining: 2.12s
54: learn: 0.4107215    total: 2.53s    remaining: 2.07s
55: learn: 0.4084986    total: 2.57s    remaining: 2.02s
56: learn: 0.4067278    total: 2.61s    remaining: 1.97s
57: learn: 0.4058478    total: 2.67s    remaining: 1.93s
58: learn: 0.4050000    total: 2.73s    remaining: 1.89s
59: learn: 0.4039232    total: 2.77s    remaining: 1.85s
60: learn: 0.4032417    total: 2.8s remaining: 1.79s
61: learn: 0.4025092    total: 2.85s    remaining: 1.75s
62: learn: 0.4019447    total: 2.92s    remaining: 1.71s
63: learn: 0.3978013    total: 2.97s    remaining: 1.67s
64: learn: 0.3971572    total: 3.03s    remaining: 1.63s
65: learn: 0.3952749    total: 3.07s    remaining: 1.58s
66: learn: 0.3943706    total: 3.11s    remaining: 1.53s
67: learn: 0.3935298    total: 3.15s    remaining: 1.48s
68: learn: 0.3928379    total: 3.18s    remaining: 1.43s
69: learn: 0.3922042    total: 3.22s    remaining: 1.38s
70: learn: 0.3915744    total: 3.25s    remaining: 1.33s
71: learn: 0.3909309    total: 3.29s    remaining: 1.28s
72: learn: 0.3902807    total: 3.33s    remaining: 1.23s
73: learn: 0.3897191    total: 3.37s    remaining: 1.18s
74: learn: 0.3870658    total: 3.4s remaining: 1.14s
75: learn: 0.3854908    total: 3.44s    remaining: 1.09s
76: learn: 0.3849864    total: 3.49s    remaining: 1.04s
77: learn: 0.3843592    total: 3.54s    remaining: 1000ms
78: learn: 0.3837757    total: 3.59s    remaining: 954ms
79: learn: 0.3834153    total: 3.63s    remaining: 907ms
80: learn: 0.3820647    total: 3.67s    remaining: 860ms
81: learn: 0.3815409    total: 3.73s    remaining: 819ms
82: learn: 0.3809143    total: 3.77s    remaining: 771ms
83: learn: 0.3804150    total: 3.81s    remaining: 725ms
84: learn: 0.3783076    total: 3.84s    remaining: 678ms
85: learn: 0.3777422    total: 3.89s    remaining: 633ms
86: learn: 0.3773108    total: 3.93s    remaining: 587ms
87: learn: 0.3759948    total: 3.99s    remaining: 545ms
88: learn: 0.3733624    total: 4.04s    remaining: 499ms
89: learn: 0.3728116    total: 4.07s    remaining: 453ms
90: learn: 0.3722956    total: 4.12s    remaining: 407ms
91: learn: 0.3718745    total: 4.17s    remaining: 363ms
92: learn: 0.3709713    total: 4.21s    remaining: 317ms
93: learn: 0.3705749    total: 4.25s    remaining: 271ms
94: learn: 0.3701589    total: 4.29s    remaining: 226ms
95: learn: 0.3696644    total: 4.34s    remaining: 181ms
96: learn: 0.3693308    total: 4.37s    remaining: 135ms
97: learn: 0.3689769    total: 4.42s    remaining: 90.1ms
98: learn: 0.3685521    total: 4.46s    remaining: 45.1ms
99: learn: 0.3681502    total: 4.5s remaining: 0us
0:  learn: 0.6787347    total: 36.7ms   remaining: 3.64s
1:  learn: 0.6629308    total: 80.4ms   remaining: 3.94s
2:  learn: 0.6521186    total: 112ms    remaining: 3.63s
3:  learn: 0.6424923    total: 154ms    remaining: 3.69s
4:  learn: 0.6330328    total: 185ms    remaining: 3.52s
5:  learn: 0.6254169    total: 219ms    remaining: 3.42s
6:  learn: 0.6134426    total: 258ms    remaining: 3.43s
7:  learn: 0.6037043    total: 296ms    remaining: 3.41s
8:  learn: 0.5960655    total: 331ms    remaining: 3.34s
9:  learn: 0.5894701    total: 372ms    remaining: 3.35s
10: learn: 0.5801976    total: 411ms    remaining: 3.32s
11: learn: 0.5686701    total: 458ms    remaining: 3.36s
12: learn: 0.5604865    total: 496ms    remaining: 3.32s
13: learn: 0.5518040    total: 538ms    remaining: 3.31s
14: learn: 0.5465118    total: 590ms    remaining: 3.34s
15: learn: 0.5355247    total: 628ms    remaining: 3.3s
16: learn: 0.5289492    total: 665ms    remaining: 3.25s
17: learn: 0.5228831    total: 710ms    remaining: 3.23s
18: learn: 0.5187277    total: 750ms    remaining: 3.2s
19: learn: 0.5136091    total: 791ms    remaining: 3.16s
20: learn: 0.5089900    total: 841ms    remaining: 3.16s
21: learn: 0.5028772    total: 884ms    remaining: 3.13s
22: learn: 0.4980801    total: 951ms    remaining: 3.18s
23: learn: 0.4936861    total: 991ms    remaining: 3.14s
24: learn: 0.4904537    total: 1.03s    remaining: 3.09s
25: learn: 0.4867408    total: 1.07s    remaining: 3.04s
26: learn: 0.4836632    total: 1.1s remaining: 2.99s
27: learn: 0.4810250    total: 1.14s    remaining: 2.94s
28: learn: 0.4770696    total: 1.18s    remaining: 2.89s
29: learn: 0.4747817    total: 1.24s    remaining: 2.9s
30: learn: 0.4702333    total: 1.28s    remaining: 2.86s
31: learn: 0.4681339    total: 1.32s    remaining: 2.81s
32: learn: 0.4661564    total: 1.37s    remaining: 2.78s
33: learn: 0.4627045    total: 1.42s    remaining: 2.75s
34: learn: 0.4593287    total: 1.46s    remaining: 2.71s
35: learn: 0.4577672    total: 1.51s    remaining: 2.68s
36: learn: 0.4541266    total: 1.55s    remaining: 2.64s
37: learn: 0.4509724    total: 1.6s remaining: 2.61s
38: learn: 0.4492433    total: 1.65s    remaining: 2.58s
39: learn: 0.4454138    total: 1.69s    remaining: 2.53s
40: learn: 0.4440516    total: 1.73s    remaining: 2.48s
41: learn: 0.4413669    total: 1.76s    remaining: 2.44s
42: learn: 0.4383825    total: 1.82s    remaining: 2.42s
43: learn: 0.4364719    total: 1.86s    remaining: 2.37s
44: learn: 0.4338141    total: 1.91s    remaining: 2.33s
45: learn: 0.4326615    total: 1.98s    remaining: 2.32s
46: learn: 0.4310823    total: 2.02s    remaining: 2.28s
47: learn: 0.4274267    total: 2.06s    remaining: 2.23s
48: learn: 0.4263108    total: 2.1s remaining: 2.19s
49: learn: 0.4232592    total: 2.15s    remaining: 2.15s
50: learn: 0.4219268    total: 2.21s    remaining: 2.13s
51: learn: 0.4209638    total: 2.27s    remaining: 2.09s
52: learn: 0.4189954    total: 2.31s    remaining: 2.05s
53: learn: 0.4156722    total: 2.35s    remaining: 2s
54: learn: 0.4141122    total: 2.39s    remaining: 1.96s
55: learn: 0.4131974    total: 2.44s    remaining: 1.91s
56: learn: 0.4091712    total: 2.49s    remaining: 1.88s
57: learn: 0.4073386    total: 2.53s    remaining: 1.83s
58: learn: 0.4061417    total: 2.57s    remaining: 1.79s
59: learn: 0.4051938    total: 2.62s    remaining: 1.75s
60: learn: 0.4029454    total: 2.68s    remaining: 1.71s
61: learn: 0.4023675    total: 2.73s    remaining: 1.68s
62: learn: 0.4017586    total: 2.77s    remaining: 1.63s
63: learn: 0.4010712    total: 2.81s    remaining: 1.58s
64: learn: 0.3987395    total: 2.86s    remaining: 1.54s
65: learn: 0.3979852    total: 2.89s    remaining: 1.49s
66: learn: 0.3974124    total: 2.94s    remaining: 1.45s
67: learn: 0.3950991    total: 2.98s    remaining: 1.4s
68: learn: 0.3943587    total: 3.02s    remaining: 1.36s
69: learn: 0.3924520    total: 3.06s    remaining: 1.31s
70: learn: 0.3895471    total: 3.1s remaining: 1.27s
71: learn: 0.3887278    total: 3.15s    remaining: 1.22s
72: learn: 0.3879781    total: 3.19s    remaining: 1.18s
73: learn: 0.3863183    total: 3.24s    remaining: 1.14s
74: learn: 0.3858390    total: 3.31s    remaining: 1.1s
75: learn: 0.3837355    total: 3.35s    remaining: 1.06s
76: learn: 0.3831220    total: 3.4s remaining: 1.01s
77: learn: 0.3825741    total: 3.46s    remaining: 975ms
78: learn: 0.3819807    total: 3.5s remaining: 930ms
79: learn: 0.3816032    total: 3.54s    remaining: 884ms
80: learn: 0.3810282    total: 3.58s    remaining: 840ms
81: learn: 0.3788452    total: 3.63s    remaining: 797ms
82: learn: 0.3784096    total: 3.67s    remaining: 753ms
83: learn: 0.3777615    total: 3.72s    remaining: 709ms
84: learn: 0.3773304    total: 3.77s    remaining: 665ms
85: learn: 0.3768781    total: 3.81s    remaining: 621ms
86: learn: 0.3744644    total: 3.85s    remaining: 576ms
87: learn: 0.3740947    total: 3.9s remaining: 532ms
88: learn: 0.3737719    total: 3.96s    remaining: 489ms
89: learn: 0.3727752    total: 4s   remaining: 445ms
90: learn: 0.3724632    total: 4.05s    remaining: 400ms
91: learn: 0.3720264    total: 4.09s    remaining: 356ms
92: learn: 0.3715933    total: 4.13s    remaining: 311ms
93: learn: 0.3701342    total: 4.17s    remaining: 266ms
94: learn: 0.3697976    total: 4.22s    remaining: 222ms
95: learn: 0.3694249    total: 4.26s    remaining: 178ms
96: learn: 0.3674265    total: 4.3s remaining: 133ms
97: learn: 0.3660059    total: 4.34s    remaining: 88.6ms
98: learn: 0.3650624    total: 4.38s    remaining: 44.3ms
99: learn: 0.3641319    total: 4.43s    remaining: 0us
0:  learn: 0.6755975    total: 36.6ms   remaining: 3.62s
1:  learn: 0.6640494    total: 73.8ms   remaining: 3.62s
2:  learn: 0.6479160    total: 118ms    remaining: 3.8s
3:  learn: 0.6353381    total: 159ms    remaining: 3.81s
4:  learn: 0.6256086    total: 194ms    remaining: 3.69s
5:  learn: 0.6123194    total: 242ms    remaining: 3.8s
6:  learn: 0.6004987    total: 288ms    remaining: 3.83s
7:  learn: 0.5877596    total: 327ms    remaining: 3.76s
8:  learn: 0.5787817    total: 396ms    remaining: 4.01s
9:  learn: 0.5685038    total: 458ms    remaining: 4.12s
10: learn: 0.5621513    total: 490ms    remaining: 3.96s
11: learn: 0.5530775    total: 554ms    remaining: 4.07s
12: learn: 0.5472004    total: 592ms    remaining: 3.96s
13: learn: 0.5408390    total: 630ms    remaining: 3.87s
14: learn: 0.5362334    total: 672ms    remaining: 3.81s
15: learn: 0.5291432    total: 715ms    remaining: 3.76s
16: learn: 0.5192261    total: 758ms    remaining: 3.7s
17: learn: 0.5160676    total: 794ms    remaining: 3.62s
18: learn: 0.5102759    total: 877ms    remaining: 3.74s
19: learn: 0.5053539    total: 935ms    remaining: 3.74s
20: learn: 0.5018072    total: 1.01s    remaining: 3.81s
21: learn: 0.4984342    total: 1.06s    remaining: 3.75s
22: learn: 0.4946264    total: 1.1s remaining: 3.67s
23: learn: 0.4895979    total: 1.14s    remaining: 3.6s
24: learn: 0.4864200    total: 1.17s    remaining: 3.52s
25: learn: 0.4833633    total: 1.22s    remaining: 3.47s
26: learn: 0.4787359    total: 1.26s    remaining: 3.41s
27: learn: 0.4742556    total: 1.3s remaining: 3.35s
28: learn: 0.4722703    total: 1.34s    remaining: 3.29s
29: learn: 0.4680890    total: 1.38s    remaining: 3.22s
30: learn: 0.4646848    total: 1.43s    remaining: 3.18s
31: learn: 0.4624585    total: 1.48s    remaining: 3.15s
32: learn: 0.4603976    total: 1.52s    remaining: 3.08s
33: learn: 0.4586205    total: 1.56s    remaining: 3.03s
34: learn: 0.4554294    total: 1.6s remaining: 2.97s
35: learn: 0.4521939    total: 1.66s    remaining: 2.94s
36: learn: 0.4490775    total: 1.7s remaining: 2.9s
37: learn: 0.4476623    total: 1.74s    remaining: 2.84s
38: learn: 0.4459154    total: 1.81s    remaining: 2.83s
39: learn: 0.4442376    total: 1.85s    remaining: 2.77s
40: learn: 0.4415100    total: 1.89s    remaining: 2.71s
41: learn: 0.4402119    total: 1.93s    remaining: 2.66s
42: learn: 0.4388914    total: 1.96s    remaining: 2.6s
43: learn: 0.4362361    total: 2s   remaining: 2.54s
44: learn: 0.4351212    total: 2.03s    remaining: 2.48s
45: learn: 0.4339355    total: 2.12s    remaining: 2.49s
46: learn: 0.4316856    total: 2.18s    remaining: 2.46s
47: learn: 0.4303935    total: 2.21s    remaining: 2.4s
48: learn: 0.4290825    total: 2.25s    remaining: 2.35s
49: learn: 0.4268229    total: 2.31s    remaining: 2.31s
50: learn: 0.4258701    total: 2.34s    remaining: 2.25s
51: learn: 0.4247373    total: 2.39s    remaining: 2.21s
52: learn: 0.4237794    total: 2.42s    remaining: 2.15s
53: learn: 0.4227810    total: 2.47s    remaining: 2.1s
54: learn: 0.4179922    total: 2.51s    remaining: 2.05s
55: learn: 0.4168096    total: 2.55s    remaining: 2s
56: learn: 0.4159020    total: 2.58s    remaining: 1.95s
57: learn: 0.4149399    total: 2.63s    remaining: 1.9s
58: learn: 0.4110033    total: 2.66s    remaining: 1.85s
59: learn: 0.4100967    total: 2.71s    remaining: 1.8s
60: learn: 0.4083306    total: 2.75s    remaining: 1.76s
61: learn: 0.4072024    total: 2.79s    remaining: 1.71s
62: learn: 0.4065295    total: 2.83s    remaining: 1.66s
63: learn: 0.4058783    total: 2.86s    remaining: 1.61s
64: learn: 0.4039702    total: 2.93s    remaining: 1.58s
65: learn: 0.4022050    total: 2.98s    remaining: 1.53s
66: learn: 0.4003304    total: 3.02s    remaining: 1.49s
67: learn: 0.3997262    total: 3.06s    remaining: 1.44s
68: learn: 0.3982798    total: 3.1s remaining: 1.39s
69: learn: 0.3976596    total: 3.16s    remaining: 1.35s
70: learn: 0.3971132    total: 3.21s    remaining: 1.31s
71: learn: 0.3964351    total: 3.25s    remaining: 1.26s
72: learn: 0.3945348    total: 3.29s    remaining: 1.22s
73: learn: 0.3939425    total: 3.33s    remaining: 1.17s
74: learn: 0.3932463    total: 3.37s    remaining: 1.12s
75: learn: 0.3928738    total: 3.41s    remaining: 1.08s
76: learn: 0.3911859    total: 3.46s    remaining: 1.03s
77: learn: 0.3907312    total: 3.5s remaining: 988ms
78: learn: 0.3896094    total: 3.57s    remaining: 948ms
79: learn: 0.3890563    total: 3.6s remaining: 901ms
80: learn: 0.3884495    total: 3.65s    remaining: 856ms
81: learn: 0.3878721    total: 3.68s    remaining: 809ms
82: learn: 0.3870853    total: 3.75s    remaining: 768ms
83: learn: 0.3864268    total: 3.79s    remaining: 721ms
84: learn: 0.3836755    total: 3.83s    remaining: 675ms
85: learn: 0.3831474    total: 3.87s    remaining: 631ms
86: learn: 0.3824414    total: 3.91s    remaining: 585ms
87: learn: 0.3820372    total: 3.95s    remaining: 539ms
88: learn: 0.3811514    total: 4.01s    remaining: 496ms
89: learn: 0.3785666    total: 4.05s    remaining: 450ms
90: learn: 0.3777927    total: 4.12s    remaining: 407ms
91: learn: 0.3773483    total: 4.18s    remaining: 363ms
92: learn: 0.3768092    total: 4.21s    remaining: 317ms
93: learn: 0.3747709    total: 4.25s    remaining: 272ms
94: learn: 0.3738006    total: 4.29s    remaining: 226ms
95: learn: 0.3726374    total: 4.36s    remaining: 182ms
96: learn: 0.3714779    total: 4.4s remaining: 136ms
97: learn: 0.3708958    total: 4.43s    remaining: 90.5ms
98: learn: 0.3704787    total: 4.49s    remaining: 45.3ms
99: learn: 0.3693287    total: 4.54s    remaining: 0us
0:  learn: 0.6854808    total: 44.5ms   remaining: 3.07s
1:  learn: 0.6789342    total: 76.1ms   remaining: 2.59s
2:  learn: 0.6710810    total: 119ms    remaining: 2.67s
3:  learn: 0.6652697    total: 153ms    remaining: 2.53s
4:  learn: 0.6584662    total: 198ms    remaining: 2.57s
5:  learn: 0.6511288    total: 233ms    remaining: 2.49s
6:  learn: 0.6465899    total: 272ms    remaining: 2.45s
7:  learn: 0.6414408    total: 306ms    remaining: 2.37s
8:  learn: 0.6351526    total: 367ms    remaining: 2.49s
9:  learn: 0.6288849    total: 408ms    remaining: 2.45s
10: learn: 0.6214170    total: 447ms    remaining: 2.4s
11: learn: 0.6170727    total: 506ms    remaining: 2.45s
12: learn: 0.6105342    total: 544ms    remaining: 2.39s
13: learn: 0.6051710    total: 585ms    remaining: 2.34s
14: learn: 0.5995441    total: 623ms    remaining: 2.28s
15: learn: 0.5955350    total: 672ms    remaining: 2.27s
16: learn: 0.5899862    total: 715ms    remaining: 2.23s
17: learn: 0.5845524    total: 752ms    remaining: 2.17s
18: learn: 0.5800335    total: 833ms    remaining: 2.24s
19: learn: 0.5748668    total: 874ms    remaining: 2.18s
20: learn: 0.5706926    total: 912ms    remaining: 2.13s
21: learn: 0.5644252    total: 970ms    remaining: 2.12s
22: learn: 0.5606822    total: 1.01s    remaining: 2.06s
23: learn: 0.5553719    total: 1.05s    remaining: 2.01s
24: learn: 0.5526506    total: 1.08s    remaining: 1.95s
25: learn: 0.5490226    total: 1.12s    remaining: 1.9s
26: learn: 0.5437315    total: 1.16s    remaining: 1.85s
27: learn: 0.5406485    total: 1.2s remaining: 1.8s
28: learn: 0.5372565    total: 1.24s    remaining: 1.76s
29: learn: 0.5327823    total: 1.28s    remaining: 1.71s
30: learn: 0.5298844    total: 1.32s    remaining: 1.66s
31: learn: 0.5265301    total: 1.36s    remaining: 1.61s
32: learn: 0.5244033    total: 1.42s    remaining: 1.59s
33: learn: 0.5204570    total: 1.5s remaining: 1.59s
34: learn: 0.5186000    total: 1.54s    remaining: 1.54s
35: learn: 0.5166380    total: 1.58s    remaining: 1.49s
36: learn: 0.5138762    total: 1.64s    remaining: 1.47s
37: learn: 0.5113238    total: 1.7s remaining: 1.43s
38: learn: 0.5079826    total: 1.74s    remaining: 1.38s
39: learn: 0.5063132    total: 1.77s    remaining: 1.33s
40: learn: 0.5034582    total: 1.81s    remaining: 1.28s
41: learn: 0.5016317    total: 1.85s    remaining: 1.23s
42: learn: 0.4989641    total: 1.89s    remaining: 1.19s
43: learn: 0.4968961    total: 1.93s    remaining: 1.14s
44: learn: 0.4955183    total: 1.97s    remaining: 1.09s
45: learn: 0.4940431    total: 2s   remaining: 1.04s
46: learn: 0.4912095    total: 2.05s    remaining: 1s
47: learn: 0.4897417    total: 2.11s    remaining: 968ms
48: learn: 0.4874051    total: 2.16s    remaining: 924ms
49: learn: 0.4853273    total: 2.2s remaining: 879ms
50: learn: 0.4830105    total: 2.24s    remaining: 833ms
51: learn: 0.4792982    total: 2.28s    remaining: 788ms
52: learn: 0.4773667    total: 2.31s    remaining: 743ms
53: learn: 0.4751678    total: 2.36s    remaining: 699ms
54: learn: 0.4720366    total: 2.4s remaining: 655ms
55: learn: 0.4702685    total: 2.46s    remaining: 615ms
56: learn: 0.4690686    total: 2.5s remaining: 571ms
57: learn: 0.4670308    total: 2.54s    remaining: 527ms
58: learn: 0.4659758    total: 2.59s    remaining: 482ms
59: learn: 0.4648619    total: 2.66s    remaining: 444ms
60: learn: 0.4637065    total: 2.7s remaining: 398ms
61: learn: 0.4616158    total: 2.78s    remaining: 359ms
62: learn: 0.4606571    total: 2.82s    remaining: 313ms
63: learn: 0.4580198    total: 2.86s    remaining: 268ms
64: learn: 0.4564792    total: 2.9s remaining: 223ms
65: learn: 0.4555989    total: 2.95s    remaining: 179ms
66: learn: 0.4544228    total: 3.03s    remaining: 136ms
67: learn: 0.4533427    total: 3.07s    remaining: 90.2ms
68: learn: 0.4518842    total: 3.1s remaining: 45ms
69: learn: 0.4510303    total: 3.14s    remaining: 0us
0:  learn: 0.6858449    total: 42.3ms   remaining: 2.92s
1:  learn: 0.6778718    total: 94.4ms   remaining: 3.21s
2:  learn: 0.6719399    total: 168ms    remaining: 3.74s
3:  learn: 0.6664632    total: 211ms    remaining: 3.48s
4:  learn: 0.6591999    total: 310ms    remaining: 4.03s
5:  learn: 0.6538062    total: 377ms    remaining: 4.02s
6:  learn: 0.6478654    total: 422ms    remaining: 3.79s
7:  learn: 0.6426394    total: 469ms    remaining: 3.63s
8:  learn: 0.6359268    total: 509ms    remaining: 3.45s
9:  learn: 0.6298517    total: 564ms    remaining: 3.38s
10: learn: 0.6239089    total: 616ms    remaining: 3.3s
11: learn: 0.6164559    total: 677ms    remaining: 3.27s
12: learn: 0.6120916    total: 742ms    remaining: 3.25s
13: learn: 0.6081299    total: 786ms    remaining: 3.15s
14: learn: 0.6042900    total: 832ms    remaining: 3.05s
15: learn: 0.6012153    total: 868ms    remaining: 2.93s
16: learn: 0.5954841    total: 929ms    remaining: 2.9s
17: learn: 0.5903460    total: 974ms    remaining: 2.81s
18: learn: 0.5836323    total: 1.01s    remaining: 2.72s
19: learn: 0.5803133    total: 1.06s    remaining: 2.64s
20: learn: 0.5767913    total: 1.1s remaining: 2.56s
21: learn: 0.5733150    total: 1.14s    remaining: 2.48s
22: learn: 0.5701430    total: 1.19s    remaining: 2.44s
23: learn: 0.5658711    total: 1.24s    remaining: 2.37s
24: learn: 0.5615695    total: 1.29s    remaining: 2.32s
25: learn: 0.5583077    total: 1.34s    remaining: 2.27s
26: learn: 0.5553097    total: 1.38s    remaining: 2.19s
27: learn: 0.5524697    total: 1.42s    remaining: 2.13s
28: learn: 0.5488063    total: 1.47s    remaining: 2.07s
29: learn: 0.5454557    total: 1.54s    remaining: 2.05s
30: learn: 0.5418739    total: 1.63s    remaining: 2.05s
31: learn: 0.5372384    total: 1.73s    remaining: 2.05s
32: learn: 0.5339865    total: 1.79s    remaining: 2.01s
33: learn: 0.5300285    total: 1.85s    remaining: 1.96s
34: learn: 0.5281090    total: 1.9s remaining: 1.9s
35: learn: 0.5250143    total: 1.95s    remaining: 1.84s
36: learn: 0.5220368    total: 2s   remaining: 1.78s
37: learn: 0.5189688    total: 2.05s    remaining: 1.73s
38: learn: 0.5161286    total: 2.1s remaining: 1.67s
39: learn: 0.5132831    total: 2.17s    remaining: 1.62s
40: learn: 0.5106647    total: 2.21s    remaining: 1.56s
41: learn: 0.5089302    total: 2.25s    remaining: 1.5s
42: learn: 0.5070844    total: 2.3s remaining: 1.44s
43: learn: 0.5044440    total: 2.34s    remaining: 1.38s
44: learn: 0.5024906    total: 2.38s    remaining: 1.32s
45: learn: 0.4986218    total: 2.42s    remaining: 1.26s
46: learn: 0.4959638    total: 2.47s    remaining: 1.21s
47: learn: 0.4937223    total: 2.51s    remaining: 1.15s
48: learn: 0.4906317    total: 2.55s    remaining: 1.09s
49: learn: 0.4879473    total: 2.6s remaining: 1.04s
50: learn: 0.4855137    total: 2.64s    remaining: 983ms
51: learn: 0.4841537    total: 2.68s    remaining: 928ms
52: learn: 0.4825360    total: 2.72s    remaining: 872ms
53: learn: 0.4809738    total: 2.76s    remaining: 818ms
54: learn: 0.4797089    total: 2.79s    remaining: 762ms
55: learn: 0.4785221    total: 2.86s    remaining: 715ms
56: learn: 0.4771580    total: 2.9s remaining: 661ms
57: learn: 0.4757980    total: 2.95s    remaining: 610ms
58: learn: 0.4746702    total: 2.99s    remaining: 558ms
59: learn: 0.4734499    total: 3.03s    remaining: 505ms
60: learn: 0.4714873    total: 3.08s    remaining: 455ms
61: learn: 0.4704030    total: 3.13s    remaining: 404ms
62: learn: 0.4691505    total: 3.17s    remaining: 352ms
63: learn: 0.4682198    total: 3.21s    remaining: 301ms
64: learn: 0.4674057    total: 3.25s    remaining: 250ms
65: learn: 0.4662900    total: 3.29s    remaining: 200ms
66: learn: 0.4646490    total: 3.33s    remaining: 149ms
67: learn: 0.4637058    total: 3.39s    remaining: 99.7ms
68: learn: 0.4626845    total: 3.45s    remaining: 50ms
69: learn: 0.4617493    total: 3.49s    remaining: 0us
0:  learn: 0.6842231    total: 35.3ms   remaining: 2.44s
1:  learn: 0.6781632    total: 72.9ms   remaining: 2.48s
2:  learn: 0.6688444    total: 109ms    remaining: 2.43s
3:  learn: 0.6616521    total: 149ms    remaining: 2.46s
4:  learn: 0.6564388    total: 193ms    remaining: 2.5s
5:  learn: 0.6512287    total: 232ms    remaining: 2.47s
6:  learn: 0.6456828    total: 288ms    remaining: 2.59s
7:  learn: 0.6406841    total: 326ms    remaining: 2.53s
8:  learn: 0.6343532    total: 371ms    remaining: 2.52s
9:  learn: 0.6305959    total: 412ms    remaining: 2.47s
10: learn: 0.6247688    total: 489ms    remaining: 2.63s
11: learn: 0.6188381    total: 547ms    remaining: 2.64s
12: learn: 0.6147997    total: 587ms    remaining: 2.57s
13: learn: 0.6101684    total: 625ms    remaining: 2.5s
14: learn: 0.6038660    total: 676ms    remaining: 2.48s
15: learn: 0.5987693    total: 718ms    remaining: 2.42s
16: learn: 0.5932746    total: 782ms    remaining: 2.44s
17: learn: 0.5881489    total: 846ms    remaining: 2.44s
18: learn: 0.5816394    total: 891ms    remaining: 2.39s
19: learn: 0.5752610    total: 928ms    remaining: 2.32s
20: learn: 0.5723198    total: 983ms    remaining: 2.29s
21: learn: 0.5674529    total: 1.03s    remaining: 2.25s
22: learn: 0.5631400    total: 1.08s    remaining: 2.21s
23: learn: 0.5587330    total: 1.14s    remaining: 2.19s
24: learn: 0.5548008    total: 1.19s    remaining: 2.15s
25: learn: 0.5512702    total: 1.24s    remaining: 2.1s
26: learn: 0.5484515    total: 1.29s    remaining: 2.05s
27: learn: 0.5449410    total: 1.33s    remaining: 2s
28: learn: 0.5409617    total: 1.38s    remaining: 1.94s
29: learn: 0.5375626    total: 1.44s    remaining: 1.92s
30: learn: 0.5345770    total: 1.48s    remaining: 1.86s
31: learn: 0.5322886    total: 1.52s    remaining: 1.8s
32: learn: 0.5300507    total: 1.57s    remaining: 1.77s
33: learn: 0.5282686    total: 1.61s    remaining: 1.71s
34: learn: 0.5253057    total: 1.67s    remaining: 1.67s
35: learn: 0.5223315    total: 1.71s    remaining: 1.62s
36: learn: 0.5194561    total: 1.76s    remaining: 1.57s
37: learn: 0.5176734    total: 1.8s remaining: 1.52s
38: learn: 0.5151183    total: 1.85s    remaining: 1.47s
39: learn: 0.5121722    total: 1.89s    remaining: 1.42s
40: learn: 0.5087551    total: 1.95s    remaining: 1.38s
41: learn: 0.5061796    total: 2s   remaining: 1.33s
42: learn: 0.5041603    total: 2.03s    remaining: 1.28s
43: learn: 0.5008195    total: 2.08s    remaining: 1.23s
44: learn: 0.4981981    total: 2.11s    remaining: 1.17s
45: learn: 0.4961635    total: 2.17s    remaining: 1.13s
46: learn: 0.4943916    total: 2.21s    remaining: 1.08s
47: learn: 0.4928103    total: 2.27s    remaining: 1.04s
48: learn: 0.4912153    total: 2.31s    remaining: 988ms
49: learn: 0.4891246    total: 2.34s    remaining: 938ms
50: learn: 0.4876870    total: 2.39s    remaining: 891ms
51: learn: 0.4860567    total: 2.43s    remaining: 842ms
52: learn: 0.4847793    total: 2.47s    remaining: 792ms
53: learn: 0.4837673    total: 2.52s    remaining: 746ms
54: learn: 0.4817976    total: 2.55s    remaining: 697ms
55: learn: 0.4803827    total: 2.59s    remaining: 649ms
56: learn: 0.4791580    total: 2.63s    remaining: 600ms
57: learn: 0.4770999    total: 2.67s    remaining: 553ms
58: learn: 0.4760986    total: 2.71s    remaining: 505ms
59: learn: 0.4739569    total: 2.75s    remaining: 458ms
60: learn: 0.4719559    total: 2.8s remaining: 413ms
61: learn: 0.4708329    total: 2.86s    remaining: 369ms
62: learn: 0.4696806    total: 2.92s    remaining: 324ms
63: learn: 0.4681599    total: 2.96s    remaining: 278ms
64: learn: 0.4672418    total: 3.01s    remaining: 232ms
65: learn: 0.4653938    total: 3.05s    remaining: 185ms
66: learn: 0.4643815    total: 3.09s    remaining: 138ms
67: learn: 0.4634560    total: 3.15s    remaining: 92.6ms
68: learn: 0.4620317    total: 3.19s    remaining: 46.2ms
69: learn: 0.4608927    total: 3.23s    remaining: 0us
0:  learn: 0.6820864    total: 79.4ms   remaining: 715ms
1:  learn: 0.6717586    total: 179ms    remaining: 718ms
2:  learn: 0.6653624    total: 280ms    remaining: 654ms
3:  learn: 0.6535887    total: 402ms    remaining: 603ms
4:  learn: 0.6436591    total: 528ms    remaining: 528ms
5:  learn: 0.6357576    total: 614ms    remaining: 410ms
6:  learn: 0.6283024    total: 726ms    remaining: 311ms
7:  learn: 0.6206824    total: 816ms    remaining: 204ms
8:  learn: 0.6120411    total: 913ms    remaining: 101ms
9:  learn: 0.6045663    total: 1.01s    remaining: 0us
0:  learn: 0.6812725    total: 79.5ms   remaining: 715ms
1:  learn: 0.6682593    total: 168ms    remaining: 670ms
2:  learn: 0.6600218    total: 251ms    remaining: 585ms
3:  learn: 0.6502116    total: 334ms    remaining: 502ms
4:  learn: 0.6418241    total: 429ms    remaining: 429ms
5:  learn: 0.6320138    total: 536ms    remaining: 357ms
6:  learn: 0.6246990    total: 623ms    remaining: 267ms
7:  learn: 0.6172527    total: 739ms    remaining: 185ms
8:  learn: 0.6107522    total: 835ms    remaining: 92.8ms
9:  learn: 0.6046327    total: 938ms    remaining: 0us
0:  learn: 0.6778932    total: 77.7ms   remaining: 700ms
1:  learn: 0.6699917    total: 169ms    remaining: 676ms
2:  learn: 0.6630856    total: 263ms    remaining: 614ms
3:  learn: 0.6541323    total: 359ms    remaining: 539ms
4:  learn: 0.6465458    total: 449ms    remaining: 449ms
5:  learn: 0.6383303    total: 535ms    remaining: 357ms
6:  learn: 0.6319545    total: 621ms    remaining: 266ms
7:  learn: 0.6249272    total: 739ms    remaining: 185ms
8:  learn: 0.6182948    total: 829ms    remaining: 92.2ms
9:  learn: 0.6095659    total: 920ms    remaining: 0us
0:  learn: 0.6788336    total: 59.1ms   remaining: 532ms
1:  learn: 0.6641919    total: 123ms    remaining: 493ms
2:  learn: 0.6520778    total: 193ms    remaining: 449ms
3:  learn: 0.6378013    total: 291ms    remaining: 436ms
4:  learn: 0.6280943    total: 359ms    remaining: 359ms
5:  learn: 0.6183372    total: 426ms    remaining: 284ms
6:  learn: 0.6045327    total: 512ms    remaining: 219ms
7:  learn: 0.5970878    total: 593ms    remaining: 148ms
8:  learn: 0.5855487    total: 675ms    remaining: 75ms
9:  learn: 0.5745267    total: 749ms    remaining: 0us
0:  learn: 0.6754175    total: 63.9ms   remaining: 575ms
1:  learn: 0.6657609    total: 137ms    remaining: 549ms
2:  learn: 0.6497813    total: 210ms    remaining: 490ms
3:  learn: 0.6385731    total: 276ms    remaining: 414ms
4:  learn: 0.6243063    total: 347ms    remaining: 347ms
5:  learn: 0.6121638    total: 420ms    remaining: 280ms
6:  learn: 0.6001453    total: 502ms    remaining: 215ms
7:  learn: 0.5917983    total: 582ms    remaining: 146ms
8:  learn: 0.5795535    total: 653ms    remaining: 72.5ms
9:  learn: 0.5703570    total: 728ms    remaining: 0us
0:  learn: 0.6736660    total: 59.9ms   remaining: 539ms
1:  learn: 0.6632992    total: 132ms    remaining: 529ms
2:  learn: 0.6518791    total: 201ms    remaining: 468ms
3:  learn: 0.6420412    total: 271ms    remaining: 406ms
4:  learn: 0.6300900    total: 341ms    remaining: 341ms
5:  learn: 0.6202904    total: 414ms    remaining: 276ms
6:  learn: 0.6090917    total: 484ms    remaining: 207ms
7:  learn: 0.6016861    total: 551ms    remaining: 138ms
8:  learn: 0.5888547    total: 621ms    remaining: 69ms
9:  learn: 0.5786878    total: 686ms    remaining: 0us
0:  learn: 0.6870159    total: 181ms    remaining: 12.5s
1:  learn: 0.6828002    total: 375ms    remaining: 12.8s
2:  learn: 0.6777351    total: 559ms    remaining: 12.5s
3:  learn: 0.6726369    total: 751ms    remaining: 12.4s
4:  learn: 0.6657397    total: 935ms    remaining: 12.2s
5:  learn: 0.6593904    total: 1.13s    remaining: 12s
6:  learn: 0.6543954    total: 1.34s    remaining: 12s
7:  learn: 0.6501775    total: 1.54s    remaining: 12s
8:  learn: 0.6455502    total: 1.74s    remaining: 11.8s
9:  learn: 0.6401297    total: 1.95s    remaining: 11.7s
10: learn: 0.6358096    total: 2.14s    remaining: 11.5s
11: learn: 0.6307778    total: 2.33s    remaining: 11.3s
12: learn: 0.6262099    total: 2.53s    remaining: 11.1s
13: learn: 0.6222219    total: 2.72s    remaining: 10.9s
14: learn: 0.6177531    total: 2.91s    remaining: 10.7s
15: learn: 0.6144299    total: 3.12s    remaining: 10.5s
16: learn: 0.6097591    total: 3.31s    remaining: 10.3s
17: learn: 0.6060434    total: 3.51s    remaining: 10.1s
18: learn: 0.6016303    total: 3.69s    remaining: 9.91s
19: learn: 0.5982357    total: 3.89s    remaining: 9.73s
20: learn: 0.5944408    total: 4.11s    remaining: 9.59s
21: learn: 0.5904644    total: 4.3s remaining: 9.39s
22: learn: 0.5864407    total: 4.5s remaining: 9.19s
23: learn: 0.5830545    total: 4.68s    remaining: 8.97s
24: learn: 0.5795563    total: 4.87s    remaining: 8.76s
25: learn: 0.5759693    total: 5.08s    remaining: 8.61s
26: learn: 0.5728544    total: 5.28s    remaining: 8.42s
27: learn: 0.5695674    total: 5.48s    remaining: 8.22s
28: learn: 0.5663685    total: 5.67s    remaining: 8.02s
29: learn: 0.5634423    total: 5.86s    remaining: 7.82s
30: learn: 0.5609288    total: 6.08s    remaining: 7.64s
31: learn: 0.5570840    total: 6.28s    remaining: 7.45s
32: learn: 0.5529839    total: 6.47s    remaining: 7.26s
33: learn: 0.5504068    total: 6.67s    remaining: 7.06s
34: learn: 0.5476205    total: 6.87s    remaining: 6.87s
35: learn: 0.5446545    total: 7.07s    remaining: 6.68s
36: learn: 0.5419831    total: 7.27s    remaining: 6.48s
37: learn: 0.5380515    total: 7.47s    remaining: 6.29s
38: learn: 0.5342235    total: 7.66s    remaining: 6.09s
39: learn: 0.5308639    total: 7.86s    remaining: 5.89s
40: learn: 0.5278329    total: 8.08s    remaining: 5.71s
41: learn: 0.5255189    total: 8.29s    remaining: 5.52s
42: learn: 0.5228636    total: 8.48s    remaining: 5.33s
43: learn: 0.5199856    total: 8.68s    remaining: 5.13s
44: learn: 0.5169885    total: 8.86s    remaining: 4.92s
45: learn: 0.5145012    total: 9.06s    remaining: 4.73s
46: learn: 0.5123452    total: 9.26s    remaining: 4.53s
47: learn: 0.5101961    total: 9.45s    remaining: 4.33s
48: learn: 0.5079208    total: 9.64s    remaining: 4.13s
49: learn: 0.5054776    total: 9.83s    remaining: 3.93s
50: learn: 0.5032262    total: 10s  remaining: 3.74s
51: learn: 0.5011123    total: 10.3s    remaining: 3.55s
52: learn: 0.4990094    total: 10.5s    remaining: 3.35s
53: learn: 0.4964270    total: 10.7s    remaining: 3.16s
54: learn: 0.4943683    total: 10.9s    remaining: 2.96s
55: learn: 0.4922463    total: 11s  remaining: 2.76s
56: learn: 0.4904286    total: 11.2s    remaining: 2.56s
57: learn: 0.4879954    total: 11.4s    remaining: 2.36s
58: learn: 0.4860014    total: 11.6s    remaining: 2.17s
59: learn: 0.4842943    total: 11.8s    remaining: 1.97s
60: learn: 0.4824070    total: 12s  remaining: 1.78s
61: learn: 0.4800439    total: 12.2s    remaining: 1.58s
62: learn: 0.4783785    total: 12.5s    remaining: 1.38s
63: learn: 0.4759009    total: 12.6s    remaining: 1.19s
64: learn: 0.4739579    total: 12.8s    remaining: 986ms
65: learn: 0.4718904    total: 13s  remaining: 790ms
66: learn: 0.4704376    total: 13.2s    remaining: 592ms
67: learn: 0.4687568    total: 13.4s    remaining: 394ms
68: learn: 0.4675015    total: 13.6s    remaining: 197ms
69: learn: 0.4661309    total: 13.8s    remaining: 0us
0:  learn: 0.6867160    total: 188ms    remaining: 13s
1:  learn: 0.6817983    total: 375ms    remaining: 12.8s
2:  learn: 0.6763436    total: 564ms    remaining: 12.6s
3:  learn: 0.6707665    total: 745ms    remaining: 12.3s
4:  learn: 0.6655522    total: 943ms    remaining: 12.3s
5:  learn: 0.6595188    total: 1.15s    remaining: 12.2s
6:  learn: 0.6541515    total: 1.33s    remaining: 12s
7:  learn: 0.6491243    total: 1.53s    remaining: 11.9s
8:  learn: 0.6447512    total: 1.72s    remaining: 11.7s
9:  learn: 0.6392623    total: 1.92s    remaining: 11.5s
10: learn: 0.6346267    total: 2.11s    remaining: 11.3s
11: learn: 0.6298721    total: 2.31s    remaining: 11.2s
12: learn: 0.6248463    total: 2.51s    remaining: 11s
13: learn: 0.6205205    total: 2.69s    remaining: 10.8s
14: learn: 0.6170393    total: 2.88s    remaining: 10.6s
15: learn: 0.6130706    total: 3.07s    remaining: 10.4s
16: learn: 0.6087552    total: 3.27s    remaining: 10.2s
17: learn: 0.6044712    total: 3.47s    remaining: 10s
18: learn: 0.6013824    total: 3.67s    remaining: 9.85s
19: learn: 0.5975130    total: 3.86s    remaining: 9.66s
20: learn: 0.5936635    total: 4.08s    remaining: 9.52s
21: learn: 0.5902263    total: 4.28s    remaining: 9.34s
22: learn: 0.5872904    total: 4.5s remaining: 9.2s
23: learn: 0.5843652    total: 4.72s    remaining: 9.05s
24: learn: 0.5793123    total: 4.92s    remaining: 8.86s
25: learn: 0.5761650    total: 5.13s    remaining: 8.69s
26: learn: 0.5729976    total: 5.33s    remaining: 8.49s
27: learn: 0.5698271    total: 5.54s    remaining: 8.3s
28: learn: 0.5652784    total: 5.73s    remaining: 8.1s
29: learn: 0.5622549    total: 5.94s    remaining: 7.92s
30: learn: 0.5596275    total: 6.14s    remaining: 7.73s
31: learn: 0.5564927    total: 6.34s    remaining: 7.53s
32: learn: 0.5535853    total: 6.53s    remaining: 7.32s
33: learn: 0.5510388    total: 6.72s    remaining: 7.12s
34: learn: 0.5488515    total: 6.92s    remaining: 6.92s
35: learn: 0.5457913    total: 7.12s    remaining: 6.73s
36: learn: 0.5426674    total: 7.32s    remaining: 6.53s
37: learn: 0.5399439    total: 7.52s    remaining: 6.33s
38: learn: 0.5369225    total: 7.71s    remaining: 6.13s
39: learn: 0.5336043    total: 7.91s    remaining: 5.93s
40: learn: 0.5303811    total: 8.11s    remaining: 5.74s
41: learn: 0.5277906    total: 8.31s    remaining: 5.54s
42: learn: 0.5240578    total: 8.51s    remaining: 5.34s
43: learn: 0.5219273    total: 8.71s    remaining: 5.15s
44: learn: 0.5193463    total: 8.9s remaining: 4.94s
45: learn: 0.5172282    total: 9.1s remaining: 4.75s
46: learn: 0.5144832    total: 9.3s remaining: 4.55s
47: learn: 0.5121899    total: 9.49s    remaining: 4.35s
48: learn: 0.5101026    total: 9.68s    remaining: 4.15s
49: learn: 0.5079813    total: 9.87s    remaining: 3.95s
50: learn: 0.5051492    total: 10.1s    remaining: 3.75s
51: learn: 0.5027441    total: 10.3s    remaining: 3.56s
52: learn: 0.5009599    total: 10.5s    remaining: 3.36s
53: learn: 0.4981433    total: 10.7s    remaining: 3.16s
54: learn: 0.4962993    total: 10.8s    remaining: 2.96s
55: learn: 0.4936682    total: 11s  remaining: 2.76s
56: learn: 0.4916918    total: 11.2s    remaining: 2.56s
57: learn: 0.4891126    total: 11.4s    remaining: 2.36s
58: learn: 0.4869325    total: 11.6s    remaining: 2.17s
59: learn: 0.4848522    total: 11.8s    remaining: 1.97s
60: learn: 0.4829020    total: 12s  remaining: 1.77s
61: learn: 0.4812473    total: 12.2s    remaining: 1.58s
62: learn: 0.4795380    total: 12.4s    remaining: 1.38s
63: learn: 0.4775164    total: 12.6s    remaining: 1.18s
64: learn: 0.4757711    total: 12.8s    remaining: 985ms
65: learn: 0.4738938    total: 13s  remaining: 789ms
66: learn: 0.4717643    total: 13.2s    remaining: 593ms
67: learn: 0.4699461    total: 13.5s    remaining: 396ms
68: learn: 0.4679298    total: 13.7s    remaining: 198ms
69: learn: 0.4661554    total: 13.9s    remaining: 0us
0:  learn: 0.6856073    total: 185ms    remaining: 12.8s
1:  learn: 0.6813658    total: 369ms    remaining: 12.5s
2:  learn: 0.6763579    total: 565ms    remaining: 12.6s
3:  learn: 0.6714679    total: 763ms    remaining: 12.6s
4:  learn: 0.6660735    total: 973ms    remaining: 12.6s
5:  learn: 0.6590186    total: 1.16s    remaining: 12.4s
6:  learn: 0.6539830    total: 1.36s    remaining: 12.2s
7:  learn: 0.6491584    total: 1.55s    remaining: 12s
8:  learn: 0.6447890    total: 1.75s    remaining: 11.8s
9:  learn: 0.6392684    total: 1.96s    remaining: 11.8s
10: learn: 0.6345571    total: 2.16s    remaining: 11.6s
11: learn: 0.6311585    total: 2.37s    remaining: 11.4s
12: learn: 0.6257382    total: 2.73s    remaining: 12s
13: learn: 0.6210128    total: 3.08s    remaining: 12.3s
14: learn: 0.6167537    total: 3.33s    remaining: 12.2s
15: learn: 0.6117552    total: 3.54s    remaining: 11.9s
16: learn: 0.6068090    total: 3.76s    remaining: 11.7s
17: learn: 0.6026308    total: 3.97s    remaining: 11.5s
18: learn: 0.5994881    total: 4.17s    remaining: 11.2s
19: learn: 0.5950413    total: 4.37s    remaining: 10.9s
20: learn: 0.5911287    total: 4.56s    remaining: 10.6s
21: learn: 0.5879195    total: 4.78s    remaining: 10.4s
22: learn: 0.5846708    total: 4.98s    remaining: 10.2s
23: learn: 0.5808136    total: 5.18s    remaining: 9.94s
24: learn: 0.5771825    total: 5.38s    remaining: 9.68s
25: learn: 0.5729025    total: 5.57s    remaining: 9.42s
26: learn: 0.5697713    total: 5.78s    remaining: 9.21s
27: learn: 0.5665399    total: 5.99s    remaining: 8.98s
28: learn: 0.5630845    total: 6.18s    remaining: 8.74s
29: learn: 0.5605371    total: 6.38s    remaining: 8.51s
30: learn: 0.5576014    total: 6.58s    remaining: 8.29s
31: learn: 0.5548633    total: 6.8s remaining: 8.07s
32: learn: 0.5512260    total: 7s   remaining: 7.85s
33: learn: 0.5485768    total: 7.19s    remaining: 7.61s
34: learn: 0.5459721    total: 7.39s    remaining: 7.39s
35: learn: 0.5433590    total: 7.58s    remaining: 7.16s
36: learn: 0.5404806    total: 7.79s    remaining: 6.95s
37: learn: 0.5376623    total: 8s   remaining: 6.74s
38: learn: 0.5352702    total: 8.22s    remaining: 6.53s
39: learn: 0.5318593    total: 8.43s    remaining: 6.33s
40: learn: 0.5287440    total: 8.65s    remaining: 6.12s
41: learn: 0.5263023    total: 8.85s    remaining: 5.9s
42: learn: 0.5238787    total: 9.04s    remaining: 5.68s
43: learn: 0.5211830    total: 9.24s    remaining: 5.46s
44: learn: 0.5192002    total: 9.44s    remaining: 5.24s
45: learn: 0.5169270    total: 9.64s    remaining: 5.03s
46: learn: 0.5140122    total: 9.86s    remaining: 4.82s
47: learn: 0.5117085    total: 10.1s    remaining: 4.61s
48: learn: 0.5090780    total: 10.3s    remaining: 4.4s
49: learn: 0.5069044    total: 10.5s    remaining: 4.18s
50: learn: 0.5048360    total: 10.6s    remaining: 3.96s
51: learn: 0.5030125    total: 10.8s    remaining: 3.75s
52: learn: 0.5011650    total: 11s  remaining: 3.54s
53: learn: 0.4981879    total: 11.2s    remaining: 3.32s
54: learn: 0.4960101    total: 11.4s    remaining: 3.11s
55: learn: 0.4932337    total: 11.6s    remaining: 2.89s
56: learn: 0.4912376    total: 11.8s    remaining: 2.69s
57: learn: 0.4887712    total: 12s  remaining: 2.48s
58: learn: 0.4868622    total: 12.2s    remaining: 2.27s
59: learn: 0.4853113    total: 12.4s    remaining: 2.06s
60: learn: 0.4832912    total: 12.6s    remaining: 1.85s
61: learn: 0.4806880    total: 12.8s    remaining: 1.65s
62: learn: 0.4787688    total: 13s  remaining: 1.44s
63: learn: 0.4769648    total: 13.2s    remaining: 1.23s
64: learn: 0.4750955    total: 13.4s    remaining: 1.03s
65: learn: 0.4730873    total: 13.5s    remaining: 821ms
66: learn: 0.4714787    total: 13.8s    remaining: 616ms
67: learn: 0.4698039    total: 14s  remaining: 411ms
68: learn: 0.4680462    total: 14.2s    remaining: 205ms
69: learn: 0.4665342    total: 14.4s    remaining: 0us
0:  learn: 0.6788336    total: 66.7ms   remaining: 1.27s
1:  learn: 0.6641919    total: 134ms    remaining: 1.21s
2:  learn: 0.6520778    total: 205ms    remaining: 1.16s
3:  learn: 0.6378013    total: 277ms    remaining: 1.11s
4:  learn: 0.6280943    total: 352ms    remaining: 1.06s
5:  learn: 0.6183372    total: 418ms    remaining: 975ms
6:  learn: 0.6045327    total: 486ms    remaining: 902ms
7:  learn: 0.5970878    total: 552ms    remaining: 828ms
8:  learn: 0.5855487    total: 656ms    remaining: 801ms
9:  learn: 0.5745267    total: 727ms    remaining: 727ms
10: learn: 0.5635858    total: 804ms    remaining: 658ms
11: learn: 0.5562680    total: 875ms    remaining: 583ms
12: learn: 0.5494956    total: 962ms    remaining: 518ms
13: learn: 0.5434607    total: 1.04s    remaining: 444ms
14: learn: 0.5366582    total: 1.11s    remaining: 369ms
15: learn: 0.5306572    total: 1.19s    remaining: 296ms
16: learn: 0.5249508    total: 1.26s    remaining: 222ms
17: learn: 0.5189863    total: 1.33s    remaining: 148ms
18: learn: 0.5142055    total: 1.4s remaining: 73.6ms
19: learn: 0.5078640    total: 1.49s    remaining: 0us
0:  learn: 0.6754175    total: 62.9ms   remaining: 1.2s
1:  learn: 0.6657609    total: 128ms    remaining: 1.15s
2:  learn: 0.6497813    total: 209ms    remaining: 1.19s
3:  learn: 0.6385731    total: 275ms    remaining: 1.1s
4:  learn: 0.6243063    total: 363ms    remaining: 1.09s
5:  learn: 0.6121638    total: 429ms    remaining: 1s
6:  learn: 0.6001453    total: 497ms    remaining: 923ms
7:  learn: 0.5917983    total: 561ms    remaining: 842ms
8:  learn: 0.5795535    total: 628ms    remaining: 768ms
9:  learn: 0.5703570    total: 703ms    remaining: 703ms
10: learn: 0.5607939    total: 775ms    remaining: 634ms
11: learn: 0.5511210    total: 843ms    remaining: 562ms
12: learn: 0.5461723    total: 910ms    remaining: 490ms
13: learn: 0.5394976    total: 979ms    remaining: 420ms
14: learn: 0.5329465    total: 1.05s    remaining: 351ms
15: learn: 0.5261160    total: 1.13s    remaining: 281ms
16: learn: 0.5212908    total: 1.19s    remaining: 210ms
17: learn: 0.5171155    total: 1.27s    remaining: 141ms
18: learn: 0.5100957    total: 1.44s    remaining: 75.8ms
19: learn: 0.5040100    total: 1.54s    remaining: 0us
0:  learn: 0.6736660    total: 66.6ms   remaining: 1.26s
1:  learn: 0.6632992    total: 139ms    remaining: 1.25s
2:  learn: 0.6518791    total: 227ms    remaining: 1.28s
3:  learn: 0.6420412    total: 302ms    remaining: 1.21s
4:  learn: 0.6300900    total: 392ms    remaining: 1.18s
5:  learn: 0.6202904    total: 462ms    remaining: 1.08s
6:  learn: 0.6090917    total: 547ms    remaining: 1.01s
7:  learn: 0.6016861    total: 613ms    remaining: 920ms
8:  learn: 0.5888547    total: 711ms    remaining: 869ms
9:  learn: 0.5786878    total: 780ms    remaining: 780ms
10: learn: 0.5690928    total: 853ms    remaining: 698ms
11: learn: 0.5605502    total: 923ms    remaining: 616ms
12: learn: 0.5552464    total: 1s   remaining: 541ms
13: learn: 0.5451069    total: 1.07s    remaining: 460ms
14: learn: 0.5388667    total: 1.16s    remaining: 388ms
15: learn: 0.5330966    total: 1.24s    remaining: 310ms
16: learn: 0.5254825    total: 1.31s    remaining: 232ms
17: learn: 0.5204746    total: 1.38s    remaining: 154ms
18: learn: 0.5137134    total: 1.48s    remaining: 78.1ms
19: learn: 0.5077613    total: 1.55s    remaining: 0us
0:  learn: 0.6661865    total: 83.7ms   remaining: 6.61s
1:  learn: 0.6442901    total: 182ms    remaining: 7.1s
2:  learn: 0.6296108    total: 283ms    remaining: 7.27s
3:  learn: 0.6041061    total: 387ms    remaining: 7.35s
4:  learn: 0.5846803    total: 482ms    remaining: 7.23s
5:  learn: 0.5694868    total: 570ms    remaining: 7.03s
6:  learn: 0.5564804    total: 653ms    remaining: 6.81s
7:  learn: 0.5362603    total: 753ms    remaining: 6.77s
8:  learn: 0.5249824    total: 860ms    remaining: 6.78s
9:  learn: 0.5156833    total: 947ms    remaining: 6.63s
10: learn: 0.5074373    total: 1.06s    remaining: 6.63s
11: learn: 0.4966087    total: 1.17s    remaining: 6.62s
12: learn: 0.4843238    total: 1.25s    remaining: 6.44s
13: learn: 0.4751487    total: 1.33s    remaining: 6.28s
14: learn: 0.4667416    total: 1.45s    remaining: 6.26s
15: learn: 0.4601999    total: 1.56s    remaining: 6.23s
16: learn: 0.4506270    total: 1.65s    remaining: 6.1s
17: learn: 0.4443271    total: 1.74s    remaining: 5.99s
18: learn: 0.4364742    total: 1.82s    remaining: 5.85s
19: learn: 0.4287354    total: 1.91s    remaining: 5.72s
20: learn: 0.4225620    total: 2.01s    remaining: 5.65s
21: learn: 0.4179536    total: 2.13s    remaining: 5.6s
22: learn: 0.4146607    total: 2.21s    remaining: 5.48s
23: learn: 0.4116159    total: 2.3s remaining: 5.38s
24: learn: 0.4072502    total: 2.4s remaining: 5.27s
25: learn: 0.4033408    total: 2.5s remaining: 5.18s
26: learn: 0.3992202    total: 2.58s    remaining: 5.07s
27: learn: 0.3950707    total: 2.67s    remaining: 4.96s
28: learn: 0.3920537    total: 2.77s    remaining: 4.87s
29: learn: 0.3892721    total: 2.86s    remaining: 4.77s
30: learn: 0.3878469    total: 2.91s    remaining: 4.6s
31: learn: 0.3853206    total: 3.01s    remaining: 4.51s
32: learn: 0.3825435    total: 3.1s remaining: 4.42s
33: learn: 0.3797944    total: 3.2s remaining: 4.33s
34: learn: 0.3776274    total: 3.3s remaining: 4.25s
35: learn: 0.3749218    total: 3.4s remaining: 4.16s
36: learn: 0.3730313    total: 3.48s    remaining: 4.05s
37: learn: 0.3697761    total: 3.58s    remaining: 3.95s
38: learn: 0.3679260    total: 3.68s    remaining: 3.87s
39: learn: 0.3655824    total: 3.77s    remaining: 3.77s
40: learn: 0.3625396    total: 3.85s    remaining: 3.67s
41: learn: 0.3606557    total: 3.95s    remaining: 3.58s
42: learn: 0.3586780    total: 4.06s    remaining: 3.49s
43: learn: 0.3569128    total: 4.14s    remaining: 3.39s
44: learn: 0.3547151    total: 4.23s    remaining: 3.29s
45: learn: 0.3530752    total: 4.32s    remaining: 3.2s
46: learn: 0.3516675    total: 4.41s    remaining: 3.1s
47: learn: 0.3500730    total: 4.5s remaining: 3s
48: learn: 0.3482186    total: 4.6s remaining: 2.91s
49: learn: 0.3469220    total: 4.72s    remaining: 2.83s
50: learn: 0.3451109    total: 4.81s    remaining: 2.73s
51: learn: 0.3426953    total: 4.89s    remaining: 2.63s
52: learn: 0.3411557    total: 4.99s    remaining: 2.54s
53: learn: 0.3385266    total: 5.11s    remaining: 2.46s
54: learn: 0.3368309    total: 5.2s remaining: 2.36s
55: learn: 0.3338607    total: 5.31s    remaining: 2.27s
56: learn: 0.3305002    total: 5.39s    remaining: 2.18s
57: learn: 0.3288964    total: 5.51s    remaining: 2.09s
58: learn: 0.3273422    total: 5.61s    remaining: 2s
59: learn: 0.3262397    total: 5.7s remaining: 1.9s
60: learn: 0.3245780    total: 5.79s    remaining: 1.8s
61: learn: 0.3225558    total: 5.88s    remaining: 1.71s
62: learn: 0.3211791    total: 5.99s    remaining: 1.62s
63: learn: 0.3197995    total: 6.09s    remaining: 1.52s
64: learn: 0.3185145    total: 6.19s    remaining: 1.43s
65: learn: 0.3174419    total: 6.33s    remaining: 1.34s
66: learn: 0.3148381    total: 6.42s    remaining: 1.24s
67: learn: 0.3134987    total: 6.51s    remaining: 1.15s
68: learn: 0.3118657    total: 6.61s    remaining: 1.05s
69: learn: 0.3106146    total: 6.7s remaining: 957ms
70: learn: 0.3095362    total: 6.79s    remaining: 861ms
71: learn: 0.3084200    total: 6.88s    remaining: 765ms
72: learn: 0.3071627    total: 6.98s    remaining: 669ms
73: learn: 0.3056916    total: 7.07s    remaining: 573ms
74: learn: 0.3042122    total: 7.18s    remaining: 479ms
75: learn: 0.3030383    total: 7.27s    remaining: 383ms
76: learn: 0.3018889    total: 7.36s    remaining: 287ms
77: learn: 0.3005356    total: 7.45s    remaining: 191ms
78: learn: 0.2986284    total: 7.55s    remaining: 95.6ms
79: learn: 0.2973939    total: 7.65s    remaining: 0us
0:  learn: 0.6643290    total: 94.2ms   remaining: 7.45s
1:  learn: 0.6454704    total: 186ms    remaining: 7.27s
2:  learn: 0.6267988    total: 278ms    remaining: 7.13s
3:  learn: 0.6045659    total: 389ms    remaining: 7.4s
4:  learn: 0.5871720    total: 499ms    remaining: 7.49s
5:  learn: 0.5693765    total: 590ms    remaining: 7.28s
6:  learn: 0.5530513    total: 682ms    remaining: 7.12s
7:  learn: 0.5386130    total: 776ms    remaining: 6.98s
8:  learn: 0.5240635    total: 866ms    remaining: 6.83s
9:  learn: 0.5159297    total: 959ms    remaining: 6.71s
10: learn: 0.5085569    total: 1.07s    remaining: 6.74s
11: learn: 0.5003657    total: 1.17s    remaining: 6.64s
12: learn: 0.4913964    total: 1.3s remaining: 6.71s
13: learn: 0.4835945    total: 1.46s    remaining: 6.87s
14: learn: 0.4764090    total: 1.59s    remaining: 6.9s
15: learn: 0.4699141    total: 1.73s    remaining: 6.9s
16: learn: 0.4647210    total: 1.84s    remaining: 6.84s
17: learn: 0.4583236    total: 1.96s    remaining: 6.74s
18: learn: 0.4507625    total: 2.06s    remaining: 6.63s
19: learn: 0.4434956    total: 2.21s    remaining: 6.62s
20: learn: 0.4369761    total: 2.33s    remaining: 6.55s
21: learn: 0.4315171    total: 2.44s    remaining: 6.42s
22: learn: 0.4271261    total: 2.55s    remaining: 6.31s
23: learn: 0.4231828    total: 2.65s    remaining: 6.19s
24: learn: 0.4176501    total: 2.76s    remaining: 6.06s
25: learn: 0.4142370    total: 2.88s    remaining: 5.99s
26: learn: 0.4112396    total: 2.99s    remaining: 5.87s
27: learn: 0.4080917    total: 3.09s    remaining: 5.74s
28: learn: 0.4024743    total: 3.2s remaining: 5.63s
29: learn: 0.3995273    total: 3.3s remaining: 5.5s
30: learn: 0.3932476    total: 3.4s remaining: 5.37s
31: learn: 0.3922751    total: 3.45s    remaining: 5.18s
32: learn: 0.3900948    total: 3.54s    remaining: 5.05s
33: learn: 0.3873775    total: 3.64s    remaining: 4.92s
34: learn: 0.3846040    total: 3.76s    remaining: 4.84s
35: learn: 0.3808225    total: 3.9s remaining: 4.77s
36: learn: 0.3775607    total: 3.99s    remaining: 4.64s
37: learn: 0.3753230    total: 4.08s    remaining: 4.51s
38: learn: 0.3731365    total: 4.19s    remaining: 4.4s
39: learn: 0.3709968    total: 4.29s    remaining: 4.29s
40: learn: 0.3669675    total: 4.38s    remaining: 4.17s
41: learn: 0.3630283    total: 4.47s    remaining: 4.04s
42: learn: 0.3595623    total: 4.56s    remaining: 3.93s
43: learn: 0.3582165    total: 4.66s    remaining: 3.81s
44: learn: 0.3565920    total: 4.75s    remaining: 3.69s
45: learn: 0.3545405    total: 4.85s    remaining: 3.59s
46: learn: 0.3517645    total: 4.94s    remaining: 3.47s
47: learn: 0.3496004    total: 5.04s    remaining: 3.36s
48: learn: 0.3469607    total: 5.13s    remaining: 3.25s
49: learn: 0.3450449    total: 5.23s    remaining: 3.14s
50: learn: 0.3409540    total: 5.33s    remaining: 3.03s
51: learn: 0.3393811    total: 5.42s    remaining: 2.92s
52: learn: 0.3370172    total: 5.51s    remaining: 2.81s
53: learn: 0.3350501    total: 5.61s    remaining: 2.7s
54: learn: 0.3336867    total: 5.7s remaining: 2.59s
55: learn: 0.3326568    total: 5.79s    remaining: 2.48s
56: learn: 0.3305981    total: 5.9s remaining: 2.38s
57: learn: 0.3291170    total: 5.99s    remaining: 2.27s
58: learn: 0.3267238    total: 6.08s    remaining: 2.16s
59: learn: 0.3254530    total: 6.18s    remaining: 2.06s
60: learn: 0.3240547    total: 6.28s    remaining: 1.96s
61: learn: 0.3227733    total: 6.38s    remaining: 1.85s
62: learn: 0.3208503    total: 6.47s    remaining: 1.75s
63: learn: 0.3192012    total: 6.58s    remaining: 1.65s
64: learn: 0.3183069    total: 6.69s    remaining: 1.54s
65: learn: 0.3166566    total: 6.78s    remaining: 1.44s
66: learn: 0.3146433    total: 6.89s    remaining: 1.34s
67: learn: 0.3127463    total: 6.98s    remaining: 1.23s
68: learn: 0.3114390    total: 7.08s    remaining: 1.13s
69: learn: 0.3101015    total: 7.21s    remaining: 1.03s
70: learn: 0.3092973    total: 7.31s    remaining: 927ms
71: learn: 0.3082230    total: 7.41s    remaining: 824ms
72: learn: 0.3068674    total: 7.5s remaining: 719ms
73: learn: 0.3050543    total: 7.6s remaining: 617ms
74: learn: 0.3042546    total: 7.71s    remaining: 514ms
75: learn: 0.3032710    total: 7.81s    remaining: 411ms
76: learn: 0.3022287    total: 7.9s remaining: 308ms
77: learn: 0.3010763    total: 8s   remaining: 205ms
78: learn: 0.3000623    total: 8.14s    remaining: 103ms
79: learn: 0.2986710    total: 8.24s    remaining: 0us
0:  learn: 0.6562429    total: 81.1ms   remaining: 6.4s
1:  learn: 0.6335780    total: 169ms    remaining: 6.6s
2:  learn: 0.6186449    total: 262ms    remaining: 6.73s
3:  learn: 0.6001972    total: 377ms    remaining: 7.16s
4:  learn: 0.5860519    total: 472ms    remaining: 7.08s
5:  learn: 0.5703638    total: 562ms    remaining: 6.93s
6:  learn: 0.5581365    total: 649ms    remaining: 6.77s
7:  learn: 0.5453623    total: 767ms    remaining: 6.9s
8:  learn: 0.5348661    total: 861ms    remaining: 6.79s
9:  learn: 0.5213708    total: 951ms    remaining: 6.65s
10: learn: 0.5124921    total: 1.05s    remaining: 6.56s
11: learn: 0.5043456    total: 1.14s    remaining: 6.46s
12: learn: 0.4954577    total: 1.23s    remaining: 6.34s
13: learn: 0.4879698    total: 1.33s    remaining: 6.26s
14: learn: 0.4769865    total: 1.42s    remaining: 6.15s
15: learn: 0.4697842    total: 1.52s    remaining: 6.08s
16: learn: 0.4611150    total: 1.62s    remaining: 6.02s
17: learn: 0.4545033    total: 1.72s    remaining: 5.93s
18: learn: 0.4492953    total: 1.82s    remaining: 5.85s
19: learn: 0.4422798    total: 1.92s    remaining: 5.75s
20: learn: 0.4376908    total: 2.02s    remaining: 5.69s
21: learn: 0.4331509    total: 2.12s    remaining: 5.6s
22: learn: 0.4293290    total: 2.22s    remaining: 5.51s
23: learn: 0.4249046    total: 2.32s    remaining: 5.42s
24: learn: 0.4221392    total: 2.42s    remaining: 5.32s
25: learn: 0.4183462    total: 2.51s    remaining: 5.22s
26: learn: 0.4133569    total: 2.62s    remaining: 5.15s
27: learn: 0.4094390    total: 2.73s    remaining: 5.06s
28: learn: 0.4065924    total: 2.83s    remaining: 4.98s
29: learn: 0.4036392    total: 2.94s    remaining: 4.9s
30: learn: 0.3975599    total: 3.03s    remaining: 4.79s
31: learn: 0.3945920    total: 3.12s    remaining: 4.68s
32: learn: 0.3920131    total: 3.23s    remaining: 4.6s
33: learn: 0.3894455    total: 3.33s    remaining: 4.51s
34: learn: 0.3871030    total: 3.44s    remaining: 4.43s
35: learn: 0.3852647    total: 3.54s    remaining: 4.33s
36: learn: 0.3819382    total: 3.63s    remaining: 4.22s
37: learn: 0.3796748    total: 3.77s    remaining: 4.16s
38: learn: 0.3773800    total: 3.87s    remaining: 4.07s
39: learn: 0.3748525    total: 3.97s    remaining: 3.97s
40: learn: 0.3725744    total: 4.06s    remaining: 3.87s
41: learn: 0.3689606    total: 4.15s    remaining: 3.76s
42: learn: 0.3665860    total: 4.24s    remaining: 3.65s
43: learn: 0.3626355    total: 4.35s    remaining: 3.56s
44: learn: 0.3595666    total: 4.45s    remaining: 3.46s
45: learn: 0.3567451    total: 4.59s    remaining: 3.39s
46: learn: 0.3537801    total: 4.68s    remaining: 3.29s
47: learn: 0.3515180    total: 4.79s    remaining: 3.19s
48: learn: 0.3484534    total: 4.89s    remaining: 3.1s
49: learn: 0.3460548    total: 4.98s    remaining: 2.99s
50: learn: 0.3446576    total: 5.1s remaining: 2.9s
51: learn: 0.3422310    total: 5.18s    remaining: 2.79s
52: learn: 0.3402758    total: 5.27s    remaining: 2.69s
53: learn: 0.3388304    total: 5.36s    remaining: 2.58s
54: learn: 0.3369734    total: 5.47s    remaining: 2.49s
55: learn: 0.3352839    total: 5.58s    remaining: 2.39s
56: learn: 0.3338208    total: 5.69s    remaining: 2.3s
57: learn: 0.3321561    total: 5.8s remaining: 2.2s
58: learn: 0.3305014    total: 5.91s    remaining: 2.1s
59: learn: 0.3292382    total: 6s   remaining: 2s
60: learn: 0.3277298    total: 6.1s remaining: 1.9s
61: learn: 0.3264114    total: 6.19s    remaining: 1.8s
62: learn: 0.3242588    total: 6.32s    remaining: 1.7s
63: learn: 0.3229963    total: 6.4s remaining: 1.6s
64: learn: 0.3218746    total: 6.53s    remaining: 1.51s
65: learn: 0.3198690    total: 6.61s    remaining: 1.4s
66: learn: 0.3186453    total: 6.73s    remaining: 1.31s
67: learn: 0.3171265    total: 6.86s    remaining: 1.21s
68: learn: 0.3158560    total: 6.97s    remaining: 1.11s
69: learn: 0.3146361    total: 7.06s    remaining: 1.01s
70: learn: 0.3126894    total: 7.14s    remaining: 905ms
71: learn: 0.3115763    total: 7.23s    remaining: 803ms
72: learn: 0.3098584    total: 7.32s    remaining: 702ms
73: learn: 0.3088046    total: 7.42s    remaining: 601ms
74: learn: 0.3078307    total: 7.53s    remaining: 502ms
75: learn: 0.3053759    total: 7.65s    remaining: 403ms
76: learn: 0.3038827    total: 7.76s    remaining: 303ms
77: learn: 0.3023627    total: 7.87s    remaining: 202ms
78: learn: 0.3009514    total: 7.96s    remaining: 101ms
79: learn: 0.2996157    total: 8.07s    remaining: 0us
0:  learn: 0.6870159    total: 182ms    remaining: 1.64s
1:  learn: 0.6828002    total: 368ms    remaining: 1.47s
2:  learn: 0.6777351    total: 568ms    remaining: 1.32s
3:  learn: 0.6726369    total: 767ms    remaining: 1.15s
4:  learn: 0.6657397    total: 955ms    remaining: 955ms
5:  learn: 0.6593904    total: 1.16s    remaining: 772ms
6:  learn: 0.6543954    total: 1.35s    remaining: 581ms
7:  learn: 0.6501775    total: 1.56s    remaining: 390ms
8:  learn: 0.6455502    total: 1.78s    remaining: 198ms
9:  learn: 0.6401297    total: 1.99s    remaining: 0us
0:  learn: 0.6867160    total: 189ms    remaining: 1.7s
1:  learn: 0.6817983    total: 396ms    remaining: 1.58s
2:  learn: 0.6763436    total: 586ms    remaining: 1.37s
3:  learn: 0.6707665    total: 785ms    remaining: 1.18s
4:  learn: 0.6655522    total: 973ms    remaining: 973ms
5:  learn: 0.6595188    total: 1.18s    remaining: 784ms
6:  learn: 0.6541515    total: 1.4s remaining: 599ms
7:  learn: 0.6491243    total: 1.62s    remaining: 405ms
8:  learn: 0.6447512    total: 1.83s    remaining: 203ms
9:  learn: 0.6392623    total: 2.03s    remaining: 0us
0:  learn: 0.6856073    total: 173ms    remaining: 1.56s
1:  learn: 0.6813658    total: 441ms    remaining: 1.76s
2:  learn: 0.6763579    total: 659ms    remaining: 1.54s
3:  learn: 0.6714679    total: 863ms    remaining: 1.29s
4:  learn: 0.6660735    total: 1.06s    remaining: 1.06s
5:  learn: 0.6590186    total: 1.29s    remaining: 859ms
6:  learn: 0.6539830    total: 1.5s remaining: 645ms
7:  learn: 0.6491584    total: 1.7s remaining: 426ms
8:  learn: 0.6447890    total: 1.9s remaining: 211ms
9:  learn: 0.6392684    total: 2.1s remaining: 0us
0:  learn: 0.6665327    total: 93.2ms   remaining: 7.36s
1:  learn: 0.6464428    total: 194ms    remaining: 7.55s
2:  learn: 0.6274255    total: 294ms    remaining: 7.55s
3:  learn: 0.6038900    total: 396ms    remaining: 7.53s
4:  learn: 0.5853056    total: 500ms    remaining: 7.51s
5:  learn: 0.5711453    total: 598ms    remaining: 7.38s
6:  learn: 0.5518932    total: 699ms    remaining: 7.29s
7:  learn: 0.5371312    total: 812ms    remaining: 7.31s
8:  learn: 0.5246082    total: 911ms    remaining: 7.19s
9:  learn: 0.5108871    total: 1.01s    remaining: 7.11s
10: learn: 0.5017540    total: 1.12s    remaining: 7.04s
11: learn: 0.4914030    total: 1.24s    remaining: 7.01s
12: learn: 0.4829017    total: 1.33s    remaining: 6.87s
13: learn: 0.4753740    total: 1.43s    remaining: 6.74s
14: learn: 0.4684517    total: 1.57s    remaining: 6.82s
15: learn: 0.4595894    total: 1.67s    remaining: 6.69s
16: learn: 0.4516042    total: 1.78s    remaining: 6.59s
17: learn: 0.4452736    total: 1.88s    remaining: 6.48s
18: learn: 0.4382162    total: 1.99s    remaining: 6.4s
19: learn: 0.4326054    total: 2.1s remaining: 6.29s
20: learn: 0.4268372    total: 2.19s    remaining: 6.16s
21: learn: 0.4202041    total: 2.29s    remaining: 6.03s
22: learn: 0.4167642    total: 2.4s remaining: 5.95s
23: learn: 0.4122437    total: 2.53s    remaining: 5.9s
24: learn: 0.4093793    total: 2.68s    remaining: 5.89s
25: learn: 0.4039919    total: 2.79s    remaining: 5.79s
26: learn: 0.4004563    total: 2.9s remaining: 5.7s
27: learn: 0.3975197    total: 3s   remaining: 5.58s
28: learn: 0.3932144    total: 3.12s    remaining: 5.48s
29: learn: 0.3904852    total: 3.24s    remaining: 5.4s
30: learn: 0.3880708    total: 3.33s    remaining: 5.27s
31: learn: 0.3859222    total: 3.44s    remaining: 5.16s
32: learn: 0.3831005    total: 3.54s    remaining: 5.04s
33: learn: 0.3810144    total: 3.65s    remaining: 4.94s
34: learn: 0.3785504    total: 3.75s    remaining: 4.82s
35: learn: 0.3761624    total: 3.87s    remaining: 4.73s
36: learn: 0.3737088    total: 3.97s    remaining: 4.61s
37: learn: 0.3712488    total: 4.06s    remaining: 4.49s
38: learn: 0.3685311    total: 4.17s    remaining: 4.38s
39: learn: 0.3662991    total: 4.26s    remaining: 4.26s
40: learn: 0.3634880    total: 4.38s    remaining: 4.16s
41: learn: 0.3606838    total: 4.48s    remaining: 4.05s
42: learn: 0.3587348    total: 4.59s    remaining: 3.95s
43: learn: 0.3567398    total: 4.7s remaining: 3.84s
44: learn: 0.3548707    total: 4.83s    remaining: 3.75s
45: learn: 0.3520839    total: 4.92s    remaining: 3.64s
46: learn: 0.3492787    total: 5.03s    remaining: 3.53s
47: learn: 0.3467724    total: 5.14s    remaining: 3.43s
48: learn: 0.3430803    total: 5.24s    remaining: 3.31s
49: learn: 0.3411632    total: 5.36s    remaining: 3.22s
50: learn: 0.3396453    total: 5.46s    remaining: 3.1s
51: learn: 0.3377685    total: 5.57s    remaining: 3s
52: learn: 0.3362602    total: 5.69s    remaining: 2.9s
53: learn: 0.3349568    total: 5.79s    remaining: 2.79s
54: learn: 0.3336092    total: 5.9s remaining: 2.68s
55: learn: 0.3318186    total: 6s   remaining: 2.57s
56: learn: 0.3287766    total: 6.13s    remaining: 2.47s
57: learn: 0.3268753    total: 6.22s    remaining: 2.36s
58: learn: 0.3251920    total: 6.31s    remaining: 2.25s
59: learn: 0.3227292    total: 6.43s    remaining: 2.14s
60: learn: 0.3213152    total: 6.54s    remaining: 2.04s
61: learn: 0.3192716    total: 6.63s    remaining: 1.93s
62: learn: 0.3183261    total: 6.74s    remaining: 1.82s
63: learn: 0.3171622    total: 6.85s    remaining: 1.71s
64: learn: 0.3159563    total: 6.98s    remaining: 1.61s
65: learn: 0.3143367    total: 7.09s    remaining: 1.5s
66: learn: 0.3133019    total: 7.2s remaining: 1.4s
67: learn: 0.3122973    total: 7.31s    remaining: 1.29s
68: learn: 0.3103480    total: 7.4s remaining: 1.18s
69: learn: 0.3093429    total: 7.52s    remaining: 1.07s
70: learn: 0.3081943    total: 7.63s    remaining: 967ms
71: learn: 0.3067472    total: 7.73s    remaining: 859ms
72: learn: 0.3058445    total: 7.83s    remaining: 751ms
73: learn: 0.3047133    total: 7.93s    remaining: 643ms
74: learn: 0.3035925    total: 8.03s    remaining: 535ms
75: learn: 0.3022633    total: 8.13s    remaining: 428ms
76: learn: 0.3011552    total: 8.23s    remaining: 321ms
77: learn: 0.2998250    total: 8.35s    remaining: 214ms
78: learn: 0.2985347    total: 8.45s    remaining: 107ms
79: learn: 0.2977599    total: 8.55s    remaining: 0us
RandomizedSearchCV(cv=3, error_score=&amp;#39;raise&amp;#39;,
                   estimator=&amp;lt;catboost.core.CatBoostClassifier object at 0x000002835E232280&amp;gt;,
                   param_distributions={&amp;#39;depth&amp;#39;: [4, 5, 6, 7, 8, 9, 10],
                                        &amp;#39;iterations&amp;#39;: [10, 20, 30, 40, 50, 60,
                                                       70, 80, 90, 100],
                                        &amp;#39;learning_rate&amp;#39;: [0.01, 0.02, 0.03,
                                                          0.04, 0.05]},
                   random_state=123)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;After the tuning, I will print out the grid search result. The best
hyperparameter values we have are learning_rate = 0.05, iterations = 80,
and depth = 8. I will then let the machine learn from the data by
fitting the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot; Results from Grid Search &amp;quot; )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; Results from Grid Search &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;\n The best estimator across ALL searched params:\n&amp;quot;,Cat_random.best_estimator_)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 The best estimator across ALL searched params:
 &amp;lt;catboost.core.CatBoostClassifier object at 0x00000283655C1640&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;\n The best score across ALL searched params:\n&amp;quot;,Cat_random.best_score_)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 The best score across ALL searched params:
 0.8718094516047511&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;\n The best parameters across ALL searched params:\n&amp;quot;,Cat_random.best_params_)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 The best parameters across ALL searched params:
 {&amp;#39;learning_rate&amp;#39;: 0.05, &amp;#39;iterations&amp;#39;: 80, &amp;#39;depth&amp;#39;: 8}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;CBC_tuned = CatBoostClassifier(learning_rate = 0.05, iterations = 80, depth = 8, random_state=RANDOM_STATE)

CBC_tuned.fit(X_train_hybrid_ext, y_train_hybrid_ext, cat_features = cat_features)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0:  learn: 0.6665327    total: 76.9ms   remaining: 6.07s
1:  learn: 0.6464428    total: 162ms    remaining: 6.3s
2:  learn: 0.6274255    total: 252ms    remaining: 6.46s
3:  learn: 0.6038900    total: 338ms    remaining: 6.41s
4:  learn: 0.5853056    total: 434ms    remaining: 6.51s
5:  learn: 0.5711453    total: 541ms    remaining: 6.67s
6:  learn: 0.5518932    total: 642ms    remaining: 6.7s
7:  learn: 0.5371312    total: 735ms    remaining: 6.61s
8:  learn: 0.5246082    total: 822ms    remaining: 6.49s
9:  learn: 0.5108871    total: 927ms    remaining: 6.49s
10: learn: 0.5017540    total: 1.02s    remaining: 6.39s
11: learn: 0.4914030    total: 1.11s    remaining: 6.28s
12: learn: 0.4829017    total: 1.2s remaining: 6.18s
13: learn: 0.4753740    total: 1.32s    remaining: 6.22s
14: learn: 0.4684517    total: 1.44s    remaining: 6.23s
15: learn: 0.4595894    total: 1.54s    remaining: 6.16s
16: learn: 0.4516042    total: 1.63s    remaining: 6.05s
17: learn: 0.4452736    total: 1.77s    remaining: 6.11s
18: learn: 0.4382162    total: 1.87s    remaining: 5.99s
19: learn: 0.4326054    total: 1.98s    remaining: 5.93s
20: learn: 0.4268372    total: 2.08s    remaining: 5.86s
21: learn: 0.4202041    total: 2.21s    remaining: 5.83s
22: learn: 0.4167642    total: 2.31s    remaining: 5.71s
23: learn: 0.4122437    total: 2.42s    remaining: 5.64s
24: learn: 0.4093793    total: 2.54s    remaining: 5.6s
25: learn: 0.4039919    total: 2.65s    remaining: 5.5s
26: learn: 0.4004563    total: 2.75s    remaining: 5.39s
27: learn: 0.3975197    total: 2.84s    remaining: 5.28s
28: learn: 0.3932144    total: 2.94s    remaining: 5.17s
29: learn: 0.3904852    total: 3.04s    remaining: 5.06s
30: learn: 0.3880708    total: 3.13s    remaining: 4.95s
31: learn: 0.3859222    total: 3.23s    remaining: 4.84s
32: learn: 0.3831005    total: 3.32s    remaining: 4.73s
33: learn: 0.3810144    total: 3.42s    remaining: 4.63s
34: learn: 0.3785504    total: 3.54s    remaining: 4.55s
35: learn: 0.3761624    total: 3.63s    remaining: 4.44s
36: learn: 0.3737088    total: 3.72s    remaining: 4.33s
37: learn: 0.3712488    total: 3.83s    remaining: 4.24s
38: learn: 0.3685311    total: 3.94s    remaining: 4.14s
39: learn: 0.3662991    total: 4.06s    remaining: 4.06s
40: learn: 0.3634880    total: 4.17s    remaining: 3.96s
41: learn: 0.3606838    total: 4.28s    remaining: 3.87s
42: learn: 0.3587348    total: 4.39s    remaining: 3.77s
43: learn: 0.3567398    total: 4.49s    remaining: 3.67s
44: learn: 0.3548707    total: 4.59s    remaining: 3.57s
45: learn: 0.3520839    total: 4.7s remaining: 3.48s
46: learn: 0.3492787    total: 4.84s    remaining: 3.4s
47: learn: 0.3467724    total: 4.96s    remaining: 3.3s
48: learn: 0.3430803    total: 5.05s    remaining: 3.19s
49: learn: 0.3411632    total: 5.14s    remaining: 3.09s
50: learn: 0.3396453    total: 5.25s    remaining: 2.98s
51: learn: 0.3377685    total: 5.35s    remaining: 2.88s
52: learn: 0.3362602    total: 5.46s    remaining: 2.78s
53: learn: 0.3349568    total: 5.57s    remaining: 2.68s
54: learn: 0.3336092    total: 5.68s    remaining: 2.58s
55: learn: 0.3318186    total: 5.77s    remaining: 2.47s
56: learn: 0.3287766    total: 5.86s    remaining: 2.37s
57: learn: 0.3268753    total: 5.96s    remaining: 2.26s
58: learn: 0.3251920    total: 6.06s    remaining: 2.16s
59: learn: 0.3227292    total: 6.16s    remaining: 2.05s
60: learn: 0.3213152    total: 6.26s    remaining: 1.95s
61: learn: 0.3192716    total: 6.39s    remaining: 1.86s
62: learn: 0.3183261    total: 6.52s    remaining: 1.76s
63: learn: 0.3171622    total: 6.61s    remaining: 1.65s
64: learn: 0.3159563    total: 6.71s    remaining: 1.55s
65: learn: 0.3143367    total: 6.82s    remaining: 1.45s
66: learn: 0.3133019    total: 6.92s    remaining: 1.34s
67: learn: 0.3122973    total: 7.01s    remaining: 1.24s
68: learn: 0.3103480    total: 7.11s    remaining: 1.13s
69: learn: 0.3093429    total: 7.21s    remaining: 1.03s
70: learn: 0.3081943    total: 7.3s remaining: 926ms
71: learn: 0.3067472    total: 7.41s    remaining: 823ms
72: learn: 0.3058445    total: 7.52s    remaining: 721ms
73: learn: 0.3047133    total: 7.63s    remaining: 619ms
74: learn: 0.3035925    total: 7.73s    remaining: 515ms
75: learn: 0.3022633    total: 7.83s    remaining: 412ms
76: learn: 0.3011552    total: 7.95s    remaining: 310ms
77: learn: 0.2998250    total: 8.05s    remaining: 207ms
78: learn: 0.2985347    total: 8.17s    remaining: 103ms
79: learn: 0.2977599    total: 8.28s    remaining: 0us
&amp;lt;catboost.core.CatBoostClassifier object at 0x000002835E53F520&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We have 38 variables. We can use all of them, but we can also
further reduce them for to look for the most relevant variables to the
model. We can trim the variable with recursive feature elimination
(RFE), which is a feature selection method that fits the model and
remove the weakest feature (or predictor) iteratively until the optimal
number of features is found (&lt;a
href="https://link.springer.com/content/pdf/10.1023/A:1012487302797.pdf"&gt;Guyon
et al., 2022&lt;/a&gt;). Note that this process is entirely data-driven,
meaning that the machine decides which variable solely based on the
data, not the theory. In this post, I use a variant of RFE called RFE
with cross validation (RFECV) that selects the best subset of features
based on the cross-validation score of the model. RFECV is a
bit&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I have computed RFECV in advance to save time. Below is the
result. Performance of the model jumped at 20 features and fluctuated
after that, meaning that the optimal number of features is 20.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;rfecv_model = RFECV(estimator=CBC_tuned, step=1, cv=5 ,scoring=&amp;#39;accuracy&amp;#39;)
rfecv = rfecv_model.fit(X_train_hybrid_ext, y_train_hybrid_ext)

print(&amp;#39;Optimal number of features :&amp;#39;, rfecv.n_features_)
print(&amp;#39;Best features :&amp;#39;, X_train_hybrid_ext.columns[rfecv.support_])
print(&amp;#39;Original features :&amp;#39;, X_train_hybrid_ext.columns)

plt.figure(figsize=(10, 15), dpi=800)
plt.xlabel(&amp;quot;Number of features selected&amp;quot;)
plt.ylabel(&amp;quot;Cross validation score \n of number of selected features&amp;quot;)
plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://taridwong.github.io//posts/2022-08-06-edmrfecv.PNG" style="width:50.0%" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The optimal set of features are &lt;code&gt;X1MOMEDU&lt;/code&gt;,
&lt;code&gt;X1DADEDU&lt;/code&gt;, &lt;code&gt;X1MTHEFF&lt;/code&gt;, &lt;code&gt;X1SCIUTI&lt;/code&gt;,
&lt;code&gt;X1SCIEFF&lt;/code&gt;, &lt;code&gt;X1SCHOOLBEL&lt;/code&gt;,
&lt;code&gt;X1SCHOOLENG&lt;/code&gt;, &lt;code&gt;X1STUEDEXPCT&lt;/code&gt;,
&lt;code&gt;X1SCHOOLCLI&lt;/code&gt;, &lt;code&gt;X1COUPERCOU&lt;/code&gt;,
&lt;code&gt;X1COUPERPRI&lt;/code&gt;, &lt;code&gt;X3TGPA9TH&lt;/code&gt;, &lt;code&gt;S1NOHWDN&lt;/code&gt;,
&lt;code&gt;S1NOPAPER&lt;/code&gt;, &lt;code&gt;S1GETINTOCLG&lt;/code&gt;,
&lt;code&gt;S1WORKING&lt;/code&gt;, &lt;code&gt;S1HRMHOMEWK&lt;/code&gt;,
&lt;code&gt;S1HRSHOMEWK&lt;/code&gt;, &lt;code&gt;S1HROTHHOMWK&lt;/code&gt;,
&lt;code&gt;X4PSENRSTLV&lt;/code&gt;. I will reduce the number of variable based on
the RFECV result and create a training and a testing data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;X_hybrid_extreme_trim = X_hybrid_extreme[[&amp;#39;X1MOMEDU&amp;#39;, &amp;#39;X1DADEDU&amp;#39;, &amp;#39;X1MTHEFF&amp;#39;, &amp;#39;X1SCIUTI&amp;#39;, &amp;#39;X1SCIEFF&amp;#39;,&amp;#39;X1SCHOOLBEL&amp;#39;, &amp;#39;X1SCHOOLENG&amp;#39;, &amp;#39;X1STUEDEXPCT&amp;#39;, &amp;#39;X1SCHOOLCLI&amp;#39;,
&amp;#39;X1COUPERCOU&amp;#39;, &amp;#39;X1COUPERPRI&amp;#39;, &amp;#39;X3TGPA9TH&amp;#39;, &amp;#39;S1NOHWDN&amp;#39;, &amp;#39;S1NOPAPER&amp;#39;,
&amp;#39;S1GETINTOCLG&amp;#39;, &amp;#39;S1WORKING&amp;#39;, &amp;#39;S1HRMHOMEWK&amp;#39;, &amp;#39;S1HRSHOMEWK&amp;#39;, &amp;#39;S1HROTHHOMWK&amp;#39;, &amp;#39;X4PSENRSTLV&amp;#39;]]

X_train_hybrid_ext, X_test_hybrid_ext, y_train_hybrid_ext, y_test_hybrid_ext = train_test_split(X_hybrid_extreme_trim, y_hybrid_extreme, test_size = 0.30, random_state = RANDOM_STATE)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Then, I will fit the CatBoost model I created earlier with this new
data set and use it to predict the testing data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;cat_features_post_trim = [0, 1, 7, 12, 13, 14, 15,16, 17, 19]

CBC_tuned.fit(X_train_hybrid_ext, y_train_hybrid_ext, cat_features = cat_features_post_trim)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0:  learn: 0.6630968    total: 61.4ms   remaining: 4.85s
1:  learn: 0.6347421    total: 136ms    remaining: 5.3s
2:  learn: 0.6135360    total: 209ms    remaining: 5.36s
3:  learn: 0.5958557    total: 293ms    remaining: 5.57s
4:  learn: 0.5789939    total: 368ms    remaining: 5.53s
5:  learn: 0.5568907    total: 441ms    remaining: 5.44s
6:  learn: 0.5378680    total: 526ms    remaining: 5.49s
7:  learn: 0.5226018    total: 603ms    remaining: 5.43s
8:  learn: 0.5087498    total: 680ms    remaining: 5.36s
9:  learn: 0.5009270    total: 755ms    remaining: 5.28s
10: learn: 0.4841922    total: 856ms    remaining: 5.37s
11: learn: 0.4746953    total: 941ms    remaining: 5.33s
12: learn: 0.4681778    total: 1.03s    remaining: 5.33s
13: learn: 0.4612541    total: 1.13s    remaining: 5.32s
14: learn: 0.4532992    total: 1.23s    remaining: 5.31s
15: learn: 0.4450863    total: 1.31s    remaining: 5.24s
16: learn: 0.4375271    total: 1.4s remaining: 5.2s
17: learn: 0.4308885    total: 1.48s    remaining: 5.11s
18: learn: 0.4254128    total: 1.59s    remaining: 5.1s
19: learn: 0.4213425    total: 1.7s remaining: 5.09s
20: learn: 0.4177958    total: 1.78s    remaining: 5.01s
21: learn: 0.4131347    total: 1.87s    remaining: 4.93s
22: learn: 0.4089653    total: 1.97s    remaining: 4.87s
23: learn: 0.4055672    total: 2.07s    remaining: 4.84s
24: learn: 0.4030118    total: 2.17s    remaining: 4.77s
25: learn: 0.3997708    total: 2.26s    remaining: 4.69s
26: learn: 0.3945504    total: 2.35s    remaining: 4.61s
27: learn: 0.3909571    total: 2.44s    remaining: 4.54s
28: learn: 0.3874681    total: 2.53s    remaining: 4.45s
29: learn: 0.3842159    total: 2.62s    remaining: 4.36s
30: learn: 0.3817295    total: 2.71s    remaining: 4.28s
31: learn: 0.3793464    total: 2.8s remaining: 4.2s
32: learn: 0.3760956    total: 2.88s    remaining: 4.11s
33: learn: 0.3744325    total: 2.97s    remaining: 4.02s
34: learn: 0.3711698    total: 3.06s    remaining: 3.94s
35: learn: 0.3690638    total: 3.15s    remaining: 3.85s
36: learn: 0.3659461    total: 3.25s    remaining: 3.78s
37: learn: 0.3637044    total: 3.34s    remaining: 3.69s
38: learn: 0.3621688    total: 3.42s    remaining: 3.6s
39: learn: 0.3601269    total: 3.51s    remaining: 3.51s
40: learn: 0.3580019    total: 3.59s    remaining: 3.42s
41: learn: 0.3567636    total: 3.71s    remaining: 3.35s
42: learn: 0.3544761    total: 3.79s    remaining: 3.26s
43: learn: 0.3505973    total: 3.88s    remaining: 3.17s
44: learn: 0.3495149    total: 3.97s    remaining: 3.09s
45: learn: 0.3476525    total: 4.07s    remaining: 3.01s
46: learn: 0.3462577    total: 4.18s    remaining: 2.93s
47: learn: 0.3445400    total: 4.26s    remaining: 2.84s
48: learn: 0.3427600    total: 4.36s    remaining: 2.76s
49: learn: 0.3408402    total: 4.45s    remaining: 2.67s
50: learn: 0.3381710    total: 4.54s    remaining: 2.58s
51: learn: 0.3364197    total: 4.63s    remaining: 2.49s
52: learn: 0.3339518    total: 4.74s    remaining: 2.41s
53: learn: 0.3311018    total: 4.86s    remaining: 2.34s
54: learn: 0.3294152    total: 4.94s    remaining: 2.25s
55: learn: 0.3280687    total: 5.04s    remaining: 2.16s
56: learn: 0.3269127    total: 5.13s    remaining: 2.07s
57: learn: 0.3252681    total: 5.22s    remaining: 1.98s
58: learn: 0.3243089    total: 5.3s remaining: 1.89s
59: learn: 0.3212165    total: 5.39s    remaining: 1.8s
60: learn: 0.3201323    total: 5.49s    remaining: 1.71s
61: learn: 0.3189120    total: 5.58s    remaining: 1.62s
62: learn: 0.3178599    total: 5.67s    remaining: 1.53s
63: learn: 0.3169985    total: 5.75s    remaining: 1.44s
64: learn: 0.3154376    total: 5.87s    remaining: 1.35s
65: learn: 0.3142858    total: 5.95s    remaining: 1.26s
66: learn: 0.3130732    total: 6.05s    remaining: 1.17s
67: learn: 0.3115498    total: 6.14s    remaining: 1.08s
68: learn: 0.3106751    total: 6.24s    remaining: 995ms
69: learn: 0.3104327    total: 6.31s    remaining: 901ms
70: learn: 0.3094918    total: 6.34s    remaining: 803ms
71: learn: 0.3084260    total: 6.42s    remaining: 713ms
72: learn: 0.3070031    total: 6.5s remaining: 624ms
73: learn: 0.3052639    total: 6.61s    remaining: 536ms
74: learn: 0.3043254    total: 6.71s    remaining: 447ms
75: learn: 0.3038159    total: 6.76s    remaining: 356ms
76: learn: 0.3029093    total: 6.85s    remaining: 267ms
77: learn: 0.3014591    total: 6.93s    remaining: 178ms
78: learn: 0.2992314    total: 7.02s    remaining: 88.8ms
79: learn: 0.2984094    total: 7.12s    remaining: 0us
&amp;lt;catboost.core.CatBoostClassifier object at 0x000002835E53F520&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Results from classification report are satisfactory as seen from the
macro average of precision, recall, and f1-score. I also show the
receiver operating characteristic curve below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;from sklearn.metrics import roc_auc_score
from sklearn.metrics import classification_report

pred_ext = CBC_tuned.predict(X_test_hybrid_ext)

print(classification_report(y_test_hybrid_ext, pred_ext))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;              precision    recall  f1-score   support

           0       0.86      0.92      0.88      3351
           1       0.91      0.85      0.88      3433

    accuracy                           0.88      6784
   macro avg       0.88      0.88      0.88      6784
weighted avg       0.88      0.88      0.88      6784&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;roc_auc_score(y_test_hybrid_ext, pred_ext)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.8827700805886101&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;from sklearn import metrics

y_pred_proba_cat = CBC_tuned.predict_proba(X_test_hybrid_ext)[::,1]
fpr_cat, tpr_cat, _ = metrics.roc_curve(y_test_hybrid_ext,  y_pred_proba_cat)

auc_cat = metrics.roc_auc_score(y_test_hybrid_ext, y_pred_proba_cat)

#create ROC curve
plt.plot(fpr_cat,tpr_cat, label=&amp;quot;ROC_AUC=&amp;quot;+str(auc_cat.round(3)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;lt;matplotlib.lines.Line2D object at 0x000002835E591550&amp;gt;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.legend(loc=&amp;quot;lower right&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.legend.Legend object at 0x000002835E5915B0&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;True Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;True Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.xlabel(&amp;#39;False Positive Rate&amp;#39;)

# displaying the title&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;False Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Area Under Curve&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Area Under Curve&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file366c53d3573_files/figure-html/unnamed-chunk-23-9.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I also visualize feature importance of the model below. The most
impactful predictor to students’ high school dropout is their last year
GPA, followed by hours spent doing homework on typical school days, and
their self-efficacy in mathematics.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;from matplotlib.pyplot import figure

importances_cat = pd.Series(CBC_tuned.feature_importances_, index = X_hybrid_extreme_trim.columns)

sorted_importance_cat = importances_cat.sort_values()

#Horizontal bar plot
sorted_importance_cat.plot(kind=&amp;#39;barh&amp;#39;, color=&amp;#39;lightgreen&amp;#39;); 
plt.xlabel(&amp;#39;Feature Importance Score&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;Feature Importance Score&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;Features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;Features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Visualizing Important Features&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Visualizing Important Features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.rcParams[&amp;quot;figure.figsize&amp;quot;] = (8, 4)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file366c53d3573_files/figure-html/unnamed-chunk-24-11.png" width="1152" data-distill-preview=1 /&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The point of this post is to demonstrate how EDM can be used with
large-scale educational data to derive insights and potentially apply it
to practice. We started out with a lot of variables (4014), then we
reduce it based on the relevant theory to 67, based on missing data to
51, based on correlation coefficient to 38, and based on RFECV to
20.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We might want to select variables that are actionable for the
model to be meaningful. For example, saying that a student is likely to
dropout of their high school because of their socio-economic status
might not be as helpful because you cannot change their family income in
a matter of days or months. However, saying that their GPA and hours
spent on home work are influencing factors might allow students to
adjust their learning behavior.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With a meaningful model, an early warning system can be developed
to alert teachers of potential under-performing students for an early
intervention. However, I do not mean that results from the model is
perfect. It should be used in conjunction with other indicators such as
student record, parents’ observation, and behavior note. As education
goes online or semi-online, records of student data can be leveraged to
better understand them and ultimately benefit the teaching
practice.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>53861e41352b60b38bf303d190c7b01f</distill:md5>
      <category>R</category>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-08-06-edm</guid>
      <pubDate>Sat, 06 Aug 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-08-06-edm/edm_files/figure-html5/unnamed-chunk-24-11.png" medium="image" type="image/png" width="2304" height="1536"/>
    </item>
    <item>
      <title>Item Response Theory</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-05-15-irt</link>
      <description>


&lt;h2 id="introduction-to-item-response-theory"&gt;Introduction to Item
Response Theory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://taridwong.github.io/posts/2022-01-15-ctt/"&gt;My
previous post on Classical Test Theory (CTT)&lt;/a&gt; discussed how it has
several disadvantages that limit its interpretation to a certain group
of population and therefore reduces its utility to test development.
Specifically, generalizability of the test scores from CTT is quite
limited due to item/test dependency; item parameters such as item
difficulty, item discrimination, and reliability estimates are dependent
upon test scores, which are derived from a group of sample (&lt;a
href="https://www.jstor.org/stable/41219505?casa_token=PZl4ti6FHboAAAAA%3AEsuXjTVftCbh7pnyqQVhGv5cCEG9nAWMNV36MC7nXp5svyUaJhXBQG6jYtcmYNNf3S3b7HkhG48gjFTDo_ftMvXvm6vwRAuQwiEretfpuoKpwFiUQw&amp;amp;seq=1"&gt;DeVellis,
2006&lt;/a&gt;). If we change our sample, those parameters might change. Also,
If we administer two forms of the same test (i.e., form A and form B) to
the same examinee, we still cannot guarantee that they will obtain the
same score on both tests. Raw scores of a CTT-based test do not reflect
learning progress of an examinee as CTT-based scores are not comparable
across time (&lt;a
href="https://www.jstor.org/stable/41219505?casa_token=PZl4ti6FHboAAAAA%3AEsuXjTVftCbh7pnyqQVhGv5cCEG9nAWMNV36MC7nXp5svyUaJhXBQG6jYtcmYNNf3S3b7HkhG48gjFTDo_ftMvXvm6vwRAuQwiEretfpuoKpwFiUQw&amp;amp;seq=1"&gt;DeVellis,
2006&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Such limitations can be addressed by Item Response Theory (IRT).
IRT is able to link information from test items to examinee performance
on the same scale to provide information on the specific domain of
interest and ability of the examinee (θ) (&lt;a
href="https://www.routledge.com/Handbook-of-Item-Response-Theory-Volume-3-Applications/Linden/p/book/9780367221188"&gt;Hambleton
&amp;amp; Zenisky, 2018&lt;/a&gt;). The relationship between observable items and
examinee performance is explained through &lt;em&gt;Item Characteristic Curve
(ICC)&lt;/em&gt;, which explains the probability of getting an item(s)
correctly given the current ability level and parameter (&lt;a
href="https://doi.org/10.1111/j.1745-3992.1993.tb00543.x"&gt;Hambleton
&amp;amp; Jones, 1993&lt;/a&gt;). Therefore, IRT allows researchers to predict
examinees’ expected test score given their ability level. In this post,
I will be examining characteristics of test items based on the IRT
framework. The R packages I will be using are &lt;a
href="https://cran.r-project.org/web/packages/ltm/ltm.pdf"&gt;&lt;code&gt;ltm&lt;/code&gt;&lt;/a&gt;
and &lt;a
href="https://cran.r-project.org/web/packages/mirt/mirt.pdf"&gt;&lt;code&gt;mirt&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(ltm)
library(mirt)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;First, let’s load in a data set. I will be using the data from Law
School Admission Test (LSAT), N = 1000, 5 items. The data can be called
with &lt;code&gt;data(LSAT).&lt;/code&gt;As an initial step, we can use
&lt;code&gt;ltm::descript&lt;/code&gt; for descriptive statistics.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;data(LSAT)
ltm::descript(LSAT)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Descriptive statistics for the &amp;#39;LSAT&amp;#39; data-set

Sample:
 5 items and 1000 sample units; 0 missing values

Proportions for each level of response:
           0     1  logit
Item 1 0.076 0.924 2.4980
Item 2 0.291 0.709 0.8905
Item 3 0.447 0.553 0.2128
Item 4 0.237 0.763 1.1692
Item 5 0.130 0.870 1.9010


Frequencies of total scores:
     0  1  2   3   4   5
Freq 3 20 85 237 357 298


Point Biserial correlation with Total Score:
       Included Excluded
Item 1   0.3620   0.1128
Item 2   0.5668   0.1532
Item 3   0.6184   0.1728
Item 4   0.5344   0.1444
Item 5   0.4354   0.1216


Cronbach&amp;#39;s alpha:
                  value
All Items        0.2950
Excluding Item 1 0.2754
Excluding Item 2 0.2376
Excluding Item 3 0.2168
Excluding Item 4 0.2459
Excluding Item 5 0.2663


Pairwise Associations:
   Item i Item j p.value
1       1      5   0.565
2       1      4   0.208
3       3      5   0.113
4       2      4   0.059
5       1      2   0.028
6       2      5   0.009
7       1      3   0.003
8       4      5   0.002
9       3      4   7e-04
10      2      3   4e-04&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;From the output above, inspection of non significant results can be
used to reveal ‘problematic’ items in pairwise association. Latent
variable models assume that the high associations between items can be
explained by a set of latent variables, so any pair of items that is not
related to each other violates this assumption. Additionally, Item 1
seems to be the easiest item as seen from its highest proportion of
correct response.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="dichotomous-item"&gt;Dichotomous Item&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We will be performing IRT analyses on dichotomous items, which are
items that only have two possible scores of incorrect (0) and correct
(1). The three most common dichotomous IRT models are Rasch/1-parameter
logistics model (1PL), 2-parameter logistics model (2PL), and
3-parameter logistics model(3PL).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="rasch-model-1pl"&gt;Rasch Model (1PL)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We will fit the original Rasch model, which fixes the item
discrimination (aka &lt;em&gt;a&lt;/em&gt; parameter) of all items to 1 to the data.
The 1PL (also called &lt;em&gt;Rasch model&lt;/em&gt;) model describes test items in
terms of only one parameter, &lt;em&gt;item difficulty&lt;/em&gt; (aka &lt;em&gt;b&lt;/em&gt;
parameter). Item difficulty is simply how hard an item is (how high does
one’s latent ability level need to be in order to have a 50% chance of
getting the item right?). &lt;em&gt;b-parameter&lt;/em&gt; is estimated for each
item of the test.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;ltm::rasch()&lt;/code&gt; assumes equal a-parameter across
items with an estimated value. In order to impose the constraint = 1,
the &lt;code&gt;constraint&lt;/code&gt; argument is used. This argument accepts a
two-column matrix where the first column denotes the parameter and the
second column indicates the value at which the corresponding parameter
should be fixed.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_rasch &amp;lt;- rasch(LSAT, constraint = cbind(length(LSAT) + 1, 1))

summary(mod_rasch)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = cbind(length(LSAT) + 1, 1))

Model Summary:
   log.Lik      AIC      BIC
 -2473.054 4956.108 4980.646

Coefficients:
                value std.err   z.vals
Dffclt.Item 1 -2.8720  0.1287 -22.3066
Dffclt.Item 2 -1.0630  0.0821 -12.9458
Dffclt.Item 3 -0.2576  0.0766  -3.3635
Dffclt.Item 4 -1.3881  0.0865 -16.0478
Dffclt.Item 5 -2.2188  0.1048 -21.1660
Dscrmn         1.0000      NA       NA

Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 6.3e-05 
quasi-Newton: BFGS &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The results of the descriptive analysis are also validated by the
model fit, where items 3 and 1 are the most difficult and the easiest
respectively (the lower the &lt;em&gt;b&lt;/em&gt;-parameter value, the easier). The
parameter estimates can be transformed to probability estimates using
the &lt;code&gt;coef()&lt;/code&gt; method&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;coef(mod_rasch, prob = TRUE, order = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           Dffclt Dscrmn P(x=1|z=0)
Item 1 -2.8719712      1  0.9464434
Item 5 -2.2187785      1  0.9019232
Item 4 -1.3880588      1  0.8002822
Item 2 -1.0630294      1  0.7432690
Item 3 -0.2576109      1  0.5640489&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The last column denotes the probability of a positive response
(getting the item correctly) to the &lt;em&gt;i&lt;/em&gt;th item for the average
individual. The argument &lt;code&gt;order = TRUE&lt;/code&gt; indicates the output
to sort the items according to the difficulty estimates. In order to
check the fit of the model to the data, the argument
&lt;code&gt;GoF.rasch()&lt;/code&gt; and &lt;code&gt;margins()&lt;/code&gt; can be used. The
former argument performs a parametric Bootstrap goodness-of-fit test
using Pearson’s Chi-square statistics, while the latter examines the
two- and three-way chi-square residual analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;GoF.rasch(mod_rasch, B = 199) # B = Bootstrap sample&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Bootstrap Goodness-of-Fit using Pearson chi-squared

Call:
rasch(data = LSAT, constraint = cbind(length(LSAT) + 1, 1))

Tobs: 30.6 
# data-sets: 200 
p-value: 0.27 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Based on 200 data-points, the non significant p-value suggests an
acceptable fit of the model. The null hypothesis states that the
observed data and the model fit with each other (no differences). Now,
for two-way margin analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_rasch)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = cbind(length(LSAT) + 1, 1))

Fit on the Two-Way Margins

Response: (0,0)
  Item i Item j Obs   Exp (O-E)^2/E  
1      2      4  81 98.69      3.17  
2      1      5  12 18.45      2.25  
3      3      5  67 80.04      2.12  

Response: (1,0)
  Item i Item j Obs    Exp (O-E)^2/E  
1      3      5  63  51.62      2.51  
2      2      4 156 139.78      1.88  
3      3      4 108  99.42      0.74  

Response: (0,1)
  Item i Item j Obs    Exp (O-E)^2/E  
1      2      4 210 193.47      1.41  
2      2      3 135 125.07      0.79  
3      1      4  53  47.24      0.70  

Response: (1,1)
  Item i Item j Obs    Exp (O-E)^2/E  
1      2      4 553 568.06      0.40  
2      3      5 490 501.43      0.26  
3      2      3 418 427.98      0.23  &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;From the above output, using the 3.5 rule of thumb, the value of all
two-way combinations are below the cut-off (same way of how statistical
hypothesis works) and therefore indicate a good fit to the two-way
margins. Next, we will examine the fit to the three-way margins.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_rasch, type = &amp;quot;three-way&amp;quot;, nprint = 2) #nprint returns 2 highest residual values for each combinations&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = cbind(length(LSAT) + 1, 1))

Fit on the Three-Way Margins

Response: (0,0,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E    
1      2      3      4  48 66.07      4.94 ***
2      1      3      5   6 13.58      4.23 ***

Response: (1,0,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      2      4  70 82.01      1.76  
2      2      4      5  28 22.75      1.21  

Response: (0,1,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      2      5   3  7.73      2.90  
2      3      4      5  37 45.58      1.61  

Response: (1,1,0)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      3      4      5  48  36.91      3.33  
2      1      2      4 144 126.35      2.47  

Response: (0,0,1)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      3      5  41 34.58      1.19  
2      2      4      5  64 72.26      0.94  

Response: (1,0,1)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      1      2      4 190 174.87      1.31  
2      1      2      3 126 114.66      1.12  

Response: (0,1,1)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      2      5  42 34.35      1.70  
2      1      4      5  46 38.23      1.58  

Response: (1,1,1)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      3      4      5 397 416.73      0.93  
2      2      3      4 343 361.18      0.91  

&amp;#39;***&amp;#39; denotes a chi-squared residual greater than 3.5 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The three-way margins suggest a problematic fit for two triplets of
items, both containing item 3. We can try fitting the unconstrained
version of Rasch model (not fixing the &lt;em&gt;a&lt;/em&gt;-parameter to 1) to see
the difference. This time, no need for the &lt;code&gt;constraint&lt;/code&gt;
argument.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_1pl &amp;lt;- rasch(LSAT, constraint = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;After fitting the 1PL model, we will request for Item
Characteristics Curve (ICC), Item Information Curve (IIC), Test
Information Function (TIF), Latent Ability Curve of the examinees, and
Uni-dimensionality Plot of the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mfrow = c(2, 3))

plot(mod_1pl, type=c(&amp;quot;ICC&amp;quot;), 
     legend = TRUE, cx = &amp;quot;bottomright&amp;quot;, lwd = 2, 
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;1PL Item Characteristics Curve&amp;quot;)

plot(mod_1pl,type=c(&amp;quot;IIC&amp;quot;),
     legend = TRUE, cx = &amp;quot;topright&amp;quot;, lwd = 2,
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;1PL Item Information Curve&amp;quot;)

plot(mod_1pl,type=c(&amp;quot;IIC&amp;quot;),items=c(0), lwd = 2,
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;Test Information Function&amp;quot;)

plot(0:1, 0:1, type = &amp;quot;n&amp;quot;, ann = FALSE, axes = FALSE)
info1 &amp;lt;- information(mod_1pl, c(-4, 0)) #Item information on the negative area of theta (-4 to 0)
info2 &amp;lt;- information(mod_1pl, c(0, 4)) #Item information on the positive area of theta (0 to 4)
text(0.5, 0.5, labels = paste(&amp;quot;Total Information:&amp;quot;, round(info1$InfoTotal, 3),
                               &amp;quot;\n\nInformation in (-4, 0):&amp;quot;, round(info1$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info1$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;),
                               &amp;quot;\n\nInformation in (0, 4):&amp;quot;, round(info2$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info2$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;)))

theta.rasch&amp;lt;-ltm::factor.scores(mod_1pl)
summary(theta.rasch$score.dat$z1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-1.9104 -0.9594 -0.4660 -0.6867 -0.4660  0.5930 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(theta.rasch, main = &amp;quot;Latent Ability of the Examinee&amp;quot;)

unitest_1pl &amp;lt;- unidimTest(mod_1pl,LSAT)
plot(unitest_1pl, type = &amp;quot;b&amp;quot;, pch = 1:2, main = &amp;quot;Modified Parallel Analysis Plot&amp;quot;)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Real Data&amp;quot;, &amp;quot;Average Simulated Data&amp;quot;), lty = 1, 
    pch = 1:2, col = 1:2, bty = &amp;quot;n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file366c48bb7554_files/figure-html/unnamed-chunk-11-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The first plot is ICC of out 1PL model. ICC shows the
relationship between examinee ability (θ) and the probability of
examinees answering an item correctly based on their ability. On ICC,
item discrimination is represented by the steepness of the curve, and
item difficulty is represented by the position where the probability of
getting the item correct is 0.5.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Try comparing item 1 and item 3. For item 1, the point where the
probability of getting the item correctly is 0.5 matches with the
ability point -4 on the X-axis. However, for item 3, the point where the
probability of getting the item correctly is 0.5 matches with the
ability point 0 on the X-axis. In other words, you need more ability to
get item 3 correct than item 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The second plot is IIC. IIC shows how much “information” about
the latent trait ability an item can provide. Item information curves
peak at the point of difficulty value, where the item has the highest
discrimination and the probability of answering the item correctly is
0.5. To put it in plain language, a very difficult item will provide
very little information about persons with low ability (because the item
is already too hard), and very easy items will provide little
information about persons with high ability levels (because it is too
easy).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The third plot is TIF of the whole test. This is simply the sum
of the individual IICs above. The curves shows how much information this
test offers in terms of ability level of examinees. Ideally, we want a
test which provides fairly good coverage of a wide range of latent
ability levels. Otherwise, the test is only good at identifying a
limited range of examinees. The current TIF shows that more information
is yielded around examinees with -2 ability level. The test could use
more items for people with high ability (more difficult item is needed).
In particular, the amount of Test Information for ability levels in the
interval (-4 - 0) is almost 60%, and the item that seems to distinguish
between respondents with higher ability levels is item 3 as it is the
most difficult item.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Latent Ability Curve shows the distribution of examinee’s
latent ability level. The plot shows that most examinees are located
around 0 to 1 ability level.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;unitest_1pl&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Unidimensionality Check using Modified Parallel Analysis

Call:
rasch(data = LSAT, constraint = NULL)

Matrix of tertachoric correlations
       Item 1 Item 2 Item 3 Item 4 Item 5
Item 1 1.0000 0.1703 0.2275 0.1072 0.0665
Item 2 0.1703 1.0000 0.1891 0.1111 0.1724
Item 3 0.2275 0.1891 1.0000 0.1867 0.1055
Item 4 0.1072 0.1111 0.1867 1.0000 0.2009
Item 5 0.0665 0.1724 0.1055 0.2009 1.0000

Alternative hypothesis: the second eigenvalue of the observed data is substantially larger 
            than the second eigenvalue of data under the assumed IRT model

Second eigenvalue in the observed data: 0.2254
Average of second eigenvalues in Monte Carlo samples: 0.2561
Monte Carlo samples: 100
p-value: 0.6535&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The last plot is a test for unidimensionality of the test with
modified parallel analysis &lt;a
href="https://psycnet.apa.org/record/1983-31736-001"&gt;(Drasgpw &amp;amp;
Lissak, 1983&lt;/a&gt;). The output above shows that the result is
non-significant, meaning that the 1PL model fits the data well and we
are actually measuring a single trait here. The data is a law school
test, so it should be measuring contents about law, not maths or
English. The unidimensionality analysis shows that the test is measuring
what it is intended to measure.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;summary(mod_1pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = NULL)

Model Summary:
   log.Lik      AIC      BIC
 -2466.938 4945.875 4975.322

Coefficients:
                value std.err   z.vals
Dffclt.Item 1 -3.6153  0.3266 -11.0680
Dffclt.Item 2 -1.3224  0.1422  -9.3009
Dffclt.Item 3 -0.3176  0.0977  -3.2518
Dffclt.Item 4 -1.7301  0.1691 -10.2290
Dffclt.Item 5 -2.7802  0.2510 -11.0743
Dscrmn         0.7551  0.0694  10.8757

Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 2.9e-05 
quasi-Newton: BFGS &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;coef(mod_1pl, prob = TRUE, order = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           Dffclt    Dscrmn P(x=1|z=0)
Item 1 -3.6152665 0.7551347  0.9387746
Item 5 -2.7801716 0.7551347  0.8908453
Item 4 -1.7300903 0.7551347  0.7869187
Item 2 -1.3224208 0.7551347  0.7307844
Item 3 -0.3176306 0.7551347  0.5596777&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The output above suggests that the discrimination parameter of our
unconstrained model is different from 1, meaning that our constrained
and unconstrained Rasch models are different. The difference can be
tested with a likelihood ratio test using &lt;code&gt;anova().&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;anova(mod_rasch, mod_1pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 Likelihood Ratio Table
              AIC     BIC  log.Lik   LRT df p.value
mod_rasch 4956.11 4980.65 -2473.05                 
mod_1pl   4945.88 4975.32 -2466.94 12.23  1  &amp;lt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;By comparing model summary of the constrained and unconstrained
version, the latter is more suitable for the LSAT data due to its
smaller Akaike’s Information Criterion (AIC) and Bayesian Information
Criterion (BIC) values. AIC and BIC are measures of model performance
that account for model complexity. AIC is a measure that determines
which model fits the data better. The lower the score, the better fit
the model is. Similarly for BIC, the score measures complexity of the
model. BIC penalizes the model more for its complexity, meaning that
more complex models will have a worse (larger) score and will, in turn,
be less likely to be selected.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can double check the result by testing the unconstrained model
with the three-way margins, which yields a problematic fit with the
constrained model.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_1pl, type = &amp;quot;three-way&amp;quot;, nprint = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = NULL)

Fit on the Three-Way Margins

Response: (0,0,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      3      5   6  9.40      1.23  
2      3      4      5  30 25.85      0.67  

Response: (1,0,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      2      4      5  28 22.75      1.21  
2      2      3      4  81 74.44      0.58  

Response: (0,1,0)
  Item i Item j Item k Obs  Exp (O-E)^2/E  
1      1      2      5   3 7.58      2.76  
2      1      3      4   5 9.21      1.92  

Response: (1,1,0)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      2      4      5  51 57.49      0.73  
2      3      4      5  48 42.75      0.64  

Response: (0,0,1)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      1      3      5  41  33.07      1.90  
2      2      3      4 108 101.28      0.45  

Response: (1,0,1)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      2      3      4 210 218.91      0.36  
2      1      2      4 190 185.56      0.11  

Response: (0,1,1)
  Item i Item j Item k Obs   Exp (O-E)^2/E  
1      1      3      5  23 28.38      1.02  
2      1      4      5  46 42.51      0.29  

Response: (1,1,1)
  Item i Item j Item k Obs    Exp (O-E)^2/E  
1      1      2      4 520 526.36      0.08  
2      1      2      3 398 393.30      0.06  &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The new three-way margins suggests a good fit with the unconstrained
Rasch model. Finally, we investigate two more possible extensions of the
unconstrained Rasch model, the two-parameter logistic (2PL) model that
assumes a different discrimination parameter per item, and Rasch model
that incorporates a guessing parameter (3PL).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="parameter-logistics-model-2pl"&gt;2 Parameter Logistics Model
(2PL)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The 2PL model has the same equation as the 1PL model, but unlike
1PL, 2PL allows item discrimination and item difficulty to vary across
items instead of fixing it to a constant value.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The 2PL model can also be fitted with &lt;code&gt;ltm()&lt;/code&gt;.The
formula of &lt;code&gt;ltm()&lt;/code&gt; is two-sided, where its left is either a
data frame or a matrix, and its right allows only &lt;code&gt;z1&lt;/code&gt; and/or
&lt;code&gt;z2&lt;/code&gt;. Latent variables with &lt;code&gt;z2&lt;/code&gt; serves in the
case of interaction.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_2pl &amp;lt;- ltm(LSAT ~ z1)
summary(mod_2pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
ltm(formula = LSAT ~ z1)

Model Summary:
   log.Lik      AIC      BIC
 -2466.653 4953.307 5002.384

Coefficients:
                value std.err  z.vals
Dffclt.Item 1 -3.3597  0.8669 -3.8754
Dffclt.Item 2 -1.3696  0.3073 -4.4565
Dffclt.Item 3 -0.2799  0.0997 -2.8083
Dffclt.Item 4 -1.8659  0.4341 -4.2982
Dffclt.Item 5 -3.1236  0.8700 -3.5904
Dscrmn.Item 1  0.8254  0.2581  3.1983
Dscrmn.Item 2  0.7229  0.1867  3.8721
Dscrmn.Item 3  0.8905  0.2326  3.8281
Dscrmn.Item 4  0.6886  0.1852  3.7186
Dscrmn.Item 5  0.6575  0.2100  3.1306

Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 0.024 
quasi-Newton: BFGS &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The output above shows that the model estimated both item difficulty
and item discrimination. Next, we can try comparing our 1PL model with
the newly fitted 2PL model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;#compare
anova(mod_1pl, mod_2pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 Likelihood Ratio Table
            AIC     BIC  log.Lik  LRT df p.value
mod_1pl 4945.88 4975.32 -2466.94                
mod_2pl 4953.31 5002.38 -2466.65 0.57  4   0.967&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The comparison suggested no significant difference between both
models. Next, we ca nrequest for ICC, IIC, TIF, Latent Ability
Distribution, and Unidimensionality Plot like we did with the 1PL
model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mfrow = c(2, 3))

plot(mod_2pl, type=c(&amp;quot;ICC&amp;quot;), 
     legend = TRUE, cx = &amp;quot;bottomright&amp;quot;, lwd = 2, 
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;2PL Item Characteristics Curve&amp;quot;)

plot(mod_2pl,type=c(&amp;quot;IIC&amp;quot;),
     legend = TRUE, cx = &amp;quot;topright&amp;quot;, lwd = 2,
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;2PL Item Information Curve&amp;quot;)

plot(mod_2pl,type=c(&amp;quot;IIC&amp;quot;),items=c(0), lwd = 2,
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;Test Information Function&amp;quot;)

plot(0:1, 0:1, type = &amp;quot;n&amp;quot;, ann = FALSE, axes = FALSE)
info1_2pl &amp;lt;- information(mod_2pl, c(-4, 0)) #Item information on the negative area of theta (-4 to 0)
info2_2pl &amp;lt;- information(mod_2pl, c(0, 4)) #Item information on the positive area of theta (0 to 4)
text(0.5, 0.5, labels = paste(&amp;quot;Total Information:&amp;quot;, round(info1_2pl$InfoTotal, 3),
                               &amp;quot;\n\nInformation in (-4, 0):&amp;quot;, round(info1_2pl$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info1_2pl$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;),
                               &amp;quot;\n\nInformation in (0, 4):&amp;quot;, round(info2_2pl$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info2_2pl$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;)))

theta.2pl&amp;lt;-ltm::factor.scores(mod_2pl)
summary(theta.2pl$score.dat$z1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-1.8953 -1.0026 -0.5397 -0.6629 -0.3572  0.6064 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(theta.2pl, main = &amp;quot;Latent ability scores of the participants 2PL&amp;quot;)

unitest_2pl &amp;lt;- unidimTest(mod_2pl,LSAT)
plot(unitest_2pl, type = &amp;quot;b&amp;quot;, pch = 1:2)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Real Data&amp;quot;, &amp;quot;Average Simulated Data&amp;quot;), lty = 1, 
    pch = 1:2, col = 1:2, bty = &amp;quot;n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file366c48bb7554_files/figure-html/unnamed-chunk-18-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ICC and IIC of the two models are different, meaning that when we
allow item discrimination to vary, characteristics and yielded
information of each item also changed accordingly.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;unitest_2pl&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Unidimensionality Check using Modified Parallel Analysis

Call:
ltm(formula = LSAT ~ z1)

Matrix of tertachoric correlations
       Item 1 Item 2 Item 3 Item 4 Item 5
Item 1 1.0000 0.1703 0.2275 0.1072 0.0665
Item 2 0.1703 1.0000 0.1891 0.1111 0.1724
Item 3 0.2275 0.1891 1.0000 0.1867 0.1055
Item 4 0.1072 0.1111 0.1867 1.0000 0.2009
Item 5 0.0665 0.1724 0.1055 0.2009 1.0000

Alternative hypothesis: the second eigenvalue of the observed data is substantially larger 
            than the second eigenvalue of data under the assumed IRT model

Second eigenvalue in the observed data: 0.2254
Average of second eigenvalues in Monte Carlo samples: 0.2423
Monte Carlo samples: 100
p-value: 0.604&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The above results of unidimensionality testing also suggests that
the test measures only one construct. When considering two models
together, it might be more preferable for us to choose the 1PL model as
there is no difference between both 1PL and 2PL; however, by nature, 1PL
model is more simple and easier to explain comparing to 2PL. Let us try
fitting the data to a 3PL model just in case.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="parameter-logistics-model-3pl"&gt;3 Parameter Logistics Model
(3PL)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The 3PL model is very similar to the 2PL model; however, the model
includes an additional parameter: lower asymptote (also known as the
guessing parameter). Under this model, individuals with zero ability
have a nonzero chance of correctly answering any item just by guessing
randomly. A 3PL model can be fitted with &lt;code&gt;tpm()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_3pl &amp;lt;- tpm(LSAT, type = &amp;quot;latent.trait&amp;quot;, max.guessing = 1)

summary(mod_3pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
tpm(data = LSAT, type = &amp;quot;latent.trait&amp;quot;, max.guessing = 1)

Model Summary:
  log.Lik      AIC      BIC
 -2466.66 4963.319 5036.935

Coefficients:
                value std.err  z.vals
Gussng.Item 1  0.0374  0.8650  0.0432
Gussng.Item 2  0.0777  2.5282  0.0307
Gussng.Item 3  0.0118  0.2815  0.0419
Gussng.Item 4  0.0353  0.5769  0.0612
Gussng.Item 5  0.0532  1.5596  0.0341
Dffclt.Item 1 -3.2965  1.7788 -1.8532
Dffclt.Item 2 -1.1451  7.5166 -0.1523
Dffclt.Item 3 -0.2490  0.7527 -0.3308
Dffclt.Item 4 -1.7658  1.6162 -1.0925
Dffclt.Item 5 -2.9902  4.0606 -0.7364
Dscrmn.Item 1  0.8286  0.2877  2.8797
Dscrmn.Item 2  0.7604  1.3774  0.5520
Dscrmn.Item 3  0.9016  0.4190  2.1516
Dscrmn.Item 4  0.7007  0.2574  2.7219
Dscrmn.Item 5  0.6658  0.3282  2.0284

Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Optimizer: optim (BFGS)
Convergence: 0 
max(|grad|): 0.028 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The model summary above suggested that all three item parameters of
item discrimination (&lt;em&gt;a&lt;/em&gt;), item difficulty (&lt;em&gt;b&lt;/em&gt;), and item
guessing (&lt;em&gt;c&lt;/em&gt;) are allowed to vary. Like what we did, we can try
requesting for ICC, IIC, TIF, latent ability distribution, and
unidimensionality plot.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mfrow = c(2, 3))

plot(mod_3pl, type=c(&amp;quot;ICC&amp;quot;), 
     legend = TRUE, cx = &amp;quot;bottomright&amp;quot;, lwd = 2, 
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;3PL Item Characteristics Curve&amp;quot;)

plot(mod_3pl,type=c(&amp;quot;IIC&amp;quot;),
     legend = TRUE, cx = &amp;quot;topright&amp;quot;, lwd = 2,
     cex.main = 1.5, cex.lab = 1.3, cex = 0.6, main = &amp;quot;3PL Item Information Curve&amp;quot;)

plot(mod_3pl,type=c(&amp;quot;IIC&amp;quot;),items=c(0))

plot(0:1, 0:1, type = &amp;quot;n&amp;quot;, ann = FALSE, axes = FALSE)
info1_3pl &amp;lt;- information(mod_3pl, c(-4, 0)) #Item information on the negative area of theta (-4 to 0)
info2_3pl &amp;lt;- information(mod_3pl, c(0, 4)) #Item information on the positive area of theta (0 to 4)
text(0.5, 0.5, labels = paste(&amp;quot;Total Information:&amp;quot;, round(info1_3pl$InfoTotal, 3),
                               &amp;quot;\n\nInformation in (-4, 0):&amp;quot;, round(info1_3pl$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info1_3pl$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;),
                               &amp;quot;\n\nInformation in (0, 4):&amp;quot;, round(info2_3pl$InfoRange, 3),
                               paste(&amp;quot;(&amp;quot;, round(100 * info2_3pl$PropRange, 2), &amp;quot;%)&amp;quot;, sep = &amp;quot;&amp;quot;)))

theta.3pl&amp;lt;-ltm::factor.scores(mod_3pl)
summary(theta.3pl$score.dat$z1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-1.8706 -0.9992 -0.5368 -0.6584 -0.3590  0.6116 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;plot(theta.3pl, main = &amp;quot;Latent ability scores of the participants 3PL&amp;quot;)

unitest_3pl &amp;lt;- unidimTest(mod_3pl,LSAT)
plot(unitest_3pl, type = &amp;quot;b&amp;quot;, pch = 1:2)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Real Data&amp;quot;, &amp;quot;Average Simulated Data&amp;quot;), lty = 1, 
    pch = 1:2, col = 1:2, bty = &amp;quot;n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file366c48bb7554_files/figure-html/unnamed-chunk-21-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;unitest_3pl&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Unidimensionality Check using Modified Parallel Analysis

Call:
tpm(data = LSAT, type = &amp;quot;latent.trait&amp;quot;, max.guessing = 1)

Matrix of tertachoric correlations
       Item 1 Item 2 Item 3 Item 4 Item 5
Item 1 1.0000 0.1703 0.2275 0.1072 0.0665
Item 2 0.1703 1.0000 0.1891 0.1111 0.1724
Item 3 0.2275 0.1891 1.0000 0.1867 0.1055
Item 4 0.1072 0.1111 0.1867 1.0000 0.2009
Item 5 0.0665 0.1724 0.1055 0.2009 1.0000

Alternative hypothesis: the second eigenvalue of the observed data is substantially larger 
            than the second eigenvalue of data under the assumed IRT model

Second eigenvalue in the observed data: 0.2254
Average of second eigenvalues in Monte Carlo samples: 0.2495
Monte Carlo samples: 100
p-value: 0.5842&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The unidimensionality testing above suggested non-significant
result, meaning that the test measures one construct as intended.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;anova(mod_1pl, mod_3pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 Likelihood Ratio Table
            AIC     BIC  log.Lik  LRT df p.value
mod_1pl 4945.88 4975.32 -2466.94                
mod_3pl 4963.32 5036.94 -2466.66 0.56  9       1&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The comparison between the 1PL and the 3PL model also suggests no
statistical differences. The 3PL model also has larger AIC and BIC;
therefore, it is more preferable for us to use 1PL model with this data.
Finally, we can request for ability estimates for all response pattern
with the &lt;code&gt;factor.scores()&lt;/code&gt; function.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;factor.scores(mod_1pl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = NULL)

Scoring Method: Empirical Bayes

Factor-Scores for observed response patterns:
   Item 1 Item 2 Item 3 Item 4 Item 5 Obs     Exp     z1 se.z1
1       0      0      0      0      0   3   2.364 -1.910 0.790
2       0      0      0      0      1   6   5.468 -1.439 0.793
3       0      0      0      1      0   2   2.474 -1.439 0.793
4       0      0      0      1      1  11   8.249 -0.959 0.801
5       0      0      1      0      0   1   0.852 -1.439 0.793
6       0      0      1      0      1   1   2.839 -0.959 0.801
7       0      0      1      1      0   3   1.285 -0.959 0.801
8       0      0      1      1      1   4   6.222 -0.466 0.816
9       0      1      0      0      0   1   1.819 -1.439 0.793
10      0      1      0      0      1   8   6.063 -0.959 0.801
11      0      1      0      1      1  16  13.288 -0.466 0.816
12      0      1      1      0      1   3   4.574 -0.466 0.816
13      0      1      1      1      0   2   2.070 -0.466 0.816
14      0      1      1      1      1  15  14.749  0.049 0.836
15      1      0      0      0      0  10  10.273 -1.439 0.793
16      1      0      0      0      1  29  34.249 -0.959 0.801
17      1      0      0      1      0  14  15.498 -0.959 0.801
18      1      0      0      1      1  81  75.060 -0.466 0.816
19      1      0      1      0      0   3   5.334 -0.959 0.801
20      1      0      1      0      1  28  25.834 -0.466 0.816
21      1      0      1      1      0  15  11.690 -0.466 0.816
22      1      0      1      1      1  80  83.310  0.049 0.836
23      1      1      0      0      0  16  11.391 -0.959 0.801
24      1      1      0      0      1  56  55.171 -0.466 0.816
25      1      1      0      1      0  21  24.965 -0.466 0.816
26      1      1      0      1      1 173 177.918  0.049 0.836
27      1      1      1      0      0  11   8.592 -0.466 0.816
28      1      1      1      0      1  61  61.235  0.049 0.836
29      1      1      1      1      0  28  27.709  0.049 0.836
30      1      1      1      1      1 298 295.767  0.593 0.862&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;By default, &lt;code&gt;factor.scores()&lt;/code&gt; produces ability estimates
for the observed response patterns (every combination available); if
ability estimates are required for non observed or specific response
patterns, these could be specified using the &lt;code&gt;resp.patterns&lt;/code&gt;
argument.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;factor.scores(mod_1pl, resp.patterns = rbind(c(1,1,1,1,1), c(0,0,0,0,0)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
rasch(data = LSAT, constraint = NULL)

Scoring Method: Empirical Bayes

Factor-Scores for specified response patterns:
  Item 1 Item 2 Item 3 Item 4 Item 5 Obs     Exp     z1 se.z1
1      1      1      1      1      1 298 295.767  0.593 0.862
2      0      0      0      0      0   3   2.364 -1.910 0.790&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The specified response patterns above are for examinees who got
all items correctly and incorrectly respectively. The results suggested
that the examinee who got all item correctly has ability level of 0.50
and the examinee who got all item incorrectly has ability level of
-1.91.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The model we discussed so far, Rasch, 1PL,2PL, 3PL are all
suitable for dichotomous test items (True/False), but what if item
responses have more than 2 categories like in a survey (i.e., 1 =
Strongly disagree 2 = Disagree 3 = Agree 4 = Strongly Agree)? This is
when we use polytomous IRT models.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="polytomous-item"&gt;Polytomous Item&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The data we consider here comes from the Environment section of the
1990 British Social Attitudes Survey, N = 291, 6 items, 3 ordinal
response options. The data can be loaded with
&lt;code&gt;data(Environment)&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;data(Environment)
descript(Environment)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Descriptive statistics for the &amp;#39;Environment&amp;#39; data-set

Sample:
 6 items and 291 sample units; 0 missing values

Proportions for each level of response:
             very concerned slightly concerned not very concerned
LeadPetrol           0.6151             0.3265             0.0584
RiverSea             0.8007             0.1753             0.0241
RadioWaste           0.7457             0.1924             0.0619
AirPollution         0.6495             0.3196             0.0309
Chemicals            0.7491             0.1924             0.0584
Nuclear              0.5155             0.3265             0.1581


Frequencies of total scores:
      6  7  8  9 10 11 12 13 14 15 16 17 18
Freq 96 51 37 27 26 18 13  7  6  6  1  1  2


Cronbach&amp;#39;s alpha:
                        value
All Items              0.8215
Excluding LeadPetrol   0.8218
Excluding RiverSea     0.7990
Excluding RadioWaste   0.7767
Excluding AirPollution 0.7751
Excluding Chemicals    0.7790
Excluding Nuclear      0.8058


Pairwise Associations:
   Item i Item j p.value
1       1      2   0.001
2       1      3   0.001
3       1      4   0.001
4       1      5   0.001
5       1      6   0.001
6       2      3   0.001
7       2      4   0.001
8       2      5   0.001
9       2      6   0.001
10      3      4   0.001&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;From the descriptive output, the first response, “&lt;em&gt;very
concerned&lt;/em&gt;”, has the highest frequency. The p-values for the
pairwise associations indicate significant associations between all
items. An alternative method to explore the degree of association
between pairs of items can be done with &lt;code&gt;rcor.test()&lt;/code&gt; for
non-parametric correlation coefficients.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;rcor.test(Environment, method = &amp;quot;kendall&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
             LeadPetrol RiverSea RadioWaste AirPollution Chemicals
LeadPetrol    *****      0.385    0.260      0.457        0.305   
RiverSea     &amp;lt;0.001      *****    0.399      0.548        0.403   
RadioWaste   &amp;lt;0.001     &amp;lt;0.001    *****      0.506        0.623   
AirPollution &amp;lt;0.001     &amp;lt;0.001   &amp;lt;0.001      *****        0.504   
Chemicals    &amp;lt;0.001     &amp;lt;0.001   &amp;lt;0.001     &amp;lt;0.001        *****   
Nuclear      &amp;lt;0.001     &amp;lt;0.001   &amp;lt;0.001     &amp;lt;0.001       &amp;lt;0.001   
             Nuclear
LeadPetrol    0.279 
RiverSea      0.320 
RadioWaste    0.484 
AirPollution  0.382 
Chemicals     0.463 
Nuclear       ***** 

upper diagonal part contains correlation coefficient estimates 
lower diagonal part contains corresponding p-values&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;rcor.test()&lt;/code&gt; provides two options for nonparametric
calculation, Kendall’s &lt;em&gt;Tau&lt;/em&gt; and Spearman’s &lt;em&gt;rho&lt;/em&gt; in
&lt;code&gt;method&lt;/code&gt; argument. Initially, we will fit the partial credit
model (PCM), which is a Polytomous version of the Rasch model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="partial-credit-model"&gt;Partial Credit Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;PCM fixes the discrimination parameter of all item as 1 in the
same way as what Rasch’s model does. The threshold (or the parameter
that represents the trait level necessary for an examinee to have 50% to
pick a response category) of PCM is allowed to vary.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For the &lt;code&gt;gpcm()&lt;/code&gt; function, If
&lt;code&gt;constraint = "rasch"&lt;/code&gt;, then the discrimination parameter is
assumed equal for all items and fixed at one. If
&lt;code&gt;constraint = "1PL"&lt;/code&gt;, then the discrimination parameter βi is
assumed equal for all items but is estimated. Here, we are fixing all
discrimination parameter to 1.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_pcm_rasch &amp;lt;- gpcm(Environment, constraint = &amp;quot;rasch&amp;quot;)
summary(mod_pcm_rasch)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
gpcm(data = Environment, constraint = &amp;quot;rasch&amp;quot;)

Model Summary:
   log.Lik      AIC      BIC
 -1147.176 2318.351 2362.431

Coefficients:
$LeadPetrol
        value std.err z.value
Catgr.1 0.680   0.153   4.450
Catgr.2 2.785   0.292   9.541
Dscrmn  1.000      NA      NA

$RiverSea
        value std.err z.value
Catgr.1 1.822   0.180  10.149
Catgr.2 3.385   0.435   7.781
Dscrmn  1.000      NA      NA

$RadioWaste
        value std.err z.value
Catgr.1 1.542   0.174   8.879
Catgr.2 2.328   0.302   7.709
Dscrmn  1.000      NA      NA

$AirPollution
        value std.err z.value
Catgr.1 0.822   0.153   5.363
Catgr.2 3.517   0.376   9.343
Dscrmn  1.000      NA      NA

$Chemicals
        value std.err z.value
Catgr.1 1.555   0.174   8.949
Catgr.2 2.399   0.308   7.788
Dscrmn  1.000      NA      NA

$Nuclear
        value std.err z.value
Catgr.1 0.316   0.156   2.029
Catgr.2 1.498   0.208   7.218
Dscrmn  1.000      NA      NA


Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 0.0079 
optimizer: nlminb &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The model summary provides AIC and BIC of the model. Same as what we
did with our dichotomous data, we can request for parameter estimates of
each item.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;coef(mod_pcm_rasch, prob = TRUE, order = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;             Catgr.1 Catgr.2 Dscrmn
LeadPetrol     0.680   2.785      1
RiverSea       1.822   3.385      1
RadioWaste     1.542   2.328      1
AirPollution   0.822   3.517      1
Chemicals      1.555   2.399      1
Nuclear        0.316   1.498      1&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;In order to check the fit of the model to the data, the argument
&lt;code&gt;GoF.gpcm&lt;/code&gt; and &lt;code&gt;margins()&lt;/code&gt; can be used.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;GoF.gpcm(mod_pcm_rasch, B = 199) # B = Bootstrap sample&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Parametric Bootstrap Approximation to Pearson chi-squared Goodness-of-Fit Measure

Call:
gpcm(data = Environment, constraint = &amp;quot;rasch&amp;quot;)

Tobs: 1001.41 
# data-sets: 200 
p-value: 0.08 &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Based on 200 data-points, the non significant p-value suggests an
acceptable fit of the model. The null hypothesis states that the
observed data and the model fit with each other (no differences). We can
move on to the two-way margin analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_pcm_rasch)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
gpcm(data = Environment, constraint = &amp;quot;rasch&amp;quot;)

Fit on the Two-Way Margins

             LeadPetrol RiverSea RadioWaste AirPollution Chemicals
LeadPetrol   &amp;lt;NA&amp;gt;       35.47     3.04      34.43         7.41    
RiverSea     ***        &amp;lt;NA&amp;gt;     20.07      77.50        18.75    
RadioWaste                       &amp;lt;NA&amp;gt;       44.64        68.12    
AirPollution ***        ***      ***        &amp;lt;NA&amp;gt;         33.29    
Chemicals                        ***        ***          &amp;lt;NA&amp;gt;     
Nuclear                          ***                              
             Nuclear
LeadPetrol    4.71  
RiverSea     10.36  
RadioWaste   43.35  
AirPollution 16.56  
Chemicals    27.68  
Nuclear      &amp;lt;NA&amp;gt;   

&amp;#39;***&amp;#39; denotes pairs of items with lack-of-fit&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The upper diagonal part of the output contains the residuals, and
the lower diagonal part indicates the pairs for which the residuals
exceed the threshold value. The two-way margin analysis above suggests
problematic fit of the data with the PCM model, meaning that PCM might
not be suitable for this data. We can try using the next model, the
Graded Response Model (GRM).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="graded-response-model"&gt;Graded Response Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;GRM is the polytomous version of the 2PL model (&lt;a
href="https://www.frontiersin.org/articles/10.3389/feduc.2021.721963/full"&gt;Dai
et al., 2021&lt;/a&gt;). Despite able to constrain the discrimination
parameter, GRM works differently than PCM. PCM estimates separate
category response parameters for each item, while the GRM model further
assumes that the thresholds for category response are also equal across
items (&lt;a
href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4520411/#:~:text=The%20difference%20between%20the%20two,are%20also%20equal%20across%20items."&gt;Nguyen
et al., 2014&lt;/a&gt;). Initially, we can try fitting the constrained version
of Graded Response Model (GRM) that assumes equal &lt;em&gt;a&lt;/em&gt; parameter
across items (similar to Rasch model). The model is fitted by
&lt;code&gt;grm()&lt;/code&gt; as follows&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_grm &amp;lt;- grm(Environment, constrained = TRUE)
summary(mod_grm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment, constrained = TRUE)

Model Summary:
   log.Lik      AIC      BIC
 -1106.193 2238.386 2286.139

Coefficients:
$LeadPetrol
        value
Extrmt1 0.395
Extrmt2 1.988
Dscrmn  2.218

$RiverSea
        value
Extrmt1 1.060
Extrmt2 2.560
Dscrmn  2.218

$RadioWaste
        value
Extrmt1 0.832
Extrmt2 1.997
Dscrmn  2.218

$AirPollution
        value
Extrmt1 0.483
Extrmt2 2.448
Dscrmn  2.218

$Chemicals
        value
Extrmt1 0.855
Extrmt2 2.048
Dscrmn  2.218

$Nuclear
        value
Extrmt1 0.062
Extrmt2 1.266
Dscrmn  2.218


Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 0.0049 
quasi-Newton: BFGS &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;If standard errors for the parameter estimates are required, you can
add the argument &lt;code&gt;Hessian = T&lt;/code&gt; to the function
&lt;code&gt;grm()&lt;/code&gt;. Similarly to our dichotomous case, the fit of the
model can be checked using &lt;code&gt;margins()&lt;/code&gt; for two-way
margins.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_grm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment, constrained = TRUE)

Fit on the Two-Way Margins

             LeadPetrol RiverSea RadioWaste AirPollution Chemicals
LeadPetrol   -          10.03     9.98       5.19         7.85    
RiverSea                -         5.06      17.12         2.56    
RadioWaste                       -           6.78        20.60    
AirPollution                                -             4.49    
Chemicals                                                -        
Nuclear                                                           
             Nuclear
LeadPetrol   16.93  
RiverSea      7.14  
RadioWaste   12.09  
AirPollution  4.57  
Chemicals     3.85  
Nuclear      -      &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The output above looks good as there is no indication of poor fit.
Next, we will try with the three-way margins.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;margins(mod_grm, type = &amp;quot;three&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment, constrained = TRUE)

Fit on the Three-Way Margins

   Item i Item j Item k (O-E)^2/E  
1       1      2      3     28.52  
2       1      2      4     34.26  
3       1      2      5     29.91  
4       1      2      6     42.74  
5       1      3      4     33.03  
6       1      3      5     66.72  
7       1      3      6     65.31  
8       1      4      5     25.48  
9       1      4      6     34.46  
10      1      5      6     39.49  
11      2      3      4     29.63  
12      2      3      5     37.74  
13      2      3      6     32.50  
14      2      4      5     27.08  
15      2      4      6     36.77  
16      2      5      6     19.49  
17      3      4      5     38.99  
18      3      4      6     26.91  
19      3      5      6     39.62  
20      4      5      6     22.25  &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Both the two- and three-way residuals show a good fit of the
constrained model to the data, but checking the fit of the model in the
margins does not correspond to an overall goodness-of-fit test. As a
result, we will fit the unconstrained version of the GRM as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;mod_grm_unconstrained &amp;lt;- grm(Environment) #unconstrained GRM

summary(mod_grm_unconstrained)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment)

Model Summary:
   log.Lik      AIC      BIC
 -1090.404 2216.807 2282.927

Coefficients:
$LeadPetrol
        value
Extrmt1 0.487
Extrmt2 2.584
Dscrmn  1.378

$RiverSea
        value
Extrmt1 1.058
Extrmt2 2.499
Dscrmn  2.341

$RadioWaste
        value
Extrmt1 0.779
Extrmt2 1.793
Dscrmn  3.123

$AirPollution
        value
Extrmt1 0.457
Extrmt2 2.157
Dscrmn  3.283

$Chemicals
        value
Extrmt1 0.809
Extrmt2 1.868
Dscrmn  2.947

$Nuclear
        value
Extrmt1 0.073
Extrmt2 1.427
Dscrmn  1.761


Integration:
method: Gauss-Hermite
quadrature points: 21 

Optimization:
Convergence: 0 
max(|grad|): 0.003 
quasi-Newton: BFGS &lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We can use a likelihood ratio test to check if the unconstrained
version GRM is better than its constrained one.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;anova(mod_grm, mod_grm_unconstrained)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 Likelihood Ratio Table
                          AIC     BIC  log.Lik   LRT df p.value
mod_grm               2238.39 2286.14 -1106.19                 
mod_grm_unconstrained 2216.81 2282.93 -1090.40 31.58  5  &amp;lt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The likelihood ratio test indicates that the unconstrained GRM is
preferable for the Environment data. We can plot the Item Characteristic
Curve (ICC) of all 6 items, Item Information Curve (IIC), and Test
Information Curve (TIC) below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mar=c(2,6,2,2), mfrow = c(3, 3))

plot(mod_grm_unconstrained, lwd = 2, cex = 0.6, legend = TRUE, cx = &amp;quot;left&amp;quot;, 
     xlab = &amp;quot;Latent Trait&amp;quot;, cex.main = 0.7, cex.lab = 0.7, cex.axis = 1)

##############################################################

plot(mod_grm_unconstrained, type = &amp;quot;IIC&amp;quot;, lwd = 2, cex = 0.6, legend = TRUE, cx = &amp;quot;topleft&amp;quot;,
     xlab = &amp;quot;Latent Trait&amp;quot;, cex.main = 0.7, cex.lab = 0.7, cex.axis = 1)

plot(mod_grm_unconstrained, type = &amp;quot;IIC&amp;quot;, items = 0, lwd = 2, xlab = &amp;quot;Latent Trait&amp;quot;,cex.main = 0.7, cex.lab = 0.7, cex.axis = 1)

info3 &amp;lt;- information(mod_grm_unconstrained, c(-4, 0))
info4 &amp;lt;- information(mod_grm_unconstrained, c(0, 4))

text(-1.9, 8, labels = paste(&amp;quot;Information in (-4, 0):&amp;quot;,
                             paste(round(100 * info3$PropRange, 1), &amp;quot;%&amp;quot;, sep = &amp;quot;&amp;quot;),
                             &amp;quot;\n\nInformation in (0, 4):&amp;quot;,
                             paste(round(100 * info4$PropRange, 1), &amp;quot;%&amp;quot;, sep = &amp;quot;&amp;quot;)), cex = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file366c48bb7554_files/figure-html/unnamed-chunk-37-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;From the Item Characteristic Curve, we observe that there is low
probability of endorsing the the first option, “very concerned”, for
relatively high latent trait levels, which means that the questions
asked are not considered as major environmental issues by the
respondent.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Test Information Curve also tells us that the test provides
89% of the total information for high latent trait levels.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally, the Item Information Curve indicates that items in
LeadPetrol and Nuclear provide little information in the whole latent
trait continuum. We can check this in detail using
&lt;code&gt;information()&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;information(mod_grm_unconstrained, c(-4, 4))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment)

Total Information = 26.97
Information in (-4, 4) = 26.7 (98.97%)
Based on all the items&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;For item 1 and item 6&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;information(mod_grm_unconstrained, c(-4, 4), items = c(1, 6)) #for item 1 and 6&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
grm(data = Environment)

Total Information = 5.36
Information in (-4, 4) = 5.17 (96.38%)
Based on items 1, 6&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We observe that item 1 and 6 provide only the 5.36% of the total
information (from the total of 26.97); Thus, they could probably be
excluded from a similar future study. Finally, a useful comparison is to
plot the ICC of each response option separately.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r"&gt;&lt;code&gt;par(mar=c(2,6,2,2), mfrow = c(2, 2))

plot(mod_grm_unconstrained, category = 1, lwd = 2, cex = 0.7, legend = TRUE, cx = -4.5,
     cy = 0.85, xlab = &amp;quot;Latent Trait&amp;quot;, cex.main = 0.7, cex.lab = 0.7,
     cex.axis = 1)

for (ctg in 2:3) 
  {
  plot(mod_grm_unconstrained, category = ctg, lwd = 2, cex = 0.5, annot = FALSE,
      xlab = &amp;quot;Latent Trait&amp;quot;, cex.main = 0.7, cex.lab = 0.7,
      cex.axis = 1)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file366c48bb7554_files/figure-html/unnamed-chunk-40-1.png" width="672" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;From the plot, the response option for RadioWaste and Chemicals have
nearly identical characteristic curves for all categories, indicating
that these two items are probably regarded to have the same effect on
the environment.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;So far, we have discussed what IRT is, including its model for
dichotomous and polytomous test items. There are several models for us
to choose from, and each model has its parameter we can adjust to suit
our needs as well depending on how complex we want our model to be.
There is no right or wrong answer model selection. For example, if we
want the model to be as simple as possible with a dichotomous test (say,
a math test), we could go for Rasch model. If we want our model to be
more realistic, we may want to use 2PL or 3PL, which comes with expenses
such as the need for more sample size or more difficulty to fit the
model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, given how useful IRT is in analyzing test items with
several unique parameters, it doesn’t mean that we need to disregard the
concept of CTT in our practice. Both theories have its own contribution
and usefulness. For example, CTT is more practical to implement in the
classroom setting where there is small amount of students and there is
no need to investigate items at a deeper level. For example, if we want
to use a classroom assessment for formative purposes (i.e., a practice
quiz to help students prepare for the final exam), using CTT might be
sufficient. If we want to develop a large-scale test to measure students
at a national level, maybe IRT might be more appropriate to improve the
test with a more realistic model.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a
href="https://d1wqtxts1xzle7.cloudfront.net/47596077/ITEMS_Module_16-with-cover-page-v2.pdf?Expires=1652597267&amp;amp;Signature=dRiumBzQzxIiFeQvRCtF3cuBjpeaui-mWZXaskGDaQYlwenB7ZZVx0dBUxSnl47nyQnveBh0ZBHdY3IbzNnPHZqAhAOdIN~QIQVCfSQ11d0ZV3mef~kmo8nEzXj459sakTPeZCeTwZSVxklbbtIE52mzRfFItBw4ZF70kJMT9S3FC9YaI~lDDXN5YUbb4VL7ZgCH-lsBIthNbTSDUXxqGuwgeHgxKfotwPdlQRr5iLEEwlLkea2Fr1zow4uLrNg38yK31MdNg55m73x9iwJZA2s~6wlj9IfsWINVJlwPoIOjT5Zm0bIuyIfzGbUKRk~3AiXLjThGUjIi0wlc1Y6flQ__&amp;amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"&gt;Hambleton
and Jones (1993&lt;/a&gt;) did a really great job in comparing the difference
between CTT and IRT in their work. I have presented the table below for
your information.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;The Comparison between CTT and IRT Models (&lt;a
href="https://d1wqtxts1xzle7.cloudfront.net/47596077/ITEMS_Module_16-with-cover-page-v2.pdf?Expires=1652597267&amp;amp;Signature=dRiumBzQzxIiFeQvRCtF3cuBjpeaui-mWZXaskGDaQYlwenB7ZZVx0dBUxSnl47nyQnveBh0ZBHdY3IbzNnPHZqAhAOdIN~QIQVCfSQ11d0ZV3mef~kmo8nEzXj459sakTPeZCeTwZSVxklbbtIE52mzRfFItBw4ZF70kJMT9S3FC9YaI~lDDXN5YUbb4VL7ZgCH-lsBIthNbTSDUXxqGuwgeHgxKfotwPdlQRr5iLEEwlLkea2Fr1zow4uLrNg38yK31MdNg55m73x9iwJZA2s~6wlj9IfsWINVJlwPoIOjT5Zm0bIuyIfzGbUKRk~3AiXLjThGUjIi0wlc1Y6flQ__&amp;amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"&gt;Hambleton
&amp;amp; Jones, 1993&lt;/a&gt;, p.43)&lt;/caption&gt;
&lt;colgroup&gt;
&lt;col width="24%" /&gt;
&lt;col width="34%" /&gt;
&lt;col width="41%" /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th&gt;Area&lt;/th&gt;
&lt;th&gt;Classical Test Theory&lt;/th&gt;
&lt;th&gt;Item Response Theory&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;Model&lt;/td&gt;
&lt;td&gt;Linear&lt;/td&gt;
&lt;td&gt;Non-linear&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;Level&lt;/td&gt;
&lt;td&gt;Test&lt;/td&gt;
&lt;td&gt;Item&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;Assumptions&lt;/td&gt;
&lt;td&gt;Weak (i.e., easy to fit with test data)&lt;/td&gt;
&lt;td&gt;Strong (i.e., more difficult to meet with test data)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;Item-Ability Relationship&lt;/td&gt;
&lt;td&gt;Not Specified&lt;/td&gt;
&lt;td&gt;Item Characteristics Function&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;Examinee Ability&lt;/td&gt;
&lt;td&gt;Represented by test scores or estimated true scores&lt;/td&gt;
&lt;td&gt;Represented by latent ability (Theta/θ)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;Invariance of item and person statistics&lt;/td&gt;
&lt;td&gt;Unavailable / item and person parameters are sample dependent&lt;/td&gt;
&lt;td&gt;Item and person parameters are sample dependent if the model fits
the data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td&gt;Item Statistics&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;em&gt;p&lt;/em&gt; = item difficulty&lt;/p&gt;
&lt;p&gt;&lt;em&gt;r&lt;/em&gt; = item discrimination&lt;/p&gt;&lt;/td&gt;
&lt;td&gt;Item discrimination (&lt;em&gt;a&lt;/em&gt;), Item difficulty (&lt;em&gt;b&lt;/em&gt;),
guessing parameter (&lt;em&gt;c&lt;/em&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td&gt;Sample Size (for item parameter estimation)&lt;/td&gt;
&lt;td&gt;200-500&lt;/td&gt;
&lt;td&gt;More than 500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Similar to CTT, IRT can be used to develop tests, scales,
surveys, or other measurement tools. The model can analyze item-level
data of both dichotomous (i.e., exams with true/false) and polytomous
(i.e., surveys with no right/wrong answers) tests to provide information
on sensitivity of measurement across a range of latent trait. Knowing
information like item difficulty, item discrimination, and item guessing
is useful when building tests as we can examine which item is a good
item and which item is a not-so-useful one. For example, a test item
that is easy to guess might not be appropriate because everyone can do
it, so it doesn’t really measure anything.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IRT also allows us to put ability levels and difficulties of
items into the same scale to match the trait levels of a target
population. If we want a test to measure someone with “just enough”
knowledge to pass (say, a driver license exam), we can build a test to
measure people with low to medium knowledge. I mean, a taxi driver or a
racing driver know how to drive a car, and that is enough. However, if
we want to develop a test to select the best-of-the-best candidates for
scholarship selection, we might want to build a difficult test to
separate low-to-mid tier students to high performance students. Anyway,
that is all for this post. Thank you so much for reading this! Have a
good day!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>2530d599a3222722a8076a89ee971d4a</distill:md5>
      <category>R</category>
      <category>Psychometric</category>
      <guid>https://taridwong.github.io/posts/2022-05-15-irt</guid>
      <pubDate>Sun, 15 May 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-05-15-irt/IIC.png" medium="image" type="image/png" width="656" height="551"/>
    </item>
    <item>
      <title>Making Sense of Machine Learning with Explanable Artificial Intelligence</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-28-xai</link>
      <description>


&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In my &lt;a
href="https://taridwong.github.io/posts/2022-04-09-ensemble/"&gt;previous
post&lt;/a&gt; on ensemble machine learning models, I mentioned that one major
drawback in the artificial intelligence (AI) field is &lt;a
href="https://www.technologyreview.com/2017/04/11/5113/the-dark-secret-at-the-heart-of-ai/"&gt;the
black box problem&lt;/a&gt;, which hampers interpretability of the results
from complex algorithms such as Random Forest or Extreme Gradient
Boosting. Not knowing how the algorithm works behind the prediction
could reduce applicability of the method itself as the audience can’t
fully comprehend the result and therefore unable to use it to inform
their decisions; this problem could therefore damage trust from the
stakeholders (users, policy makers, general audience) to the field as
well &lt;a href="https://doi.org/10.1175/BAMS-D-18-0195.1"&gt;(McGovern et
al., 2019)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On the developer’s side, fully understanding the machine learning
models through the explanable approach (aka the white-box approach)
allows developers to identify potential problems such as &lt;a
href="https://www.kaggle.com/code/alexisbcook/data-leakage/tutorial"&gt;data
leakage&lt;/a&gt; in the algorithm and fix (or debug) it with relative ease
(&lt;a
href="https://ieeexplore.ieee.org/abstract/document/8882211"&gt;Loyola-Gonzalez,
2019)&lt;/a&gt;. Further, knowing which variable affects the prediction the
most can inform feature engineering to reduce model complexity and
direct future data collection as well by focusing on collecting the
variables that matter &lt;a
href="https://www.kaggle.com/code/dansbecker/use-cases-for-model-insights"&gt;(Becker
&amp;amp; Cook, 2021)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On stakeholder’s side, it is important to emphasize model
explanability especially in industries such as healthcare, finances, and
military to foster trust between the people inside and outside of the
field that could lead to the extent that the result is used to inform
decisions made by humans such as financial credit approval &lt;a
href="https://www.kaggle.com/code/dansbecker/use-cases-for-model-insights"&gt;(Becker
&amp;amp; Cook, 2021&lt;/a&gt;; &lt;a
href="https://ieeexplore.ieee.org/abstract/document/8882211"&gt;Loyola-Gonzalez,
2019)&lt;/a&gt;. Clearly Understanding how, where, and why the model also
benefits the model itself as users are able to identify potential
problems in its performance and provide the develoeprs with their
feedback &lt;a
href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=9401991"&gt;(Velez
et al., 2021)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The above examples knowing how to extract human-understandable
insights from a complex machine learning model is important, especially
in social science data where the theoretical part is as important as the
methodological and the practical part. For that reason, I will be
applying the methods of Explanable Artificial Intelligence (XAI) to
extract interpretable insights from a classification model that predicts
students’ grade repetition. We will begin by setting up the environment
as usual.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

from imblearn.combine import SMOTEENN

from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score

import warnings
warnings.filterwarnings(&amp;quot;ignore&amp;quot;)

RANDOM_STATE = 123&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;I will be using the same data set as my &lt;a
href="https://taridwong.github.io/posts/2022-02-27-statlearning/"&gt;previous
post about statistical learning&lt;/a&gt;, namely the Programme for
International Student Assessment (PISA) 2018 (&lt;a
href="https://www.oecd.org/education/pisa-2018-results-volume-i-5f07c754-en.htm"&gt;OECD,
2019&lt;/a&gt;). However, the set of variables that I am examining will be
different as PISA contains several school-related variables that can be
shifted as the researcher sees fit. For this post, I will predict
students’ class repetition from 25 predictors (or features as called in
the field of machine learning) such as students’ socio-economic status,
history of bullying involvement, and their learning motivation. The data
is collected from Thai student in 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;df = pd.read_csv(&amp;quot;PISA_TH.csv&amp;quot;)

X = df.drop(&amp;#39;REPEAT&amp;#39;, axis=1)
y = df[&amp;#39;REPEAT&amp;#39;]

df.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   REPEAT    ESCS  DAYSKIP  ...  Invest_effort  WEALTH  Home_resource
0       0 -0.7914        1  ...              6  0.0721        -1.4469
1       0  0.8188        1  ...              8 -0.3429         1.1793
2       0  0.4509        1  ...             10  0.3031         1.1793
3       0  0.7086        1  ...             10 -0.5893        -0.1357
4       0  0.8361        1  ...             10  0.5406         1.1793

[5 rows x 25 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="addressing-sample-imbalance"&gt;Addressing Sample Imbalance&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The problem is that our targeted variable is imbalance; that is,
the number of students who repeated a class is smaller than the number
of students who did not. This situation makes sense in the real-world
data as normal samples are usually more prevalent than the abnormal
ones, but it is undesirable in the machine learning scenario as the
model could recognize minority samples as unimportant and therefore
disregard them as noises &lt;a
href="https://arxiv.org/pdf/1106.1813.pdf"&gt;(Chawla et al., 2022)&lt;/a&gt;. As
a result, the model could give misleadingly optimistic performance on
classification datasets as it classifies only students who did not
repeat a class.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;See the t-Distributed Stochastic Neighbor Embedding (tSNE) plot
below for the visualization. There isn’t much samples of repeaters in
contrary to non-repeater students. Plus, the pattern is not prominent
enough as the blut dots (repeaters) stay very close to the red dots
(non-repeaters). This could make the pattern difficult to be learned by
the machine due to its ambiguity. One way we can mitigate this problem
is to perform data augmentation via oversampling and undersampling,
which synthesizes more minority samples and deletes or merges majority
samples to improve performance of the machine (&lt;a
href="https://ieeexplore.ieee.org/abstract/document/9034624?casa_token=P33Jkz0x1zEAAAAA:Xtz22PhKDSZ_ktb6X7w-Le7PHkxHwfzzRzvrL3qJcJIDwmyaAMizIr1lUBSK5Lpz1qyk4Ls"&gt;Budhiman
et al., 2019&lt;/a&gt;; &lt;a
href="https://ieeexplore.ieee.org/abstract/document/7797091"&gt;Wong et
al;., 2016&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;Counter(y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Counter({0: 8044, 1: 589})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;tsne = TSNE(n_components=2, random_state=RANDOM_STATE)

TSNE_result = tsne.fit_transform(X)

plt.figure(figsize=(12,8))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1200x800 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;sns.scatterplot(TSNE_result[:,0], TSNE_result[:,1], hue=y, legend=&amp;#39;full&amp;#39;, palette=&amp;quot;hls&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;AxesSubplot:&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To balance the data, I will use both oversampling and
undersampling. Normal oversampling methods duplicates minority samples
for more sample size; however, this approach does not add any more
information to the model (more of the same, basically). Instead, we can
&lt;em&gt;synthesize&lt;/em&gt; minority samples by creating samples that are
&lt;em&gt;similar&lt;/em&gt; to the existing minority samples; this technique is
named as &lt;strong&gt;Synthetic Minority Oversampling TEchnique
(SMOTE)&lt;/strong&gt; &lt;a
href="https://www.wiley.com/en-us/Imbalanced+Learning%3A+Foundations%2C+Algorithms%2C+and+Applications-p-9781118074626"&gt;(He
and Ma, 2013)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Also, we can further enhance the effectiveness of SMOTE by adding
undersampling into the process &lt;a
href="https://arxiv.org/pdf/1106.1813.pdf"&gt;(Chawla et al., 2022)&lt;/a&gt;.
Instead of randomly delete our majority samples, we will use the
&lt;strong&gt;Edited Nearest Neighbor (ENN)&lt;/strong&gt; method, which deletes
data points based on their neighbors to make the difference between
majority and minority samples &lt;a
href="https://www.researchgate.net/profile/Duke-T-J-Ludera/publication/348663430_Credit_Card_Fraud_Detection_by_Combining_Synthetic_Minority_Oversampling_and_Edited_Nearest_Neighbours/links/6009f844a6fdccdcb86fc68c/Credit-Card-Fraud-Detection-by-Combining-Synthetic-Minority-Oversampling-and-Edited-Nearest-Neighbours.pdf"&gt;(Ludera,
2021)&lt;/a&gt;. The combination of these two techniques is called &lt;a
href="https://imbalanced-learn.org/stable/auto_examples/combine/plot_comparison_combine.html#sphx-glr-auto-examples-combine-plot-comparison-combine-py"&gt;&lt;strong&gt;SMOTEENN&lt;/strong&gt;&lt;/a&gt;
See Figure 1 for the example of ENN from &lt;a
href="https://doi.org/10.1016/j.ins.2009.02.011"&gt;Guan et al.,
(2009)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-xaienn.png" style="width:70.0%" alt="" /&gt;
&lt;p class="caption"&gt;ENN Editing with 1-NN Classifier. No copyright
infringement is intended&lt;/p&gt;
&lt;/div&gt;
&lt;pre class="python"&gt;&lt;code&gt;smote_enn = SMOTEENN(random_state=RANDOM_STATE, sampling_strategy = &amp;#39;minority&amp;#39;, n_jobs=-1)

X_resampled, y_resampled = smote_enn.fit_resample(X, y)

Counter(y_resampled)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Counter({1: 8040, 0: 4794})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;tsne = TSNE(n_components=2, random_state=RANDOM_STATE)

TSNE_result = tsne.fit_transform(X_resampled)

plt.figure(figsize=(12,8))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1200x800 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;sns.scatterplot(TSNE_result[:,0], TSNE_result[:,1], hue=y_resampled, legend=&amp;#39;full&amp;#39;, palette=&amp;quot;hls&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;AxesSubplot:&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The second tSNE plot shows a more noticable pattern between student
repeaters and non-repeaters. The number of repeaters is increased while
the number of non-repeaters is decreased. Next, we can put our augmented
data into the Random Forest model for prediction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="random-forest-ensemble"&gt;Random Forest Ensemble&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We will be splitting the data set into a training and a testing set
as usual. For a quick recap, Random Forest is a machine learning model
that consists of several unique and uncorrelated decision trees; hence
the word Random in its name. Those trees work together to improve the
predictive accuracy of that dataset than a single decision tree &lt;a
href="https://mitpress.mit.edu/books/introduction-machine-learning-second-edition"&gt;(Kubat,
2017)&lt;/a&gt;. The model will be evaluated with the repeated stratified
10-folds technique to test our model prediction on different sets of
unseen data to ensure its accuracy, especially in the case of imbalanced
data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;CV = RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=RANDOM_STATE)


X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size = 0.30, 
                                                    random_state = RANDOM_STATE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;# random forest model creation
clf_rfc = RandomForestClassifier(random_state=RANDOM_STATE)
clf_rfc.fit(X_train, y_train)

# predictions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;RandomForestClassifier(random_state=123)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;rfc_predict = clf_rfc.predict(X_test)

rfc_cv_score = cross_val_score(clf_rfc, X_resampled, y_resampled, cv=CV, scoring=&amp;#39;roc_auc&amp;#39;)

print(&amp;quot;=== All AUC Scores ===&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;=== All AUC Scores ===&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(rfc_cv_score)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[0.98907675 0.99357898 0.99228597 0.99276793 0.99409399 0.99378369
 0.9931644  0.9901627  0.98967714 0.99287747 0.99384328 0.99252177
 0.99452348 0.99187137 0.99040419 0.99201929 0.9896265  0.99140778
 0.99115331 0.99457436]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;#39;\n&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;=== Mean AUC Score ===&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;=== Mean AUC Score ===&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(&amp;quot;Mean AUC Score - RandForest: &amp;quot;, rfc_cv_score.mean())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Mean AUC Score - RandForest:  0.9921707177188173&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;#define metrics for normal RF
from sklearn import metrics

y_pred_proba_rf = clf_rfc.predict_proba(X_test)[::,1]
fpr_rf, tpr_rf, _ = metrics.roc_curve(y_test,  y_pred_proba_rf)

auc_rf = metrics.roc_auc_score(y_test, y_pred_proba_rf)
plt.plot(fpr_rf,tpr_rf, label=&amp;quot;AUC for Random Forest Classifier = &amp;quot;+str(auc_rf.round(3)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[&amp;lt;matplotlib.lines.Line2D object at 0x000002836A80FDF0&amp;gt;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.legend(loc=&amp;quot;lower right&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.legend.Legend object at 0x000002836A80F580&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;True Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;True Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.xlabel(&amp;#39;False Positive Rate&amp;#39;)
           &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;False Positive Rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Receiver-Operator Curve (ROC)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Receiver-Operator Curve (ROC)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file366c664c513a_files/figure-html/unnamed-chunk-10-1.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The results will be evaluated with the Receiver Operator
Characteristic (ROC) curve, which shows the diagnostic ability of binary
classifiers. One approach to use ROC is to evaluate its Area Under Curve
(AUC), which measures of the ability of a classifier to distinguish
between classes and is used as a summary of the ROC curve. The higher
the AUC, the better the performance of the model at distinguishing
between the positive and negative classes. The mean of 20 rounds of
testing (randomly splitting the data into 10 stratified parts, repeated
it for 2 times) looks good is around 0.99, meaning that there is a 99%
chance that the model is able to correctly predict which student is a
reapeater and which is not based on the data used to train the machine.
Now we know that the model works well with our data, let us move on to
interpreting it with XAI techniques.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="explaining-ai"&gt;Explaining AI&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;XAI is a set of methods that allows a machine learning model and its
results understandable to human in terms of how it works in terms of
prediction, including the impace of variables to the prediction results
&lt;a
href="https://link.springer.com/book/10.1007/978-3-030-68640-6"&gt;(Gianfagna
&amp;amp; Di Cecco, 2021)&lt;/a&gt;. The XAI methods that we will extract insights
are permutation importance, partial dependence plot, and Shapley
Additive explanations (SHAP) values.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="permutation-importance"&gt;Permutation Importance&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;One of the most basic questions we might ask of a model is: What
features have the biggest impact on predictions? This quention could be
answered through the examination of &lt;strong&gt;feature importance&lt;/strong&gt;.
There are multiple ways to measure feature importance. One way is to
extract the feature importance plot from the model itself as
demonstrated below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;# Create a pd.Series of features importances
importances_rf = pd.Series(clf_rfc.feature_importances_, index = X_resampled.columns)

# Sort importances_rf
sorted_importance_rf = importances_rf.sort_values()

#Horizontal bar plot
sorted_importance_rf.plot(kind=&amp;#39;barh&amp;#39;, color=&amp;#39;lightgreen&amp;#39;); 
plt.xlabel(&amp;#39;Feature Importance Score&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;Feature Importance Score&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;#39;Features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;Features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Visualizing Important Features&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Visualizing Important Features&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file366c664c513a_files/figure-html/unnamed-chunk-11-3.png" width="1152" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Another way, which we will focus on in this post, is to use the
permutation importance score from the area of XAI. Permutation
importance is calculated by asking the following question: “If I
randomly shuffle a single column of the validation data, leaving the
target and all other columns in place, how would that affect the
accuracy of predictions in that now-shuffled data?”. Randomly
re-ordering a single column should cause less accurate predictions,
since the resulting data no longer corresponds to anything observed in
the real world. Model accuracy especially suffers if we shuffle a column
that the model relied on heavily for predictions. In our case, if we
mess with the “BEINGBULLIED” variable, the model would be severely
affected by the reduced prediction accuracy. The same would happen to
the variable “Parent_emosup”, “Positive_feel” and so forth as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;import eli5
from eli5.sklearn import PermutationImportance

FEATURES = X_test.columns.tolist()

perm = PermutationImportance(clf_rfc, random_state=RANDOM_STATE).fit(X_test, y_test)
eli5.show_weights(perm, feature_names = FEATURES, top = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;IPython.core.display.HTML object&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-xaiper-imp.png" style="width:40.0%" alt="" /&gt;
&lt;p class="caption"&gt;Permutation Importance&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The permutation importance results are consistent with the
feature importance score we extracted from the model. The values towards
the top are the most important features, and those towards the bottom
matter least. The first number in each row shows how much model
performance decreased with a random shuffling (in this case, using
“accuracy” as the performance metric). Like most things in data science,
there is some randomness to the exact performance change from a
shuffling a column. We measure the amount of randomness in our
permutation importance calculation by repeating the process with
multiple shuffles. The number after the ± measures how performance
varied from one-reshuffling to the next.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In our example, the most important feature was “BEINGBULLIED”,
which is the index of exposure to bullying. The index was constructed
from questions that ask if students have experienced bullying in the
past 12 months from statements such as “Other students left me out of
things on purpose”; “Other students made fun of me”; “I was threatened
by other students”. Positive values on this scale indicate that the
student was more exposed to bullying at school than the average student
in OECD countries; negative values on this scale indicate that the
student was less exposed to bullying at school than the average student
across OECD countries. This result is consistent with the literature
that students’ grade repetition is associated with the likelihood of
being bullied (&lt;a
href="https://journals.plos.org/Plosmedicine/article?id=10.1371/journal.pmed.1003846"&gt;Lian
et al., 2021&lt;/a&gt;; &lt;a
href="https://www.tandfonline.com/doi/full/10.1080/21683603.2019.1699215?casa_token=OB1MKY8CNuMAAAAA%3A5gwLS94ZgeACsQtZhKmiDLGtJUCu_qUMTtNyy_ftGl14WekcRoUErjezdZcOvI1s6bJEg_HYFIo"&gt;Ozada
Nazim &amp;amp; Duyan, 2019&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="partial-dependence-plots"&gt;Partial Dependence Plots&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;While feature importance shows what variables most affect
predictions, partial dependence plots show how a feature affects
predictions. For our case, partial dependence plots can be used to
answer questions such as “Controlling for all variables, what impact
does the index of exposure to bullying have on the prediction of grade
repetition?”. The interpretation of partial dependence plot is somewhat
similar to the interpretation of linear or logistic regression. On this
plot, The y axis is interpreted as change in the prediction from what it
would be predicted at the baseline or leftmost value. A blue shaded area
indicates level of confidence.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The plot below indicates that being subjected to bullying (as
reflected by having positive value of the variable) increases the
likelihood of students to repeat a grade. Positive values in this index
indicate that the student is more exposed to bullying at school than the
average student in OECD countries. Negative values in this index
indicate that the student is less exposed to bullying at school than the
average student in OECD countries; therefore, having zero does not mean
students did not experience any form of bullying, but rather
experiencing bullying to some degree (i.e., being bullied a bit).
However, the predicting power does not change much after 0, meaning that
the amount of exposure to bullying does not matter in predicting
students’ grade repetition.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;from pdpbox import pdp

pdp_bullied = pdp.pdp_isolate(model=clf_rfc, dataset=X_test, model_features=FEATURES, feature=&amp;#39;BEINGBULLIED&amp;#39;)

pdp.pdp_plot(pdp_bullied, &amp;#39;BEINGBULLIED&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(&amp;lt;Figure size 1500x950 with 2 Axes&amp;gt;, {&amp;#39;title_ax&amp;#39;: &amp;lt;AxesSubplot:&amp;gt;, &amp;#39;pdp_ax&amp;#39;: &amp;lt;AxesSubplot:xlabel=&amp;#39;BEINGBULLIED&amp;#39;&amp;gt;})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file366c664c513a_files/figure-html/unnamed-chunk-13-5.png" width="1440" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Partial Dependence Plots can also be used to examine interactions
between variables as well. The graph below shows predictions for any
combination of students’ exposure to bullying and the amount of
emotional support from parents. The prediction power is highest when
students score 0 in the index of exposure to bullying (i.e., being
bullied a bit) and having scores on the index of parents’ emotional
support between -1.7 to +0.5. Positive values on this scale mean that
students perceived greater levels of emotional support from their
parents than did the average student across OECD countries while
negative value means otherwise. Having higher exposure to bullying
reduces prediction power of the model as indicated by the changing color
from yellow to green, and when the score in the index of emotional
support reaches 1, the score of the exposure to bullying index becomes
less matter as the prediction power reduces.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;features_to_plot = [&amp;#39;BEINGBULLIED&amp;#39;, &amp;#39;Parent_emosup&amp;#39;]

inter1  =  pdp.pdp_interact(model=clf_rfc, dataset=X_test, model_features=FEATURES, features=features_to_plot)

pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type=&amp;#39;contour&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(&amp;lt;Figure size 750x950 with 3 Axes&amp;gt;, {&amp;#39;title_ax&amp;#39;: &amp;lt;AxesSubplot:&amp;gt;, &amp;#39;pdp_inter_ax&amp;#39;: &amp;lt;AxesSubplot:xlabel=&amp;#39;BEINGBULLIED&amp;#39;, ylabel=&amp;#39;Parent_emosup&amp;#39;&amp;gt;})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file366c664c513a_files/figure-html/unnamed-chunk-14-7.png" width="720" data-distill-preview=1 /&gt;&lt;/p&gt;
&lt;h3 id="shap-values"&gt;SHAP Values&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Finally, SHAP value allows us to interpret the prediction at a
fine-grained level to the components of individual predictions to show
the impact of each feature. For our case, CHAP value can be used to
answer questions like “On what basis did the model predict that student
A is likely to repeat a grade?”. The plot is quite straightforward to
interpret. The red part shows what increases the likelihood of repeating
a grade, and the blue part shows what decreases the likelihood of
repeating a grade.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On the plot below, the prediction is at the base alue of 0.60,
meaning that it is the average of the model output. For this particular
student, their likelihood to repeat a grade is increased by being
exposed to bullying (BEINGBULLIED), having mediocre emotional support
from parents (Parent_emosup), and having poor overall social standing as
indicated by -0.9 the variable the index of socio-economic, social and
cultural status (ESCS). However, the likelihood is decreased by their
educational resources at home (Home_resource), having cooperative class
(Stu_coop), their parents’ occupational status (Parent_occupation), and
having low record of class skipping (CLASSKIP).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-xaishap_10th_java.png" style="width:85.0%" alt="" /&gt;
&lt;p class="caption"&gt;SHAP Value for a prediction&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id="shap-summary-plot"&gt;SHAP Summary Plot&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In addition to the breakdown of each individual prediction, we
can also visualize groups of SHAP values with SHAP summary plot and SHAP
dependence contribution plot. SHAP summary plots give us an overview of
feature importance and what is driving the prediction. This plot is made
of many dots. Each dot has three characteristics as follows: a)
horizontal location (the x-axis) that indicates whether the effect of
that value caused a higher or lower prediction; b) vertical location
(the y-axis) that indicates the variable name, in order of importance
from top to bottom. c) Gradient color indicates the original value for
that variable. In booleans (i.e., yes/no variable), it will take two
colors, but in number it can contain the whole spectrum.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For example, the left most point in the ‘Parent_emosup’ row is
red in color, meaning that for that particular student, having greater
levels of emotional support from their parents reduces their likelihood
of repeating a grade by roughly 0.3. Seeing variables have a wide spread
in range can be inferred that permutation importance is high; however,
it is best to use permutation importance to measure which variable is
important to the prediction.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Some features such as &lt;code&gt;Home_resource&lt;/code&gt; (educational
resource at home) have reasonably clear separation between the blue and
pink dots, which implies a straightforward meaning that the increase in
the variable value lower (i.e., more resource) the likelihood of
repeating a grade while the decrease in educational resource impacts the
variable in the other direction (higher chance to repeat a
grade).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;However, some variables such as &lt;code&gt;Stu_coop&lt;/code&gt; (the degree
of cooperativeness within classrooms) have blue and pink dots jumbled
together, suggesting that the increase in this variable leads to higher
predictions, and other times it leads to a lower prediction. In other
words, both high and low values of the variable can have both positive
and negative effects on the prediction. The most likely explanation for
this “jumbling” of effects is that the variable (in this case
&lt;code&gt;Stu_coop&lt;/code&gt;) has an interaction effect with other variables.
For example, there may be some situations where cooperating with other
students lead to &lt;a
href="https://www.simplypsychology.org/social-loafing.html"&gt;social
loafing&lt;/a&gt; - when stduents contribute less effort when working as a
group, and therefore learns less. This interaction needs further
investigation.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;shap_values_summary = explainer.shap_values(X_test)
shap.summary_plot(shap_values[1], X_test)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-xaishap_summary.png" style="width:45.0%" alt="" /&gt;
&lt;p class="caption"&gt;SHAP Summary Plot&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id="shap-dependence-contribution-plot"&gt;SHAP Dependence Contribution
Plot&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The earlier Partial Dependence Plots to show how a single feature
impacts predictions. This is insightful and relevant for many real-world
use cases. The interpretation is also friendly to non-technical audience
as well. However, there is a lot that we still don’t know; for example,
what is the distribution of effects? Is the effect of having a certain
value pretty constant, or does it vary a lot depending on the values of
other feaures. SHAP dependence contribution plots provide a similar
insight to the partial dependence plot, but they add a lot more detail.
The plot shows scatter dots that explain how the effect a single feature
has on the predictions made by the model. The plot can be read as
follows: a) The x-axis is the value of the feature; b) The y-axis is the
SHAP value for that feature, which represents how much knowing that
feature’s value changes the output of the model prediction; c) The color
corresponds to a second feature that may have an interaction effect with
the feature we are plotting.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The plot below shows the relatively flat trend of the
&lt;code&gt;BEINGBULLIED&lt;/code&gt; feature, meaning that this variable does
impact the prediction regardless of the value; this trend is consistent
with the partial dependence plot shown earlier in the post. However,
there is a sign of interaction as there are points with similar value
that produce different outcome. See the left of the 2D pane, for
example. For some students, being less exposed to bullying gives them
more chance to repeat a grade while some students got less chance. There
might be other features that interact with this variable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;While the primary trend is that being bullied increases the
chance of repeating a grade , there are some variations that can be
explained by the interaction of features as well. For a concrete
explanation, see the right of the 2D pane. Being positioned overthere
means that those students experience a lot of bullying, but their chance
of repeating a grade is relatively lower than those who experience less
bullying. One explanation is that some of those students have positive
feelings for themselves (indicated by the red color), which could make
them more resilient toward being bullied.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;shap.dependence_plot(&amp;#39;BEINGBULLIED&amp;#39;, shap_values_summary[1], X_test, interaction_index=&amp;quot;Positive_feel&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class="figure"&gt;
&lt;img src="https://taridwong.github.io//posts/2022-04-28-xaishap_dependence_bulliedXpositivefeel.png" style="width:60.0%"
alt="" /&gt;
&lt;p class="caption"&gt;SHAP Dependence Contribution Plot&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;What we have done so far is making a prediction with a Random Forest
Ensemble model, which has high predictive power at the price of being
challenging to explain due to its complexity (&lt;a
href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2822360/"&gt;Zhang &amp;amp;
Wang, 2009&lt;/a&gt;). XAI tools such as permutation importance, partial
dependence plot, and SHAP values, allow us to understand outputs of the
model at various levels from the overall picture to fine-grained
individual cases. Knowing how predictions are made also allow
establishes venues for future studies as well. XAI results are important
to bridge the knowledge gap between technical (e.g., developers) and
non-technical (e.g., customers, users) audiences, which could build
trust and confidence when putting the AI models into the actual use. XAI
also helps an organization develop a responsible approach to AI
development by avoiding the reliance on results that we do not
understand to inform our decisions.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;However, note that XAI is not perfect. Its results are
context-dependent, meaning that if the context changes, so does the
result (&lt;a
href="https://www.sciencedirect.com/science/article/pii/S0740624X21001027"&gt;de
Brujin et al., 2021&lt;/a&gt;). The prediction and how it happens can only be
used as a factor to be considered along with other lines of evidence
such as expert opinion, counter explanations, and potential
consequences. Regardless, XAI is still a useful too to have in expanding
the knowledge we get from machine learning. Thank you very much for
reading!&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>f40f1e89b04d8b250adbd706645111a3</distill:md5>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-28-xai</guid>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-28-xai/xai_files/figure-html5/unnamed-chunk-14-11.png" medium="image" type="image/png" width="1440" height="1824"/>
    </item>
    <item>
      <title>Addressing Data Imbalance with Semi-Supervised Learning</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-28-semisupervised</link>
      <description>For this post, I will use semi-supervised learning approach to perform a classification task with a highly imbalance data.  

(7 min read)</description>
      <category>Python</category>
      <category>Unsupervised Machine Learning</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-28-semisupervised</guid>
      <pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-28-semisupervised/semi-ml.png" medium="image" type="image/png" width="900" height="450"/>
    </item>
    <item>
      <title>Examining Customer Cluster with Unsupervised Machine Learning</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-18-unsupervisedml</link>
      <description>In this post, I will be using two unsupervised learning techniques with a data set, namely K-means clustering and Hierarchical clustering, to determine groups of customers from their age, income, and spending behavior data.  

(8 min read)</description>
      <category>Python</category>
      <category>Unsupervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-18-unsupervisedml</guid>
      <pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-18-unsupervisedml/unsupervisedml_files/figure-html5/unnamed-chunk-14-19.png" medium="image" type="image/png" width="2880" height="1152"/>
    </item>
    <item>
      <title>Combining Multiple Machine Learning Models with the Ensemble Methods</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-04-09-ensemble</link>
      <description>This entry explores different ways to combine supervised machine learning models to maximize their predictive capability.  

(13 min read)</description>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-04-09-ensemble</guid>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-04-09-ensemble/robot.png" medium="image" type="image/png" width="626" height="528"/>
    </item>
    <item>
      <title>Examining PISA 2018 Data Set with Statistical Learning Approach</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-02-27-statlearning</link>
      <description>In this post, I will be developing two statistical learning models, namely Linear Regression and Polynomial Regression, and apply it to Thai student data from the Programme for International Student Assessment (PISA) 2018 data set to examine the impact of classroom competition and cooperation to students' academic performance.  

(14 min read)</description>
      <category>R</category>
      <category>Statistics</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2022-02-27-statlearning</guid>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-02-27-statlearning/statlearn.jpeg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Classical Test Theory in R</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-01-15-ctt</link>
      <description>For this post, I will be analyzing characteristics of test items based on the framework of Classical Test Theory (CTT).

(13 min read)</description>
      <category>R</category>
      <category>Psychometric</category>
      <guid>https://taridwong.github.io/posts/2022-01-15-ctt</guid>
      <pubDate>Sat, 15 Jan 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-01-15-ctt/ctt_files/figure-html5/unnamed-chunk-28-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Examining the Big 5 personality Dataset with factor analysis</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2022-01-01-efacfa</link>
      <description>For this entry, I will be examining the Big 5 personality Inventory data set with Exploratory Data Analysis to identify potential structures of personality trait and verify them with Confirmatory Factor Analysis.

(8 min read)</description>
      <category>R</category>
      <category>Statistics</category>
      <guid>https://taridwong.github.io/posts/2022-01-01-efacfa</guid>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2022-01-01-efacfa/corrmatrix.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Measuring Text Similarity with Movie plot data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-29-movie-similarity</link>
      <description>For this post, I will be analyzing textual data of movie plots to determine their similarity with TF-IDF and Clustering.

(7 min read)</description>
      <category>Python</category>
      <category>Natural Language Processing</category>
      <category>Unsupervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-29-movie-similarity</guid>
      <pubDate>Wed, 29 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-29-movie-similarity/movie-similarity_files/figure-html5/unnamed-chunk-11-1.png" medium="image" type="image/png" width="484" height="483"/>
    </item>
    <item>
      <title>Missing Data Analysis</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-27-missingdata</link>
      <description>For this post, I will examine missing data in a large-scale dataset and discuss about numerous ways we can clean them as a part of data preparation.

(10 min read)</description>
      <category>R</category>
      <category>Data Visualization</category>
      <category>Statistics</category>
      <guid>https://taridwong.github.io/posts/2021-12-27-missingdata</guid>
      <pubDate>Mon, 27 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-27-missingdata/missingpic.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Applying Machine Learning to Audio Data: Visualization, Classification, and Recommendation</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data</link>
      <description>For this entry, I am trying my hands on audio data to extract its features for exploratory data analysis (EDA), using machine learning algorithms to perform music classification, and finally build up on that result to develop a recommendation system for music of similar characteristics.

(13 min read)</description>
      <category>Python</category>
      <category>Data Visualization</category>
      <category>Supervised Machine Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data</guid>
      <pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-11-applying-machine-learning-to-audio-data/applying-machine-learning-to-audio-data_files/figure-html5/unnamed-chunk-13-11.png" medium="image" type="image/png" width="3072" height="1152"/>
    </item>
    <item>
      <title>Interactive plots for Suicide Data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data</link>
      <description>For this entry, will be visualizing suicide data from 1958 to 2015 with interactive plots to communicate insights to non-technical audience.  

(14 min read)</description>
      <category>R</category>
      <category>Data Visualization</category>
      <category>Quantitative research</category>
      <guid>https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data</guid>
      <pubDate>Thu, 09 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-08-interactive-dashboard-for-suicide-data/preview.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Image Recognition with Artificial Neural Networks</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks</link>
      <description>In this entry, we will be developing a deep learning algorithm - a sub-field of machine learning inspired by the structure of human brain (neural networks) - to classify images of single digit number (0-9).  

(9 min read)</description>
      <category>Python</category>
      <category>Supervised Machine Learning</category>
      <category>Deep Learning</category>
      <guid>https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks</guid>
      <pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-07-image-recognition-with-artificial-neural-networks/deepnn.jpg" medium="image" type="image/jpeg"/>
    </item>
    <item>
      <title>Anomaly Detection with New York City taxi data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data</link>
      <description>In this entry, I will be conducting anomaly detection to identify points of anomaly in the taxi passengers data in New York City from July 2014 to January 2015 at half-hourly intervals.
 
 (4 min read)</description>
      <category>Unsupervised Machine Learning</category>
      <category>R</category>
      <guid>https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data</guid>
      <pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-12-04-anomaly-detection-with-new-york-city-taxi-data/anomaly-detection-with-new-york-city-taxi-data_files/figure-html5/unnamed-chunk-9-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Crime mapping in San Francisco with police data</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data</link>
      <description>In this entry, we will explore San Francisco crime data from 2016 to 2018 to understand the relationship between civilian-reported incidents of crime and police-reported incidents of crime. Along the way we will use table intersection methods to subset our data, aggregation methods to calculate important statistics, and simple visualizations to understand crime trends.  

(7 min read)</description>
      <category>Data Visualization</category>
      <category>R</category>
      <category>Quantitative research</category>
      <guid>https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data</guid>
      <pubDate>Tue, 30 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-11-30-crime-mapping-in-san-francisco-with-police-data/crime-mapping-in-san-francisco-with-police-data_files/figure-html5/unnamed-chunk-6-1.png" medium="image" type="image/png" width="1920" height="1920"/>
    </item>
    <item>
      <title>Exploring COVID-19 data from twitter with topic modeling</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds</link>
      <description>


&lt;h2 id="covid-19-situation-in-alberta-canada"&gt;COVID-19 situation in
Alberta, Canada&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The province of Alberta, Canada, has suffered from the COVID-19
pandemic like all other places. Alberta has gone through cycles of
reopening and returning to provincial lock down since April 2020. The
province, however, has lifted almost all restrictions and enacted its
reopening plan on the recent &lt;a
href="https://calgary.ctvnews.ca/alberta-moves-to-stage-3-of-reopening-plan-on-canada-day-1.5475913"&gt;Canada
day&lt;/a&gt; when 70% of Alberta population has received at least one dose of
&lt;a
href="https://www.canada.ca/en/public-health/services/diseases/coronavirus-disease-covid-19/vaccines.html"&gt;approved
COVID-19 vaccination&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The navigation of the province through this pandemic was led by
the Alberta’s Chief Medical Officer of Health, &lt;a
href="https://www.alberta.ca/office-of-the-chief-medical-officer-of-health.aspx"&gt;Dr. Deena
Hinshaw&lt;/a&gt;. Dr.Hinshaw usually held public health briefings almost
every day during wave 1 to wave 3 of the pandemic, but her communication
channel has changed in wave 4 as less public health briefing was held
and more tweets were posted on the &lt;a
href="https://twitter.com/CMOH_Alberta"&gt;her account&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For that, I believe we could use &lt;a
href="https://en.wikipedia.org/wiki/Natural_language_processing"&gt;Natural
Language Processing (NLP)&lt;/a&gt; techniques to extract themes and
characteristics from Dr.Hinshaw’s tweet to examine the essence of public
health messages since the provincial reopening date, specifically from
July 1st to October 31st, 2021.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="text-mining-and-word-cloud-fundamentals"&gt;Text mining and word
cloud fundamentals&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For this post, we will use text mining and word clouds to
initially explore characteristics of the data set. Text mining is an
exploratory method for textual data under Natural Language Processing
(NLP), a branch of Artificial Intelligence concerning the understanding
of words and spoken texts. NLP is also a type of unsupervised machine
learning approach to discover hidden structures in the data to inform
decisions made by experts of the subject matter.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Word cloud is also a popular way to to communicate findings from
textual data in a visually engaging way. The more frequent a word appear
in the data set (or corpus) the bigger that word will be in the
cloud.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will use Python to perform this analysis on R platform with
&lt;code&gt;reticulate::repl_python()&lt;/code&gt;. First of all, we will be
importing necessary modules and twitter data set that we mined from
Dr. Hinshaw’s account with &lt;code&gt;pd.read_csv&lt;/code&gt;. There are 538
tweets in total, and we can print out examples of the tweets via
&lt;code&gt;tweets_df.Text.head(5)&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;#Import necessary modules

import numpy as np #for numpy array
import pandas as pd #for data reading and processing
import matplotlib.pyplot as plt #for plotting
import re #for Regex text cleaning
from wordcloud import WordCloud, STOPWORDS #for word clouds
from nltk.stem import WordNetLemmatizer #to reduce text to base form
from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation as LDA #for topic modeling
import warnings

warnings.filterwarnings(&amp;quot;ignore&amp;quot;) #suppress the warning that Python kindly gave me

tweets_df = pd.read_csv(&amp;quot;text-query-tweets.csv&amp;quot;)

tweets_df.shape

# Print out the first rows of papers&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(538, 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print(tweets_df.Text.head(5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    We all have the ability to take small actions ...
1    As we head into Halloween weekend, I encourage...
2    Sadly, 9 new deaths related to COVID-19 were a...
3    Over the past 24 hours, we ID’d 603 new cases ...
4    Here is a summary of the latest #COVID19AB num...
Name: Text, dtype: object&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="lets-clean-the-text-first"&gt;Let’s clean the text first&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;After we imported our data into the system, we have to clean our
data to get rid of textual elements that we do not need such as
punctuation, numbers, as well as convert all words to lower case.
Painful as it may be, this has to be done. It took me days (not that
much, but I felt it that way) to clean all of this and make sure that no
junk is left behind (well, there could be. Do let me know if you find
any).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The phrase “Garbage in, garbage out” is really applicable here in
data work context. If you let any junk (corrupted data) in, the most you
will get is processed junk. After we cleaned the text, let us print them
out again to see what they look like. All numbers are gone. All texts
are in lowercase. All URLs and punctuation is gone. Good
riddance!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ATTN nerds: Note that in the code below, we will pass the
original &lt;code&gt;Text&lt;/code&gt; column in &lt;code&gt;tweets_df&lt;/code&gt; to the
&lt;code&gt;re.sub&lt;/code&gt; function only once. For the second cleaning function
onward, we will pass &lt;code&gt;tweets_df['Text_processed']&lt;/code&gt; instead to
stack our text cleaning results on the same column. Yes, I wrote this to
remind myself because I struggled on it for hours (half an hour,
actually).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;
#remove all numbers from the text with list comprehension
tweets_df[&amp;#39;Text_processed&amp;#39;] = tweets_df[&amp;#39;Text&amp;#39;].map(lambda x: re.sub(r&amp;#39;[0-9]+&amp;#39;, &amp;#39;&amp;#39;, x))

# Remove punctuation
tweets_df[&amp;#39;Text_processed&amp;#39;] = tweets_df[&amp;#39;Text_processed&amp;#39;].map(lambda x: re.sub(r&amp;#39;[^\w\s\,\.!?]&amp;#39;, &amp;#39;&amp;#39;, x))

# Convert the tweets to lowercase
tweets_df[&amp;#39;Text_processed&amp;#39;] = tweets_df[&amp;#39;Text_processed&amp;#39;].map(lambda x: x.lower())

#Clean out URLs
tweets_df[&amp;#39;Text_processed&amp;#39;] = tweets_df[&amp;#39;Text_processed&amp;#39;].map(lambda x: re.sub(r&amp;quot;http\S+&amp;quot;, &amp;quot;&amp;quot;, x))

# Print the processed titles of the first rows 
print(tweets_df[&amp;#39;Text_processed&amp;#39;].head())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    we all have the ability to take small actions ...
1    as we head into halloween weekend, i encourage...
2    sadly,  new deaths related to covid were also ...
3    over the past  hours, we idd  new cases amp co...
4    here is a summary of the latest covidab number...
Name: Text_processed, dtype: object&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="so-this-is-whats-happening-over-time"&gt;So this is what’s
happening over time&lt;/h2&gt;
&lt;pre class="python"&gt;&lt;code&gt;
#Change datetime format to datetime
tweets_df[&amp;#39;Datetime&amp;#39;] = pd.to_datetime(tweets_df[&amp;#39;Datetime&amp;#39;])

#Extract month from datetime
tweets_df[&amp;#39;Month&amp;#39;] = tweets_df[&amp;#39;Datetime&amp;#39;].dt.month

# Group the papers by year
groups = tweets_df.groupby(&amp;#39;Month&amp;#39;)

# Determine the size of each group
counts = groups.size()

# Visualize the counts as a bar plot

# Vertical lines
plt.axvline(x = 7.0, color = &amp;#39;forestgreen&amp;#39;, label = &amp;#39;The reopening date&amp;#39;, linestyle=&amp;#39;--&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.lines.Line2D object at 0x00000283881BD430&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.axvline(x = 8.0, color = &amp;#39;firebrick&amp;#39;, label = &amp;#39;Wave 4 started&amp;#39;, linestyle=&amp;#39;--&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.lines.Line2D object at 0x00000283881BD940&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.legend(bbox_to_anchor = (1.0, 1), loc = &amp;#39;upper right&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.legend.Legend object at 0x00000283881BD400&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.title(&amp;quot;Tweet count across months&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 1.0, &amp;#39;Tweet count across months&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.ylabel(&amp;quot;Tweet count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0, 0.5, &amp;#39;Tweet count&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.xlabel(&amp;quot;Month&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Text(0.5, 0, &amp;#39;Month&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;counts.plot()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;AxesSubplot:title={&amp;#39;center&amp;#39;:&amp;#39;Tweet count across months&amp;#39;}, xlabel=&amp;#39;Month&amp;#39;, ylabel=&amp;#39;Tweet count&amp;#39;&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file366c12431b17_files/figure-html/unnamed-chunk-5-1.png" width="720" style="display: block; margin: auto;" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The line plot above represents tweet counts across months after the
provincial reopening date. The x-axis indicates months and the y-axis
indicates the number of twitter post of Dr. Hinshaw. The number of tweet
dropped slightly from July to August as cases decreased, but wave 4 of
the pandemic started in August as cases were on the rise again. We can
see that the number of cases aligns with the number of tweets posted on
Dr. Hinshaw’s account.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="lets-see-the-big-picture-with-word-cloud"&gt;Let’s see the big
picture with word cloud&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Now that we know the frequency of tweets over months, we can plot a
word cloud from our processed text to see the big picture of twitter
data. There are 114,362 words in total after combining all 538 tweets
together. The word cloud below suggests that “covid” was mentioned the
most during the past four months, following by “vaccine”, “new cases”,
and “unvaccinated”.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;
text_all = &amp;quot; &amp;quot;.join(tweet for tweet in tweets_df.Text_processed)
print (&amp;quot;There are {} words in the combination of all tweets&amp;quot;.format(len(text_all)))

#lemmatize all words&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;There are 114362 words in the combination of all tweets&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;lemmatizer = WordNetLemmatizer()
text_all = &amp;quot;&amp;quot;.join([lemmatizer.lemmatize(i) for i in text_all])

# Create Stopword list:
stopwords_cloud = set(STOPWORDS)
stopwords_cloud.update([&amp;quot;https://&amp;quot;, &amp;quot;(/)&amp;quot;, &amp;quot;Online:&amp;quot;, 
                        &amp;quot;Twitter:&amp;quot;, &amp;quot;Join&amp;quot;, &amp;quot;us&amp;quot;, &amp;quot;virtually&amp;quot;,
                        &amp;quot;pm&amp;quot;, &amp;quot;:&amp;quot;, &amp;quot;https&amp;quot;, &amp;quot;t&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;co&amp;quot;, &amp;quot;amp&amp;quot;, &amp;quot;will&amp;quot;])
                      
#Generate a word cloud image
wordcloud_tweet = WordCloud(stopwords=stopwords_cloud, background_color=&amp;quot;white&amp;quot;,random_state=7).generate(text_all)

#Display the generated image:
#the matplotlib way:
plt.figure(figsize=[10,10])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 1000x1000 with 0 Axes&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.imshow(wordcloud_tweet, interpolation=&amp;#39;bilinear&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage object at 0x00000283881B1730&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.axis(&amp;quot;off&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(-0.5, 399.5, 199.5, -0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file366c12431b17_files/figure-html/unnamed-chunk-6-3.png" width="960" style="display: block; margin: auto;" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The thing is, word cloud can only provide a rough visual
presentation for the characteristics of our textual data. We would need
to dive a little bit deeper to graphs and numbers to examine what is
truly going on. Let us visualize them all on a bar plot.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2
id="common-word-bar-plot-and-text-preprocessing-for-topic-modeling"&gt;Common
word bar plot and text preprocessing for topic modeling&lt;/h2&gt;
&lt;pre class="python"&gt;&lt;code&gt;
# Helper function to count common words

def plot_10_most_common_words(count_data, tfidf_vectorizer):
    import matplotlib.pyplot as plt
    words = tfidf_vectorizer.get_feature_names()
    total_counts = np.zeros(len(words))
    for t in count_data:
        total_counts+=t.toarray()[0]
    
    count_dict = (zip(words, total_counts))
    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]
    words = [w[0] for w in count_dict]
    counts = [w[1] for w in count_dict]
    x_pos = np.arange(len(words)) 

    plt.bar(x_pos, counts,align=&amp;#39;center&amp;#39;)
    plt.xticks(x_pos, words, rotation=90) 
    plt.xlabel(&amp;#39;words&amp;#39;)
    plt.ylabel(&amp;#39;counts&amp;#39;)
    plt.title(&amp;#39;10 most common words&amp;#39;)
    plt.show()

#Make your own list of stop words
my_additional_stop_words = (&amp;quot;https://&amp;quot;, &amp;quot;(/)&amp;quot;, &amp;quot;➡Online:&amp;quot;, 
                        &amp;quot;➡Twitter:&amp;quot;, &amp;quot;Join&amp;quot;, &amp;quot;us&amp;quot;, &amp;quot;virtually&amp;quot;,
                        &amp;quot;pm&amp;quot;, &amp;quot;:&amp;quot;, &amp;quot;https&amp;quot;, &amp;quot;t&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;co&amp;quot;, &amp;quot;amp&amp;quot;, &amp;quot;today&amp;quot;, &amp;quot;new&amp;quot;, &amp;quot;covid&amp;quot;,
                        &amp;quot;covidab&amp;quot;, &amp;quot;hours&amp;quot;, &amp;quot;completed&amp;quot;)
                        
stop_words_lda = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)    

# Initialize the count vectorizer with the English stop words
tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words_lda)

# Fit and transform the processed titles
count_data = tfidf_vectorizer.fit_transform(tweets_df[&amp;#39;Text_processed&amp;#39;])

# Visualise the 10 most common words
plot_10_most_common_words(count_data, tfidf_vectorizer)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="../file366c12431b17_files/figure-html/unnamed-chunk-7-5.png" width="960" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The bar plot above gave us a more detailed information of which
word occurs more frequently than the others based on the term
frequency–inverse document frequency (TFIDF) statistics. TFIDF gives
each word a weight that reflects its importance to a document. For
TFIDF, words that occur too frequent like “the” provides little meaning
while words rarely occur doesn’t tell us much as well. We are taking
about the COVID-19 pandemic here, so it is obvious that “vaccinated” is
going to be mentioned the most in Dr. Hinshaw’s tweet. “Cases” and
“unvaccinated” seem to be reasonable to be mentioned as the second- and
third most important words as the government of Alberta has been putting
more effort in identifying more cases in the province and encourage
unvaccinated individuals to get their vaccine.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will also create a &lt;code&gt;tfidf_vectorizer&lt;/code&gt; model with
our own list of stopwords (or words that have little meaning such as
“is, am, are”) to prepare our data for Latent Dirichlet Allocation (LDA)
topic modeling.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2
id="finally-lets-see-potential-topics-from-dr.-hinshaws-tweet"&gt;Finally,
let’s see potential topics from Dr. Hinshaw’s tweet&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Latent Dirichlet Allocation is a powerful natural language
processing technique that discovers hidden patterns in topic from
unstructured textual data with statistical models &lt;a
href="https://link.springer.com/content/pdf/10.1007/s11042-018-6894-4.pdf"&gt;(Jelodar
et al., 2019)&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Here, we can use LDA to discover potential topics among the sea
of tweets posted by Dr. Hinshaw to find out what she talked about since
the provincial reopening and wave 4 of the pandemic. I have specified
the model to extract 8 topics from the data, with 5 words per topics.
Note that these numbers are arbitrary chosen.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If we extracted too few topics, we might not be able to capture
the whole picture of the data. On the other hand, extracting too much
topics could just give us more of the same overlapping themes. We need
to find the middle ground.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="python"&gt;&lt;code&gt;
# Helper function to print out the topics
def print_topics(model, tfidf_vectorizer, n_top_words):
    words = tfidf_vectorizer.get_feature_names()
    for topic_idx, topic in enumerate(model.components_):
        print(&amp;quot;\nTopic #%d:&amp;quot; % topic_idx)
        print(&amp;quot; &amp;quot;.join([words[i]
                        for i in topic.argsort()[:-n_top_words - 1:-1]]))
                        
#How many topic and words per topic we want to see
number_topics = 8
number_words = 5 
                      
# Create and fit the LDA model
lda = LDA(n_components=number_topics, random_state = 1)
lda.fit(count_data)

# Print the topics found by the LDA model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;LatentDirichletAllocation(n_components=8, random_state=1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="python"&gt;&lt;code&gt;print_topics(lda, tfidf_vectorizer, number_words)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Topic #0:
twitter online join video transcript

Topic #1:
possible information protection protect soon

Topic #2:
vaccines protect vaccine dose book

Topic #3:
cases tests partially unvaccinated idd

Topic #4:
reported deaths sadly condolences alberta

Topic #5:
oct age steps important dr

Topic #6:
matter pandemic report continue health

Topic #7:
ahs participating prevent book available&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="wrapping-up-here.-what-can-we-conclude"&gt;Wrapping up here. What
can we conclude?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The topics we discovered above can be inferred as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Topic 0: An invitation for the general population to join a live
update video on Twitter.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Topic 1: The availability of possible information on COVID-19
protection&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Topic 2: Encouragement to book for a vaccination for more
protection.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Topic 3: The proportion of unvaxxed vs vaxxed vs partiallyvaxxed
patients.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Topic 4: Covid-related death.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The insights that we gained could also be further supported by
opinion from public health experts as they could provide information at
a greater depth into their field.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;From what we have discussed so far, we can see that with the
right tool, LDA for our case, we could take advantage of the vast
availability of textual data that revolves around us in our everyday
lives and use that information to deepen our understanding of social
phenomena. We could explore how students opinion changed from pre- to
post-COVID era, or we could use this technique to media transcription of
social events such as political protests, election speech, or even
product review in the marketing field. Thank you for your
reading!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>150e32d23689517567eba0ddc47378ec</distill:md5>
      <category>Python</category>
      <category>Natural Language Processing</category>
      <category>Unsupervised Machine Learning</category>
      <category>COVID-19</category>
      <guid>https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds</guid>
      <pubDate>Tue, 23 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://taridwong.github.io/posts/2021-11-18-exploring-covid-19-data-from-twitter-with-word-clouds/word-cloud-pv.png" medium="image" type="image/png" width="1920" height="1920"/>
    </item>
    <item>
      <title>Finding a home among the paradigm push-back with Dialectical Pluralism</title>
      <dc:creator>Tarid Wongvorachan</dc:creator>
      <link>https://taridwong.github.io/posts/2021-11-07-introducing-dialectical-pluralism</link>
      <description>This entry discusses the reconciliation of quantitative and qualitative worldviews amidst the paradigm wars with pluralistic stance.

(2 min read)</description>
      <category>Mixed methods research</category>
      <guid>https://taridwong.github.io/posts/2021-11-07-introducing-dialectical-pluralism</guid>
      <pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate>
      <media:content url="https://www.relevantinsights.com/wp-content/uploads/2020/05/Qualitative-Quantitative-Research-For-New-Product-Development.png" medium="image" type="image/png"/>
    </item>
  </channel>
</rss>
